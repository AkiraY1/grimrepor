title,body,labels,comments,state
IdeficsImageProcessor raises unexpected ValueError,"### System Info

- `transformers` version: 4.47.1
- Platform: Linux-5.15.0-126-generic-x86_64-with-glibc2.35
- Python version: 3.13.1
- Huggingface_hub version: 0.27.0
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed

### Who can help?

@amyeroberts @qubvel @zucchini-nlp 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

When I was running the following code:
```python
# ignore imports...

checkpoint = ""HuggingFaceM4/idefics-9b-instruct""
model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)
processor = AutoProcessor.from_pretrained(checkpoint)
images = [
    PIL.Image.open(...),
    PIL.Image.open(...),
]
batched_text = [""<image><image>"", ""<image><image>""] 
batched_image = [images, images]
processor(batched_image, batched_text)
```
I got the following exception:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[4], line 5
      3 batched_text =  [""<image><image>"", ""<image><image>""] 
      4 batched_images = [images, images]
----> 5 processor(batched_images, batched_text)

File ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/utils/deprecation.py:165, in deprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func(*args, **kwargs)
    161 elif minimum_action in (Action.NOTIFY, Action.NOTIFY_ALWAYS):
    162     # DeprecationWarning is ignored by default, so we use FutureWarning instead
    163     warnings.warn(message, FutureWarning, stacklevel=2)
--> 165 return func(*args, **kwargs)

File ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/models/idefics/processing_idefics.py:430, in IdeficsProcessor.__call__(self, images, text, audio, videos, **kwargs)
    427 if add_eos_token:
    428     full_text += self.tokenizer.eos_token
--> 430 image_objects = self.image_processor(image_objects, **output_kwargs[""images_kwargs""])
    432 all_prompts.append(full_text)
    433 all_images.append(image_objects)

File ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/image_processing_utils.py:41, in BaseImageProcessor.__call__(self, images, **kwargs)
     39 def __call__(self, images, **kwargs) -> BatchFeature:
     40     """"""Preprocess an image or a batch of images.""""""
---> 41     return self.preprocess(images, **kwargs)

File ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/models/idefics/image_processing_idefics.py:134, in IdeficsImageProcessor.preprocess(self, images, image_num_channels, image_size, image_mean, image_std, transform, return_tensors, **kwargs)
    131 if isinstance(images, list) and len(images) == 0:
    132     return []
--> 134 images = make_list_of_images(images)
    136 if not valid_images(images):
    137     raise ValueError(
    138         ""Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, ""
    139         ""torch.Tensor, tf.Tensor or jax.ndarray.""
    140     )

File ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/image_utils.py:206, in make_list_of_images(images, expected_ndims)
    201         raise ValueError(
    202             f""Invalid image shape. Expected either {expected_ndims + 1} or {expected_ndims} dimensions, but got""
    203             f"" {images.ndim} dimensions.""
    204         )
    205     return images
--> 206 raise ValueError(
    207     ""Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or ""
    208     f""jax.ndarray, but got {type(images)}.""
    209 )

ValueError: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'list'>.
```
Note that this bug is not exist in v4.45.2.

### Expected behavior

Even I have followed instruction provided [here](https://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src/transformers/models/idefics/processing_idefics.py#L354-L358), I still got such a bug, why?","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
"Update doc for `metric_for_best_model` when `save_strategy=""best""`.","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Updates the docstring for `TrainingArguments.metric_for_best_model`, `Trainer._determine_best_metric`, and adds a new test.

Specifically, when `save_strategy=""best""` we need to specify a value for `metric_for_best_model`. This clashes with the previous logic that `metric_for_best_model` would default to loss.

Brought up in this comment: https://github.com/huggingface/transformers/pull/31817#discussion_r1891105193


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?


@muellerzr @SunMarc (cc. @shcheklein - Author of comment)

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.


Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
ModernBERT inference fails on CPU: ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?),"### System Info

- `transformers` version: 4.48.0.dev0
- Platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
- Python version: 3.12.5
- Huggingface_hub version: 0.25.1
- Safetensors version: 0.4.5
- Accelerate version: 0.34.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 4090

### Who can help?

@Rocketknight1 @Arthu

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

When one runs the below code, taken exactly from the Hugging Face ModernBERT's README except for the addition of `device = 'cpu'`, they get the error `ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)`:
```python
import torch
from transformers import pipeline
from pprint import pprint

pipe = pipeline(
    ""fill-mask"",
    model=""answerdotai/ModernBERT-base"",
    torch_dtype=torch.bfloat16,
    device='cpu',
)

input_text = ""He walked to the [MASK].""
results = pipe(input_text)
pprint(results)
```

Here is the full traceback of the error:
```
ValueError                                Traceback (most recent call last)
Cell In[1], line 13
      5 pipe = pipeline(
      6     ""fill-mask"",
      7     model=""answerdotai/ModernBERT-base"",
      8     torch_dtype=torch.bfloat16,
      9     device='cpu',
     10 )
     12 input_text = ""He walked to the [MASK].""
---> 13 results = pipe(input_text)
     14 pprint(results)

File ~/dev/.venv/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:270, in FillMaskPipeline.__call__(self, inputs, **kwargs)
    248 def __call__(self, inputs, **kwargs):
    249     """"""
    250     Fill the masked token in the text(s) given as inputs.
    251 
   (...)
    268         - **token_str** (str) -- The predicted token (to replace the masked one).
    269     """"""
--> 270     outputs = super().__call__(inputs, **kwargs)
    271     if isinstance(inputs, list) and len(inputs) == 1:
    272         return outputs[0]

File ~/dev/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1301, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1293     return next(
   1294         iter(
   1295             self.get_iterator(
   (...)
   1298         )
   1299     )
   1300 else:
-> 1301     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~/dev/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1308, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1306 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
   1307     model_inputs = self.preprocess(inputs, **preprocess_params)
-> 1308     model_outputs = self.forward(model_inputs, **forward_params)
   1309     outputs = self.postprocess(model_outputs, **postprocess_params)
   1310     return outputs

File ~/dev/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1208, in Pipeline.forward(self, model_inputs, **forward_params)
   1206     with inference_context():
   1207         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
-> 1208         model_outputs = self._forward(model_inputs, **forward_params)
   1209         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu""))
   1210 else:

File ~/dev/.venv/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:127, in FillMaskPipeline._forward(self, model_inputs)
    126 def _forward(self, model_inputs):
--> 127     model_outputs = self.model(**model_inputs)
    128     model_outputs[""input_ids""] = model_inputs[""input_ids""]
    129     return model_outputs

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:1059, in ModernBertForMaskedLM.forward(self, input_ids, attention_mask, sliding_window_mask, position_ids, labels, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict, **kwargs)
   1054         with torch.no_grad():
   1055             input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(
   1056                 inputs=input_ids, attention_mask=attention_mask, position_ids=position_ids, labels=labels
   1057             )
-> 1059 outputs = self.model(
   1060     input_ids,
   1061     attention_mask=attention_mask,
   1062     sliding_window_mask=sliding_window_mask,
   1063     position_ids=position_ids,
   1064     indices=indices,
   1065     cu_seqlens=cu_seqlens,
   1066     max_seqlen=max_seqlen,
   1067     batch_size=batch_size,
   1068     seq_len=seq_len,
   1069     output_attentions=output_attentions,
   1070     output_hidden_states=output_hidden_states,
   1071     return_dict=return_dict,
   1072 )
   1073 last_hidden_state = outputs[0]
   1075 if self.sparse_prediction and labels is not None:
   1076     # flatten labels and output first

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:913, in ModernBertModel.forward(self, input_ids, attention_mask, sliding_window_mask, position_ids, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict)
    902     layer_outputs = self._gradient_checkpointing_func(
    903         encoder_layer.__call__,
    904         hidden_states,
   (...)
    910         output_attentions,
    911     )
    912 else:
--> 913     layer_outputs = encoder_layer(
    914         hidden_states,
    915         attention_mask=attention_mask,
    916         sliding_window_mask=sliding_window_mask,
    917         position_ids=position_ids,
    918         cu_seqlens=cu_seqlens,
    919         max_seqlen=max_seqlen,
    920         output_attentions=output_attentions,
    921     )
    922 hidden_states = layer_outputs[0]
    923 if output_attentions and len(layer_outputs) > 1:

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:529, in ModernBertEncoderLayer.forward(self, hidden_states, attention_mask, sliding_window_mask, position_ids, cu_seqlens, max_seqlen, output_attentions)
    519 def forward(
    520     self,
    521     hidden_states: torch.Tensor,
   (...)
    527     output_attentions: Optional[bool] = False,
    528 ) -> torch.Tensor:
--> 529     attn_outputs = self.attn(
    530         self.attn_norm(hidden_states),
    531         attention_mask=attention_mask,
    532         sliding_window_mask=sliding_window_mask,
    533         position_ids=position_ids,
    534         cu_seqlens=cu_seqlens,
    535         max_seqlen=max_seqlen,
    536         output_attentions=output_attentions,
    537     )
    538     hidden_states = hidden_states + attn_outputs[0]
    539     mlp_output = (
    540         self.compiled_mlp(hidden_states)
    541         if self.config.reference_compile
    542         else self.mlp(self.mlp_norm(hidden_states))
    543     )

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:487, in ModernBertAttention.forward(self, hidden_states, output_attentions, **kwargs)
    484 else:
    485     qkv = qkv.view(bs, -1, 3, self.num_heads, self.head_dim)
--> 487 attn_outputs = MODERNBERT_ATTENTION_FUNCTION[self.config._attn_implementation](
    488     self,
    489     qkv=qkv,
    490     rotary_emb=self.rotary_emb,
    491     local_attention=self.local_attention,
    492     bs=bs,
    493     dim=self.all_head_size,
    494     output_attentions=output_attentions,
    495     **kwargs,
    496 )
    497 hidden_states = attn_outputs[0]
    498 hidden_states = self.out_drop(self.Wo(hidden_states))

File ~/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:349, in flash_attention_forward(module, qkv, rotary_emb, cu_seqlens, max_seqlen, local_attention, bs, dim, target_dtype, **_kwargs)
    336 def flash_attention_forward(
    337     module: ""ModernBertAttention"",
    338     qkv: torch.Tensor,
   (...)
    347 ) -> Tuple[torch.Tensor]:
    348     # (total_seqlen, 3, nheads, headdim)
--> 349     qkv = rotary_emb(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)
    351     convert_dtype = qkv.dtype not in (torch.float16, torch.bfloat16)
    352     if convert_dtype:
    353         # FA2 implementation only supports fp16 and bf16. If FA2 is supported,
    354         # bfloat16 must be supported as of FA2 2.5.7. (Turing GPUs not supported)

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:178, in ModernBertUnpaddedRotaryEmbedding.forward(self, qkv, cu_seqlens, max_seqlen)
    175 if max_seqlen is not None:
    176     self._update_cos_sin_cache(max_seqlen, device=qkv.device, dtype=qkv.dtype)
--> 178 qkv = apply_rotary_unpadded(
    179     qkv,
    180     self._cos_cached,
    181     self._sin_cached,
    182     cu_seqlens=cu_seqlens,
    183     max_seqlen=max_seqlen,
    184 )
    186 return qkv

File ~/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:136, in apply_rotary_unpadded(qkv, cos, sin, cu_seqlens, max_seqlen)
    113 def apply_rotary_unpadded(
    114     qkv,
    115     cos,
   (...)
    118     max_seqlen: Optional[int] = None,
    119 ):
    120     """"""
    121     Arguments:
    122         qkv: (total_nnz, 3, nheads, headdim) - input tensor for packed QKV.
   (...)
    134     Apply rotary embedding to the first rotary_dim of x.
    135     """"""
--> 136     return ApplyRotaryEmbUnpad.apply(qkv, cos, sin, cu_seqlens, max_seqlen)

File ~/dev/.venv/lib/python3.12/site-packages/torch/autograd/function.py:575, in Function.apply(cls, *args, **kwargs)
    572 if not torch._C._are_functorch_transforms_active():
    573     # See NOTE: [functorch vjp and autograd interaction]
    574     args = _functorch.utils.unwrap_dead_wrappers(args)
--> 575     return super().apply(*args, **kwargs)  # type: ignore[misc]
    577 if not is_setup_ctx_defined:
    578     raise RuntimeError(
    579         ""In order to use an autograd.Function with functorch transforms ""
    580         ""(vmap, grad, jvp, jacrev, ...), it must override the setup_context ""
    581         ""staticmethod. For more details, please see ""
    582         ""https://pytorch.org/docs/main/notes/extending.func.html""
    583     )

File ~/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:75, in ApplyRotaryEmbUnpad.forward(ctx, qkv, cos, sin, cu_seqlens, max_seqlen)
     71 # We need qkv to be contiguous so that when we reshape to combine (3, nheads) dimensions,
     72 # we get the same tensor
     73 # qk = rearrange(qkv[:, :2], ""b_s t h d -> b_s (t h) d"")
     74 qk = qkv[:, :2].view(total_nnz, -1, headdim)
---> 75 apply_rotary(
     76     qk,
     77     cos,
     78     sin,
     79     seqlen_offsets=0,
     80     cu_seqlens=cu_seqlens,
     81     max_seqlen=max_seqlen,
     82     interleaved=False,
     83     inplace=True,
     84 )
     86 ctx.save_for_backward(cos, sin, cu_seqlens)
     87 ctx.max_seqlen = max_seqlen

File ~/dev/.venv/lib/python3.12/site-packages/flash_attn/ops/triton/rotary.py:202, in apply_rotary(x, cos, sin, seqlen_offsets, cu_seqlens, max_seqlen, interleaved, inplace, conjugate)
    199 # Need this, otherwise Triton tries to launch from cuda:0 and we get
    200 # ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)
    201 with torch.cuda.device(x.device.index):
--> 202     rotary_kernel[grid](
    203         output,  # data ptrs
    204         x,
    205         cos,
    206         sin,
    207         cu_seqlens,
    208         seqlen_offsets,
    209         seqlen,  # shapes
    210         rotary_dim,
    211         seqlen_ro,
    212         output.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0
    213         output.stride(-3),  # seqlen_stride or total_seqlen_stride
    214         output.stride(-2),  # nheads_stride
    215         output.stride(-1),  # headdim_stride
    216         x.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0
    217         x.stride(-3),  # seqlen stride or total_seqlen_stride
    218         x.stride(-2),  # nheads stride
    219         x.stride(-1),  # headdim stride
    220         BLOCK_K,
    221         isinstance(seqlen_offsets, torch.Tensor),
    222         is_varlen,
    223         interleaved,
    224         conjugate,
    225         BLOCK_M,
    226     )
    227 return output

File ~/dev/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:345, in KernelInterface.__getitem__.<locals>.<lambda>(*args, **kwargs)
    339 def __getitem__(self, grid) -> T:
    340     """"""
    341     A JIT function is launched with: fn[grid](*args, **kwargs).
    342     Hence JITFunction.__getitem__ returns a callable proxy that
    343     memorizes the grid.
    344     """"""
--> 345     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)

File ~/dev/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:691, in JITFunction.run(self, grid, warmup, *args, **kwargs)
    689     # launch kernel
    690     launch_metadata = kernel.launch_metadata(grid, stream, *non_constexpr_vals)
--> 691     kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,
    692                self.CompiledKernel.launch_enter_hook, self.CompiledKernel.launch_exit_hook, *non_constexpr_vals)
    693 return kernel

File ~/dev/.venv/lib/python3.12/site-packages/triton/backends/nvidia/driver.py:365, in CudaLauncher.__call__(self, *args, **kwargs)
    364 def __call__(self, *args, **kwargs):
--> 365     self.launch(*args, **kwargs)

ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)
```

### Expected behavior

It works.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Potentially incorrect calculation of `total_updates` on >=4.46.0 since #34198 affecting multi gpu training,"### System Info

Okay I have been pulling my hair out for a few hours and turns out this bug only happens when `average_tokens_across_devices` is True and epochs > 1

Simplest case to reproduce

DDP
**world size 2**
dataset length = 4
**epochs = 2**
micro batch size  = 1 (aka per gpu batch size)
gradient accumulation = 1
**average_tokens_across_devices = True**

So every epoch, total 2 steps on both devices

but as first epoch finishes, we get

```
[rank1]:   File ""/home/jovyan/axolotl/src/axolotl/train.py"", line 191, in train
[rank1]:     trainer.train(resume_from_checkpoint=resume_from_checkpoint)
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/transformers/trainer.py"", line 2164, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/transformers/trainer.py"", line 2473, in _inner_training_loop
[rank1]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)
[rank1]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/transformers/trainer.py"", line 5142, in get_batch_samples
[rank1]:     num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum().item()
[rank1]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/accelerator.py"", line 2459, in gather
[rank1]:     return gather(tensor)
[rank1]:            ^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 376, in wrapper
[rank1]:     return function(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 437, in gather
[rank1]:     return _gpu_gather(tensor)
[rank1]:            ^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 356, in _gpu_gather
[rank1]:     return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 129, in recursively_apply
[rank1]:     raise TypeError(
[rank1]: TypeError: Unsupported types (<class 'NoneType'>) passed to `_gpu_gather_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
```

The main culprit here is 
https://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src/transformers/trainer.py#L2468-L2473

`steps_in_epoch` per rank is correctly calculated as 2 but total updates is 3
Normally that is harmless because dataloader would be exhausted and would result in empty batch and it won't enter the loop on 2473.
However, when using the recently added option `average_tokens_across_devices`, it will try to gather number of total items in batches across all ranks and gather doesn't like broadcasting `None`

https://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src/transformers/trainer.py#L5139-L5156

This problem does not surface with 1 gpu because `average_tokens_across_devices` is auto set to `False` and neither under epoch = 1 because `DefaultFlowCallback` stops the training process considering global step and expected max steps

### Who can help?

@muellerzr

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Train any LM on more than one gpu, set at least average_tokens_across_devices = True and epochs  > 1

### Expected behavior

Either we fix `total_updates` count or we handle `None` for gather","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
modernbert logits do not have gradient,"### System Info

latest transformers version (from source), python 3.10

### Who can help?

@Arthurz

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

model_id = ""answerdotai/ModernBERT-base""
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForMaskedLM.from_pretrained(model_id).to(""cuda"")
# Create a simple input
inputs = {
""input_ids"": torch.randint(0, 1000, (1, 10)).cuda(),
""attention_mask"": torch.ones(1, 10).cuda()
}

# Set to train mode and check all parameters
model.train()
for name, param in model.named_parameters():
    print(f""{name}: requires_grad = {param.requires_grad}"")

# Do forward pass
outputs = model(**inputs)
print(""\nOutput logits requires_grad:"", outputs.logits.requires_grad)
print(""Output logits grad_fn:"", outputs.logits.grad_fn)

### Expected behavior

When I do this, the output is:
Output logits requires_grad: False
Output logits grad_fn: None

Despite explicitly setting all the parameters to requires_grad = True! And when printing all the params, they all are correctly set to requires_grad = True.

Just to sanity check, I ran the same code but set model_id = ""bert-base-uncased"", and got:
Output logits requires_grad: True
Output logits grad_fn: <ViewBackward0 object at 0x7f0ca6abf370>

So it's def a ModernBERT specific problem!","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Support modernBERT for encoder-decoder models,"### Feature request

The docs state that the [EncoderDecoderModel](https://huggingface.co/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel) can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the encoder. Though [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base) isn't supported:
```
File ""/content/syntax_transformer/data/../models/encoderDecoder.py"", line 40, in __init__
    self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py"", line 538, in from_encoder_decoder_pretrained
    decoder = AutoModelForCausalLM.from_pretrained(decoder_pretrained_model_name_or_path, **kwargs_decoder)
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py"", line 567, in from_pretrained
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.modernbert.configuration_modernbert.ModernBertConfig'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GlmConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig.
```

### Motivation

ModernBert has a better performance and a longer context length.

### Your contribution

How is it possible to support monderBERT? It isn't that different from other BERT models.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
CLIP conversion script - Change fairseq to OpenAI,"# What does this PR do?

Change fairseq to OpenAI in the CLIP conversion script

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

@ArthurZucker",[],0,open
MultiModalityCausalLM does not support Flash Attention 2.0 yet,"### System Info

transformers version 4.47.1
Google colab
Python 3.10.12

-------------------

I attempted to use Flash Attention with the [Janus-1.3B](https://github.com/deepseek-ai/Janus) model, but encountered the following error:

`ValueError: MultiModalityCausalLM does not support Flash Attention 2.0 yet.`

This error was raised by the transformers/modeling_utils.py file:
```
if not cls._supports_flash_attn_2:
    raise ValueError(
        f""{cls.__name__} does not support Flash Attention 2.0 yet. Please request to add support where""
        f"" the model is hosted, on its model hub page: https://huggingface.co/{config._name_or_path}/discussions/new""
        "" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new""
    )
```

Installed FlashAttention-2 using the command:
`pip install flash-attn --no-build-isolation`

 Here is the code I used:
```
import torch
from transformers import AutoModelForCausalLM
from janus.models import MultiModalityCausalLM, VLChatProcessor
from janus.utils.io import load_pil_images

# specify the path to the model
model_path = ""deepseek-ai/Janus-1.3B""
vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)
tokenizer = vl_chat_processor.tokenizer

vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(
    model_path, trust_remote_code=True,  attn_implementation=""flash_attention_2""
)
vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()

conversation = [
    {
        ""role"": ""User"",
        ""content"": ""<image_placeholder>\nConvert the formula into latex code."",
        ""images"": [""images/equation.png""],
    },
    {""role"": ""Assistant"", ""content"": """"},
]

# load images and prepare for inputs
pil_images = load_pil_images(conversation)
prepare_inputs = vl_chat_processor(
    conversations=conversation, images=pil_images, force_batchify=True
).to(vl_gpt.device)

# # run image encoder to get the image embeddings
inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)

# # run the model to get the response
outputs = vl_gpt.language_model.generate(
    inputs_embeds=inputs_embeds,
    attention_mask=prepare_inputs.attention_mask,
    pad_token_id=tokenizer.eos_token_id,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    max_new_tokens=512,
    do_sample=False,
    use_cache=True,
)

answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)
print(f""{prepare_inputs['sft_format'][0]}"", answer)
```



",[],0,open
"`RuntimeError: self and mat2 must have the same dtype, but got Float and BFloat16` when training with `torch_compile`","### System Info

- `transformers` version: 4.48.0.dev0
- Platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
- Python version: 3.12.5
- Huggingface_hub version: 0.25.1
- Safetensors version: 0.4.5
- Accelerate version: 0.34.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 4090

### Who can help?

@ArthurZucker @muellerzr @SunMarc

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

When I try continual pretraining ModernBERT for a MLM objective with the `torch_compile` flag of my `TrainingArguments` set to `True`, I get the below error:
```python
  0%|                                                                                   | 0/1223301 [00:00<?, ?it/s]
/home/dev/.venv/lib/python3.12/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.
  param_schemas = callee.param_schemas()
/home/dev/.venv/lib/python3.12/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.
  param_schemas = callee.param_schemas()
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0] Graph break from `Tensor.item()`, consider setting:
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0]     torch._dynamo.config.capture_scalar_outputs = True
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0] or:
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0] to include these operations in the captured graph.
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0]
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0] Graph break: from user code at:
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0]   File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py"", line 711, in torch_dynamo_resume_in__unpad_modernbert_input_at_710
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0]     max_seqlen_in_batch = int(seqlens_in_batch.max().item())
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0]
W1221 15:33:48.046000 14779 torch/_dynamo/variables/tensor.py:776] [4/0]
Traceback (most recent call last):
  File ""/home/dev/encoder/scripts/train/train_modernbert.py"", line 206, in <module>
    trainer.train(
  File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/trainer.py"", line 2163, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/trainer.py"", line 2523, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/trainer.py"", line 3668, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/trainer.py"", line 3722, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py"", line 465, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py"", line 820, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py"", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py"", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py"", line 1023, in forward
    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)
  File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py"", line 1055, in torch_dynamo_resume_in_forward_at_1055
    input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py"", line 913, in forward
    layer_outputs = encoder_layer(
                    ^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py"", line 519, in forward
    def forward(
  File ""/home/dev/.venv/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py"", line 529, in torch_dynamo_resume_in_forward_at_529
    attn_outputs = self.attn(
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py"", line 632, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py"", line 1100, in forward
    return compiled_fn(full_args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 308, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py"", line 124, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py"", line 98, in g
    return f(*args)
           ^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/autograd/function.py"", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 1525, in forward
    fw_outs = call_func_at_runtime_with_args(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py"", line 124, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 488, in wrapper
    return compiled_fn(runtime_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py"", line 667, in inner_fn
    outs = compiled_fn(args)
           ^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_inductor/codecache.py"", line 1478, in __call__
    return self.current_callable(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.venv/lib/python3.12/site-packages/torch/_inductor/utils.py"", line 1977, in run
    return model(new_inputs)
           ^^^^^^^^^^^^^^^^^
  File ""/tmp/torchinductor_/y7/cy7xv2rrzhznbq3e2wurnq5pmygfytvnpovxlh5bugtoa3ebwy6f.py"", line 277, in call
    extern_kernels.addmm(buf9, buf7, reinterpret_tensor(buf8, (1152, 768), (1, 1152), 0), alpha=1, beta=1, out=buf10)
RuntimeError: self and mat2 must have the same dtype, but got Float and BFloat16
```

This does not occur when finetuning for a classification task.

I am using `bfloat16` mixed precision.

### Expected behavior

The training works.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
Add support for H2O cache eviction with LLaMA,"# What does this PR do?

We implement the [Heavy-Hitter Oracle (H2O)](https://arxiv.org/abs/2306.14048) cache eviction strategy in Huggingface transformers, which selectively retains a balance of KV pairs that are recent or contribute most to the cumulative attention scores while evicting less important ones to maintain a fixed cache size. Our implementation identifies and preserves these “heavy hitter” tokens during inference, maintaining generation quality while dramatically reducing memory requirements. 

<p float=""left"">
  <img src=""https://github.com/user-attachments/assets/c2cdb9c6-f40e-4ce2-bc17-1f2e1da69248"" width=""32%"" />
<img src=""https://github.com/user-attachments/assets/b4ad4398-236f-4d0d-bc18-54f15faec5a9"" width=""32%"" />
  <img src=""https://github.com/user-attachments/assets/0019abaa-724a-46d4-8496-5cdc18bdaba6"" width=""32%"" />
</p>

Key features:
- Dynamic tracking of token importance through attention scores
- Configurable ratio between recent and heavy-hitter sections
- Added support with LLaMA through ""post_processing"" the KV cache in the LlamaAttention

Fixes #30758

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
      Yes, discussed in issue #30758
- [N/A] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
      Added informative docstrings to public methods following guidelines
- [x] Did you write any new necessary tests?
      Added test to validate that H2OCache works correctly with LLaMA model generation by ensuring it can produce non-empty text responses while using cache, specifically testing with TinyLlama-1.1B-Chat in 8-bit quantization on CUDA devices.

## Who can review?

@gante as this relates to generation functionality since this touches core caching infrastructure.


## Outline of code
Files modified:
* `src/transformers/cache_utils.py`: Added a new class `H2OCache`
* `src/transformers/models/llama/modeling_llama.py`: Added post processing function to track attention weights to identify heavy hitters
* `benchmark/h20`: Added benchmarking scripts to compare H2OCache performance with DynamicCache
* `tests/h2O`: Added tests to run LLM with H20Cache


## Executing code
To test an LLM with the H20 cache mechanism, run the following:
```
python -m pytest -n auto --dist=loadfile -s -v ./tests/h2O/test_h2O.py
```

## Results

We demonstrate that H2O achieves over 80% reduction in KV cache size while incurring less than a 5% reduction in throughput. This represents a significant improvement over QuantizedCache, which introduces substantially higher overhead for similar memory savings.",[],0,open
is_causal arg appears twice in FAttention call from GPT2Attention.forward(),"### System Info

4.48.dev ubuntu18, py3.11

### Who can help?

@ArthurZucker based on #35235
@Cyrilvallez  based on  #35342

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

use GPT2 with flash attn.

in https://github.com/huggingface/transformers/blob/608e163b527eaee41e650ffb9eb4c422d2679902/src/transformers/integrations/flash_attention.py#L47

it inserts `is_causal` argument twice, from kwargs and explicitely. causes `TypeError: transformers.modeling_flash_attention_utils._flash_attention_forward() got multiple values for keyword argument 'is_causal'`

the kwargs get `is_causal` from GPT2Attention.forward()


### Expected behavior

use `is_causal` just once

please add GPT2 to your release tests suite","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Add support for modular with fast image processors,"# What does this PR do?

Add support for using modular with fast image processors

Also add a fix for using modular on files other than modeling, which seem to have been broken by a recent change

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->


## Who can review?

@Cyrilvallez 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Fix: bamba error handling kwargs with forward pass ,"This PR adds a small change to handle additional kwargs passed to the forward function of `BambaModel` architecture. 

Fixes a behavior when tuning Bamba models - when HF Trainer would pass additional LossArgs to `BambaForCausalLM.forward()` , which are  passed to `BambaModel.forward` , and need to be ignored. 
This would fix the error - `num_items_in_batch` is an unexpected arg , while tuning the model.

Additional Context:
 [LlamaForCausalLM](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L784) class passes all kwargs to self.model.forward , hence they need to be handled. 
In the future, we would add support for flashAttentionKwargs in BambaModel.forward() . 

cc: @fabianlim 

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
",[],1,open
Integrate xlstm cleanly.,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR integrates `xLSTM` via the `xlstm`-library including certain optimizations (potentially use torch.compile and cuda graphs for speed up). This enables using the `NX-AI/xLSTM-7b` without a special fork of transformers.



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests? Yes, I adapted the tests of the recurrent Mamba2 model.


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker
",[],0,open
Several fixes related to rotary position embeddings,"# What does this PR do?

- Changes related to `position_embeddings` being a mandatory argument
- Remove `position_ids` argument of `apply_rotary_pos_emb`
- Replace `torch.stack` by `torch.cat`, former requires equal shapes
- `esm`: RoPE depends on `position_ids`, which was ignored.
  Fix changes behavior, but should improve.
- `gpt_neox`: Selection of attention compute type via class removed
- `gptj`, `codegen`: RoPE must be applied per head, and some shape issues.
  Probably changes behavior.
- `nemotron`: `config.partial_rotary_factor` was not implemented. This is
  why default changed to 1, so that behavior in default case does not change.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #35233. This is the first of two PRs providing the fix. I split it for easier reviewing.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
      https://github.com/huggingface/transformers/issues/35233
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?
      The second PR fixes the bug the issue is about, and I am adding new tests to confirm it is fixed.


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Zamba new attention standard,"Incorporates new attention standard #35235 in the Zamba architecture

## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?


## Who can review?

@ArthurZucker @Cyrilvallez ",[],0,open
Add support for post-processing kwargs in image-text-to-text pipeline,"# What does this PR do?

This PR adds support for post-processing kwargs for the image-text-to-text pipeline. Following what is done in the text-generation pipeline, for now only `clean_up_tokenization_spaces` is supported, but more can be added in the future.

It also fixes two bugs with the pipeline
- One where the preprocessing kwargs given in the init of the pipeline would be overriden when calling it with no preprocessing kwargs.
- Another where the pipeline would crash with chat templates when `images=image` where image is not a list

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

## Who can review?

@Rocketknight1 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
update modular_modernbert -- add inputs_embeds param to ModernBertModel,"# What does this PR do?
Hi! Congrats on the release of ModernBERT; it looks amazing. I'm interested in using ModernBERT eventually to train a new [Contextual Document Embeddings](https://arxiv.org/abs/2410.02525) model. 

One desired feature is to pass the contextual and word embeddings together in the second stage, which requires setting the `inputs_embeds` kwarg so that we can pass hidden states directly. This is a feature of typical BERT and other transformer implementations but isn't yet allowed by ModernBERT, so I added it. It's only a few additional lines of code.

cc: @warner-benjamin @tomaarsen @orionw @staghado @bclavie @NohTow @ArthurZucker

",[],0,open
'do_sample' model default cannot be overridden,"### System Info

transformers 4.47.1, python 3.10.

Basically while using Qwen-2-VL-Instruct (default config sets do_sample=True), if I set the model_kwargs with do_sample=False, I am unable to override the model_config. I had to make changes to the generations/utils.py to override....

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

1. Use a model like Qwen2-VL-Instruct with default config using do_sample = True
2. Try to override the behavior through kwargs used during model.generate

### Expected behavior

If I set do_sample=False during generation, the default config of the model should be overridden","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
"Model loaded with `PretrainedModel.from_pretrained` and `with torch.device(""cuda""):` decorator leads to unexpected errors compared to `.to(""cuda"")`","### System Info

```
- `transformers` version: 4.48.0.dev0
- Platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.39
- Python version: 3.10.14
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.5
- Accelerate version: 0.34.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+rocm6.2 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: AMD Instinct MI250X/MI250
```
transformers commit 4567ee80572f51859f1454db687cacdf2ec12b13

### Who can help?

@mht-sharma maybe you know

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoConfig
import torch

model_id = ""Qwen/Qwen1.5-MoE-A2.7B-Chat""

cfg = AutoConfig.from_pretrained(model_id)
# cfg.num_hidden_layers = 4

with torch.device(""cuda""):
   model = AutoModelForCausalLM.from_config(cfg, torch_dtype=torch.bfloat16)

# with torch.device(""cuda""):
#    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)

param_size = 0
for name, param in model.named_parameters():
    param_size += param.nelement() * param.element_size()
    print(name, param.dtype)
buffer_size = 0
for name, buffer in model.named_buffers():
    buffer_size += buffer.nelement() * buffer.element_size()
    print(name, buffer.dtype)

size_all_gb = (param_size + buffer_size) * 1e-9
print('model size: {:.3f} GB'.format(size_all_gb))

tokenizer = AutoTokenizer.from_pretrained(model_id)

inp = tokenizer(""Hello my friends, how are you?"", return_tensors=""pt"").to(""cuda"")

gen_config = GenerationConfig(
    max_new_tokens=100,
    min_new_tokens=100,
    use_cache=True,
    num_beams=1,
    do_sample=False,
)

print(""generating"")
res = model.generate(**inp, generation_config=gen_config)

print(tokenizer.batch_decode(res))
```

When using `with torch.device(""cuda"")`, to load a model on device, I am getting various unexpected errors as `HIPBLAS_STATUS_INTERNAL_ERROR when calling hipblasLtMatmul` or `RuntimeError: HIP error: no kernel image is available for execution on the device`.

However, when loading a (dummy) model with
```
with torch.device(""cuda""):
   model = AutoModelForCausalLM.from_config(cfg, torch_dtype=torch.bfloat16)
```
everything is fine at runtime, no error.

Similarly, when loading with
```
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(""cuda"")
``` 
there is no error at runtime.

I do not have access to an Nvidia GPU at the time so could not reproduce there to see if this issue exists also on Nvidia distribution pytorch. So I am not sure if this is a ROCm, PyTorch or Transformers bug, this might need some investigations.

I could reproduce the issue on a few previous Transformers versions (4.45, 4.46, 4.47).

Filling for awareness, this might need some more investigation and/or extended testing in Transformers CI.

Interestingly I could not reproduce this issue with `peft-internal-testing/tiny-random-qwen-1.5-MoE`, but only with `Qwen/Qwen1.5-MoE-A2.7B-Chat`.

### Expected behavior

No error","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Update missing model error message,"This PR updates the missing model error message to suggest installing from `main` for very new models. This can be important because a lot of users want to download new models, but often don't understand why it won't work!

Fixes #35362 ",[],2,open
Issue with Idefics3 sample code ,"This sample code is given in  huggingface idefics3 documentation: [here](https://huggingface.co/docs/transformers/v4.47.1/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration.forward.example)
```

`import requests
import torch
from PIL import Image
from io import BytesIO

from transformers import AutoProcessor, AutoModelForVision2Seq
from transformers.image_utils import load_image

## Note that passing the image urls (instead of the actual pil images) to the processor is also possible
image1 = load_image(""https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"")
image2 = load_image(""https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg"")
image3 = load_image(""https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg"")

processor = AutoProcessor.from_pretrained(""HuggingFaceM4/Idefics3-8B-Llama3"")
model = AutoModelForVision2Seq.from_pretrained(""HuggingFaceM4/Idefics3-8B-Llama3"", torch_dtype=torch.bfloat16, device_map=""auto"")

## Create inputs
messages = [
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image""},
            {""type"": ""text"", ""text"": ""In this image, we can see the city of New York, and more specifically the Statue of Liberty.""},
            {""type"": ""image""},
            {""type"": ""text"", ""text"": ""What can we see in this image?""},
        ]
    },
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image""},
            {""type"": ""text"", ""text"": ""In which city is that bridge located?""},
        ]
    }
]

prompts = [processor.apply_chat_template([message], add_generation_prompt=True) for message in messages]
images = [[image1, image2], [image3]]
inputs = processor(text=prompts, images=images, padding=True, return_tensors=""pt"").to(model.device)

## Generate
generated_ids = model.generate(**inputs, max_new_tokens=256)
generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)

print(generated_texts[0])

print(generated_texts[1])`
```
 
When i use the above code, i get an output like this: 
User:<image>In this image, we can see the city of New York, and more specifically the Statue of Liberty.<image>What can we see in this image?
Assistant:orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_
User:<image>In which city is that bridge located?
Assistant:                                                                                                                                                                                                   ?       ?     ?   ?   ?   ? ?   ? ? ? ? ? ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?   


Can anybody please tell me what the issue is? also, for this single inference, it takes several minutes to generate the output.",[],0,open
Disable  `.github/workflows/self-comment-ci.yml` for now,"# What does this PR do?

Let's rework it for more security later.",[],1,open
"FineTuning AutoModelForSequenceClassification.from_pretrained(meta-llama/Llama-3.2-1B) Bug:RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward) and awq importing","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ x] Did you read the [contributor guideline] (https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker


Library:

- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- trainer: @muellerzr and @SunMarc


Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
 
 I added two contributions.
 1) The first one regarding the mismatch about the batch of the devices then training the model with the attribute device_map='auto'. Discussed here for AutoModelForSequenceClassification.from_pretrained.  Thasnks to [hust] who provided the solution. Working also in my case with llama 3.3 1B https://discuss.huggingface.co/t/fine-tune-meta-llama-llama-2-7b-hf-bug-expected-all-tensors-to-be-on-the-same-device-but-found-at-least-two-devices-cuda-1-and-cuda-0-when-checking-argument-for-argument-target-in-method-wrapper-cuda-nll-loss-forward/129341/1
 2) The seconds is regarding the checking of availability of the package awq for loading quantized models.  'problems importing in awq # `importlib.metadata.version` doesn't work with `awq` (line 144 src/transformers/utils/import_utils.py) . mportlib.metadata.version(pkg_name) works with 'autoawq' and not 'awq' while importlib.util.find_spec(""awq"") works only with awq and not autoawq. Both of thems need to be taken into account (is the same package)
",[],0,open
bugfix Idefics3 processor - handle gracefully cases with text and no images,"# What does this PR do?

Fixing Idefics3 processor to work with batches that do not include images


## Who can review?
@andimarafioti 

",[],4,open
Transformers cannot load ModernBERT for sequence classification,"### System Info

I am trying to test the new ModernBER, following this notebook from the official documentation: https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/finetune_modernbert_on_glue.ipynb model for sequence classification but I am getting the following error: 

```
Traceback (most recent call last):
  File ""/home/ubuntu/req_datalab/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py"", line 1038, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict[""model_type""]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/req_datalab/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py"", line 740, in __getitem__
    raise KeyError(key)
KeyError: 'modernbert'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ubuntu/caf_requirements_training/caf_requirements_training/train_full_fine_tuning.py"", line 75, in <module>
    train_model_ft(tmp_folder_dataset.name, args)
  File ""/home/ubuntu/caf_requirements_training/caf_requirements_training/train_full_fine_tuning.py"", line 39, in train_model_ft
    orchestrate_training_with_epoch_artifacts(dataset=dataset, args=args)
  File ""/home/ubuntu/caf_requirements_training/caf_requirements_training/utils/training/training_utils.py"", line 153, in orchestrate_training_with_epoch_artifacts
    tokenizer, model = get_model_tokenizer(args)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/caf_requirements_training/caf_requirements_training/utils/training/training_utils.py"", line 46, in get_model_tokenizer
    model = AutoModelForSequenceClassification.from_pretrained(training_model_name,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/req_datalab/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py"", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/req_datalab/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py"", line 1040, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `modernbert` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
```
I am using:
- Python version: 3.12.7
- Tranformers version: 4.47.1
- Tranformers information:

```
- `transformers` version: 4.47.1
- Platform: Linux-6.8.0-1018-aws-x86_64-with-glibc2.35
- Python version: 3.12.7
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.5
- Accelerate version: 1.2.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?:  no
- Using GPU in script?: yes
- GPU type: NVIDIA A10G
```

The code snippet used is this:

```
model = AutoModelForSequenceClassification.from_pretrained(""answerdotai/ModernBERT-base"", cache_dir=model_saving_path,
                                                               num_labels=12, compile=False)
```

Thank you very much!

### Who can help?

@ArthurZucker
@stevhliu

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Just executing 
```
model = AutoModelForSequenceClassification.from_pretrained(""answerdotai/ModernBERT-base"", cache_dir=model_saving_path,
                                                         num_labels=12, compile=False)
```
the problem will arise 

### Expected behavior

To load the model normally","[{'id': 1834081910, 'node_id': 'MDU6TGFiZWwxODM0MDgxOTEw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Usage', 'name': 'Usage', 'color': 'e28436', 'default': False, 'description': 'General questions about the library'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
More model refactoring!,"# What does this PR do?

Add more model refactoring as per https://github.com/huggingface/transformers/pull/35235

Adding list here for simplicity:

- [x] Cohere
- [ ] Chameleon
- [ ] DBRX
- [x] Gemma
- [x] Gemma2
- [x] GLM (modular donc rien à faire je crois)
- [ ] gpt_neoX et GPT2
- [x] Granite
- [ ] Jamba
- [ ] JetMoe
- [ ] Mimi
- [x] Mistral
- [x] Mixtral
- [ ] Mllama
- [ ] Moshi
- [ ] Nemotron
- [ ] OPT
- [x] Phi
- [ ] Ph3
- [ ] PhiMoe
- [x] Qwen2
- [ ] qwen2Moe
- [ ] qwen2VL
- [ ] SableML
- [x] StartCoder2 -> Modular normalement oK
- [ ] Idefics1,2,3
- [x] Olmo
- [x] Olmo2
- [ ] Siglip
- [ ] Whisper
",[],1,open
Default value for mean_resizing in resize_token_embeddings should be False,"### System Info

transformers>=4.46

### Who can help?

@Arthur

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

When running resize_token_embeddings with additional tokens

### Expected behavior

It is much slower to run resize_token_embeddings with mean_resizing=True.
Maybe it's because nowadays token embedding size became much larger than before (like gemma2, qwen2, ...).

So I think the default value for mean_resizing in resize_token_embeddings should be False now,
or implementation has to be fix to keep resizing speed as before.

Note: Maybe it happens if I'm using deepspeed stage3, but I didn't thoroughly investigate that.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
`inv_freq_expanded` does not move to correct device in Qwen2,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->
When running InternVL2_5-78B example with 8bit quantization on multiple GPU
```python
from glob import glob
import os
import traceback
from tqdm import tqdm
import numpy as np
import pandas as pd
import math
import torch
import torchvision.transforms as T
from decord import VideoReader, cpu
from PIL import Image
from torchvision.transforms.functional import InterpolationMode
from transformers import AutoModel, AutoTokenizer

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def split_model(model_name):
    device_map = {}
    world_size = torch.cuda.device_count()
    num_layers = {
        'InternVL2_5-1B': 24, 'InternVL2_5-2B': 24, 'InternVL2_5-4B': 36, 'InternVL2_5-8B': 32,
        'InternVL2_5-26B': 48, 'InternVL2_5-38B': 64, 'InternVL2_5-78B': 80}[model_name]
    # Since the first GPU will be used for ViT, treat it as half a GPU.
    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))
    num_layers_per_gpu = [num_layers_per_gpu] * world_size
    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)
    layer_cnt = 0
    for i, num_layer in enumerate(num_layers_per_gpu):
        for j in range(num_layer):
            device_map[f'language_model.model.layers.{layer_cnt}'] = i
            layer_cnt += 1
    device_map['vision_model'] = 0
    device_map['mlp1'] = 0
    device_map['language_model.model.tok_embeddings'] = 0
    device_map['language_model.model.embed_tokens'] = 0
    device_map['language_model.output'] = 0
    device_map['language_model.model.norm'] = 0
    device_map['language_model.lm_head'] = 0
    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0

    return device_map

def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    # calculate the existing image aspect ratio
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

    # find the closest aspect ratio to the target
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    # calculate the target width and height
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

    # resize the image
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        # split the image
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

def load_image(image_file, input_size=448, max_num=12):
    if isinstance(image_file, str):
        image = Image.open(image_file).convert('RGB')
    else:
        image = image_file
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(image) for image in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values

# If you want to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.
path = ""OpenGVLab/InternVL2_5-78B""
device_map = split_model('InternVL2_5-78B')
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_8bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    device_map=device_map,
    trust_remote_code=True).eval()

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)

# video multi-round conversation (视频多轮对话)
def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):
    if bound:
        start, end = bound[0], bound[1]
    else:
        start, end = -100000, 100000
    start_idx = max(first_idx, round(start * fps))
    end_idx = min(round(end * fps), max_frame)
    seg_size = float(end_idx - start_idx) / num_segments
    frame_indices = np.array([
        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))
        for idx in range(num_segments)
    ])
    return frame_indices

def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):
    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)
    max_frame = len(vr) - 1
    fps = float(vr.get_avg_fps())

    pixel_values_list, num_patches_list = [], []
    transform = build_transform(input_size=input_size)
    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)
    for frame_index in frame_indices:
        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')
        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)
        pixel_values = [transform(tile) for tile in img]
        pixel_values = torch.stack(pixel_values)
        num_patches_list.append(pixel_values.shape[0])
        pixel_values_list.append(pixel_values)
    pixel_values = torch.cat(pixel_values_list)
    return pixel_values, num_patches_list

pixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()
generation_config = dict(max_new_tokens=1024, do_sample=True)
question = '<image>\nPlease describe the image shortly.'
response = model.chat(tokenizer, pixel_values, question, generation_config)
print(f'User: {question}\nAssistant: {response}')
```

return following error

```
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
  File ""/workspace/CogVideo/transformers/src/transformers/generation/utils.py"", line 2254, in generate
    result = self._sample(
  File ""/workspace/CogVideo/transformers/src/transformers/generation/utils.py"", line 3253, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File ""/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/workspace/CogVideo/transformers/src/transformers/models/qwen2/modeling_qwen2.py"", line 822, in forward
    outputs = self.model(
  File ""/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/workspace/CogVideo/transformers/src/transformers/models/qwen2/modeling_qwen2.py"", line 557, in forward
    position_embeddings = self.rotary_emb(hidden_states, position_ids)
  File ""/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
  File ""/workspace/CogVideo/transformers/src/transformers/models/qwen2/modeling_qwen2.py"", line 338, in forward
    freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)
```


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
Maybe the way SequenceClassification Model calculates the last non-pad token is not reasonable.,"### System Info

None

### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

SequenceClassification Model  finds the last token that is not a padding token in each row  by cal the first position of pad token when a `pad_token_id` is defined in the configuration. 
the code in https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L951

```
sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1
```

When pad_token is eos token, what is calculated using this method will be the position of the first eos token. However, in some LLM templates, eos is often added at the end of the prompt. Also, when a model such llama does not have a default pad token, using eos as a pad token is a common practice.

For example, in llama, eos token is <|eot_id|>

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer
model = AutoModelForSequenceClassification.from_pretrained(""meta-llama/Llama-3.2-1B-Instruct"", num_labels=1)
tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-3.2-1B-Instruct"")
tokenizer.eos_token, model.config.pad_token_id, tokenizer.pad_token
# ('<|eot_id|>', None, None)
```
Using chat template to encode QA pairs we can get

```
prompt = ""what is 1+1?""
response1 = ""1+1=2""
response2 = ""1+1=3""

conv1 = [{""role"": ""user"", ""content"": prompt}, {""role"": ""assistant"", ""content"": response1}]
conv2 = [{""role"": ""user"", ""content"": prompt}, {""role"": ""assistant"", ""content"": response2}]

conv1_tokenized = tokenizer.apply_chat_template(conv1, tokenize=True, return_tensors=""pt"")
conv2_tokenized = tokenizer.apply_chat_template(conv2, tokenize=True, return_tensors=""pt"")

# conv1_tokenized(input_ids)
# (tensor([[128000, 128006,    882, 128007,    271,  12840,    374,    220,     16,
#             10,     16,     30, 128009, 128006,  78191, 128007,    271,     16,
#              10,     16,     28,     17, 128009, 128006,  78191, 128007,    271]]),
# conv1_tokenized(tokens)
# '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nwhat is 1+1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n1+1=2<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n')
```
In Llama chat template, eos_token is added on the end of prompt. If we do not pad the input_ids and don't set pad_token_id, the score is correct. 

```
with torch.no_grad():
    score1 = model(conv1_tokenized).logits[0][0].item()
    score2 = model(conv2_tokenized).logits[0][0].item()
print(f""Score for response 1: {score1}"")
print(f""Score for response 2: {score2}"")
# Score for response 1: 1.7297050952911377
# Score for response 2: 1.43972647190094
```

If we set pad_token_id = eos_token_id, we get the same score for different QA pairs with the same prompt

```
model.config.pad_token_id = tokenizer.eos_token_id
with torch.no_grad():
    score1 = model(conv1_tokenized).logits[0][0].item()
    score2 = model(conv2_tokenized).logits[0][0].item()
print(f""Score for response 1: {score1}"")
print(f""Score for response 2: {score2}"")
# Score for response 1: -1.857212781906128
# Score for response 2: -1.857212781906128
```

This is because the score of SequenceClassification Model is the last non-pad token's logits，and the last non-pad token is the token before the first eos token. This is incorrect especially when training reward models with preference pairs that have the same prompt.

The complete recurrence script is as follows

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer
model = AutoModelForSequenceClassification.from_pretrained(""meta-llama/Llama-3.2-1B-Instruct"", num_labels=1)
tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-3.2-1B-Instruct"")
prompt = ""what is 1+1?""
response1 = ""1+1=2""
response2 = ""1+1=3""

conv1 = [{""role"": ""user"", ""content"": prompt}, {""role"": ""assistant"", ""content"": response1}]
conv2 = [{""role"": ""user"", ""content"": prompt}, {""role"": ""assistant"", ""content"": response2}]

conv1_tokenized1 = tokenizer.apply_chat_template(conv1, tokenize=True, return_tensors=""pt"")
conv2_tokenized1 = tokenizer.apply_chat_template(conv2, tokenize=True, return_tensors=""pt"")
model.config.pad_token_id = tokenizer.eos_token_id
with torch.no_grad():
    score1 = model(conv1_tokenized1).logits[0][0].item()
    score2 = model(conv2_tokenized1).logits[0][0].item()
print(f""Score for response 1: {score1}"")
print(f""Score for response 2: {score2}"")
```

### Expected behavior


Maybe we should prioritize using attention mask to calculate  the  position of the last non-pad token.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Any plans to add AIMv2 in the model?,"### Model description

Is there a plan to add AIMv2 https://huggingface.co/collections/apple/aimv2-6720fe1558d94c7805f7688c as part of Huggingface Model? AIMv2 showed better performance than SigLIP. I think it will be helpful for the community to have AIMv2 in Huggingface Model.



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Model implementation: https://github.com/apple/ml-aim
Model weights: https://huggingface.co/collections/apple/aimv2-6720fe1558d94c7805f7688c","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
SinkCache (StreamLLM) implemented over Post-RoPE Key cache might result in confused position for inference,"### System Info

The current implementation of SinkCache might result in confusion position for attention computation.

1. HF Key Cache: Post-RoPE
2. After Pre-filling over sequence of length N
	i) Post-RoPE Key cache with length N: [ 0, 1, 2, …, N-1]
3. During first token generation (Current Query & Key position is N, since KV cache size is N.), what does StreamLLM update based (Sink_size = S, Recent_window = R) 

    i) Initial: Post-RoPE Key cache: [0, 1, …, S-1] + [N - R +1, …, N-1] + [N]; len([N - R +1, …, N-1] = R - 1
    ii) Rotate: 
        HF applies Rotation over R-1 keys with position (N - R +1, …, N-1) to make their position as (S, S +1, …, S + R - 2)  and keep this in StreamLLM KV cache.
    iii) Updated StreamLLM Key Cache position: [ 0, 1, …, S-1] + [S, …, S + R - 2] + [N], that is: the last (S+R)th element with actual position N, since len([S, …, S + R - 2]) = R - 1
    
5. Continue next token prediction.
   i) Current Query and Key position are depends on Stream KV cache size = S + R
   ii) StreamLLM Key Cache position update:
   	   a) Initial: [ 0, 1, …, S-1] + [S + 1, …, S + R - 2, N] + [S + R] (note: N is the position of the (S+R - 1)th element.
   	   b) Rotate (all keep ones minus 1)
   	       [0, 1, …, S-1] + [S, …, S + R - 3, N-1] + [S + R] (position S+R is not involved Rotation, please refer `https://github.com/huggingface/transformers/blob/main/src/transformers/cache_utils.py, SinkCache, row: 1038`)
               len([S, …, S + R - 3, N-1]) = R - 1
   	
 Now: We get S + R Key cache with positions = [0, 1, …, S-1] + [S, …, S + R - 3, N-1] + [S + R]. Since the current query position is S+R.   For long context inference, N - 1 >> S + R. This means that Query will interact with a Key with future position.

**Note**: all the number in [] means the position used for token generation. 

### Who can help?

@gante @ArthurZucker 

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Post RoPE key cache:

```
https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py (row: 277 --283)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {""sin"": sin, ""cos"": cos, ""cache_position"": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
```

Inference stage:
next token position = Key cache length

```
https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L489 (Row: 556 - 563)
        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)
```

### Expected behavior

During inference: Query position should be larger or equal to Key position.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
A warning message showing that `MultiScaleDeformableAttention.so` is not found in `/root/.cache/torch_extensions` if `ninja` is installed with `transformers`,"### System Info

* `transformers`: `4.47.1`
* `torch`: `2.5.1`
* `timm`: `1.0.12`
* `ninja`: `1.11.1.3`
* `python`: `3.10.14`
* `pip`: `23.0.1`
* CUDA runtime installed by `torch`: `nvidia-cuda-runtime-cu12==12.4.127`
* OS (in container): Debian GNU/Linux 12 (bookworm)
* OS (native device): Windows 11 Enterprise 23H2 (`10.0.22631 Build 22631`)
* Docker version: `27.3.1, build ce12230`
* NVIDIA Driver: `565.57.02`


### Who can help?

I am asking help for [`DeformableDetrModel`](https://huggingface.co/docs/transformers/v4.47.1/en/model_doc/deformable_detr#transformers.DeformableDetrModel)
vision models: @amyeroberts, @qubvel

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

1. Start a new docker container by
    ```sh
    docker run --gpus all -it --rm --shm-size=1g python:3.10-slim bash
    ```
2. Install dependencies
    ```sh
    pip install transformers[torch] requests pillow timm
    ```
3. Run the following script (copied from [the document](https://huggingface.co/docs/transformers/v4.47.1/en/model_doc/deformable_detr#transformers.DeformableDetrModel.forward.example)), it works fine and does not show any message.
    ```python
    from transformers import AutoImageProcessor, DeformableDetrModel
    from PIL import Image
    import requests
    
    url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
    image = Image.open(requests.get(url, stream=True).raw)
    
    image_processor = AutoImageProcessor.from_pretrained(""SenseTime/deformable-detr"")
    model = DeformableDetrModel.from_pretrained(""SenseTime/deformable-detr"")
    
    inputs = image_processor(images=image, return_tensors=""pt"")
    
    outputs = model(**inputs)
    
    last_hidden_states = outputs.last_hidden_state
    list(last_hidden_states.shape)

    ```
4.  Install ninja:
    ```sh
    pip install ninja
    ```
5.  Run [the same script](https://huggingface.co/docs/transformers/v4.47.1/en/model_doc/deformable_detr#transformers.DeformableDetrModel.forward.example) again, this time, the following warning messages will show
    ```text
        
                                   !! WARNING !!
    
    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    Your compiler (c++) is not compatible with the compiler Pytorch was
    built with for this platform, which is g++ on linux. Please
    use g++ to to compile your extension. Alternatively, you may
    compile PyTorch from source using c++, and then you can also use
    c++ to compile your extension.
    
    See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
    with compiling PyTorch from source.
    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    
                                  !! WARNING !!
    
      warnings.warn(WRONG_COMPILER_WARNING.format(
    Could not load the custom kernel for multi-scale deformable attention: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    Could not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory
    ```
    Certainly, `/root/.cache/torch_extensions/py310_cu124/MultiScaleDeformableAttention/` is empty.

The issue happens only when both `ninja` and `transformers` are installed. I believe that the following issue may be related to this issue:

https://app.semanticdiff.com/gh/huggingface/transformers/pull/32834/overview


### Expected behavior

It seems that ninja will let `DeformableDetrModel` throw unexpected error messages (despite that the script still works). That's may be because I am using a container without any compiler or CUDA preinstalled (the CUDA run time is installed by `pip`).

I think there should be a check that automatically turn of the `ninja` related functionalities even if `ninja` is installed by `pip`, as long as the requirements like compiler version, CUDA path, or something, are not fulfilled.
","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Add DINOv2 with registers,"# What does this PR do?

This PR adds DINOv2 with registers, this time using the new modular tool.

To do:

- [ ] make @bernardzach a co-author","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",2,open
[`Mamba2`] Varlen implementation,"### Feature request

Use varlen implementations (cu_seq_lens) of mamba2 and conv1d when requirements are met, i.e. mostly version dependencies.

### Motivation

It's similar to how fa2 works with varlen and it should boost performance while guaranteeing correct performance on batches.

### Your contribution

I can make a PR, not sure when I'll get to it - prolly after xmas days.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Fix new BNB test failures,"# What does this PR do?

Fixes test failures on T4 runners as a result of merging #34713.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. ([Slack thread](https://huggingface.slack.com/archives/C01NE71C4F7/p1734464357810819))
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
@LysandreJik","[{'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}]",2,open
"Llama model, torch.compile output for custom device does not match with eager/cpu when generation_config.use_cache set to True","### System Info

- `transformers` version: 4.43.2
- Platform: Linux-5.15.0-126-generic-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.0a0+gitee1b680 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@ArthurZucker @gone

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

For a custom device I am working on adding `torch.compile()` with `CPP`  inductor backend.
I am trying run `""TinyLlama/TinyLlama-1.1B-Chat-v1.0""` and it has output difference when using KV cache in generation.
if I use following config output of compiled mode matches with eager mode.
`compiled_model.generation_config.use_cache = False`

And for large content length generation I see output similar to https://github.com/huggingface/transformers/issues/30347

### Expected behavior

Please help me debugging this further so that my backend generates correct output with compile mode even with KV cache.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
[DON'T MERGE] Commont bot CI for other jobs (`generation` / `quantization`),"# What does this PR do?

See example (dummy) run [here](https://github.com/huggingface/new-workflow-addition/pull/5#issuecomment-2553778995)",[],1,open
Fix syntax in HfQuantizer docstring,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes syntax in `HfQuantizer` docstring: the Attributes section is not rendered correctly.

See: https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/quantization#transformers.quantizers.HfQuantizer


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
[Whisper] fix docstrings typo,"# What does this PR do?

Fix mini typo in rendering links to GenerationMixin's generate :)",[],1,open
"Option to Disable Model Caching When Using ""pipeline""","### Feature request

Add a new parameter `disable_caching` to the `pipeline` function to control model caching. This feature would be useful for users who need to load models temporarily without storing them in cache, reducing unnecessary disk usage and improving performance for one-time use cases.

```python
from transformers import pipeline
# Example of disabling caching
pipe = pipeline(task=""zero-shot-classification"", model=""typeform/distilbert-base-uncased-mnli"", disable_caching=True)
```

### Motivation

When testing different models temporarily, it's inefficient to cache each model, especially when they will not be reused. This can lead to wasted disk space and potential performance issues and errors for users who frequently experiment with various models.

### Your contribution

I am not very experienced with the library but I am willing to contribute to the initial implementation and testing of this feature if no one wants to work on it.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
"Fix hardcoded `float` dtypes in DeBERTa model, which caused multiple RuntimeErrors in `bfloat16`","# What does this PR do?
Fix https://github.com/huggingface/transformers/issues/35332 by removing any hardcoded `float` dtypes.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker
",[],5,open
"Default arguments in `DebertaConfig` disable relative attention, contrary to the docs and `deberta-base`","### System Info

transformers 4.47.0

### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

The documentation for `DebertaConfig` says that

> Instantiating a configuration with the defaults will yield a similar configuration to that of the DeBERTa [microsoft/deberta-base](https://huggingface.co/microsoft/deberta-base) architecture.

Yet, the **most important part** of DeBERTa, namely the relative attention, is disabled by default in the model and in the config:

https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/modeling_deberta.py#L191

https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/configuration_deberta.py#L71-L75
https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/configuration_deberta.py#L120

Even when users request a given amount of `max_relative_positions`, relative attention stays disabled as long as that option is set to False.

https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/modeling_deberta.py#L201-L210

And indeed:
```python
from transformers import DebertaConfig

config = DebertaConfig()
print(config.relative_attention)
```
This prints False, and when you instantiate a new DeBERTa model, e.g. like

```python
from transformers import DebertaConfig, DebertaForMaskedLM

print(DebertaForMaskedLM._from_config(DebertaConfig()))
print(DebertaForMaskedLM._from_config(DebertaConfig(max_relative_positions=512)))
```

...there are **no relative positional embeddings** in the model, only absolute positional embeddings. This model will also not do any disentangled attention.

### Expected behavior

Conform to the documentation by setting `relative_attention=True` in the `DebertaConfig` by default. 

I would also add a warning when relative attention is False, so that users know very clearly that *despite* using a DeBERTa model, they are not getting the core feature offered by DeBERTa, namely the relative attention.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",6,open
enable non-cuda awq model support without modify version,"The IPEX awq linear is inherited from GEMM awq linear, see [here](https://github.com/casper-hansen/AutoAWQ/blob/main/awq/modules/linear/gemm_ipex.py#L14). So it's safe to convert GEMM to IPEX when no Cuda is found. It will let users run AWQ model on a non-cuda device without modifying the quantization config to change the version.",[],1,open
AllAboardBertweetModel,"### Model description

AllAboardBertweetModel used for AllaboardSystem

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
"DeBERTa's `DisentangledSelfAttention` hardcodes `float` dtype, which causes `bfloat16` overflow error","### System Info

transformers: 4.47.0
Python: 3.10.5
PyTorch: 2.5.1+cu124
GPU: NVIDIA GTX 980 Ti

### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I'm training a `DebertaForMaskedLM` model with a broader experimental framework, but you can reproduce the bug with simple inference as follows: instantiate such a model with datatype `bfloat16`, and send a batch through it.
```python
import torch
from transformers import DebertaConfig, DebertaForMaskedLM

model = DebertaForMaskedLM._from_config(DebertaConfig(), torch_dtype=torch.bfloat16)
model(**{""input_ids"": torch.tensor([[101,102,103,104]]),
         ""attention_mask"": torch.tensor([[1,1,1,1]])})
```
One of two errors is now thrown in `modeling_deberta.py`, both in `DisentangledSelfAttention.forward()` (and they can both be traced back to the same issue):
1. `RuntimeError: expected m1 and m2 to have the same dtype, but got: float != struct c10::BFloat16`
2. `RuntimeError: value cannot be converted to type at::BFloat16 without overflow`

Here's where they come from: two fields in DeBERTa's `DisentangledSelfAttention` are constructed by explicitly declaring their `dtype` as `torch.float`:

https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/modeling_deberta.py#L187-L188

Then, in `forward()`, we create the two tensors `query_layer` and `key_layer` that start out with the `dtype` of the hidden states, which have the `dtype` of the model, namely `bfloat16`:

https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/modeling_deberta.py#L258-L259

But then, one of these tensors, `query_layer`, is modified by adding `self.q_bias` into it. The resulting tensor inherits the `torch.float` data type:

https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/modeling_deberta.py#L268

The first RuntimeError can occur on the following line, when `query_layer` (now `torch.float`) and `key_layer` (still `torch.bfloat16`) are multiplied. I've had this line crash on one machine and work on another, so perhaps this kind of mixed precision sometimes works.

https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/modeling_deberta.py#L276

The second RuntimeError occurs even when mixed precision is supported. It happens on the following line:

https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/modeling_deberta.py#L290

`attention_scores` is of type `bfloat16`. You then ask to fill it with the minimal value *for the data type of `query_layer`, not the data type of `attention_scores`*. Because `query_layer.dtype` is `torch.float`, that minimal value (-3.40282e+38) is *more negative than the most negative `torch.bfloat16`* (-3.38953e+38). Hence, the overflow.

### Expected behavior

The `dtype` of `self.q_bias` and `self.v_bias` should be set like the rest of the modules/tensors in the model, rather than being hardcoded. That would keep everything `bfloat16`.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
MPI environment variables are not set.,"### System Info

HPC ubuntu 22.04 2nodesx8H100

LSF as scheduler

[tool.poetry.dependencies]
python = ""^3.10""

importlib-metadata = { version = ""~=1.0"", python = ""<3.8"" }
tensorboard = ""^2.16.2""
sge-data-package = {version = ""*"", source = ""sgedata""}
torch = ""2.2.1""
torchvision = ""0.17.1""
torchaudio = ""2.2.1""
transformers = ""4.42.0""
datasets = ""2.18.*""
accelerate = ""0.28.0""
deepspeed = ""0.13.4""
safetensors = ""0.4.2""
mpi4py = ""^4.0.0""

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

module load cuda-12.1.1
module load ISG/experimental/fg12/openmpi/5.0.4-cuda12.1-lsf
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

```
module load cuda-12.1.1
module load ISG/experimental/fg12/openmpi/5.0.4-cuda12.1-lsf
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

deepspeed \
    --hostfile=${HOSTFILE_PATH} \
    --launcher=OPENMPI \
    --launcher_args=""-bind-to none -map-by slot --mca pml ob1 --oversubscribe --display-allocation --display-map"" \
    --master_addr=${MASTER_ADDR} \
    --master_port=${_M_PORT} \
    --no_ssh_check \
    src/dna_mlm/runner.py
```

### Expected behavior

```
def setup_env_ranks() -> tp.Tuple[int, int, int]:

    # Map MPI environment variables to those expected by DeepSpeed/PyTorch
    if 'OMPI_COMM_WORLD_LOCAL_RANK' in os.environ:
        os.environ['LOCAL_RANK'] = os.environ['OMPI_COMM_WORLD_LOCAL_RANK']
        os.environ['RANK'] = os.environ['OMPI_COMM_WORLD_RANK']
        os.environ['WORLD_SIZE'] = os.environ['OMPI_COMM_WORLD_SIZE']
    else:
        raise EnvironmentError(
            ""MPI environment variables are not set. ""
            ""Ensure you are running the script with an MPI-compatible launcher.""
        )
 
 setup_env_ranks()
```

the function should set the env vars but instaed it raises the error","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
tokenizer decode decode with timestamp fails for extended vocabulary,"### System Info

python=3.10.13
transformers==4.44.1
torch==2.1.2

### Who can help?

@sanchit-gandhi @ylacombe @eustlb @arthurz

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Decoding with timestamps produces unexpected results when the vocabulary is extended

```
>>> from transformers import WhisperTokenizer, AddedToken
>>> tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-base', language=""English"", task=""transcribe"", predict_timestamps=True)
>>> extended_vocab = ['newword1']
>>> extended_vocab = [AddedToken(t, single_word=True, lstrip=True) for t in extended_vocab]
>>> tokenizer.add_tokens(extended_vocab)
1
>>> print(len(tokenizer))
51866
>>> print(tokenizer.convert_ids_to_tokens(51865))
newword1
>>> tokens = tokenizer('<|0.00|> newword1 <|0.22|>').input_ids
>>> tokens
[50258, 50259, 50359, 50364, 51865, 220, 50375, 50257]
>>> tokenizer.decode(tokens, skip_special_tokens=True)
'newword1 '
>>> tokenizer.decode(tokens, skip_special_tokens=False)
'<|startoftranscript|><|en|><|transcribe|>newword1 <|endoftext|>'
>>> tokenizer.decode(tokens, skip_special_tokens=False, decode_with_timestamps=True)
'<|startoftranscript|><|en|><|transcribe|><|0.00|><|30.02|> <|30.24|><|endoftext|>'
>>> tokens = tokenizer('<|0.00|> word <|0.22|>').input_ids # something in the vocabulary
>>> tokenizer.decode(tokens, skip_special_tokens=True)
' word '
>>> tokenizer.decode(tokens, skip_special_tokens=False)
'<|startoftranscript|><|en|><|transcribe|> word <|endoftext|>'
>>> tokenizer.decode(tokens, skip_special_tokens=False, decode_with_timestamps=True)
'<|startoftranscript|><|en|><|transcribe|><|0.00|> word <|0.22|><|endoftext|>'
```

The problem arises in [https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/whisper/tokenization_whisper.py#L546]( https://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/whisper/tokenization_whisper.py#L546)

see issue [20225](https://github.com/huggingface/transformers/issues/20225)


### Expected behavior

I would expect the timestamps to remain consistent from tokenizing and decoding.

```
>>> tokens = tokenizer('<|0.00|> newword1 <|0.22|>').input_ids
>>> tokenizer.decode(tokens, skip_special_tokens=False, decode_with_timestamps=True)
'<|startoftranscript|><|en|><|transcribe|><|0.00|> newword1<|0.22|><|endoftext|>'
```","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Create zero_delay_qkv_benchmark.py,"Implemented Zero-Delay QKV Compression with benchmarking for HuggingFace Transformers to optimize inference runtime. 

- Added `CombinedQKVProjection` class to replace separate query, key, and value projections with a single linear layer.
- Modified BERT's attention mechanism by replacing `query`, `key`, and `value` layers with the combined QKV projection.
- Introduced a `replace_attn_qkv_with_combined` function to automate the replacement process across all layers.
- Added a benchmarking utility to compare inference runtime performance before and after QKV compression.
- Provided a complete example script for testing and benchmarking with dummy data.

This change aims to demonstrate runtime optimizations for inference-heavy use cases in Transformer models.

# What does this PR do?

Implemented Zero-Delay QKV Compression with benchmarking for HuggingFace Transformers to optimize inference runtime. 

- Added `CombinedQKVProjection` class to replace separate query, key, and value projections with a single linear layer.
- Modified BERT's attention mechanism by replacing `query`, `key`, and `value` layers with the combined QKV projection.
- Introduced a `replace_attn_qkv_with_combined` function to automate the replacement process across all layers.
- Added a benchmarking utility to compare inference runtime performance before and after QKV compression.
- Provided a complete example script for testing and benchmarking with dummy data.

This change aims to demonstrate runtime optimizations for inference-heavy use cases in Transformer models.


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Create zero-delay-qkv-compression.py,"This code implementation example demonstrates how to implement a ""Zero-Delay QKV Compression"" approach during inference with a Hugging Face Transformer model. The idea is to modify the model's attention mechanism to combine the query/key/value (Q/K/V) projections into a single linear operation.

# What does this PR do?

This code implementation example demonstrates how to implement a ""Zero-Delay QKV Compression"" approach during inference with a Hugging Face Transformer model. The idea is to modify the model's attention mechanism to combine the query/key/value (Q/K/V) projections into a single linear operation.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
InternVL is ExecuTorch Compatible,"### Feature request


### Feature request

Enable OpenGVLab/InternVL2-1B to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow


### Motivation

See details in #32253

### Your contribution

model enablement","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",0,open
unable to convert llama 3.3 weights to hf.py,"### System Info

python == 3.10.12
transformers == 4.45.1


### Who can help?

_No response_

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

I ran the `convert_llama_weightsto_hf.py` script on the llama-3.3-70B-Instruct checkpoint weights 
`!python convert_llama_weights_to_hf.py --input_dir ""llama_3_3_model/Llama-3.3-70B-Instruct/original"" --model_size 70B --output_dir ""llama_3_3_model/Llama-3.3-70B-Instruct/hf-converted""`, 
and got the 
`ValueError: Failed to instantiate tokenizer.Pleae, make sure you have sentencepiece and protobuf.`
I tried upgrading both sentencepiece and protobuf but same error is occurring again


### Expected behavior

downloaded from HuggingFace Files.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",3,open
avoid un-necessary unwrapping of the model on each train step,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Create Zero-Delay_QKV_Compression.md,"Added an explanation documentation for Zero-Delay QKV Compression - an LLM inference runtime optimization technique for HuggingFace Transformers.

# What does this PR do?

Added an explanation documentation for Zero-Delay QKV Compression - an LLM inference runtime optimization technique for HuggingFace Transformers.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
Add JinaBERT model,"# What does this PR do?

This PR adds `JinaBERT` to transformers.
This enables running [`jinaai/jina-embeddings-v2-base-code`](https://huggingface.co/jinaai/jina-embeddings-v2-base-code) without `trust_remote_code`.

Relevant issue and discussion is here: #27035.

Note that there are two implementations in use for Jina Embeddings v2:
This PR covers [jinaai/jina-embeddings-v2-base-code](https://huggingface.co/jinaai/jina-embeddings-v2-base-code) which uses [this implementation](https://huggingface.co/jinaai/jina-bert-v2-qk-post-norm/tree/main).
Additionally, there is [jinaai/jina-embeddings-v2-base-en](https://huggingface.co/jinaai/jina-embeddings-v2-base-en) (and variants `small-en`, `base-zh`, `base-de`, `base-es`) which use [a different implementation](https://huggingface.co/jinaai/jina-bert-implementation).

I'm not sure if we can make a single `JinaBERT` implementation that works for both `base-code` and `base-en`.
Otherwise, we probably would want to have two `JinaBERT` implementations, for instance `JinaBERT` for `base-en` and variants and `JinaBERTv2` for `base-code`.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Tests
Quite some of the generated tests fail and I'd need help there to judge what we need or what must be updated.

I've updated the `test_inference_no_head_absolute_embedding` integration test to assert on the output that I get from the original implementation.
Moreover, I added an `test_encode` integration test to assert that we get the same results as in [the example provided by Jina AI](https://huggingface.co/jinaai/jina-embeddings-v2-base-code#usage).

## Who can review?

@ArthurZucker

@bwanglzu might be interested too","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}, {'id': 7081170094, 'node_id': 'LA_kwDOCUB6oc8AAAABphIUrg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Text', 'name': 'Text', 'color': '82A90B', 'default': False, 'description': ''}]",1,open
Patch GPTNeoX to use adequate FA2 if position_ids is provided,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

https://github.com/huggingface/transformers/pull/31629 introduced the ability to use `position_ids` in FlashAttention-2, but not all models were updated to support it. This PR is a straightforward extension to GPT-NeoX models.

I was looking into including GPT-2 too, but from a quick glance at the code it seems a little bit trickier.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Deepseek v2,"### Model description

There is stale PR https://github.com/huggingface/transformers/pull/31976. Is anybody working on this model? 

### Open source status

- [x] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
`train_new_from_iterator()` does not work when pre_tokenizer is null,"### System Info

transformers version 4.47.1
Ubuntu 20.04.6 LTS
Python 3.10

### Who can help?

@ArthurZucker, @itazap

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Follow the steps listed in https://huggingface.co/learn/nlp-course/chapter6/2, however use `microsoft/Phi-3.5-mini-instruct` as the model instead of `gpt2`.

```py
from datasets import load_dataset
from transformers import AutoTokenizer

raw_datasets = load_dataset(""code_search_net"", ""python"", trust_remote_code=True)

def get_training_corpus():
    dataset = raw_datasets[""train""]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples[""whole_func_string""]

training_corpus = get_training_corpus()

old_tokenizer = AutoTokenizer.from_pretrained(""microsoft/Phi-3.5-mini-instruct"")

tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

```
Traceback (most recent call last):
  File ""/home/azureuser/tokenizer.py"", line 16, in <module>
    tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
  File ""/anaconda/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py"", line 819, in train_new_from_iterator
    or tokenizer_json[""pre_tokenizer""][""type""] == ""Sequence""
TypeError: 'NoneType' object is not subscriptable
```





















### Expected behavior

The tokenizer should be trained.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Bart: new cache format,"# What does this PR do?

As per title, enables new cache format in Bart ans several models copied from Bart. Since there are too many models copying attention from Bart, I decided to not touch the audio ones and changed their ""Copied from"" statements

TODO:
- [ ] Run all tests + slow, almost passing for all models where new cache was enabled except for a few cases which needs more investigation",[],1,open
add RAdamScheduleFree optimizer,"# What does this PR do?

Integrates the new RAdamScheduleFree optimizer in meta's [schedule_free](https://github.com/facebookresearch/schedule_free).
This PR has greatly followed the previous PR to introduce schedule_free https://github.com/huggingface/transformers/pull/30079

## More details
- Added support for using RAdamScheduleFree through Trainer API
- Refined doc for trainer (clarified that we recommend to use constant LR scheduler with Schedule-Free family)
- Added test case for RAdamScheduleFree in `test_trainer.py` & confirmed the test passes

## Backgrounds
I recently made the RAdamScheduleFree optimizer, and it's been merged in schedule_free library since version 1.4.

- [The author of Schedule-Free paper's announcement post](https://x.com/aaron_defazio/status/1863722349138219459)
- [My announcement and release blog (in Japanese)](https://x.com/hamanasu_nagisa/status/1866413964235821492)

RAdamScheduleFree eliminates:
- Learning rate scheduler and terminal steps
- Warmup steps


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@SunMarc @ydshieh  @muellerzr
",[],0,open
Fixing device mismatch for InternVL2_5-78B rotary embeddings,"Fixing problem with Multi-GPU management of InternVL2_5-78B (https://huggingface.co/OpenGVLab/InternVL2_5-78B)

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)

No specific open issue fixing. I was working on inference using the documentation provided by the official model card of InternVL2_5-78B for multiple GPUs [here](https://huggingface.co/OpenGVLab/InternVL2_5-78B). I got the error of mismatching devices GPU:0 and cpu, I traced back the error to this line.

It may happen to other models, maybe to newer llama vision models (3.2) but I've no access to these models in Europe (see circleci ""copies"" error).


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts, @qubvel, @ArthurZucker (being Text+Vision, I mentioned all the related ones)

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 5151155822, 'node_id': 'LA_kwDOCUB6oc8AAAABMwhmbg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Big%20Model%20Inference', 'name': 'Big Model Inference', 'color': '006b75', 'default': False, 'description': 'Problems related to the Big Model Inference capabilities provided by Accelerate'}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",2,open
"Unclear what happens when using torchrun, multi-gpu and trainer arguments.","### System Info

- `transformers` version: 4.47.0
- Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.35
- Python version: 3.11.6
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): 2.15.1 (True)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?:  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc-per-node 4 stages/train_model.py
- Using GPU in script?: Yes
- GPU type: NVIDIA RTX A4000

### Who can help?

@muellerzr @SunMarc 

I have a piece of python code, that loads a training dataset from a file,  then sets up a Trainer with the training dataset. I am training a distilBERT model (I dont think that detail matters).

It works great. And then I can do torchrun --nproc_per_node = 4, and it seems to run great too, it seems to spawn 4 versions and trains them on the 4 GPUs.

However, I got suspicious - given that I’m passing the same dataset in everyt ime, is the Trainer basically training on just the first 25% of the training set? 

I’d assumed initially it was smarter than this, and some how only used one of the train dataset instances (presumably the first), but now I realize I cannot be certain of this. So, can anyone tell me if I need to “pre-partition” the datasets into 4 shards, and then load them keyed by LOCAL RANK?

If it's not doing the ""right"" thing - i.e. training on all the data, then a warning should be given when this is detected.

Script looks like this:
train_test_dataset = Dataset.load_from_disk('train_test_dataset_file')
trainer = Trainer(..., train_dataset=train_test_dataset['train'], validation_dataset=train_test_dataset['test'],...)


Should script looks like this:
train_test_dataset = Dataset.load_from_disk(f""one_quarter_train_test_dataset_file_{LOCAL_RANK)"")
trainer = Trainer(..., train_dataset=train_test_dataset['train'], validation_dataset=train_test_dataset['test'],...)



### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

clarity is needed on what behaviour is happening with the training data. Is the training dataset being used by all the nodes with an equal division of labor over each quarter of the dataset (or interleaved every 4 minibatches), or is it sending the same minibatch from the training set, and never getting to the final three quarters of the dataset.

### Expected behavior

A better understanding of what data is being trained on when invoking Trainer through torchrun.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
Multi-GPU training crashes with IterableDataset and different length input (e.g. Next token prediction),"### System Info

- `transformers` version: 4.46.3
- Platform: Linux-6.6.20-aufs-1-x86_64-with-glibc2.36
- Python version: 3.11.2
- Huggingface_hub version: 0.26.1
- Safetensors version: 0.4.5
- Accelerate version: 1.2.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: yes, DDP. multi-gpu
- Using GPU in script?: yes
- GPU type: NVIDIA RTX A5000

### Who can help?

@muellerz @SunMarc 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I am using Trainer with hf `IterableDataset`. using multi-gpu training with at least 2 gpus.
I found out that i need `--accelerator_config {""split_batches"":true}` for it work with varying length input (e.g. NTP prediction). I got this from https://github.com/huggingface/transformers/issues/26548

I am still having crashes in my case.
from what I understand the problem arises because my dataset has a reminder when considering the batch size.
I am getting the following stacktrace:

```
rank1]: Traceback (most recent call last):
[rank1]:   File ""/cs/labs/oabend/avishai.elma/src/train_multimodal_multistream.py"", line 249, in <module>
[rank1]:     main()
[rank1]:   File ""/cs/labs/oabend/avishai.elma/src/train_multimodal_multistream.py"", line 235, in main
[rank1]:     trainer.train()
[rank1]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py"", line 2123, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py"", line 2427, in _inner_training_loop
[rank1]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)
[rank1]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py"", line 5045, in get_batch_samples
[rank1]:     batch_samples += [next(epoch_iterator)]
[rank1]:                       ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/data_loader.py"", line 842, in __iter__
[rank1]:     batch = concatenate([batch, first_batch], dim=0)
[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 621, in concatenate
[rank1]:     return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})
[rank1]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 621, in <dictcomp>
[rank1]:     return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})
[rank1]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 624, in concatenate
[rank1]:     return torch.cat(data, dim=dim)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 1008 but got size 1000 for tensor number 1 in the list.
[rank0]: Traceback (most recent call last):
[rank0]:   File ""/cs/labs/oabend/avishai.elma/src/train_multimodal_multistream.py"", line 249, in <module>
[rank0]:     main()
[rank0]:   File ""/cs/labs/oabend/avishai.elma/src/train_multimodal_multistream.py"", line 235, in main
[rank0]:     trainer.train()
[rank0]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py"", line 2123, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py"", line 2427, in _inner_training_loop
[rank0]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)
[rank0]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py"", line 5045, in get_batch_samples
[rank0]:     batch_samples += [next(epoch_iterator)]
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/data_loader.py"", line 842, in __iter__
[rank0]:     batch = concatenate([batch, first_batch], dim=0)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 621, in concatenate
[rank0]:     return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 621, in <dictcomp>
[rank0]:     return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})
[rank0]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 624, in concatenate
[rank0]:     return torch.cat(data, dim=dim)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 1008 but got size 1000 for tensor number 1 in the list.
  0%|                                                                                                                                                                                                                  | 3/43040 [01:02<248:43:41, 20.81s/it]
[rank0]:[W1217 12:41:08.453747481 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1217 12:41:24.654000 1980229 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1980456 closing signal SIGTERM
E1217 12:41:24.771000 1980229 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 1980457) of binary: /cs/labs/oabend/avishai.elma/.lab_env_v3/bin/python
Traceback (most recent call last):
  File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/bin/torchrun"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/run.py"", line 919, in main
    run(args)
  File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/run.py"", line 910, in run
    elastic_launch(
  File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_multimodal_multistream.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-17_12:41:24
  host      : binky-03.cs.huji.ac.il
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1980457)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```

from what i understand this is because of those lines
https://github.com/huggingface/accelerate/blob/200c9eb7833cfa505907f6f224ebf5a275aa6d92/src/accelerate/data_loader.py#L840-L844

so i noticed that droping the remeinder might fix the issue. but it didn't help. this is probably because dataloader_drop_last doesn't get used when using `IterableDataset` 

https://github.com/huggingface/transformers/blob/5d7739f15a6e50de416977fe2cc9cb516d67edda/src/transformers/trainer.py#L997-L1009



### Expected behavior

Multi-gpu training should not crash when using iterable dataset","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Add support for DeepSpeed sequence parallelism (Ulysses),"# What does this PR do?

Adds support for sequence parallel to Tranformers.

**❗ The sister PR to `accelerate` (https://github.com/huggingface/accelerate/pull/3299) must be merged first ❗**

The PR mostly pulls in and updates changes made in @zeyugao's very thorough PR here:
https://github.com/huggingface/accelerate/pull/2877

And pulls in @samedjacob's PR here:
https://github.com/huggingface/transformers/pull/32305

To support changes made by @samejacobs in DeepSpeed, to make sequence parallel integration with HF cleaner here:
https://github.com/microsoft/DeepSpeed/pull/5774

And to respond to comments to the his PR to `transformers` here #32305.

Note that my testing has revealed that all of @zeyugao's changes are indeed required for this integration to work.

I've added two decorators:
- `deepspeed_ulysses_attention`: Instead of modifying the global `_flash_attention_forward` we wrap it instead as suggested by @ArthurZucker.
- `support_deepspeed_ulysses`: An extension of the concept above to add support for sequence parallel to modules by injecting required variables like `sp_group_size`.

I am not married to any of these changes, and am completely open to suggestions.

The only major addition made in my PR is to find the right place to shard the inputs for sequence parallelism to work.

I added a method to `Trainer` called `_finalize_inputs` which is meant to run right before the inputs are passed to the model. I toyed around with sharding the sequences in various parts of the data loading pipeline (see commits if you are curious), but ultimately determined that it had to be done in trainers, right before the inputs are passed to the model, otherwise libraries with custom trainers (i.e. `trl`) would have to refactor the way their trainers prepare data.

As things currently stand, **the sister PR to `accelerate` (https://github.com/huggingface/accelerate/pull/3299) must be merged first**, so that the new flags added to `HFDeepSpeedConfig` for sequence parallelism are available, but if desired we can likely make these PRs orthogonal by caching the same flags in `transformers` for now. 

So far everything appears to work (forward/backward pass tested on 4xA10s and 8xA100s). I will be doing more testing soon (i.e. testing loss is same, tuning a model & evaluating, etc.).

Note that we should also update `loss_utils.py`'s `fixed_cross_entropy` method to use 
[`vocab_sequence_parallel_cross_entropy`](https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/sequence/cross_entropy.py#L59) when sequence parallel is enabled, but this method currently does not allow you to propagate the args `ignore_index` and `reduction`. I will create a PR for this in DeepSpeed shortly.

I am also beginning to think we should use the `DistributedAttention` that is in `DeepSpeed` here instead of decorating the global flash attention function, similar to the way @zeyugao had it, as it seems to have a lot of functionality:
https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/sequence/layer.py#L300

I will likely test this soon as well.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
experiment: use `seen_tokens` instead of `cache_positions`,"**FOR REFERENCE, NOT FOR MERGING**

Tests the old strategy: keep track of the number of seen tokens (= cache write index) as an internal integer in `StaticCache`, as opposed to using the external `cache_positions` Tensor 

⚠️ the executorch+static cache test fails with this change, I think because it doesn't reset the cache

Test commands: confirm that eager == static compiled, and that the cache is correctly reset between runs:
```py
import torch
from transformers import LlamaForCausalLM, AutoTokenizer, GenerationConfig

model = LlamaForCausalLM.from_pretrained(""meta-llama/Llama-3.2-1B"", device_map=""auto"", torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-3.2-1B"")

prompts = [""Simply put, the theory of relativity states that ""]
model_inputs = tokenizer(prompts, return_tensors=""pt"").to(model.device)

print(""\neager\n"")
generated_tokens = model.generate(**model_inputs, do_sample=False, max_new_tokens=20)
generated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
print(""\n"", generated_text, ""\n"")

print(""\nstatic\n"")
generated_tokens = model.generate(**model_inputs, do_sample=False, max_new_tokens=20, cache_implementation=""static"")
generated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
print(""\n"", generated_text, ""\n"")

print(""\nstatic compiled\n"")
model.forward = torch.compile(model.forward, fullgraph=True, mode=""reduce-overhead"")
generated_tokens = model.generate(**model_inputs, do_sample=False, max_new_tokens=20, cache_implementation=""static"")
generated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
print(""\n"", generated_text, ""\n"")

print(""\nstatic compiled (rerun to confirm that the cache is reset)\n"")
model.forward = torch.compile(model.forward, fullgraph=True, mode=""reduce-overhead"")
generated_tokens = model.generate(**model_inputs, do_sample=False, max_new_tokens=20, cache_implementation=""static"")
generated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
print(""\n"", generated_text, ""\n"")
```
",[],1,open
[Question] Why doesn't `trainer.state.epoch` fall round after training?,"```python
# run.py
from datasets import Dataset
from transformers import TrainingArguments, Trainer, AutoModelForCausalLM


def main():
    model = AutoModelForCausalLM.from_pretrained(""Qwen/Qwen2-0.5B"")

    dataset = Dataset.from_dict(
        {
            ""input_ids"": [[1, 2, 3] for _ in range(260)],
            ""labels"": [[4, 5, 6] for _ in range(260)],
        }
    )

    trainer = Trainer(
        model=model,
        args=TrainingArguments(
            output_dir=""my_output_dir"",
            per_device_train_batch_size=16,
            gradient_accumulation_steps=2,
            num_train_epochs=1,
            report_to=""none"",
        ),
        train_dataset=dataset,
    )

    trainer.train()
    print(trainer.state.epoch)  # 0.9411


if __name__ == ""__main__"":
    main()
```


```
python run.py
```

In this case, I would expect `trainer.state.epoch` to be 1 after the training, but I end up with 0.9411 (=16/17). How to explain this?

@muellerzr 

## System info

- `transformers` version: 4.47.0.dev0 22834eeba1c2bf8d632e22fca238ab7c15d1b904
- Platform: Linux-5.15.0-1048-aws-x86_64-with-glibc2.31
- Python version: 3.11.10
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.2.0.dev0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.0+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA H100 80GB HBM3","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}]",1,open
Custom 4D tensor caused shape mismatch error,"### System Info

- `transformers` version: 4.46.3
- Platform: Linux-5.4.0-153-generic-x86_64-with-glibc2.35
- Python version: 3.10.15
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A800-SXM4-80GB

### Who can help?

@ArthurZucker 
Can you take a look at this? I want to pack the samples using the custom attention mask. However, when using a mask of shape [1, 1, seq_len, seq_len], it raises the following error
```
  File ""/data/miniconda3/envs/vllm/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py"", line 139, in to_4d
    expanded_attn_mask = causal_4d_mask.masked_fill(expanded_attn_mask.bool(), torch.finfo(dtype).min)
RuntimeError: The size of tensor a (324) must match the size of tensor b (18) at non-singleton dimension 3
```

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
import torch
import argparse

from contextlib import nullcontext
from transformers import AutoModelForCausalLM, AutoTokenizer


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(""--model_path"", type=str, default=""gpt2"")
    parser.add_argument(""--tokenizer_path"", type=str, default=""gpt2"")
    parser.add_argument(""--use-flash-attn"", action=""store_true"")
    parser.add_argument(""--no-grad"", action=""store_true"")

    args = parser.parse_args()
    return args


def prepare_data(tokenizer, packing: bool, device: torch.device):
    texts = [
        ""Hello, how are you?"",
        ""When is the next holiday?"",
        ""China is a great country."",
    ]
    encoded = tokenizer(texts)

    # Convert to tensor
    if packing:
        res = torch.zeros((1, sum(len(x) for x in encoded[""input_ids""])), dtype=torch.long)
        attention_mask = torch.full((1, 1, res.size(1), res.size(1)), dtype=torch.bfloat16, fill_value=float(""-inf""))

        offset = 0
        for i, (input_ids, attn_mask) in enumerate(zip(encoded[""input_ids""], encoded[""attention_mask""])):
            res[0, offset: offset + len(input_ids)] = torch.tensor(input_ids)
            attention_mask[0, 0, offset: offset + len(attn_mask), offset: offset + len(attn_mask)] = 0.
            offset += len(attn_mask)
    else:
        max_length = max(len(x) for x in encoded[""input_ids""])
        res = torch.zeros((len(encoded[""input_ids""]), max_length), dtype=torch.long)
        attention_mask = torch.zeros((len(encoded[""input_ids""]), max_length), dtype=torch.long)
        for i, (input_ids, attn_mask) in enumerate(zip(encoded[""input_ids""], encoded[""attention_mask""])):
            res[i, : len(input_ids)] = torch.tensor(input_ids)
            attention_mask[i, : len(attn_mask)] = torch.tensor(attn_mask)
    return res.to(device), attention_mask.to(device)


def main(args):
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path, use_flash_attention_2=args.use_flash_attn, device_map=""cuda"", torch_dtype=torch.bfloat16
    )
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    device = torch.device(""cuda"")

    context = torch.no_grad if args.no_grad else nullcontext
    with context():
        model.eval()
        input_ids_no_pack, attention_mask_no_pack = prepare_data(tokenizer, packing=False, device=device)
        input_ids_pack, attention_mask_pack = prepare_data(tokenizer, packing=True, device=device)

        logits_no_pack = model(input_ids=input_ids_no_pack, attention_mask=attention_mask_no_pack).logits
        logits_pack = model(input_ids=input_ids_pack, attention_mask=attention_mask_pack).logits

        logits_no_pack_flatten = torch.zeros_like(logits_pack)
        offset = 0
        for i in range(logits_no_pack.shape[0]):
            length = attention_mask_no_pack[i].sum().item()
            logits_no_pack_flatten[0, offset: offset + length] = logits_no_pack[i, :length]
            offset += length

        print((logits_no_pack_flatten - logits_pack).sum())


if __name__ == ""__main__"":
    args = parse_args()
    main(args)
```

### Expected behavior

Run without error.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
Fix: Rename keyword argument in_channels to num_channels,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes keyword argument in_channels to num_channels


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
version 4.47.0 provides different generation results when using quantized awq model,"### System Info

- `transformers` version: 4.47.0
- Platform: Linux-5.4.0-169-generic-x86_64-with-glibc2.31
- Python version: 3.9.19
- Huggingface_hub version: 0.26.5
- Safetensors version: 0.4.2
- Accelerate version: 0.27.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A100-SXM4-80GB


### Who can help?

@gante @SunMarc @MekkCyber 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

autoawq_model = ""casperhansen/opt-125m-awq""
prompt = ""One day, the little girl""
user_model = AutoModelForCausalLM.from_pretrained(autoawq_model).to('cuda:0')
tokenizer = AutoTokenizer.from_pretrained(autoawq_model)
input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to('cuda:0')
generate_kwargs = dict(do_sample=False, temperature=0.9, num_beams=4)
gen_ids = user_model.generate(input_ids, **generate_kwargs)
gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
target_text = [""One day, the little girl in the back of my mind will ask me if I'm a""]
assert gen_text == target_text, f""Expect: {target_text}\n but get: {gen_text}.""
```

### Expected behavior
When version < 4.47.0, it works well. Version 4.47.0 provides different result
```log
Traceback (most recent call last):
  File ""/data6/xinhe/fx_test/test.py"", line 13, in <module>
    assert gen_text == target_text, f""Expect: {target_text}\n but get: {gen_text}.""
AssertionError: Expect: [""One day, the little girl in the back of my mind will ask me if I'm a""]
 but get: ['One day, the little girl in the back of my mind will say, ??I??m so glad you??'].
```","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
Add config.json explicit change when using custom pipelines.,"# Improves the docs for custom pipeline.

Something I would have loved to know!

Documentation: @stevhliu ",[],4,open
Request to add D-FINE,"### Model description

D-FINE is a RT-DETR based model that uses probability distributions as an intermediate representation from which bounding boxes are predicted.  This model has achieved SOTA results in real-time object detection with additional training data. https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=d-fine-redefine-regression-task-in-detrs-as

### Open source status

- [ ] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Here is the link to the repository: https://github.com/Peterande/D-FINE. The weights are present on [Huggingface Hub](https://huggingface.co/Peterande/D-FINE/tree/main). The only github username I know of is @Peterande.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6126880899, 'node_id': 'LA_kwDOCUB6oc8AAAABbTDIgw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/contributions-welcome', 'name': 'contributions-welcome', 'color': 'F99E09', 'default': False, 'description': ''}]",16,open
Qwen2vl support for GGUF,"### Feature request

llama.cpp recently added [support for Qwen2VL](https://github.com/ggerganov/llama.cpp/commit/ba1cb19cdd0d92e012e0f6e009e0620f854b6afd), which means that we can now quantize Qwen2VL models (and I've done so, successfully!) I'd like to be able to load quantized Qwen2VL models with AutoModelForVision2Seq; currently, transformers doesn't recognize qwen2vl as a valid architecture.

### Motivation

It would be wonderful to be able to use quantized GGUF Qwen2VL models!

### Your contribution

I'm happy to work up the PR for this, if I can get some direction on where to start. I'm hacking through the code right now, but I don't know it well enough to be able to meaningfully dent the problem just yet.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Vision models don't work for non-square object,"### System Info

- `transformers` version: 4.47.0
- Platform: Linux-6.8.0-50-generic-x86_64-with-glibc2.39
- Python version: 3.9.20
- Huggingface_hub version: 0.26.5
- Safetensors version: 0.4.5
- Accelerate version: 1.2.0
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.5.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 4060 Ti


### Who can help?

 @amyeroberts, @qubvel

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```
from transformers import ViTMAEConfig,ViTMAEForPreTraining
config = ViTMAEConfig(image_size=tuple((16,1024)),patch_size=tuple((4,256)))
model = ViTMAEForPreTraining(config)
```

```
TypeError                                 Traceback (most recent call last)
Cell In[40], line 3
      1 from transformers import ViTMAEConfig,ViTMAEForPreTraining
      2 config = ViTMAEConfig(image_size=tuple((16,1024)),patch_size=tuple((4,256)))
----> 3 model = ViTMAEForPreTraining(config)

File ~/miniconda3/lib/python3.9/site-packages/transformers/models/vit_mae/modeling_vit_mae.py:994, in ViTMAEForPreTraining.__init__(self, config)
    991 self.config = config
    993 self.vit = ViTMAEModel(config)
--> 994 self.decoder = ViTMAEDecoder(config, num_patches=self.vit.embeddings.num_patches)
    996 # Initialize weights and apply final processing
    997 self.post_init()

File ~/miniconda3/lib/python3.9/site-packages/transformers/models/vit_mae/modeling_vit_mae.py:852, in ViTMAEDecoder.__init__(self, config, num_patches)
    846 self.decoder_layers = nn.ModuleList(
    847     [ViTMAELayer(decoder_config) for _ in range(config.decoder_num_hidden_layers)]
    848 )
    850 self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size, eps=config.layer_norm_eps)
    851 self.decoder_pred = nn.Linear(
--> 852     config.decoder_hidden_size, config.patch_size**2 * config.num_channels, bias=True
    853 )  # encoder to decoder
    854 self.gradient_checkpointing = False
    855 self.config = config

TypeError: unsupported operand type(s) for ** or pow(): 'tuple' and 'int'

```

### Expected behavior

I want ViTMAEForPreTraining to analyze my biological data, which should get a normal output even though it's not a real image. I've noticed that your model doesn't take into account some irregular inputs, such as non-square images, resulting in errors. So, hopefully the code can be improved to accommodate a wider range of inputs.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",1,open
KeyError: 'intern_vit_6b',"### System Info

Ubuntu 24.04.1
transformers    4.47.0


### Who can help?

I want to use the latest OpenGVLab/InternViT-300M-448px-V2_5 as the vision encoder of llava, but an error occurred when running the following code. I think it should be that transformers do not support this visual encoder. I tried to modify the code, but it didn't work.

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

import torch
from PIL import Image
from transformers import AutoModel, CLIPImageProcessor,AutoTokenizer, AutoProcessor,AutoModelForCausalLM
from transformers import LlavaForConditionalGeneration,LlavaConfig
clip_model_name_or_path = ""/home/wangyu/model/models--OpenGVLab--InternViT-300M-448px-V2_5/snapshots/8f86a5e87697180b439811ca69fabbfccd38d996""
qwen_model_name_or_path = ""/home/wangyu/model/Qwen2.5-0.5B-Instruct""
clip_model = AutoModel.from_pretrained(clip_model_name_or_path, device_map=""cuda:0"",trust_remote_code='True')
llm_model = AutoModelForCausalLM.from_pretrained(qwen_model_name_or_path, device_map=""cuda:0"")
llm_tokenizer = AutoTokenizer.from_pretrained(qwen_model_name_or_path)
vision_config = clip_model.config
text_config = llm_model.config
configuration = LlavaConfig(vision_config, text_config)
model = LlavaForConditionalGeneration(configuration)
model.save_pretrained(""slvm/model001"")

from transformers import LlavaProcessor, LlavaForConditionalGeneration
import torch
import os
from typing import Union
from transformers.configuration_utils import PretrainedConfig
from transformers.utils import logging
model_name_or_path = ""slvm/model001""  # 需要确认路径是否正确

llava_processor = LlavaProcessor.from_pretrained(model_name_or_path)
model = LlavaForConditionalGeneration.from_pretrained(
    model_name_or_path, 
    device_map=""cuda:0"", 
    torch_dtype=torch.bfloat16,
)

from PIL import Image

prompt_text = ""<image>\nWhat are these?""

messages = [
    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
    {""role"": ""user"", ""content"": prompt_text},
]

prompt = llava_processor.tokenizer.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)

image_path = ""000000039769.jpg""  
image = Image.open(image_path)

inputs = llava_processor(text=prompt, images=image, return_tensors=""pt"")


for tk in inputs.keys():
    if inputs[tk].dtype == torch.float32:  
        inputs[tk] = inputs[tk].to(dtype=torch.bfloat16)
    inputs[tk] = inputs[tk].to(model.device)

generate_ids = model.generate(**inputs, max_new_tokens=20)
gen_text = llava_processor.batch_decode(
    generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False
)[0]

print(gen_text)






error:

{
	""name"": ""KeyError"",
	""message"": ""'intern_vit_6b'"",
	""stack"": ""---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[5], line 9
      6 from transformers.utils import logging
      7 model_name_or_path = \""slvm/model001\""  # 需要确认路径是否正确
----> 9 llava_processor = LlavaProcessor.from_pretrained(model_name_or_path)
     10 model = LlavaForConditionalGeneration.from_pretrained(
     11     model_name_or_path, 
     12     device_map=\""cuda:0\"", 
     13     torch_dtype=torch.bfloat16,
     14 )
     16 from PIL import Image

File ~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/processing_utils.py:974, in ProcessorMixin.from_pretrained(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)
    971 if token is not None:
    972     kwargs[\""token\""] = token
--> 974 args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)
    975 processor_dict, kwargs = cls.get_processor_dict(pretrained_model_name_or_path, **kwargs)
    977 return cls.from_args_and_dict(args, processor_dict, **kwargs)

File ~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/processing_utils.py:1020, in ProcessorMixin._get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
   1017     else:
   1018         attribute_class = getattr(transformers_module, class_name)
-> 1020     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))
   1021 return args

File ~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:878, in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    876         config = AutoConfig.for_model(**config_dict)
    877     else:
--> 878         config = AutoConfig.from_pretrained(
    879             pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs
    880         )
    881 config_tokenizer_class = config.tokenizer_class
    882 if hasattr(config, \""auto_map\"") and \""AutoTokenizer\"" in config.auto_map:

File ~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1045, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
   1039     except KeyError:
   1040         raise ValueError(
   1041             f\""The checkpoint you are trying to load has model type `{config_dict['model_type']}` \""
   1042             \""but Transformers does not recognize this architecture. This could be because of an \""
   1043             \""issue with the checkpoint, or because your version of Transformers is out of date.\""
   1044         )
-> 1045     return config_class.from_dict(config_dict, **unused_kwargs)
   1046 else:
   1047     # Fallback: use pattern matching on the string.
   1048     # We go from longer names to shorter names to catch roberta before bert (for instance)
   1049     for pattern in sorted(CONFIG_MAPPING.keys(), key=len, reverse=True):

File ~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/configuration_utils.py:734, in PretrainedConfig.from_dict(cls, config_dict, **kwargs)
    731 # We remove it from kwargs so that it does not appear in `return_unused_kwargs`.
    732 config_dict[\""attn_implementation\""] = kwargs.pop(\""attn_implementation\"", None)
--> 734 config = cls(**config_dict)
    736 if hasattr(config, \""pruned_heads\""):
    737     config.pruned_heads = {int(key): value for key, value in config.pruned_heads.items()}

File ~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:108, in LlavaConfig.__init__(self, vision_config, text_config, ignore_index, image_token_index, projector_hidden_act, vision_feature_select_strategy, vision_feature_layer, image_seq_length, **kwargs)
    104 if isinstance(vision_config, dict):
    105     vision_config[\""model_type\""] = (
    106         vision_config[\""model_type\""] if \""model_type\"" in vision_config else \""clip_vision_model\""
    107     )
--> 108     vision_config = CONFIG_MAPPING[vision_config[\""model_type\""]](**vision_config)
    109 elif vision_config is None:
    110     vision_config = CONFIG_MAPPING[\""clip_vision_model\""](
    111         intermediate_size=4096,
    112         hidden_size=1024,
   (...)
    118         projection_dim=768,
    119     )

File ~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:740, in _LazyConfigMapping.__getitem__(self, key)
    738     return self._extra_content[key]
    739 if key not in self._mapping:
--> 740     raise KeyError(key)
    741 value = self._mapping[key]
    742 module_name = model_type_to_module_name(key)

KeyError: 'intern_vit_6b'""
}

### Expected behavior

I think I need to modify the source code and add internvit, just like clip. I hope the official staff can tell me where to modify it.

I love transformers！","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",1,open
Fix typing in docstring for `PaliGemmaProcessor`,"# What does this PR do?

This PR updates the typing for `tokenizer` in the `PaliGemmaProcessor` to be `GemmaTokenizerFast` instead of `LlamaTokenizerFast`.

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?

Not sure who to tag, maybe @molbap as he contributed to the model AFAIK, feel free to ping anyone else if needed (sorry if that's the case)",[],1,open
speed up PrefixConstrainedLogitsProcessor,"# Speed up PrefixConstrainedLogitsProcessor

The `__call__` method creates an empty tensor and then updates it repeatedly. This causes a significant slowdown when executing on a GPU since it requires updating the GPU memory over and over. This pull request creates the mask in CPU memory and then creates a tensor on GPU with one call. The tests I ran (the test case from https://github.com/worldbank/REaLTabFormer/tree/main/colab) has a speed increase of over 200% up to depending on the batch size, 300%. 




## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
microsoft/Phi-3.5-mini-instruct not working with FA2 due to position_ids,"### System Info

- `transformers` version: 4.44.0
- Platform: Linux-5.14.0-427.20.1.el9_4.x86_64-x86_64-with-glibc2.34
- Python version: 3.10.13
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.5
- Accelerate version: 0.33.0
- Accelerate config:    - compute_environment: LOCAL_MACHINE
        - distributed_type: MULTI_GPU
        - mixed_precision: bf16
        - use_cpu: False
        - debug: False
        - num_processes: 2
        - machine_rank: 0
        - num_machines: 1
        - gpu_ids: all
        - rdzv_backend: static
        - same_network: True
        - main_training_function: main
        - enable_cpu_affinity: False
        - downcast_bf16: no
        - tpu_use_cluster: False
        - tpu_use_sudo: False
        - tpu_env: []
- PyTorch version (GPU?): 2.4.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 3090


### Who can help?

@ArthurZucker @sunmarc

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3.5-mini-instruct')
model = AutoModelForCausalLM.from_pretrained(
    'microsoft/Phi-3.5-mini-instruct',
    device_map={"""": 0},
    torch_dtype=torch.bfloat16,
    attn_implementation=""flash_attention_2""  # Commenting this line will remove the issue
)
model.eval()

model = torch.compile(model)
# print(model.config.eos_token_id)
print(tokenizer.eos_token_id)

gen_kwargs = {
    ""max_new_tokens"": 1,
    ""pad_token_id"": tokenizer.eos_token_id,
    ""eos_token_id"": tokenizer.eos_token_id,
    ""do_sample"": False,
    ""top_p"": None,
    ""top_k"": None,
    ""temperature"": None,
}
ctx_len = 128
device = model.device
with torch.inference_mode():
    inputs = {
        ""input_ids"": torch.LongTensor([[42] * ctx_len]).to(device),
        ""attention_mask"": torch.ones(1, ctx_len).to(device),
        ""position_ids"": torch.arange(ctx_len).unsqueeze(0).to(device),
    }

    model.generate(**inputs, **gen_kwargs)

```

Error:

```
Traceback (most recent call last):
  File ""/home/pricie/vanroy/.config/JetBrains/PyCharm2024.1/scratches/scratch_68.py"", line 35, in <module>
    model.generate(**inputs, **gen_kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2024, in generate
    result = self._sample(
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2982, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py"", line 1247, in forward
    outputs = self.model(
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py"", line 1051, in forward
    layer_outputs = decoder_layer(
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py"", line 788, in forward
    attn_outputs, self_attn_weights, present_key_value = self.self_attn(
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py"", line 543, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
TypeError: Phi3LongRoPEScaledRotaryEmbedding.forward() missing 1 required positional argument: 'position_ids'
```

### Expected behavior

No error","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
"Strange behavior with attn_implementation=""eager""","### System Info

- `transformers` version: 4.47.0
- Platform: Linux-5.15.0-120-generic-x86_64-with-glibc2.35
- Python version: 3.10.15
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: Yes
- GPU type: NVIDIA A100-PCIE-40GB

### Who can help?

@zucchini-nlp

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I am trying to analyze the attention pattern of the `LLAVA v1.5 7B` model, so I used `attn_implementation=""eager""` when initing the model to obtain the attention weights. However, this has led to several issues. Firstly, the output IDs are incorrect, and secondly, errors may occur. I've noticed that this problem only appears with specific images and user prompts, while it does not occur in other cases, which is quite peculiar. Below is my code:
``` python
import numpy as np
import torch
from dotenv import load_dotenv
from PIL import Image
from transformers import (
    LlavaForConditionalGeneration,
    LlavaProcessor,
)
from transformers.generation.utils import GenerateDecoderOnlyOutput

np.set_printoptions(threshold=np.inf)
model_name = ""llava-hf/llava-1.5-7b-hf""

model: LlavaForConditionalGeneration = LlavaForConditionalGeneration.from_pretrained(
    model_name,
    cache_dir=""/root/llm/utils/models/hub"",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map=""cuda:0"",
    attn_implementation=""eager"",
)
processor: LlavaProcessor = LlavaProcessor.from_pretrained(
    model_name,
    cache_dir=""/root/llm/utils/models/hub"",
    padding_side=""left"",
    patch_size=model.config.vision_config.patch_size,
    vision_feature_select_strategy=model.config.vision_feature_select_strategy,
)

images = [
    Image.open(""/root/llm/utils/eval/Object_HalBench/images/339761.jpg""),
    Image.open(""/root/llm/utils/eval/Object_HalBench/images/431256.jpg""),
]
users = [
    ""Provide a thorough description of the given image."",
    ""What is this photo about? Please answer in great detail."",
]

prompts: list[str] = []
for u in users:
    conversation: list[dict[str]] = [
        {
            ""role"": ""user"",
            ""content"": [
                {""type"": ""image""},
                {""type"": ""text"", ""text"": u},
            ],
        },
    ]
    prompt: str = processor.apply_chat_template(
        conversation,
        tokenize=False,
        add_generation_prompt=True,
    )
    prompts.append(prompt)

with torch.inference_mode():
    encoded_inputs: dict[str, torch.Tensor] = processor(
        images=images,
        text=prompts,
        return_tensors=""pt"",
        return_token_type_ids=False,
        padding=True,
    ).to(""cuda:0"", torch.float16)

    output: GenerateDecoderOnlyOutput = model.generate(
        **encoded_inputs,
        max_new_tokens=50,
        num_beams=1,
        do_sample=False,
        temperature=0.7,
        output_attentions=True,
        use_cache=True,
        return_legacy_cache=True,
        return_dict_in_generate=True,
    )
generated_ids: list[torch.LongTensor] = output.sequences  # list of shape (batch_size, sequence_length)
print(generated_ids.cpu().numpy())
generated_ids = [o[len(i) :] for i, o in zip(encoded_inputs.input_ids, generated_ids)]
print()
decoded_outputs: list[str] = processor.batch_decode(
    generated_ids,
    skip_special_tokens=True,
    clean_up_tokenization_spaces=True,
)
print(decoded_outputs)
decoded_outputs = [d.rstrip(""\n"").strip("" "") for d in decoded_outputs]
print(decoded_outputs)
print(len(output.attentions))
```
Notice: the image I used is from Object_HalBench benchmark

The output is:
Some other warning:
```
/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
```
The generated_ids: (I remove a large number of `<image>` token for readability)
```
[[32001     1  3148  1001 29901 29871 32000 32000 32000 32000 32000 32000
  32000 32000 32000 32000 32000 32000 29871    13  1184 29894   680   263
  17826  6139   310   278  2183  1967 29889   319  1799  9047 13566 29901
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0]
 [    1  3148  1001 29901 29871 32000 32000 32000 32000 32000 32000 32000
  32000 32000 32000 32000 32000 29871    13  5618   338   445 15373  1048
  29973  3529  1234   297  2107  9493 29889   319  1799  9047 13566 29901
    450  1967  4332  1973   263 15007  3377   261   297  3158 29892 15859
    263  8938   373   263 15007 29899 11911   287   364  1160 29889   450
  15007  3377   261   338   297   278  7256   310   278  9088 29892   411
   1009 15007  3377  7962 19540   963 29889 29871    13    13  8439   526
   3196   916]]
```
Notice that `<image>`: 32000, `<pad>`: 32001

The output after `batch_decode` is:
```
['', 'The image captures a snowboarder in action, performing a trick on a snow-covered ramp. The snowboarder is in the middle of the scene, with their snowboard visible beneath them. \n\nThere are several other']
```
It's strange that there is token id 0 generated. 

Only set `output_attentions=False` and `return_dict_in_generate=False` without removing `attn_implementation=""eager"",` won't make any change.

Notice that removing `attn_implementation=""eager""`, and not returning dict overcome this question, the output then become correct:
```
[[32001     1  3148  1001 29901 29871 32000 32000 32000 32000 32000 32000
  32000 32000 32000 32000 32000 32000 29871    13  1184 29894   680   263
  17826  6139   310   278  2183  1967 29889   319  1799  9047 13566 29901
    450  1967  5680   263 27683  8345   411   263  2919  7933  8024 15678
    701   278 10090 29892  4969   263   301  1878   322   325  4626   424
  25005 29889   450  8024   338 24046  2978   278  1510   261  4038 29892
   4417   263  6023   310  5469   304   278  2913 29889 29871    13    13
    797   278]
 [    1  3148  1001 29901 29871 32000 32000 32000 32000 32000 32000 32000
  32000 32000 32000 32000 32000 29871    13  5618   338   445 15373  1048
  29973  3529  1234   297  2107  9493 29889   319  1799  9047 13566 29901
    450  1967  4332  1973   263 15007  3377   261   297  3158 29892 15859
    263  8938   373   263 15007 29899 11911   287   364  1160 29889   450
  15007  3377   261   338   297   278  7256   310   278  9088 29892   411
   1009 15007  3377  7962 19540   963 29889 29871    13    13  8439   526
   3196   916]]

['The image features a bathroom with a large green plant growing up the wall, creating a lush and vibrant atmosphere. The plant is situated near the shower area, adding a touch of nature to the space. \n\nIn the',
'The image captures a snowboarder in action, performing a trick on a snow-covered ramp. The snowboarder is in the middle of the scene, with their snowboard visible beneath them. \n\nThere are several other']
```
Beside this, some error may occur with `attn_implementation=""eager""`, in some other case (different Image input)
```
Loading checkpoint shards: 100%|████| 3/3 [00:03<00:00,  1.12s/it]
../aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.
Traceback (most recent call last):
  File ""/root/llm/LVLM/test2.py"", line 38, in <module>
    print(generator.gen(images, users,do_sample=True,
  File ""/root/llm/LVLM/model/generator/llava.py"", line 184, in gen
    out = gen_hf(
  File ""/root/llm/LVLM/model/generator/utils.py"", line 279, in gen_hf
    output, encoded_inputs = _gen_hf(
  File ""/root/llm/LVLM/model/generator/utils.py"", line 229, in _gen_hf
    output: GenerateDecoderOnlyOutput = model.generate(
  File ""/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
  File ""/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2252, in generate
    result = self._sample(
  File ""/root/anaconda3/envs/LVLM/lib/python3.10/site-packages/transformers/generation/utils.py"", line 3297, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

```

### Expected behavior

fix it","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",12,open
ci: mark model_parallel tests as cuda specific,"`parallelize()` API is deprecated in favor of accelerate's `device_map=""auto""` and therefore is not accepting new features. At the same time `parallelize()` implementation is currently CUDA-specific. This commit marks respective ci tests with `@require_torch_gpu`.

Fixes: #35252
CC: @ArthurZucker @SunMarc
",[],0,open
Fix all output_dir in test_trainer.py to use tmp_dir,"# What does this PR do?

Split from the PR: https://github.com/huggingface/transformers/pull/35243

Current `test_trainer.py` has so many direct specifications of `outpu_dir` to TrainingArguments.
I've replaced all of them with tmp_dir so that no unnecessary folders will be made under root dir.

The targets are:
- ""./examples""
- ""./regression""
- ""./generation""
- ""./test""
- ""None""


### Just a random thought
The current CircleCI doesn't seem to include tests for trainer. Is it as intended?

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ydshieh @SunMarc",[],0,open
inconsistent execution time,"### System Info

```
- `transformers` version: 4.28.1
- Platform: Windows-10-10.0.22631-SP0
- Python version: 3.10.6
- Huggingface_hub version: 0.13.4
- Safetensors version: not installed
- PyTorch version (GPU?): 2.0.0+cu118 (True)
- Tensorflow version (GPU?): 2.12.0 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>
```

### Who can help?

@Arther

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Basically, I have 100 rows dataframe, total 10 files. I like to send it each one by one to my 4 GPUs. I use Llama 3. I'm testing the execution or completion time. Now, either use the data-parallel or model-parallel for inference, I got the around same execution time. Let's say

```
# using 1 GPU
one dataframe - > prompt template 
response = model.generate(prompt)

execution time: 20 second.
```

```
# using 3 GPU 
# with data-parallel
one dataframe - > prompt template 
response = model.generate(prompt)

execution time: 20 second (GPU:0)
execution time: 21 second (GPU:0)
execution time: 19 second (GPU:0)
```

Model definition

```python
model_id = ""meta-llama/Meta-Llama-3-8B-Instruct""

torch_dtype = torch.float16
attn_implementation = ""eager""

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch_dtype,
    bnb_4bit_use_double_quant=True,
)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map={"""": torch.cuda.current_device()},
    attn_implementation=attn_implementation
)

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM"",
    target_modules=[
        'up_proj', 'down_proj', 'gate_proj', 
        'k_proj', 'q_proj', 'v_proj', 'o_proj'
    ]
)
model = get_peft_model(model, peft_config)

for df_file in tqdm(xcel_list):
    df = pd.read_excel(df_file)
    messages = prepare_prompt_from_excel(df)

    prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )

    inputs = tokenizer(
        prompt, 
        return_tensors='pt', 
        padding=True, 
        truncation=True
    ).to(""cuda"")

    start = time.time()
    outputs = model.generate(
        **inputs, 
        max_length=2048, 
        num_return_sequences=1
    )
    exe = time.time() - start

...
```

### Expected behavior

If I use data-parallel on multiple GPU, a replicate of model will be placed to all GPUs and data will be splitted across GPUs.  Or, if I use model-parallel (`device_map=auto'), layers of that model will be distributed across GPUs. Apart from this, if no data or model parallel, just single GPU inference, I was expecting to get longer time for inference, and less time or faster inference if I use multi-gpu, either data or model parallel. But single GPU inference time and multi-gpu inference times are almost comparable. My another concern is that, while using data-parallel, as I am sending one data-frame / prompt template to all GPUs - does this single prompt template gets splitted into many chunks? I doubt that. Each GPUs still see the full data in data-parallel setup here, and that's the cause to get similar execution time, no!","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Fix `Seq2SeqTrainingArguments` documentation,"# What does this PR do?

https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments

<img width=""944"" alt=""Screenshot 2024-12-13 at 15 01 59"" src=""https://github.com/user-attachments/assets/49f12cd9-2016-4ead-8c78-894a95e93b44"" />


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
Fix `model_accepts_loss_kwargs` for timm model,"# What does this PR do?

`TimmWrapper` model training is broken due to the `num_items_in_batch` param passed to the model forward. `TimmWrapper` model has `**kwargs` in the `transformers` model forward, but it goes straight to the `timm` model and not to the loss. This PR introduces a fix to avoid passing loss_kwargs even if `**kwargs` are in model forward.

Relevant to:
 - https://github.com/huggingface/transformers/pull/35121
 - https://github.com/huggingface/transformers/pull/34915

cc @ArthurZucker ",[],2,open
" RuntimeError: ""rshift_cuda"" not implemented for 'Half'","### System Info

Hardware details
CPU:Intel(R) Xeon(R) Silver 4410Y
GPU:NVIDIA RTX A6000
Software version
Version of relevant software such as operation system, cuda toolkit, python, auto-gptq, pytorch, transformers, accelerate, etc.
operation system:
Distributor ID: Ubuntu
Description: Ubuntu 24.04.1 LTS
Release: 24.04
Codename: noble

cuda toolkit:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Tue_Oct_29_23:50:19_PDT_2024
Cuda compilation tools, release 12.6, V12.6.85
Build cuda_12.6.r12.6/compiler.35059454_0

Python 3.10.15

Name: auto_gptq
Version: 0.7.1

pytorch:2.2.1+cu121
transformer:4.35.0
accelerate:0.26.0

### Who can help?

https://huggingface.co/jp288881/slim-llm-llama-7b

@ArthurZucker @itazap @mueller

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

https://huggingface.co/jp288881/slim-llm-llama-7b

### Expected behavior

  I used the quantization method from Aaronhuang-778/SliM-LLM (https://github.com/Aaronhuang-778/SliM-LLM?tab=readme-ov-file) and wanted to load and use the model , but encountered the issue mentioned in the title.I would like to ask if any experts know how to resolve this issue. I have also uploaded my model to Hugging Face for reference.

""Traceback (most recent call last):
  File ""/mnt/8tb_raid/david_model/SliM-LLM/AutoGPTQ/test.py"", line 31, in <module>
    generated_ids = model.generate(**input_ids)
  File ""/mnt/8tb_raid/david_model/SliM-LLM/AutoGPTQ/auto_gptq/modeling/_base.py"", line 480, in generate
    return self.model.generate(**kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2252, in generate
    result = self._sample(
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/generation/utils.py"", line 3251, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"", line 1163, in forward
    outputs = self.model(
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"", line 913, in forward
    layer_outputs = decoder_layer(
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"", line 640, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"", line 522, in forward
    query_states = self.q_proj(hidden_states)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/mnt/8tb_raid/david_model/SliM-LLM/AutoGPTQ/auto_gptq/nn_modules/qlinear/qlinear_cuda.py"", line 319, in forward
    zeros = torch.bitwise_right_shift(
RuntimeError: ""rshift_cuda"" not implemented for 'Half'
""","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Don't show warning for `inv_freq` buffers,"# What does this PR do?

We shouldn't show warning about unexpected keys when loading the model, if the state dict contains any saved keys for `inv_freq`. It was working good until the last release, when we moved RoPE under base model for several architectures

Since we are sure the inv frequencies are always recomputed for over a year now, we can make assumption that any key for `rotary_embedding.inv_freq` can be disregarded if the buffer has any RoPE parameters. Another option can be to explicitly try to reconstruct buffers for each attn layer, but I am not sure if we can safely assume all models will have the key similar to `model.layer.{0}.self_attn.rotary_emb.inv_freq`. So I opted to remove anything RoPE related",[],1,open
Mismatch Between txt img_token and Image Count in Multimodal Processor Causes Debugging,"### Feature request

As with Qwen2-VL, if the number of img_tokens input in the multimodal processor does not match the number of images, 
a warning or error should be displayed.


### Motivation

## reproduction code
```python
import requests
from PIL import Image

from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor


model = Qwen2VLForConditionalGeneration.from_pretrained(""Qwen/Qwen2-VL-7B-Instruct"")
processor = Qwen2VLProcessor.from_pretrained(""Qwen/Qwen2-VL-7B-Instruct"")

prompts = [
    f""USER: {processor.image_token}{processor.image_token}\nWhat are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT:"",
]
image1 = Image.open(requests.get(""https://llava-vl.github.io/static/images/view.jpg"", stream=True).raw)

inputs = processor(images=[image1], text=prompts, return_tensors=""pt"", padding=True)
```
## env
```
- `transformers` version: 4.47.0.dev0
- Platform: Linux-5.15.0-124-generic-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
```
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py"", line 139, in __call__
    self.image_token, ""<|placeholder|>"" * (image_grid_thw[index].prod() // merge_length), 1
IndexError: index 1 is out of bounds for dimension 0 with size 1
```

When running the code, an error like the one above occurs. 
The cause is that the number of img_tokens does not match the number of images.

However, the error is not very intuitive, so it took some time to find the cause. 
Therefore, I think it would be good to explicitly display a warning or error 
when the number of img_tokens and images do not match.

### Your contribution

It seems possible to add a statement that explicitly displays an error or warning 
when the number of img_tokens and images do not match in the multimodal processor.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",3,open
xpu: parallelize() not supported for PyTorch XPU backend,"With https://github.com/huggingface/transformers/releases/tag/v4.47.0.

Transforms gpt2, mt5, t5 and umt5 models don't support model parallelism when running on PyTorch XPU backend (on few gpu devices) as can be observed by running Transformers tests - see logs below.

Can model parallelism be supported for XPU backend?
- [ ] For GPT2 model, https://github.com/huggingface/transformers/pull/35253
- [ ] For MT5 model
- [ ] For T5 model
- [ ] For UMT5 model

```
$ cat spec.py
import torch
DEVICE_NAME = 'xpu'
MANUAL_SEED_FN = torch.xpu.manual_seed
EMPTY_CACHE_FN = torch.xpu.empty_cache
DEVICE_COUNT_FN = torch.xpu.device_count

$ TRANSFORMERS_TEST_DEVICE_SPEC=spec.py python3 -m pytest -rsf tests/models/ -k ""test_model_parallelization or test_model_parallel_equal_results""
<...>
FAILED tests/models/gpt2/test_modeling_gpt2.py::GPT2ModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero
FAILED tests/models/gpt2/test_modeling_gpt2.py::GPT2ModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled
FAILED tests/models/mt5/test_modeling_mt5.py::MT5ModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero
FAILED tests/models/mt5/test_modeling_mt5.py::MT5ModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled
FAILED tests/models/mt5/test_modeling_mt5.py::MT5EncoderOnlyModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero
FAILED tests/models/mt5/test_modeling_mt5.py::MT5EncoderOnlyModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled
FAILED tests/models/t5/test_modeling_t5.py::T5ModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero
FAILED tests/models/t5/test_modeling_t5.py::T5ModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled
FAILED tests/models/t5/test_modeling_t5.py::T5EncoderOnlyModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero
FAILED tests/models/t5/test_modeling_t5.py::T5EncoderOnlyModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled
FAILED tests/models/umt5/test_modeling_umt5.py::UMT5EncoderOnlyModelTest::test_model_parallel_equal_results - AttributeError: 'UMT5EncoderModel' object has no attribute 'parallelize'
FAILED tests/models/umt5/test_modeling_umt5.py::UMT5EncoderOnlyModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled
=============================== 12 failed, 682 skipped, 76163 deselected, 5 warnings in 24.79s ================================
```

CC: @ArthurZucker @SunMarc",[],2,open
Enhance DataCollatorForLanguageModeling with Configurable Token Replacement Probabilities,"This pull request introduces enhancements to the `DataCollatorForLanguageModeling` class, providing greater flexibility for token replacement during masked language modeling (MLM). The key changes include:

1. **Configurable Replacement Probabilities**:
   - **`mask_replace_prob`**: Specifies the probability of replacing masked tokens with the `[MASK]` token (default: 80%).
   - **`random_replace_prob`**: Specifies the probability of replacing masked tokens with random tokens from the vocabulary (default: 10%).
   - The remaining masked tokens are left unchanged (default: 10%).

2. **Edge Case Handling**:
   - Properly scales `random_replace_prob` to the remaining probability after applying `mask_replace_prob`.
   - Includes validation to ensure the sum of `mask_replace_prob` and `random_replace_prob` does not exceed 1.

3. **Backward Compatibility**:
   - Default behavior mimics the traditional 80-10-10 rule for MLM token replacement.

### Examples of New Functionality
- **Default Behavior**:
  Replace 80% of masked tokens with `[MASK]`, 10% with random tokens, and leave 10% unchanged.
- **Custom Configurations**:
  - Replace all masked tokens with `[MASK]`:
    ```python
    mask_replace_prob=1.0, random_replace_prob=0.0
    ```
  - Replace all masked tokens with random tokens:
    ```python
    mask_replace_prob=0.0, random_replace_prob=1.0
    ```
  - Balanced replacement:
    ```python
    mask_replace_prob=0.5, random_replace_prob=0.4
    ```

### Additional Notes
- Updated docstrings to reflect the new configuration options.
- Added validations for probability values and enhanced edge case handling for robust training workflows.

This enhancement gives users greater control over MLM training configurations, catering to various pretraining and fine-tuning use cases.",[],4,open
set supports_gradient_checkpointing = False in SwitchTransformer,"# What does this PR do?

Recently, CircleCI's `tests_torch` sometimes fails (Reference: [tests_torch](https://app.circleci.com/pipelines/github/huggingface/transformers/113436/workflows/802101b9-ed24-49ac-bc1d-7f2ae9dce240/jobs/1515817?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-checks-link&utm_content=summary) - Failed), because some models which don't support gradient_checkpointing or have MoE module like SwitchTransformer will occasionally have weights without gradients.

I found maybe-related PR: https://github.com/huggingface/transformers/pull/34806, and the situation described in the PR was already fixed (https://github.com/pytest-dev/pytest-subtests/pull/169 was already merged).

In https://github.com/huggingface/transformers/pull/34806, @ydshieh said: `TODO (ydshieh): use skipTest once pytest-dev/pytest-subtests/pull/169 is merged`, so I reverted the stopgap fix.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

cc @ydshieh @muellerzr @SunMarc",[],5,open
"StopStringCriteria relies on `len(tokenizer)==model.config.vocab_size`, leading to index errors","### System Info

Python: 3.12.0
Transformers: 4.46.3


### Who can help?

@gante
@ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

After fine-tuning EleutherAI/pythia-14m using transformer's Trainer, I run inference like this:

```python
checkpoint = ""models/checkpoint-166000""
device = ""cuda""

model = AutoModelForCausalLM.from_pretrained(checkpoint)
model.to(device)

tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side=""left"")
tokenizer.pad_token_id = 1
tokenizer.pad_token = ""<|padding|>""

prompts = [
    ""prompt1"",
    ""prompt2"",
]
inputs = tokenizer(
    prompts, return_tensors=""pt"", padding=True, truncation=True, max_length=512,
)

gen_config = copy.deepcopy(model.generation_config)
gen_config.update(
    max_new_tokens=max_length,
    do_sample=True,
    top_k=0,
    pad_token_id=tokenizer.pad_token_id,
    stop_strings=""end"",
)
gen_config.validate()

outputs = model.generate(
    input_ids=inputs[""input_ids""].to(device),
    attention_mask=inputs[""attention_mask""].to(device),
    num_return_sequences=32,
    generation_config=gen_config,
    output_scores=True,
    return_dict_in_generate=True,
    tokenizer=tokenizer,
)
```

Note that `tokenizer.pad_token_id` has to be set explicitly because it is not present in Pythia's `special_tokens_map.json`. This code leads to the following error (run with `CUDA_LAUNCH_BLOCKING=1`):

```
../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [1,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [1,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [1,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.      
../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [1,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
Traceback (most recent call last):                   
  File ""home/m/src/playground.py"", line 43, in <module>                 
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File ""/home/m/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context                            
    return func(*args, **kwargs)             
           ^^^^^^^^^^^^^^^^^^^^^                        
  File ""/home/m/venv/lib/python3.12/site-packages/transformers/generation/utils.py"", line 2215, in generate                             
    result = self._sample(
             ^^^^^^^^^^^^^
  File ""/home/m/venv/lib/python3.12/site-packages/transformers/generation/utils.py"", line 3262, in _sample                              
    unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)
                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/m/venv/lib/python3.12/site-packages/transformers/generation/stopping_criteria.py"", line 496, in __call__                  
    is_done = is_done | criteria(input_ids, scores, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/m/venv/lib/python3.12/site-packages/transformers/generation/stopping_criteria.py"", line 402, in __call__                  
    embedded = F.embedding(flipped_ids, self.embedding_vec)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   
  File ""/home/m/venv/lib/python3.12/site-packages/torch/nn/functional.py"", line 2551, in embedding                                      
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)  
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
```

This is due to mismatch between `len(tokenizer)` (50277) and `model.config.vocab_size` (50304 or 50432). This decision to round up the size of the embedding matrix to the next multiple of 128 or 256 was presumably made due to efficiency reasons. However, during sampling, tokens above `len(tokenizer)` can sometimes be generated. This is silently ignored by the tokenizer, converting such tokens to empty string. However, `StopStringCriteria` is implemented by indexing into an embedding with size determined by `len(tokenizer)` and therefore fails when it encounters a higher token.

A temporary fix is to explicitly suppress the unknown tokens from being generated:
```python
if len(tokenizer) < model.config.vocab_size:
    model.generation_config.suppress_tokens = list(range(len(tokenizer), model.config.vocab_size))
```

I propose that a more principled solution would to be modify `StopStringCriteria` to ignore tokens above `len(tokenizer)`.


### Expected behavior

Expected behavior of the `generate` method is to not fail.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",5,open
Applies the rest of the init refactor except to modular files,I'm sorry @ArthurZucker ,[],1,open
[i18n-Chinese] Translating perf_infer_gpu_multi.md to Chinese,"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->
I noticed that no one has translated docs/source/en/perf_infer_gpu_multi.md before. Could I translate this to Chinese?
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Pass state dict,"# What does this PR do?

FSDP + PEFT (lora) learning process crashes (`Watchdog caught collective operation timeout`) when trainer tries to get peft state dict (via **get_peft_model_state_dict** function from [huggingface/peft](https://github.com/huggingface/peft/blob/5cdade973eb345b0a8aeda52f3c2fb4199d0a028/src/peft/utils/save_and_load.py#L51)) and doesn't provide it with model's state dict. Therefore the function tries to get this data on its own and freezes because it has no information about FSDP

This PR addresses this issue.


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@SunMarc
@muellerzr

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],3,open
Shape mismatch in RoPE embeddings gpt_neox model when rotary_ndims is odd,"### System Info

- `transformers` version: 4.48.0.dev0
- Platform: macOS-15.1.1-arm64-arm-64bit
- Python version: 3.12.7
- Huggingface_hub version: 0.26.5
- Safetensors version: 0.4.5
- Accelerate version: 1.2.0
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.5.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

I just appended the following to https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neox/modeling_gpt_neox.py:

```python
def reproduce_bug():
    # Then:
    # head_size = hidden_size // num_attention_heads = 4
    # rotary_ndims = int(head_size * rotary_pct) = 3
    config = GPTNeoXConfig(
        vocab_size=96,
        max_position_embeddings=32,
        hidden_size=32,
        num_hidden_layers=2,
        num_attention_heads=8,
        intermediate_size=3 * 32,
        rotary_pct=0.75,
        use_parallel_residual=False,
    )
    model = GPTNeoXModel(config)
    input_ids = torch.randint(0, config.vocab_size, (1, config.max_position_embeddings))
    logits = model(input_ids)
    print(f""logits.shape = {logits.shape}"")


if __name__ == ""__main__"":
    reproduce_bug()
```
 Then, I ran
```bash
python -m src.transformers.models.gpt_neox.modeling_gpt_neox src/transformers/models/gpt_neox/modeling_gpt_neox.py
```

This gives me the following error output:
```text
The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py"", line 1510, in <module>
    reproduce_bug()
  File ""/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py"", line 1505, in reproduce_bug
    logits = model(input_ids)
             ^^^^^^^^^^^^^^^^
  File ""/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py"", line 910, in forward
    outputs = layer(
              ^^^^^^
  File ""/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py"", line 657, in forward
    attention_layer_outputs = self.attention(
                              ^^^^^^^^^^^^^^^
  File ""/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py"", line 319, in forward
    query, key, value, present = self._attn_projections_and_rope(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py"", line 431, in _attn_projections_and_rope
    query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py"", line 607, in apply_rotary_pos_emb
    q_embed = (q * cos) + (rotate_half(q) * sin)
               ~~^~~~~
RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 3
```

This is what I expected. Your code does not work if `rotary_ndims` is odd. Here, it is 3. The way that `cos`, `sin` are computed gives them a final dim size `2 * ceil(rotary_ndims / 2) == rotary_ndims + 1`, this is 1 too large.

Note that your code actually ""works"" if `rotary_ndims = 1`. Then, `cos`, `sin` have final dim size 2 and the code above works due to broadcasting (1 broadcast to 2), both `q`, `k` have final dim 1 too large, but that still works. But once `rotary_ndims` is odd and larger than 1, it fails.

### Expected behavior

Without this bug, `cos` and `sin` should have size `rotary_ndims` in the final dimension, no matter whether this is even or odd. My suggestions:
- Restrict `rotary_ndim` to be even, or
- Subselect `cos`, `sin` so their final dim size is `rotary_ndims`

My feeling is this does not only affect this single model, but many others as well. But I did not check.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",11,open
Fix Gemma2 synced multi-GPU generation,"# What does this PR do?

Generation with Gemma2ForCausalLM in synced multi-GPU settings crashes because the cache_position goes out of bounds. This PR addresses the issue.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.


@ArthurZucker
@gante
",[],0,open
if split_special_tokens==True，fast_tokenizer is slower than slow_tokenizer,"### System Info

if split_special_tokens==True，fast_tokenizer is slower than slow_tokenizer

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
from transformers import LlamaTokenizer, LlamaTokenizerFast
import time
tokenizer1 = LlamaTokenizer.from_pretrained(""./Llama-2-7b-chat-hf"", split_special_tokens=True) # LlamaTokenizer
tokenizer2 = LlamaTokenizerFast.from_pretrained(""./Llama-2-7b-chat-hf"", split_special_tokens=True) # LlamaTokenizer
print(tokenizer1, tokenizer2)

s_time = time.time()
for i in range(1000):
    tokenizer1.tokenize(""你好，where are you?""*100)
print(f""slow: {time.time() - s_time}"")

s_time = time.time()
for i in range(1000):
    tokenizer2.tokenize(""你好，where are you?""*100)
print(f""fast: {time.time() - s_time}"")
```
output: 
        slow: 0.6021890640258789
        fast: 0.7353882789611816

### Expected behavior

-","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Deleting quantization_config broken,"### System Info

Version 4.47.0

### Reproduction

```
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(""h2oai/h2o-danube3-500m-chat"", load_in_4bit=True)
del model.config.quantization_config
model.config
```

`TypeError: Object of type dtype is not JSON serializable`

### Expected behavior

I am unable to delete the `quantization_config` from an existing model. Whenever I do it, it just completely breaks the whole config.

I also tried setting `is_quantized=False` but it does not change anything.

Is there another way of achieving this?

I am aware that there is a `.dequantize` function, but in this case Im changing dtypes on my own and want to exclude that `quantization_config` particularly when saving the model.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
added cached tokenizer,"# What does this PR do?

This PR introduces a caching mechanism for the added_tokens_encoder property in tokenizers to improve performance. Previously, the added_tokens_encoder mapping was recomputed every time the property was accessed, leading to redundant computation during tasks that frequently access it, such as tokenization or decoding.

Motivation and Context
The motivation for this change is to optimize tokenizer performance, especially in workflows that repeatedly access the added_tokens_encoder property. By caching the result, this PR reduces overhead and improves runtime efficiency without altering the existing behavior of the library.

Key changes:

The added_tokens_encoder mapping is now cached on the first access and reused for subsequent calls.
The caching mechanism is implemented in a way that is backward-compatible and avoids unnecessary recomputation.

## Some benchmarks

### Composite Results

| Model                        | Composite WER (%) | Composite RTFx (With/Without/Improvement) |
|------------------------------|-------------------|-------------------------------------------|
| distil/whisper-distil-large-v2 | 7.92             | 278.32 / 202.95 / 36%                    |
| distil/whisper-distil-large-v3 | 7.52             | 282.46 / 214.42 / 32%                    |
| distil/whisper-distil-medium.en | 8.76             | 406.96 / 279.73 / 45%                    |
| openai/whisper-large          | 7.94             | 167.43 / 143.76 / 16%                    |
| openai/whisper-large-v2       | 7.83             | 167.95 / 144.45 / 16%                    |
| openai/whisper-large-v3       | 7.44             | 169.26 / 145.51 / 16%                    |
| openai/whisper-large-v3-turbo       | 7.83             | 268.72 / 197.98 / 36%                    |
| openai/whisper-medium.en      | 8.09             | 222.49 / 182.13 / 22%                    |
| openai/whisper-small.en       | 8.59             | 359.18 / 268.91 / 34%                    |
| openai/whisper-base.en        | 10.32            | 483.69 / 320.67 / 50%                    |
| openai/whisper-tiny.en        | 12.81            | 532.03 / 348.12 / 53%                    |

<details>

### AMI Results

| Model                        | AMI WER (%) | AMI RTFx (With/Without/Improvement) |
|------------------------------|-------------|-------------------------------------|
| distil/whisper-distil-large-v2 | 14.67       | 120.15 / 103.50 / 16%             |
| distil/whisper-distil-large-v3 | 15.16       | 119.29 / 104.33 / 14%             |
| distil/whisper-distil-medium.en | 16.12       | 189.32 / 152.03 / 25%             |
| openai/whisper-large          | 16.73       | 82.81 / 76.15 / 9%                |
| openai/whisper-large-v2       | 16.74       | 85.65 / 79.49 / 7%                |
| openai/whisper-large-v3       | 15.95       | 84.31 / 77.97 / 8%                |
| openai/whisper-large-v3-turbo       | 16.13          | 116.17 / 98.83 / 18%     |
| openai/whisper-medium.en      | 16.68       | 78.47 / 76.86 / 2%                |
| openai/whisper-small.en       | 17.93       | 197.70 / 168.88 / 17%             |
| openai/whisper-base.en        | 21.13       | 224.91 / 181.10 / 24%             |
| openai/whisper-tiny.en        | 24.24       | 271.98 / 228.77 / 19%             |

### Earnings22 Results

| Model                        | Earnings22 WER (%) | Earnings22 RTFx (With/Without/Improvement) |
|------------------------------|---------------------|-------------------------------------------|
| distil/whisper-distil-large-v2 | 12.19              | 279.17 / 212.11 / 32%                   |
| distil/whisper-distil-large-v3 | 11.79              | 281.64 / 219.27 / 28%                   |
| distil/whisper-distil-medium.en | 12.99              | 408.40 / 291.33 / 40%                   |
| openai/whisper-large          | 12.91              | 156.36 / 138.56 / 13%                   |
| openai/whisper-large-v2       | 12.05              | 173.81 / 151.92 / 14%                   |
| openai/whisper-large-v3       | 11.29              | 171.74 / 149.66 / 15%                   |
| openai/whisper-large-v3-turbo       | 11.63          | 274.35 / 202.67 / 35%     |
| openai/whisper-medium.en      | 12.63              | 251.39 / 204.49 / 23%                   |
| openai/whisper-small.en       | 12.97              | 390.44 / 303.05 / 29%                   |
| openai/whisper-base.en        | 15.09              | 554.06 / 370.98 / 49%                   |
| openai/whisper-tiny.en        | 19.12              | 439.19 / 323.27 / 36%                   |

### Gigaspeech Results

| Model                        | GigaSpeech WER (%) | GigaSpeech RTFx (With/Without/Improvement) |
|------------------------------|--------------------|-------------------------------------------|
| distil/whisper-distil-large-v2 | 10.32             | 242.64 / 178.28 / 26%                   |
| distil/whisper-distil-large-v3 | 10.08             | 245.04 / 185.02 / 32%                   |
| distil/whisper-distil-medium.en | 11.30             | 351.03 / 242.87 / 45%                   |
| openai/whisper-large          | 10.76             | 137.20 / 118.69 / 16%                   |
| openai/whisper-large-v2       | 10.67             | 139.24 / 120.05 / 15%                   |
| openai/whisper-large-v3       | 10.02             | 141.93 / 122.97 / 16%                   |
| openai/whisper-large-v3-turbo       | 10.14          | 229.71 / 168.52 / 36%     |
| openai/whisper-medium.en      | 11.03             | 177.60 / 151.70 / 17%                   |
| openai/whisper-small.en       | 11.35             | 271.56 / 213.19 / 27%                   |
| openai/whisper-base.en        | 12.83             | 357.94 / 253.20 / 41%                   |
| openai/whisper-tiny.en        | 14.08             | 421.61 / 284.52 / 48%                   |

### LibriSpeech Clean Results

| Model                        | LibriSpeech Clean WER (%) | LibriSpeech Clean RTFx (With/Without/Improvement) |
|------------------------------|--------------------------|-------------------------------------------|
| distil/whisper-distil-large-v2 | 2.94                     | 286.00 / 205.44 / 39%                    |
| distil/whisper-distil-large-v3 | 2.54                     | 288.02 / 217.52 / 32%                    |
| distil/whisper-distil-medium.en | 3.69                     | 415.82 / 280.95 / 48%                    |
| openai/whisper-large          | 2.73                     | 181.37 / 150.35 / 21%                    |
| openai/whisper-large-v2       | 2.83                     | 159.01 / 135.81 / 17%                    |
| openai/whisper-large-v3       | 2.01                     | 179.93 / 151.42 / 19%                    |
| openai/whisper-large-v3-turbo       | 2.10          | 278.29 / 201.89 / 38%     |
| openai/whisper-medium.en      | 3.02                     | 244.38 / 196.85 / 24%                    |
| openai/whisper-small.en       | 3.05                     | 408.91 / 280.23 / 46%                    |
| openai/whisper-base.en        | 4.25                     | 583.91 / 353.97 / 65%                    |
| openai/whisper-tiny.en        | 5.66                     | 639.70 / 376.14 / 70%                    |

### LibriSpeech Other Results

| Model                        | LibriSpeech Other WER (%) | LibriSpeech Other RTFx (With/Without/Improvement) |
|------------------------------|--------------------------|-------------------------------------------|
| distil/whisper-distil-large-v2 | 6.84                     | 248.08 / 177.63 / 40%                    |
| distil/whisper-distil-large-v3 | 5.19                     | 259.09 / 199.72 / 30%                    |
| distil/whisper-distil-medium.en | 8.35                     | 349.71 / 236.81 / 48%                    |
| openai/whisper-large          | 5.54                     | 164.39 / 138.73 / 18%                    |
| openai/whisper-large-v2       | 5.14                     | 162.81 / 139.05 / 17%                    |
| openai/whisper-large-v3       | 3.91                     | 163.21 / 140.22 / 16%                    |
| openai/whisper-large-v3-turbo       | 4.24          | 257.22 / 188.87 / 36%     |
| openai/whisper-medium.en      | 5.85                     | 222.76 / 181.65 / 23%                    |
| openai/whisper-small.en       | 7.25                     | 367.64 / 262.68 / 40%                    |
| openai/whisper-base.en        | 10.35                    | 445.31 / 293.26 / 52%                    |
| openai/whisper-tiny.en        | 15.45                    | 420.61 / 298.15 / 41%                    |

### SPGISpeech Results

| Model                        | SPGISpeech WER (%) | SPGISpeech RTFx (With/Without/Improvement) |
|------------------------------|--------------------|-------------------------------------------|
| distil/whisper-distil-large-v2 | 3.30               | 331.26 / 232.50 / 42%                   |
| distil/whisper-distil-large-v3 | 3.27               | 337.55 / 249.00 / 36%                   |
| distil/whisper-distil-medium.en | 3.83               | 478.64 / 318.96 / 50%                   |
| openai/whisper-large          | 3.20               | 198.02 / 167.48 / 18%                   |
| openai/whisper-large-v2       | 3.87               | 196.77 / 166.89 / 18%                   |
| openai/whisper-large-v3       | 2.94               | 197.37 / 166.92 / 18%                   |
| openai/whisper-large-v3-turbo       | 2.97          | 320.11 / 229.57 / 39%     |
| openai/whisper-medium.en      | 3.33               | 218.35 / 285.07 / 31%                   |
| openai/whisper-small.en       | 3.60               | 427.56 / 307.90 / 39%                   |
| openai/whisper-base.en        | 4.26               | 601.14 / 372.83 / 61%                   |
| openai/whisper-tiny.en        | 5.93               | 648.97 / 398.03 / 63%                   |

### TEDLIUM Results

| Model                        | TEDLIUM WER (%) | TEDLIUM RTFx (With/Without/Improvement) |
|------------------------------|----------------|-----------------------------------------|
| distil/whisper-distil-large-v2 | 4.87           | 274.60 / 197.85 / 39%                 |
| distil/whisper-distil-large-v3 | 3.86           | 294.14 / 217.54 / 35%                 |
| distil/whisper-distil-medium.en | 4.84           | 425.02 / 282.89 / 50%                 |
| openai/whisper-large          | 3.91           | 166.87 / 143.34 / 16%                 |
| openai/whisper-large-v2       | 3.90           | 166.91 / 143.77 / 16%                 |
| openai/whisper-large-v3       | 3.86           | 166.75 / 142.18 / 17%                 |
| openai/whisper-large-v3-turbo       | 3.57          | 288.34 / 199.61 / 44%     |
| openai/whisper-medium.en      | 4.11           | 237.28 / 185.40 / 28%                 |
| openai/whisper-small.en       | 4.07           | 352.07 / 263.51 / 34%                 |
| openai/whisper-base.en        | 4.87           | 507.93 / 336.00 / 51%                 |
| openai/whisper-tiny.en        | 5.97           | 571.50 / 352.79 / 62%                 |

### Voxpopuli Results

| Model                        | VoxPopuli WER (%) | VoxPopuli RTFx (With/Without/Improvement) |
|------------------------------|-------------------|------------------------------------------|
| distil/whisper-distil-large-v2 | 8.24             | 348.26 / 249.25 / 40%                  |
| distil/whisper-distil-large-v3 | 8.25             | 359.48 / 262.70 / 37%                  |
| distil/whisper-distil-medium.en | 9.00             | 525.00 / 345.95 / 52%                  |
| openai/whisper-large          | 7.76             | 218.21 / 182.69 / 19%                  |
| openai/whisper-large-v2       | 7.48             | 219.32 / 182.27 / 20%                  |
| openai/whisper-large-v3       | 9.54             | 213.33 / 180.51 / 18%                  |
| openai/whisper-large-v3-turbo       | 11.87          | 339.76 / 247.99 / 37%     |
| openai/whisper-medium.en      | 8.06             | 309.17 / 239.06 / 29%                  |
| openai/whisper-small.en       | 8.50             | 478.84 / 336.49 / 42%                  |
| openai/whisper-base.en        | 9.76             | 681.44 / 418.28 / 63%                  |
| openai/whisper-tiny.en        | 12.00            | 647.46 / 405.49 / 60%                  |

</details>

Benchmark scripts available there: https://github.com/huggingface/open_asr_leaderboard/tree/main/transformers

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?

@ArthurZucker
@Vaibhavs10 
This changes was suggested by @pzelasko
",[],2,open
Add retry hf hub decorator,"# What does this PR do?

This PR adds a `@hub_retry` decorator, which should be used on tests that give us any form of `requests`-like error when a download from the hub fails for *any reason*. It'll then wait a beat then try to rerun it again. I currently have it just located in the 
`ModelTesterMixin` class, but as time goes by and see where other places have it pop up, we should use it in those mixins/classes as well.



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ydshieh ",[],6,open
Cleanup: continue the init refactor,,[],1,open
Scale loss before backward,"# What does this PR do?


Fixes https://github.com/huggingface/trl/issues/2456

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?
",[],3,open
🔴 Video processors as a separate class,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/33504 and makes a separate Base class for video processors, only those that are from VLMs because video-only models like ViViT don't need an image and a video processor. We can deprecate them later in the run. Currently the main issue is to allow video processor operate separately from image processors and have their own set of initial attributes, as mentioned in the issue

Also, the initial design will keep video processors as child of image processors, so we will continue looping over each video frame. I thought of making a loop over each video since out transforms are performed on `np.array` lvl mostly, but the resizing would be hard in that case. Because we resize by converting an image to `PIL` first and apply the requested resampling. So I decided to leave it as a nested loop (same was as it is now)

In the future the plan is to integrate the fast processors and let the torchvision handle a batch of videos at once by feeding as input a tensor of shape `(bs, frames, c, h, w)`. Already tried that out with in very dirty local env, and that worked fine so adding the first fast video processor will be the next step along with @yonigozlan 's work on making fast image processors for VLMs

This is quite a breaking PR, because from now on some processor will not have an `image_processor`, but rather a `video_processor`

TODO: 
- [ ] have better BC for removed classes and some tests for that?

  ",[],0,open
run_mlm_flax on tpu v5-pods,"### System Info

Latest update of both transformers and jax

### Who can help?

 @ArthurZucker I am trying to use the `run_mlm_flax.py` to train a Roberta model on a v5-256 pod. However, while a single v3-8 is capable of running with `per_device_batch_size=128`, the v5-256 are only able to run with` per_device_batch_size=2`. Any ideas?

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Using default code.

### Expected behavior

I would expect a v5-256 to run a lot faster here.","[{'id': 2934977194, 'node_id': 'MDU6TGFiZWwyOTM0OTc3MTk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flax', 'name': 'Flax', 'color': '4862AD', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
logged loss is not correct with gradient accumulation,"### System Info

transformer v4.46.3

### Who can help?

@muellerzr 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

currently logged loss doesn't divide gradient accumulation steps, so it will be bigger than expected:
https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L2521-L2536
```
                    with context():
                        tr_loss_step = self.training_step(model, inputs, num_items_in_batch)

                    if (
                        args.logging_nan_inf_filter
                        and not is_torch_xla_available()
                        and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
                    ):
                        # if loss is nan or inf simply add the average of previous logged losses
                        tr_loss = tr_loss + tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)
                    else:
                        if tr_loss.device != tr_loss_step.device:
                            raise ValueError(
                                f""Calculated loss must be on the original device: {tr_loss.device} but device in use is {tr_loss_step.device}""
                            )
                        tr_loss = tr_loss + tr_loss_step
```

### Expected behavior

## how to fix
```
diff --git a/trainer.py b/trainer.py
index 1b9b80f..043c6c9 100755
--- a/trainer.py
+++ b/trainer.py
@@ -2546,7 +2546,7 @@ class Trainer:
                         self.state.global_step += 1
                         self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch
                         self.control = self.callback_handler.on_step_end(args, self.state, self.control)
-                        self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
+                        self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, num_batches)
                     else:
                         self.control = self.callback_handler.on_substep_end(args, self.state, self.control)
 
@@ -2571,7 +2571,7 @@ class Trainer:
                 self.control.should_training_stop = True
 
             self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)
-            self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
+            self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, self.args.gradient_accumulation_steps)
 
             if DebugOption.TPU_METRICS_DEBUG in self.args.debug:
                 if is_torch_xla_available():
@@ -2976,7 +2976,7 @@ class Trainer:
                 ) from exc
         return metrics
 
-    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval):
+    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, ga_steps):
         if self.control.should_log and self.state.global_step > self._globalstep_last_logged:
             if is_torch_xla_available():
                 xm.mark_step()
@@ -2990,7 +2990,7 @@ class Trainer:
             # reset tr_loss to zero
             tr_loss -= tr_loss
 
-            logs[""loss""] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)
+            logs[""loss""] = round(tr_loss_scalar/ ga_steps / (self.state.global_step - self._globalstep_last_logged), 4)
             if grad_norm is not None:
                 logs[""grad_norm""] = grad_norm.detach().item() if isinstance(grad_norm, torch.Tensor) else grad_norm
```","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
gradient calculation is not correct with gradient accumulation in  LM training ,"### System Info

transformer 4.46.3

### Who can help?

@muellerzr 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

run any llm example with gradient accumulation opened

### Expected behavior
TLDR: should use num items in micro-batch instead of num items in batch when calculate cross entropy for micro-batch, currently implementation makes grad-norm lower than usual

with same settings, I compared the grad-norm between megatron and deepspeed:

![image](https://github.com/user-attachments/assets/b722f9d2-8301-4b9a-8cde-2b1fc2844c03)

get_batch_sample should return list instead of scalar, then we should use coordinate num_items_in_batch when call `loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)`

### How to fix
```
diff --git a/trainer.py b/trainer.py
index a19737c..ddecf05 100755
--- a/trainer.py
+++ b/trainer.py
@@ -2416,7 +2416,7 @@ class Trainer:
             epoch_iterator = iter(epoch_dataloader)
             # We chunkify the epoch iterator into gradient accumulation steps `n` batches
             remainder = num_examples % args.gradient_accumulation_steps
-            num_items_in_batch = None
+            num_items_in_batches = None
             if remainder == 0:
                 remainder = args.gradient_accumulation_steps
             update_step = -1
@@ -2424,7 +2424,8 @@ class Trainer:
             for _ in range(total_updates):
                 update_step += 1
                 num_batches = args.gradient_accumulation_steps if update_step != (total_updates - 1) else remainder
-                batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)
+                batch_samples, num_items_in_batches = self.get_batch_samples(epoch_iterator, num_batches)
                 for i, inputs in enumerate(batch_samples):
                     step += 1
                     total_batched_samples += 1
@@ -5039,7 +5040,7 @@ class Trainer:
 
     def get_batch_samples(self, epoch_iterator, num_batches):
         batch_samples = []
-        num_items_in_batch = None
+        num_items_in_batches = None
         for _ in range(num_batches):
             try:
                 batch_samples += [next(epoch_iterator)]
@@ -5053,10 +5054,12 @@ class Trainer:
         if len(batch_samples) > 0 and ""labels"" in batch_samples[0]:
             # For now we don't support object detection
             try:
-                num_items_in_batch = sum([(batch[""labels""].ne(-100)).sum() for batch in batch_samples])
+                num_items_in_batches = [(batch[""labels""].ne(-100)).sum() for batch in batch_samples]
             except (TypeError, AttributeError):
                 pass
 
         if self.args.average_tokens_across_devices:
-            num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum().item()
-        return batch_samples, num_items_in_batch
+            num_items_in_batches = self.accelerator.gather(num_items_in_batches).sum().item()
+        return batch_samples, num_items_in_batches
``` 
","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
Improve tensor parallel memory usage,"### Feature request

Thanks to #34184 we can use TP for llama with only one line change. However the current implementation loads the whole model to each GPU in each rank before applying TP, significantly increasing the memory footprint.

### Motivation

We can load the model in CPU before applying TP. I tested this with llama3.1 8B on 2 GPUs. The memory usage is reduced from 60G to less than 20G. Below is my test script
```
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.distributed import device_mesh
from stainedglass_core.integrations.lm_eval.models.tensor_parallel.llama import parallelize_model

model_id = ""meta-llama/Meta-Llama-3.1-8B-Instruct""

rank = int(os.environ[""RANK""])
device = torch.device(f""cuda:{rank}"")
torch.cuda.set_device(device)
torch.distributed.init_process_group(""nccl"")

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map='cpu',
)
num_gpus = torch.cuda.device_count()
tp_mesh = device_mesh.init_device_mesh(""cuda"", (num_gpus,), mesh_dim_names=(""tp"",))
model.tensor_parallel(tp_mesh)
model.to(device)  # needed for weights and buffers that are not included by the TP plan

tokenizer = AutoTokenizer.from_pretrained(model_id)
prompt = ""Can I help""
inputs = tokenizer(prompt, return_tensors=""pt"").input_ids.to(device)

outputs = model(inputs)
print(tokenizer.decode(outputs.logits.squeeze()[-1].argmax()))
```

### Your contribution

We can set `device_map` to `cpu` in `PreTrainedModel.from_pretrained` if `tp_plan` is not `None`, and apply TP  at the end.

happy to have discussions and work on a pr for this.

CC @kwen2501 @ArthurZucker ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
PaliGemma2 Processor returns wrong labels array when <image> token is present in `text`,"### System Info

- `transformers` version: 4.47.0
- Platform: Linux-5.10.0-33-cloud-amd64-x86_64-with-glibc2.31
- Python version: 3.9.1
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.3
- Accelerate version: 0.30.1
- Accelerate config:    - compute_environment: LOCAL_MACHINE
        - distributed_type: NO
        - mixed_precision: fp16
        - use_cpu: False
        - debug: False
        - num_processes: 1
        - machine_rank: 0
        - num_machines: 1
        - gpu_ids: 0
        - rdzv_backend: static
        - same_network: True
        - main_training_function: main
        - enable_cpu_affinity: False
        - downcast_bf16: no
        - tpu_use_cluster: False
        - tpu_use_sudo: False
        - tpu_env: []
- PyTorch version (GPU?): 2.2.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: no
- Using GPU in script?: yes
- GPU type: Tesla T4

### Who can help?

@ArthurZucker @molbap we chatted about the last paligemma release :)

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Here is a script that shows the problem:
```python
from transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor
from PIL import Image
import numpy as np

hf_token = ""...""
processor = PaliGemmaProcessor.from_pretrained(
    ""google/paligemma2-3b-pt-224"", token=hf_token
)

text = [""How many shapes are green?""]
suffix = [""4""]

image = [Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))]
print(
    processor(
        images=image, text=text, suffix=suffix, return_tensors=""pt"", padding=""longest""
    ).labels
)
text = [""<image>How many shapes are green?""]
print(
    processor(
        images=image, text=text, suffix=suffix, return_tensors=""pt"", padding=""longest""
    ).labels
)
```


### Expected behavior


As you can see, the bottom one is missing the EOS token, which leads to bad finetunes! But the processor class warns me when the `<image>` token isn't present.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
[i18n-ar] Translated file: `docs/source/ar/tasks/multiple_choice.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/tasks/multiple_choice.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

### Who can review?
@stevhliu , @MKhalusova , @abodacs, may you please review this PR?

",[],0,open
[i18n-ar] Translated file: `docs/source/ar/tasks/masked_language_modeling.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/tasks/masked_language_modeling.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

### Who can review?
@stevhliu , @MKhalusova , @abodacs, may you please review this PR?

",[],0,open
[i18n-ar] Translated file: `docs/source/ar/tasks/language_modeling.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/tasks/language_modeling.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

### Who can review?
@stevhliu , @MKhalusova , @abodacs, may you please review this PR?

",[],0,open
[i18n-ar] Translated file: `docs/source/ar/tasks/question_answering.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/tasks/question_answering.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

### Who can review?
@stevhliu , @MKhalusova , @abodacs, may you please review this PR?

",[],0,open
[i18n-ar] Translated file: `docs/source/ar/tasks/summarization.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/tasks/summarization.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

### Who can review?
@stevhliu , @MKhalusova , @abodacs, may you please review this PR?

",[],0,open
[i18n-ar] Translated file: `docs/source/ar/tasks/translation.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/tasks/translation.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

### Who can review?
@stevhliu , @MKhalusova , @abodacs, may you please review this PR?

",[],0,open
[i18n-ar] Translated file : docs/source/ar/tasks/token_classification.md into Arabic,"## What does this PR do?
Translated the `docs/source/ar/tasks/token_classification.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",[],0,open
[i18n-ar] Translated file : docs/source/ar/tasks/sequence_classification.md into Arabic,"## What does this PR do?
Translated the `docs/source/ar/tasks/sequence_classification.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",[],4,open
Add Flax activation functions and corresponding tests,"The PR implements Flax version of all activation functions introduced previously in torch to be selected in configs and models.
Also a test is written to compare torch version and Flax version outputs and asses if they are close enough.","[{'id': 2934977194, 'node_id': 'MDU6TGFiZWwyOTM0OTc3MTk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flax', 'name': 'Flax', 'color': '4862AD', 'default': False, 'description': ''}]",2,open
Fix f-string to show `ACCELERATE_MIN_VERSION` on error,"# Fix f-string to show `ACCELERATE_MIN_VERSION` on error

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

If `accelerate` is not installed a helpful warning message shows the `ACCELERATE_MIN_VERSION` required - but the f-string was missing so the package version wasn't displayed. This minor PR fixes that.


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

Minor typo, no new tests.

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
 @muellerzr and @SunMarc for simple review",[],1,open
make LlamaModel._update_causal_mask torch compilable,"# What does this PR do?

This doesn't really have a performance improvement, but at least keeps torch.compile from having a graph break here.

<img width=""1031"" alt=""Screenshot 2024-12-10 at 2 44 06 PM"" src=""https://github.com/user-attachments/assets/ca5198dd-0f80-4ff6-9704-a6cb587f8ca5"">



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker 
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],3,open
How to convert my Mask2Former model (ResNet-50 backbone) to Hugging Face transformer,"### System Info

```shell
- `transformers` version: 4.34.0
- Platform: Linux-6.8.0-31-generic-x86_64-with-glibc2.17
- Python version: 3.8.20
- Huggingface_hub version: 0.17.3
- Safetensors version: 0.4.5
- Accelerate version: 0.23.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No
```


### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I found the following script, but it only supports conversion for Mask2Former model (swin backbone) https://github.com/huggingface/transformers/blob/main/src/transformers/models/mask2former/convert_mask2former_original_pytorch_checkpoint_to_pytorch.py

May I ask for some guidance on how to adjust the script so that it can support ResNet-50 architecture?

### Expected behavior

```shell
Convert my Mask2Former model (ResNet-50 backbone) to Hugging Face transformer
```


### Checklist

- [X] I have read the migration guide in the readme. ([pytorch-transformers](https://github.com/huggingface/transformers#migrating-from-pytorch-transformers-to-transformers); [pytorch-pretrained-bert](https://github.com/huggingface/transformers#migrating-from-pytorch-pretrained-bert-to-transformers))
- [X] I checked if a related official extension example runs on my machine.",[],0,open
QuantizedCache first token processing is counterintuitive / worse than in papers,"### System Info

transformers==4.46.3
torch==2.5.1
(though, it does not depend on library versions)

### Who can help?

@sunmarc @zucchini-nlp



### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

**TL;DR** current quantized KV cache implementation works poorly **with the first token**, causes problems due to attention sinks. I describe this in detail in 'expected behavior'.

To reproduce the problem, simply run LLM inference (e.g. Llama 3.2 3B) with QuantoQuantizedCache (or other QuantizedCache descendants), e.g.

```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"")
model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"", torch_dtype=torch.float16, device_map=""cuda:0"")
inputs = tokenizer(""I like rock music because"", return_tensors=""pt"").to(model.device)

model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=""quantized"", cache_config={""backend"": ""quanto"", ""nbits"": 4})
```
Note that in this example, the quantized cache should only quantize tokens that are (buffer size) positions away. However, **it mistakenly quantized the first token right away**. This is important because the first token often acts like an [Attention Sink](https://arxiv.org/abs/2309.17453) and quantizing it substantially reduces model accuracy.

I describe the problem and demonstrate the quality drawdown in detail in the ""Expected Behavior"" section (below).



### Expected behavior

### What exactly is wrong

The problem occurs every time inference is run with QuantizedCache or its descendants.

The current implementation of QuantizedCache always quantizes the very first token, adding it to the q_cache instead of storing it in the full precision cache. However, according to the docstring, quantization should only occur ""when the length goes beyond maximum capacity, and the original precision cache is discarded and moved into the quantized cache."" This implies that the first token should only be quantized when the cache length reaches its maximum capacity.

This issue may affect model quality, as recent papers on KV quantization emphasize the importance of the first token, also known as the ""attention sink"" ( https://arxiv.org/abs/2309.17453 ). In fact, some papers suggest not quantizing this token at all (e.g. https://arxiv.org/abs/2410.05265 ). Failing to store the first token in full precision could therefore be problematic.

### Proposed fix

The fix involves modifying the ""update"" method so it keeps the first token in the FP16 buffers:

https://www.diffchecker.com/Rj5qaOCj/

Here's the full code of [fixed QuantizedCache](https://gist.github.com/goodevening13/36f1d92ccf87009e03e65f3e1bc54a32) that I used for benchmarks below.


### Effect on model quality:

To verify that this fix work, I compared the two implemantations in terms of WikiText-2 perplexity (lower is better). In the table below 'Before fix' evaluates QuantoQuantizedCache and with the current code (see versions above) and 'After fix' uses the [patch](https://gist.github.com/goodevening13/36f1d92ccf87009e03e65f3e1bc54a32) described in the previous section. These were measured with Llama-3.2-3B backbone using 2-bit and 4-bit with all default hyperparameters (see details below) .

| WikiText-2 PPL | FP16 | 4-bit | 2-bit |
|----------------|------|-------|-------|
| Before fix     |    6.9865   |  7.0052     |    12.2152   |
| After fix      |    6.9865   |   7.0005    |     **11.4623**  |


The full code for obtaining these results can be found in [gist link](https://gist.github.com/goodevening13/2aa807fb79fbe135e3162a3c54601a8e). If any additional evaluations or explanations are required, I am happy to provide those.


------

Thanks to [surkovvv](https://github.com/surkovvv) for his help with the evaluation codebase
","[{'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",1,open
Add compile test for fast image processor,"# What does this PR do?

Add a test in `ImageProcessingTestMixin`, which is only ran on fast image processors. The test compiles a fast image processor, run a processing pass and compares the result with an eager image processor.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Who can review?



<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Add dtype check in check_quantized_param for bnb4,"# What does this PR do?
Adding dtype check as suggested by the comment in `check_quantized_param` for bnb4

## Who can review ?
@SunMarc ","[{'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}]",1,open
Adding Mamba2ForTokenClassification to Mamba2,"### Feature request

I’ve noticed that many newly added models do not include a `ForTokenClassification` implementation. Is this due to fundamental challenges in implementation (though I don’t perceive any major obstacles—perhaps I’ve overlooked something), or is it simply a matter of development priorities and time constraints?

### Motivation

I am currently testing a prototype based on the Mamba series models, which requires token classification outputs.

### Your contribution

If it’s merely a time issue preventing the implementation of `ForTokenClassification` in `transformers`, I’d be more than willing to contribute by adding this feature for Mamba/Mamba2. If time allows, I’d also be happy to extend the support to other models.

From my understanding, replicating the approach used in `LlamaForTokenClassification` should suffice to implement token classification model for most models. Any advice or guidance would be highly appreciated!","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
More rich documentation on pipelines,"### Feature request

For instance, in the case of pipelines, there are merely some community examples such as automatic-speech-recognition. There is no indication of who constitutes it or how the Voice Activity Detection (VAD) is involved. As a result, users are perplexed.

### Motivation

More detailed descriptions on pipline details.

### Your contribution

currently not","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",10,open
Incorrect file structure in convert_mask2former_original_pytorch_checkpoint_to_pytorch.py?,"### System Info

- `transformers` version: 4.34.0
- Platform: Linux-6.8.0-31-generic-x86_64-with-glibc2.17
- Python version: 3.8.20
- Huggingface_hub version: 0.17.3
- Safetensors version: 0.4.5
- Accelerate version: 0.23.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help?

@amyeroberts @qubvel 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I have problem when running convert_mask2former_original_pytorch_checkpoint_to_pytorch.py
1. I have the path: `~/my_project/checkpoints/coco/instance-segmentation/my_model.pkl`
2. I have the path: `~/my_project/configs/coco/instance-segmentation/my_configs.yaml`
3. I run the following in terminal: `python convert_mask2former_original_pytorch_checkpoint_to_pytorch.py     --checkpoints_dir ~/my_project/checkpoints     --configs_dir ~/my_project/configs     --mask2former_dir ~/work/Mask2Former`
4. The error happens: `coco must be wrong since acceptable values are: instance-segmentation, panoptic-segmentation, semantic-segmentation.`

### Expected behavior

In line 907, I print the input path: `~/my_project/checkpoints/coco/instance-segmentation/my_model.pkl`
In line 909, my `model_name_raw` is `instance-segmentation`, while the comment in line 908 says `# model_name_raw is something like maskformer2_swin_small_bs16_50ep.` The model_name_raw should be `my_model` from my understanding","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Detokenization discrepancy with Llama3.1,"### System Info

- `transformers` version: 4.47.0
- Platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35
- Python version: 3.12.7
- Huggingface_hub version: 0.26.5
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): not installed (NA)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: N/A

### Who can help?

@ArthurZucker @itazap


### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Spaces are being stripped from space-prefixed token `Ġ'` when followed by a common abbreviation (e.g., `n't`, `'m`, `'s`, `'ve`), even when not appropriate to do so. This is being caused because `clean_up_tokenization_spaces` is True by default for the Llama 3.1 tokenizer.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-3.1-8B-Instruct"")

original = "" plunged the long 'sword' into""
input_ids = tokenizer.encode(original, add_special_tokens=False)
tokens = tokenizer.convert_ids_to_tokens(input_ids)
decoded = tokenizer.decode(input_ids)
decoded2 = tokenizer.decode(input_ids, clean_up_tokenization_spaces=False)

print(""token ids:                "", input_ids)
print(""tokens:                   "", tokens)
print(""original:                "", original)
print(""decoded (default):       "", decoded)
print(""decoded (clean_up=False):"", decoded2)
```

Produces
```text
token ids:                 [75803, 279, 1317, 364, 80138, 6, 1139]
tokens:                    ['Ġplunged', 'Ġthe', 'Ġlong', ""Ġ'"", 'sword', ""'"", 'Ġinto']
original:                  plunged the long 'sword' into
decoded (default):         plunged the long'sword' into
decoded (clean_up=False):  plunged the long 'sword' into
```

### Expected behavior

I would expect the `original` string to match the `decoded` string in all cases unless it actually contains ""traditional"" tokenization spacing (e.g., `it 's` vs `it's`). Perhaps a good approach could be to modify the [clean_up_tokenization](https://github.com/huggingface/transformers/blob/5d7739f15a6e50de416977fe2cc9cb516d67edda/src/transformers/tokenization_utils_base.py#L3909) function to only apply this rule when the common abbreviation is followed immediately by another space.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
LlavaForConditionalGeneration._merge_input_ids_with_image_features throws error,"### System Info

- `transformers` version: 4.43.1
- Platform: Linux-6.8.5-1-default-x86_64-with-glibc2.39
- Python version: 3.11.9
- Huggingface_hub version: 0.23.5
- Safetensors version: 0.4.3
- Accelerate version: 0.29.3
- Accelerate config:    - compute_environment: LOCAL_MACHINE
        - distributed_type: MULTI_GPU
        - mixed_precision: bf16
        - use_cpu: False
        - debug: False
        - num_processes: 8
        - machine_rank: 0
        - num_machines: 1
        - gpu_ids: all
        - rdzv_backend: static
        - same_network: True
        - main_training_function: main
        - enable_cpu_affinity: False
        - downcast_bf16: no
        - tpu_use_cluster: False
        - tpu_use_sudo: False
        - tpu_env: []
        - dynamo_config: {'dynamo_backend': 'INDUCTOR'}
- PyTorch version (GPU?): 2.4.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: True
- Using GPU in script?: True
- GPU type: NVIDIA L40S

### Who can help?

_No response_

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

#### Description
I am trying to use the `AutoAWQ` library to quantize a Pixtral model (`mistral-community/Pixtral-Large-Instruct-2411`). However, I am encountering the following error:
```txt
File ""/quantization/quant/lib64/python3.11/site-packages/transformers/models/llava/modeling_llava.py"", line 303, in _merge_input_ids_with_image_features
    num_images, num_image_patches, embed_dim = image_features.shape
                                               ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'shape'
```

#### Code
Here is the code I am using:
```python
import os
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_path = r'/data/models/mistral/pixtral-large-instruct-2411' # from https://huggingface.co/mistral-community/Pixtral-Large-Instruct-2411
quant_path = r'/data/models/mistral/pixtral-large-instruct-2411-awq'
quant_config = { ""zero_point"": True, ""q_group_size"": 128, ""w_bit"": 4, ""version"": ""GEMM"" }
os.makedirs(quant_path, exist_ok=True)

# Load model
model = AutoAWQForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# Quantize
model.quantize(tokenizer, quant_config=quant_config)

# Save quantized model
model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)
print(f'Model is quantized and saved at ""{quant_path}""')
```

#### Analysis
The model I am using is `Pixtral-Large-Instruct-2411`, but its configuration is `LlavaForConditionalGeneration`. The issue arises in the `Transformers` library's source code where `image_features` remains `None` if `pixel_values` is `None`. Consequently, in the method `_merge_input_ids_with_image_features`, the first line `num_images, num_image_patches, embed_dim = image_features.shape` tries to access the `shape` attribute of `None`, resulting in an `AttributeError`.

```python
image_features = None
if pixel_values is not None:
    image_features = self.get_image_features(
        pixel_values=pixel_values,
        vision_feature_layer=vision_feature_layer,
        vision_feature_select_strategy=vision_feature_select_strategy,
    )

if legacy_processing:
    logger.warning_once(
        ""Expanding inputs for image tokens in LLaVa should be done in processing. ""
        ""Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly ""
        ""with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. ""
        ""Using processors without these attributes in the config is deprecated and will throw an error in v4.50.""
    )
    # prefill stage vs decoding stage (legacy behavior copied)
    if input_ids.shape[1] != 1:
        inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(
            image_features, inputs_embeds, input_ids, attention_mask, labels # <-- image_features is still None here
        )
        cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)
```

#### Steps to Reproduce
1. Ensure the `Pixtral-Large-Instruct-2411` model is available at the specified path.
2. Run the provided code snippet.

#### Actual Behavior
An `AttributeError` is raised due to `image_features` being `None`.


### Expected behavior

The model should be loaded, quantized, and saved without any errors.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",3,open
"DynamicCache does not support variable lengths, except for FA2","### System Info

- `transformers` version: 4.47.0
- Using GPU in script?: yes
- GPU type: NVIDIA A100-SXM4-80GB

### Who can help?

@ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

The following code will reproduce the issue: 

```python
import torch
from transformers import AutoModelForCausalLM
from transformers.cache_utils import DynamicCache


device = ""cuda:0""
ckpt = ""meta-llama/Meta-Llama-3.1-8B-Instruct""
attn_implementation = ""flash_attention_2""
cache = DynamicCache()
model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=""auto"", attn_implementation=attn_implementation).to(
    device
)


# Run first forward pass
inputs = model.dummy_inputs[""input_ids""].to(device)
with torch.no_grad():
    model(inputs, past_key_values=cache)
    print(""First forward pass"")
    print(f""Layer 0: {cache.key_cache[0].shape}"")
    print(f""Layer 1: {cache.key_cache[1].shape}"")

# Update the cache size for the first layer
print(""Removing 2 KV pairs for the first layer"")
cache.key_cache[0] = cache.key_cache[0][:, :, :-2, :]
cache.value_cache[0] = cache.value_cache[0][:, :, :-2, :]

with torch.no_grad():
    model(inputs, past_key_values=cache)
    print(""Second forward pass"")
    print(f""Layer 0: {cache.key_cache[0].shape}"")
    print(f""Layer 1: {cache.key_cache[1].shape}"")
```

### Expected behavior

In the [kvpress](https://github.com/NVIDIA/kvpress) repository, we implement various KV cache compression methods. For some of these methods, the KV cache compression is not the same from one layer to the other (see for instance https://github.com/NVIDIA/kvpress/pull/28). We noticed it's not an issue when using `attn_implementation=""flash_attention_2""` but raises an error for `attn_implementation=""sdpa""`  or `attn_implementation=""eager""`.

Here is the error from the script above:

```bash
--> [336](.../transformers/models/llama/modeling_llama.py:336)     attn_weights = attn_weights + causal_mask
    [338](.../transformers/models/llama/modeling_llama.py:338) # upcast attention to fp32
    [339](.../transformers/models/llama/modeling_llama.py:339) attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)

RuntimeError: The size of tensor a (10) must match the size of tensor b (9) at non-singleton dimension 3
```

The issue is that the `causal_mask` is not updated per-layer for `sdpa` and `eager` while it is for flash attention.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",9,open
Mimi model gives different outputs when using batch encode vs single encode,"### System Info

- `transformers` version: 4.46.3
- Platform: Linux-6.6.20-aufs-1-x86_64-with-glibc2.36
- Python version: 3.11.2
- Huggingface_hub version: 0.26.1
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: Yes
- GPU type: NVIDIA A10

### Who can help?

@ylacombe @eustlb 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```
import torchaudio
from transformers import MimiModel, AutoFeatureExtractor
model = MimiModel.from_pretrained(""kyutai/mimi"", num_quantizers=8)
feature_extractor = AutoFeatureExtractor.from_pretrained(""kyutai/mimi"")
model.cuda()
# load some audio file
inputs = feature_extractor(raw_audio=[audio.squeeze(0).numpy(),audio.squeeze(0).numpy()] , sampling_rate=feature_extractor.sampling_rate, return_tensors=""pt"")
inputs = {key:value.cuda() for key,value in inputs.items()}
out_batch = model.encode(**inputs).audio_codes
inputs = feature_extractor(raw_audio=audio.squeeze(0).numpy(), sampling_rate=feature_extractor.sampling_rate, return_tensors=""pt"")
inputs = {key:value.cuda() for key,value in inputs.items()}
out = model.encode(**inputs).audio_codes
(out_batch[0] == out[0]).all() # prints tensor(False, device='cuda:0')
```

### Expected behavior

the output for out_batch[0] and out[0] should be the same.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",11,open
NPU support SDPA,"# What does this PR do?
add features: Ascend NPU supports SDPA.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## notes
 Ascend NPU requires torch>=2.1.0 to use SDPA in Transformers.",[],0,open
Init cache on meta device,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/33147.

Initializes cache on meta device until we get the first key/values and infer the device from that. Removed `layer_device_map` as not needed anymore in most cases, because initializing cache on meta device is the default value when no `device` is given. Offloaded Static cache still would require layer device map or one device, since it prefetches key/values in advance and we cannot infer the device as soon as we see the input. 

One thing to note is that Offloaded Static cache can never run on current `main` because `torch.cuda.stream()` is not fullgraph compile compatible. Found a related issue from torch team on that: https://github.com/pytorch/pytorch/issues/92804. On current `main` I can't run even with graph breaks, and it fails after 3rd layer on `torch.cuda.default_stream(self.device).wait_stream(self._prefetch_stream)`, but with this branch it runs if we allow graph breaks until compile cache limit is reached 

Since this is the only cache that is behaving different, maybe we can make it `not instance of StaticCache` at least until the fullgraph compile is working",[],1,open
Calling Trainer.create_model_card() with an empty dataset list causes an IndexError,"### System Info

- `transformers` version: 4.46.3
- Platform: Linux-5.15.0-125-generic-x86_64-with-glibc2.35
- Python version: 3.11.10
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.5
- Accelerate version: 1.1.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA H100 80GB HBM3

### Who can help?

@muellerzr @SunMarc 

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Call `trainer.create_model_card(..., datasets=[])`.
The checks at https://github.com/huggingface/transformers/blob/de8a0b7547451f8f7ef2c0ac1f338ba77c614cec/src/transformers/modelcard.py#L492-L502 do not cover this case, we end up in the `else` branch and access `datasets[-1]`.

### Expected behavior

No exception should be raised, no dataset info should be included in the model card.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
Use modules_to_not_convert in AQLM,"# What does this PR do?

Using `modules_to_not_convert` instead of `linear_weights_not_to_quantize` in `aqlm` to standardize it in all quantizers

## Who can review ?

@SunMarc ",[],1,open
[tests] run one test but got 2 test results,"### System Info

- `transformers` version: 4.47.0.dev0
- Platform: Linux-4.18.0-425.3.1.el8.x86_64-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.26.5
- Safetensors version: 0.4.5
- Accelerate version: 1.1.0.dev0
- Accelerate config:    - compute_environment: LOCAL_MACHINE
        - distributed_type: MULTI_GPU
        - mixed_precision: bf16
        - use_cpu: False
        - debug: False
        - num_processes: 2
        - machine_rank: 0
        - num_machines: 1
        - gpu_ids: all
        - rdzv_backend: static
        - same_network: True
        - main_training_function: main
        - enable_cpu_affinity: False
        - downcast_bf16: no
        - tpu_use_cluster: False
        - tpu_use_sudo: False
        - tpu_env: []
- PyTorch version (GPU?): 2.5.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A100 80GB PCIe

### Who can help?

@ydshieh
 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

with the following test command:
```bash
pytest -rA tests/models/mra/test_modeling_mra.py::MraModelTest::test_load_with_mismatched_shapes
``` 
I will get 2 test results:
```bash
======================================================== short test summary info =========================================================
PASSED tests/models/mra/test_modeling_mra.py::MraModelTest::test_load_with_mismatched_shapes
[Testing <class 'transformers.models.mra.modeling_mra.MraForSequenceClassification'>] SUBFAIL tests/models/mra/test_modeling_mra.py::MraModelTest::test_load_with_mismatched_shapes - ValueError: sequence length must be divisible by the block_size.
================================================ 1 failed, 1 passed, 3 warnings in 6.27s =================================================
```

### Expected behavior

I expect that this command gives_ me only one test result rather than two. 

Below are the experiments I tried:

- with pytest-subtests installed, comment out ""with self.subTest(msg=f""Testing {model_class}""):"", test pass with 1 test case
- with pytest-subtests uninstalled, comment out ""with self.subTest(msg=f""Testing {model_class}""):"", test pass with 1 test case
- with pytest-subtests uninstalled, test fails with 1 test case 

I also saw your other PR #34806. Maybe this is another issue with `pytest-subtests`?","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Add reminder config to issue template and print DS version in env,"# What does this PR do?

Resolves: #34145
- Add a reminder to include config info in the code repro
- Add DeepSpeed version info to `transformers-cli env`

I ended up adding this when I was debugging the ds version related stuff in #31867, collecting ds version info here is handy and would be consistent with [TRL](https://github.com/huggingface/trl/blob/main/trl/commands/cli.py) which collects ds version info in their env scripts already.  
If you don't want to collect ds version in the env script, let me know and I can remove that commit.  🤗
  
What it looks like:
```
Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points. 

- `transformers` version: 4.48.0.dev0
- Platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.35
- Python version: 3.10.14
- Huggingface_hub version: 0.25.0
- Safetensors version: 0.4.5
- Accelerate version: 0.34.2
- Accelerate config:    not found
- DeepSpeed version: 0.16.1
- PyTorch version (GPU?): 2.4.1+cu121 (True)
- Tensorflow version (GPU?): 2.15.1 (False)
- Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)
- Jax version: 0.4.13
- JaxLib version: 0.4.13
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A100-SXM4-80GB

(test_transformers) b3schnei@growl:~/transformers_debug/transformers$ pip uninstall deepspeed
Found existing installation: deepspeed 0.16.1
Uninstalling deepspeed-0.16.1:
Proceed (Y/n)? y
  Successfully uninstalled deepspeed-0.16.1
(test_transformers) b3schnei@growl:~/transformers_debug/transformers$ transformers-cli env
Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.

- `transformers` version: 4.48.0.dev0
- Platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.35
- Python version: 3.10.14
- Huggingface_hub version: 0.25.0
- Safetensors version: 0.4.5
- Accelerate version: 0.34.2
- Accelerate config:    not found
- DeepSpeed version: not installed
- PyTorch version (GPU?): 2.4.1+cu121 (True)
- Tensorflow version (GPU?): 2.15.1 (False)
- Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)
- Jax version: 0.4.13
- JaxLib version: 0.4.13
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A100-SXM4-80GB
```

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker ",[],0,open
Adding FlexAttention Support for Qwen2 models,"# What does this PR do?

Adding flex_attention for Qwen2 models following https://github.com/huggingface/transformers/issues/34809

## Who can review ?
@ArthurZucker ",[],2,open
Impossible to change attention implementation,"### System Info

```
transformers==4.46.3
torch==2.5.1
flash-attn==2.7.1
```

### Who can help?

@amyeroberts @LysandreJik

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

The `attn_implementation` argument is not respected in `from_pretrained`.

This means it is impossible to change the attention implementation for certain models, e.g. `google/gemma-2-27b`.


```python
from transformers import AutoModelForCausalLM
from transformers.utils import is_flash_attn_2_available

print(is_flash_attn_2_available())
# True

model = AutoModelForCausalLM.from_pretrained(
    ""google/gemma-2-27b"",
    torch_dtype=""bfloat16"",
    device_map=""auto"",
    attn_implementation=""flash_attention_2"",
)
print(model.config._attn_implementation)
# 'eager'
```

I think this is related the following changes:
- @amyeroberts: https://github.com/huggingface/transformers/pull/32383
- @LysandreJik: https://huggingface.co/google/gemma-2-27b/discussions/12

### Expected behavior

It should be possible to specify the `attn_implementation`. Especially as Flash Attention 2 is officially supported for Gemma 2.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",10,open
how to load the weight of decoder.embed_tokens.weight seperately from the shared weight?,"### System Info

- `transformers` version: 4.46.3
- Platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.17
- Python version: 3.8.20
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.4.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA RTX A4000


### Who can help?

@ArthurZucker @muellerzr  @SunMarc


### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

when i use t5 1.1 on seq2seq task, which has 59744 source vocab size and only 32 target vocab size. And To correctly use softmax to calculate each token's probality and score on 32 candidates so I  set model.lm_head as below: 
```python
torch.nn.Linear(config.d_model,target_vocab_size=32,bias=False). 

And everything looks good when the model is training. But after training, I load the safetensor as below:
checkpoint_path = ""./resultstest/checkpoint-100""
config = T5Config.from_pretrained(""./onlychangelmhead/checkpoint-100/config.json"")
model = T5ForConditionalGeneration(config)
model.lm_head = torch.nn.Linear(config.d_model,target_vocab_size,bias=False)
state_dict = load_file(f""{checkpoint_path}/model.safetensors"")
model.load_state_dict(state_dict, strict=True) 
```

And the issue comes as:
```
Traceback (most recent call last):
  File ""bs_based_on_massdic_failed.py"", line 110, in <module>
    model.load_state_dict(state_dict, strict=True)
  File ""/home/zhi/anaconda3/envs/peptide_completion/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for T5ForConditionalGeneration:
	Missing key(s) in state_dict: ""encoder.embed_tokens.weight"". 
	size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([59744, 768]).
```

when I try to print the safetensors' shape it shows that the `lm_head. weight` looks fine as size of `[32, 768]`, but with no `decoder.embeded_tokens` or the way I load the safetensor can not load the embeded_tokens's weight from shared weight properly(I guess). So how can I fix that problem to correctly feat this model on my exact target vocab size as 32 but not same as the source vocab's size. It would be very appreciate if you can reply. Best.

    

### Expected behavior

Use t5 1.1 to feat on 32 target vocab size task. And load the safetensor properly.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",3,open
Qwen2vl float16 inference bug in naive attention,"### System Info

- `transformers` version: 4.46.2
- Platform: Linux-5.15.0-125-generic-x86_64-with-glibc2.35
- Python version: 3.10.15
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A10

### Who can help?

@ArthurZucker @GeLee-Q 

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Hi @ArthurZucker @GeLee-Q , thanks for #33312, which has resolved the inference bug for qwen2vl in float16. However, it still has some flaws. Although the current code solves the numerical overflow issue, it may **break causality of the autoregressive model**. See details [here](https://github.com/huggingface/transformers/issues/35151#issuecomment-2537723488).

### Expected behavior

Current: [Code](https://github.com/huggingface/transformers/blob/c8c8dffbe45ebef0a8dba4a51024e5e5e498596b/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L582)
```
        if attention_mask is not None:  # no matter the length, we just slice it
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights = attn_weights + causal_mask

        # Fix precision issues in Qwen2-VL float16 inference
        # Replace inf values with zeros in attention weights to prevent NaN propagation
        if query_states.dtype == torch.float16:
            attn_weights = torch.where(torch.isinf(attn_weights), torch.zeros_like(attn_weights), attn_weights)
```


Fixed: Replace ±inf with zero before adding attn_weights with causal_mask. (prevent converting -inf, which for causality, to zero.)
```
        # Fix precision issues in Qwen2-VL float16 inference
        # Replace inf values with zeros in attention weights to prevent NaN propagation
        if query_states.dtype == torch.float16:
            attn_weights = torch.where(torch.isinf(attn_weights), torch.zeros_like(attn_weights), attn_weights)

        if attention_mask is not None:  # no matter the length, we just slice it
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights = attn_weights + causal_mask
```","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Cuda OOM,"### System Info

I'm trying to pre-train a small LLM with Llama architecture with 1.4B parameters. I'm using a node which is 8 * H100. Every thing is working fine. I noticed a small problem. when I load a big dataset in the code it gives an OOM error regardless the batch size after some steps. At the beginning I thought it would be some samples are too long so I did some experiments when all the samples have the max length (2048) and the code worked fine. so the problem probably not from the size of the  batch and the inputs.

when I'm adding select(range(10000)) to the end of wikitext dataset object to get sub dataset it works fine, but when I did add select(range(500000)) it gives OOM error after some steps.

Where exactly the OOM comes from?

```
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from transformers import LlamaForCausalLM, LlamaConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoTokenizer
from datasets import config
from datasets import load_dataset, load_from_disk, IterableDataset
from utils import count_parameters
from tokenizers import processors
from sim_data import create_test_dataset, filter_long_sequences

# Environment settings
os.environ[""TOKENIZERS_PARALLELISM""] = ""false""
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = False


# Initialize distributed training
dist.init_process_group(backend=""nccl"")
local_rank = int(os.environ[""LOCAL_RANK""])
torch.cuda.set_device(local_rank)

# Tokenizer setup
tokenizer = AutoTokenizer.from_pretrained('/mnt/fs/huggingface/hub/tokenizers/Aranizer-PBE-86k')

tokenizer._tokenizer.post_processor = processors.TemplateProcessing(
    single=[""<s>"", ""$A"", ""</s>""],
    special_tokens=[(""<s>"", tokenizer.bos_token_id), (""</s>"", tokenizer.eos_token_id)]
)

# Special tokens setup
tokenizer.bos_token = '<s>'
tokenizer.bos_token_id = 0
tokenizer.eos_token = '</s>'
tokenizer.eos_token_id = 2
tokenizer.pad_token = '<pad>'
tokenizer.pad_token_id = 1



# Model configuration
model_config = LlamaConfig(
    vocab_size=len(tokenizer),
    hidden_size=2048,
    intermediate_size=8192,
    num_hidden_layers=16,
    num_attention_heads=16,
    max_position_embeddings=2048,
    dropout_rate=0.1,
    layer_norm_eps=1e-6,
    pad_token_id=tokenizer.pad_token_id,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    use_cache=False,
)

# Initialize model and move to appropriate GPU
model = LlamaForCausalLM(model_config)

# Enable gradient checkpointing BEFORE wrapping with DDP

print(""great"")
# Move model to GPU
model = model.to(local_rank)
# 2. Enable gradient checkpointing before DDP wrapping
model.gradient_checkpointing_enable()

# Wrap model with DDP
model = DDP(
    model, 
    device_ids=[local_rank],
    output_device=local_rank,
    find_unused_parameters=False,
)


dataset = load_dataset(""wikitext"", ""wikitext-103-v1"", split='train')


eval_dataset = load_dataset(""wikitext"", ""wikitext-103-v1"", split='validation')



def tokenize_function(examples):

    return tokenizer(
        examples[""text""],
        truncation=True,
        max_length=2046,  # 2046 - 2 to leave room for BOS and EOS
        padding=True,
        return_tensors=None,
        add_special_tokens=True,
    )


# Tokenize datasets
tokenized_datasets = dataset.map(
    tokenize_function, 
    batched=True, 
    remove_columns=[""text""],
    # num_proc=32,
    # load_from_cache_file=False
)

tokenized_eval_dataset = eval_dataset.map(
    tokenize_function, 
    batched=True, 
    remove_columns=[""text""],
    # num_proc=32,
    # load_from_cache_file=False
)

steps_var = 100

#torchrun --nproc_per_node=8 train_distributed.py

# interleave_datasets

# Training arguments
training_args = TrainingArguments(

    accelerator_config={""dispatch_batches"": False},

    # deepspeed=deepspeed_config,
    output_dir=""./Mulhem-1.4B"",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=14,
    gradient_accumulation_steps=2,
    save_steps=steps_var,
    save_total_limit=2,
    
    # Resume training settings
    resume_from_checkpoint=True, # Enable resuming from checkpoint
    # dataloader_pin_memory=True,

    # Distributed training settings
    local_rank=local_rank,

    # Optimization settings
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.01,
    adam_beta1=0.9,
    adam_beta2=0.999,
    max_grad_norm=1.0,
    
    # Evaluation and logging
    eval_strategy=""steps"",
    eval_steps=steps_var,
    eval_delay=0.1, # 
    logging_dir=""./runs"",
    logging_strategy=""steps"",
    logging_steps=steps_var,
    report_to=[""tensorboard""],
    
    # should be checked:
    remove_unused_columns=False,

    # Distributed specific
    ddp_find_unused_parameters=False,
    ddp_bucket_cap_mb=25,
    
    # Precision settings
    fp16=False,
    bf16=False,
    
)

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    train_dataset=tokenized_datasets,
    eval_dataset=tokenized_eval_dataset,
)

if __name__ == ""__main__"":
    # Start training
    trainer.train()
    
    # Cleanup
    dist.destroy_process_group()

    
```



### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from transformers import LlamaForCausalLM, LlamaConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoTokenizer
from datasets import config
from datasets import load_dataset, load_from_disk, IterableDataset
from utils import count_parameters
from tokenizers import processors
from sim_data import create_test_dataset, filter_long_sequences

# Environment settings
os.environ[""TOKENIZERS_PARALLELISM""] = ""false""
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = False


# Initialize distributed training
dist.init_process_group(backend=""nccl"")
local_rank = int(os.environ[""LOCAL_RANK""])
torch.cuda.set_device(local_rank)

# Tokenizer setup
tokenizer = AutoTokenizer.from_pretrained('/mnt/fs/huggingface/hub/tokenizers/Aranizer-PBE-86k')

tokenizer._tokenizer.post_processor = processors.TemplateProcessing(
    single=[""<s>"", ""$A"", ""</s>""],
    special_tokens=[(""<s>"", tokenizer.bos_token_id), (""</s>"", tokenizer.eos_token_id)]
)

# Special tokens setup
tokenizer.bos_token = '<s>'
tokenizer.bos_token_id = 0
tokenizer.eos_token = '</s>'
tokenizer.eos_token_id = 2
tokenizer.pad_token = '<pad>'
tokenizer.pad_token_id = 1



# Model configuration
model_config = LlamaConfig(
    vocab_size=len(tokenizer),
    hidden_size=2048,
    intermediate_size=8192,
    num_hidden_layers=16,
    num_attention_heads=16,
    max_position_embeddings=2048,
    dropout_rate=0.1,
    layer_norm_eps=1e-6,
    pad_token_id=tokenizer.pad_token_id,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    use_cache=False,
)

# Initialize model and move to appropriate GPU
model = LlamaForCausalLM(model_config)

# Enable gradient checkpointing BEFORE wrapping with DDP

print(""great"")
# Move model to GPU
model = model.to(local_rank)
# 2. Enable gradient checkpointing before DDP wrapping
model.gradient_checkpointing_enable()

# Wrap model with DDP
model = DDP(
    model, 
    device_ids=[local_rank],
    output_device=local_rank,
    find_unused_parameters=False,
)


dataset = load_dataset(""wikitext"", ""wikitext-103-v1"", split='train')


eval_dataset = load_dataset(""wikitext"", ""wikitext-103-v1"", split='validation')



def tokenize_function(examples):

    return tokenizer(
        examples[""text""],
        truncation=True,
        max_length=2046,  # 2046 - 2 to leave room for BOS and EOS
        padding=True,
        return_tensors=None,
        add_special_tokens=True,
    )


# Tokenize datasets
tokenized_datasets = dataset.map(
    tokenize_function, 
    batched=True, 
    remove_columns=[""text""],
    # num_proc=32,
    # load_from_cache_file=False
)

tokenized_eval_dataset = eval_dataset.map(
    tokenize_function, 
    batched=True, 
    remove_columns=[""text""],
    # num_proc=32,
    # load_from_cache_file=False
)

steps_var = 100

#torchrun --nproc_per_node=8 train_distributed.py

# interleave_datasets

# Training arguments
training_args = TrainingArguments(

    accelerator_config={""dispatch_batches"": False},

    # deepspeed=deepspeed_config,
    output_dir=""./Mulhem-1.4B"",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=14,
    gradient_accumulation_steps=2,
    save_steps=steps_var,
    save_total_limit=2,
    
    # Resume training settings
    resume_from_checkpoint=True, # Enable resuming from checkpoint
    # dataloader_pin_memory=True,

    # Distributed training settings
    local_rank=local_rank,

    # Optimization settings
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.01,
    adam_beta1=0.9,
    adam_beta2=0.999,
    max_grad_norm=1.0,
    
    # Evaluation and logging
    eval_strategy=""steps"",
    eval_steps=steps_var,
    eval_delay=0.1, # 
    logging_dir=""./runs"",
    logging_strategy=""steps"",
    logging_steps=steps_var,
    report_to=[""tensorboard""],
    
    # should be checked:
    remove_unused_columns=False,

    # Distributed specific
    ddp_find_unused_parameters=False,
    ddp_bucket_cap_mb=25,
    
    # Precision settings
    fp16=False,
    bf16=False,
    
)

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    train_dataset=tokenized_datasets,
    eval_dataset=tokenized_eval_dataset,
)

if __name__ == ""__main__"":
    # Start training
    trainer.train()
    
    # Cleanup
    dist.destroy_process_group()

    

### Expected behavior

to answer my question, where the error comes from?","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",8,open
Samhq model addition ,"close #31137




**Pull Request Title:** Add HQ-SAM Functionality to Transformers Library



### **Model Overview**  
HQ-SAM (Segment Anything in High Quality) is an enhanced version of the Segment Anything Model (SAM), addressing limitations in mask quality for intricate structures and challenging segmentation tasks. The model refines SAM’s predictions using a **High-Quality Output Token** and **Global-Local Feature Fusion** while preserving SAM’s efficiency and zero-shot generalization capabilities.

According to the original implementation, HQ-SAM significantly improves mask boundaries and reduces segmentation errors by introducing minimal additional parameters (<0.5%) and computational overhead. The model is designed to maintain compatibility with SAM’s existing prompt-based design and mask decoder architecture.



### **Repository and Weights**  
The HQ-SAM implementation and pre-trained weights are available in the following repository:  
[https://github.com/SysCV/sam-hq](https://github.com/SysCV/sam-hq)  

HQ-SAM provides three pre-trained weight variants:  
- `sam_hq_vit_b` – Small vision encoder.  
- `sam_hq_vit_l` – Medium vision encoder.  
- `sam_hq_vit_h` – Large vision encoder.  

The main difference between these variants is the size of the Vision Transformer (ViT) encoder, while the prompt encoder and mask decoder remain unchanged.



### **Functionality**  
For each input (e.g., bounding boxes, 2D points, or coarse masks), HQ-SAM predicts high-quality binary masks that enhance segmentation precision. Improvements include:  
- More accurate boundaries.  
- Correction of coarse masks and segmentation errors.  
- Enhanced detail preservation for thin structures and complex object geometries.



**Reviewers:** @molbap 

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",1,open
"RuntimeError: shape '[1, 3098, 6, 5, 128]' is invalid for input of size 12689408","### System Info

conda create -yn duo python=3.10
conda activate duo

conda install -y git
conda install -y nvidia/label/cuda-12.4.0::cuda-toolkit
conda install -y nvidia::cuda-cudart-dev
conda install -y pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia

pip install transformers==4.45.2 accelerate sentencepiece datasets wandb zstandard matplotlib huggingface_hub==0.25.2
pip install tensor_parallel==2.0.0

pip install ninja packaging
pip install flash-attn==2.6.3 --no-build-isolation

# LongBench evaluation
pip install seaborn rouge_score einops pandas

pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

We encountered a shape mismatch error while trying to reproduce Duo Attention. We tested versions 4.37 to 4.47, and the issue shifted from a RuntimeError: Boolean value of Tensor with more than one value is ambiguous to a RuntimeError: shape '[1, 3098, 6, 5, 128]' is invalid for input of size 12689408. We couldn't resolve the issue by changing the versions.

We also tried different models with the following commands:

huggingface-cli download togethercomputer/Llama-2-7B-32K-Instruct --local-dir Llama-2-7B-32K-Instruct
huggingface-cli download gradientai/Llama-3-8B-Instruct-Gradient-1048k --local-dir Llama-3-8B-Instruct-Gradient-1048k
huggingface-cli download gradientai/Llama-3-8B-Instruct-Gradient-4194k --local-dir Llama-3-8B-Instruct-Gradient-4194k
huggingface-cli download mistralai/Mistral-7B-Instruct-v0.2 --local-dir Mistral-7B-Instruct-v0.2
huggingface-cli download mistralai/Mistral-7B-Instruct-v0.3 --local-dir Mistral-7B-Instruct-v0.3

However, none of these models worked. There was a previous issue suggesting that updating the transformer version could solve the problem, but we are still getting shape mismatch errors.

Could there be other packages that need to be updated as well?

### Expected behavior

A solution of RuntimeError: shape '[1, 3098, 6, 5, 128]' is invalid for input of size 12689408","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
resizing token embeddings causes output embedding to be reinitialized in `post_init` when `tie_word_embedding` is False,"### System Info

- `transformers` version: 4.46.3
- Platform: Linux-6.6.20-aufs-1-x86_64-with-glibc2.36
- Python version: 3.11.2
- Huggingface_hub version: 0.26.1
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: Yes
- GPU type: NVIDIA A10

### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

This code reproduces the problem:

```
pythia = AutoModelForCausalLM.from_pretrained(""EleutherAI/pythia-410m"")
pythia.resize_token_embeddings(502)
pythia.post_init()
```

the default value for `tie_word_embeddings` in pythia is False.
I believe the problem arises from the fact the if `tie_word_embeddings` is False, Then `resize_token_embeddings` creates a new `nn.Linear` object that doesn't have the flag `_is_hf_initialized`(causing it to be `False` when using `getattr`), and then `post_init` calls `_init_weights` on the new module.

https://github.com/huggingface/transformers/blob/c8c8dffbe45ebef0a8dba4a51024e5e5e498596b/src/transformers/modeling_utils.py#L2406
 

### Expected behavior

`post_init` should not change the weights of output_embeddings after a resize.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",7,open
FileNotFoundError: Couldn't find a module script at /home/tooko/transformers-course/glue/glue.py.,"### Model description

i run the example of hugging face NLP course(https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt#the-training-loop)
my env at Vmware-ubuntu -vscode,evaluate Version is  0.4.3 . i always encounter this error:

import evaluate
metric = evaluate.load(""glue"", ""mrpc"") #此句依旧是出现FileNotFoundError错误
model.eval()

for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()} #将batch中的数据转移到device上，batch是一个字典
        with torch.no_grad():
            outputs = model(**batch)
            
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions, references=batch[""labels""])

---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
Cell In[1], line 3
      1 import evaluate
----> 3 metric = evaluate.load(""glue"", ""mrpc"")
      4 model.eval()
      6 for epoch in range(num_epochs):

File ~/transformers-course/.env/lib/python3.11/site-packages/evaluate/loading.py:748, in load(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)
    703 """"""Load a [`~evaluate.EvaluationModule`].
    704 
    705 Args:
   (...)
    745     ```
    746 """"""
    747 download_mode = DownloadMode(download_mode or DownloadMode.REUSE_DATASET_IF_EXISTS)
--> 748 evaluation_module = evaluation_module_factory(
    749     path, module_type=module_type, revision=revision, download_config=download_config, download_mode=download_mode
    750 )
    751 evaluation_cls = import_main_class(evaluation_module.module_path)
    752 evaluation_instance = evaluation_cls(
    753     config_name=config_name,
    754     process_id=process_id,
   (...)
    760     **init_kwargs,
...
    684         ) from None
    685 else:
    686     raise FileNotFoundError(f""Couldn't find a module script at {relative_to_absolute_path(combined_path)}."")

FileNotFoundError: Couldn't find a module script at /home/tooko/transformers-course/glue/glue.py. Module 'glue' doesn't exist on the Hugging Face Hub either.

others opinion to solve this issue is to loging in hugging face account and not use proxy. B cos i am in China so i must use vpn to log in HF but after logged in no matter i used vpn or not the error was existing ,and i try to download glue/glue.py from Colab'example of HF provided but i didn't found. think you for your some advice for this problem (sorry for my bad english)

### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
addressing the issue #34611 to make FlaxDinov2 compatible with any batch size,"This PR solves the batch size issue with FlaxDinov2 model. The commonly used tests across the transformers library could not detect the error in the ""interpolate_position_encoding"" method, which only worked for single images. The issue has been fixed using jnp.tile for it's simplicity, although jnp.repeat could also be used.
The slow tests have also been modified to pass a batch of images instead of just one.

@amyeroberts could you please review the changes.

P.S. this pr does exactly what  #34620 aimed to do, however, with a few more improvements.",[],0,open
Add compute_loss_func to Seq2SeqTrainer,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Add `compute_loss_func` to `Seq2SeqTrainer` to support the `compute_loss_func` in `Trainer`.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@muellerzr and @SunMarc

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
[i18n-<languageCode>] Translating docs to Chinese  Translate agents.md into Chinese,"Hello, I will translate agents.md to Chinese.
doc url : https://github.com/huggingface/transformers/blob/main/docs/source/en/agents.md","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
logic was inverted for passing loss_kwargs to forward pass,"# What does this PR do?
Bug introduced in https://github.com/huggingface/transformers/pull/34915 inverted the loss kwargs logic.

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Special token ids are not longer typed properly in 4.47.0,"### System Info

tranformers 4.47.0, python 3.11

### Who can help?

@ArthurZucker (I think)

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

`SpecialTokensMixin` in `tokenization_utils_base` returns type of `list[str | list[str] | Unknown] | Unknown` for `bos_token_id` and other special token IDs, whereas previously it was correctly `int | None`. This is a regression caused by the following commit, where all special token types were (likely unintentionally) deleted: https://github.com/huggingface/transformers/pull/34461/files#diff-85b29486a884f445b1014a26fecfb189141f2e6b09f4ae701ee758a754fddcc1 

### Expected behavior

special token ids on tokenizers should keep returning type `id | None`","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
"(sort of) a bug with token offsets: some special tokens have (0, 0) offsets regardless of their position in the document","### System Info

- `transformers` version: 4.46.3
- Platform: macOS-14.4-arm64-arm-64bit
- Python version: 3.11.9
- Huggingface_hub version: 0.26.3
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.5.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
xlm_tok = AutoTokenizer.from_pretrained('facebook/xlm-v-base')

xlm_tok('test', return_offsets_mapping=True)
```
Output:
```
{'input_ids': [0, 1340, 2], 'attention_mask': [1, 1, 1], 'offset_mapping': [(0, 0), (0, 4), (0, 0)]}
```

### Expected behavior

The special tokens should have the offset that reflects their position in the document. In this case, (4, 4) instead of (0, 0).

Why is it a remotely a big deal? To simplify the code that iterates through the offsets. For example this piece of code will not work now:

```python
def tokenize(example: dict, tokenizer: PreTrainedTokenizer, tokenizer_name: str, max_length: int = 512) -> dict:
    ner_tags: list[int] = example['ner_tags']
    example_words: list[str] = example['tokens']
    text = ' '.join(example_words)
    
    # map words to positions in text
    word_positions: list[int] = example.get('word_positions', [])
    
    if len(word_positions) != len(example_words):
        text_iterator = 0
        for word in example_words:
            while text[text_iterator:text_iterator + len(word)] != word:
                text_iterator += 1
                assert text_iterator < len(text)
            
            word_positions.append(text_iterator)
    
    encoding: BatchEncoding = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=max_length)
    num_sub_tokens = len(encoding.offset_mapping)
    
    sub_token_iterator = 0
    sub_token_ner_tags: list[int] = []
    for word_id, ner_tag in enumerate(ner_tags):
        word_start = word_positions[word_id]
        word_end = word_start + len(example_words[word_id])
        
        # there may be some empty space between words. the sub tokens that include this empty space receive O label
        # we compare with the end ([1]) to ensure that 0-length tokens are labelled as O (for example <CLS>)
        while sub_token_iterator < num_sub_tokens and  encoding.offset_mapping[sub_token_iterator][1] <= word_start:
            sub_token_iterator += 1
            sub_token_ner_tags.append(0)  # 0 = O
            
        ext_tag = ner_tags_ext[ner_tag]
        
        if sub_token_iterator < num_sub_tokens:
            # the first sub token of a word receives original label, the rest receive extended label
            sub_token_ner_tags.append(ner_tag)
            sub_token_iterator += 1
        
        # again, we need to be careful about 0-length tokens, so we compare start ([0]) with the word end
        while sub_token_iterator < num_sub_tokens and encoding.offset_mapping[sub_token_iterator][0] < word_end:
            sub_token_iterator += 1
            sub_token_ner_tags.append(ext_tag)
    
    # any tokens at the end (like <SEP>) receive O tokens
    while sub_token_iterator < num_sub_tokens:
        sub_token_iterator += 1
        sub_token_ner_tags.append(0)
        
    return {
        'word_positions': word_positions,
        f'{tokenizer_name}_sub_tokens': encoding.input_ids,
        f'{tokenizer_name}_sub_token_offsets': encoding.offset_mapping,
        f'{tokenizer_name}_sub_token_ner_tags': sub_token_ner_tags,
    }
```

It produces entities like `Peter Blackburn</s>` for `""Peter Blackburn""` text. Not a big deal, but annoying to work around and hard to catch.","[{'id': 1834081910, 'node_id': 'MDU6TGFiZWwxODM0MDgxOTEw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Usage', 'name': 'Usage', 'color': 'e28436', 'default': False, 'description': 'General questions about the library'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Add common test for `torch.export` and fix some vision models,"# What does this PR do?

Add a common **slow** test to check if a model can be exported with no issues using `torch.export.export`

1. Add an optional test, to enable it please set `test_torch_exportable = True` flag for model-specific test.
2. Enable test for vision and video models
3. Fix most of the vision models

The main fixes include:
 - Use a compile-compatible LRU cache for models.
 - Avoid modifying model parameters in the forward pass (e.g. self.param = self.param + x).
 - Avoid modifying in-place tensors created in the forward pass.
 - Avoid creating tensors with `requires_grad=True` in the forward pass.
 
Testing is not complete, there might be code paths that can't be exported. I did additional testing with specific checkpoints. In most cases, we are safe. The only two situations I found where tests pass but checkpoint export does not pass are:
 - beit (fixed)
 - zoedepth (not fixed)

## Results

✅ - can be exported with `torch.export.export`
🔵 - export fixed in this PR
❌ - can't be exported

### Vision models

 - 🔵 beit
 - 🔵 bit
 - 🔵 conditional_detr
 - ✅ convnext
 - ✅ convnextv2
 - ✅ cvt
 - 🔵 deformable_detr
 - ✅ deit
 - ✅ depth_anything
 - 🔵 detr
 - ✅ dinat
 - ✅ dinov2
 - ✅ dit
 - ✅ dpt
 - ✅ efficientnet
 - 🔵 focalnet
 - ✅ glpn
 - ✅ hiera
 - ✅ ijepa
 - 🔵 imagegpt
 - ❌ levit (low usage, won't fix)
 - ✅ mask2former
 - 🔵 maskformer
 - ✅ mobilenet_v1
 - ✅ mobilenet_v2
 - ✅ mobilevit
 - ✅ mobilevitv2
 - ✅ poolformer
 - ✅ pvt
 - ✅ pvt_v2
 - ✅ regnet
 - ✅ resnet
 - ✅ rt_detr
 - ✅ segformer
 - 🔵 seggpt
 - ❌ superpoint (data-dependent [expression](https://github.com/huggingface/transformers/blob/main/src/transformers/models/superpoint/modeling_superpoint.py#L320))
 - ✅ swiftformer
 - ✅ swin
 - ✅ swinv2
 - 🔵 swin2sr
 - ✅ table_transformer
 - ✅ upernet
 - ✅ vit
 - ✅ vitdet
 - ✅ vit_mae
 - ✅ vitmatte
 - ✅ vit_msn
 - ✅ yolos
 - ❌  zoedept (data-dependent expression, test config pass but checkpoint not)

### Video models

 - ✅ timesformer
 - ✅ vivit
 - ✅ videomae

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}, {'id': 7891155739, 'node_id': 'LA_kwDOCUB6oc8AAAAB1ll7Gw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/torch%20export', 'name': 'torch export', 'color': '3DBA93', 'default': False, 'description': 'Issues and PRs related to torch.export compatibility'}]",11,open
Add weight norm rename in _load_state_dict_into_model,"# What does this PR do?

#33275 fixed old parametrization of weight_norm for _load_state_dict_into_meta_model, but the issue also holds for _load_state_dict_into_model when using an old version of torch, e.g., 1.13.

Namely, weights are not loaded correctly for wav2vec2.encoder.pos_conv_embed.conv.weight_g, wav2vec2.encoder.pos_conv_embed.conv.weight_v for a model trained on a PyTorch version where weight_g and weight_v were renamed to original0 and original1. 

What happens is that 
- cls._load_pretrained_model is called in from_pretrained
- _fix_key is called to ensure no more warnings for renamed keys as nicely fixed by #33275
- _load_state_dict_into_model is called, with the state_dict containing still unrenamed keys https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/modeling_utils.py#L4712
- _load_state_dict_into_model adjusts beta and gamma names but not weight_g and weight_v: https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/modeling_utils.py#L666
- This is however, done in _load_state_dict_into_meta_model creating an inconsistency between the two: https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/modeling_utils.py#L832

Related to #31970.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

cc @LysandreJik and @ArthurZucker 
",[],1,open
[WIP] Pixtral: vectorize patch embeddings,"# What does this PR do?

Continuation on the discussion from https://github.com/huggingface/transformers/pull/35110.

This PR gets rid of the loop over each image in the input for Pixtra and makes it more aligned with other VLMs. Now the model will pad the image on h/w dimensions and unpad it back after the vision patch embedding layer. That also helps us to get rid of extra dimension errors on processing code we've been having lately and remove the `BacthFeatureMix`

Test with the demo script from https://huggingface.co/mistral-community/pixtral-12b, as we don't have any test for pixtral as VLM at the moment. The generations match on text level

cc @Rocketknight1 wdyt about this? The design might need some changes as I had to make llava accept extra kwargs (`image_sizes`) to make the model work",[],4,open
Keep `image_sizes` in output of `PixtralProcessor`,"This PR keeps the `image_sizes` in the output of `PixtralProcessor.__call__`.

For the context, on vLLM we have been using[ this particular kwarg from the output of `PixtralImageProcessor` to differentiate if a model is Pixtral-hf or LLaVA](https://github.com/vllm-project/vllm/blob/db87eb6c67271eb61ba9fd8559ce811a1a398a4d/vllm/model_executor/models/llava.py#L412-L415) (since they're both defined as `LlavaForConditionalGeneration`).


We are currently in a process of migration to directly use `PixtralProcessor` (and similarly `AutoProcessor` for a lot of other multimodal models) to reduce the development overhead, but noticed this is dropped during the processor call. I hope it's not a big deal to keep this (since it's just a 1d tensor) in the output, but let me know if there's any issue or better suggestion.

cc @mgoin @DarkLight1337

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],4,open
Training config that worked with transformers v4.4.6.3 results in OOM error with v4.47.0 (using SFTTrainer),"### System Info

```
- `transformers` version: 4.47.0
- Platform: Linux-6.8.0-1015-aws-x86_64-with-glibc2.35
- Python version: 3.12.6
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: Yes
- Using GPU in script?: Yes
- GPU type: NVIDIA A100-SXM4-40GB
```

### Who can help?

@ArthurZucker @SunMarc @muellerz 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Training with transformers==4.46.3 runs as expected. Upgrading to transformers==4.47.0 (without changing anything else) leads to an OOM error in the very first training step (see stack trace below).


Run command: `accelerate launch --config_file ./accelerate_config.yaml train.py training=path/to/training_config`

### Accelerate Config
```
compute_environment: LOCAL_MACHINE                                                                                                                                           
debug: false                                                                                                                                                                 
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_use_orig_params: false
  activation_checkpointing: true
machine_rank: 0
main_training_function: main
mixed_precision: 'bf16'
num_machines: 1
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

### Training Config
```
{'accelerator_config': {'dispatch_batches': None,
                        'even_batches': True,
                        'gradient_accumulation_kwargs': None,
                        'non_blocking': False,
                        'split_batches': False,
                        'use_seedable_sampler': True},
 'adafactor': False,
 'adam_beta1': 0.9,
 'adam_beta2': 0.999,
 'adam_epsilon': 1e-08,
 'attn_implementation': 'flash_attention_2',
 'auto_find_batch_size': False,
 'average_tokens_across_devices': False,
 'batch_eval_metrics': False,
 'bf16': 'auto',
 'bf16_full_eval': False,
 'chars_per_token': '<CHARS_PER_TOKEN>',
 'data_seed': None,
 'dataloader_drop_last': False,
 'dataloader_num_workers': 0,
 'dataloader_persistent_workers': False,
 'dataloader_pin_memory': True,
 'dataloader_prefetch_factor': None,
 'dataset_batch_size': 1000,
 'dataset_kwargs': {'skip_prepare_dataset': False},
 'ddp_backend': None,
 'ddp_broadcast_buffers': None,
 'ddp_bucket_cap_mb': None,
 'ddp_find_unused_parameters': None,
 'ddp_timeout': 1800,
 'debug': [],
 'deepspeed': None,
 'delete_ckpts': False,
 'disable_tqdm': False,
 'dispatch_batches': None,
 'do_eval': True,
 'do_predict': False,
 'do_train': False,
 'early_stopping_patience': 10,
 'eval_accumulation_steps': None,
 'eval_delay': 0,
 'eval_do_concat_batches': True,
 'eval_exampleset_info_path': '',
 'eval_exampleset_path': '',
 'eval_on_start': True,
 'eval_packing': False,
 'eval_steps': 10,
 'eval_strategy': 'steps',
 'eval_use_gather_object': False,
 'evaluation_strategy': None,
 'exampleset_info_path': '',
 'exampleset_path': '',
 'force_tokenize_data': False,
 'fp16': False,
 'fp16_backend': 'auto',
 'fp16_full_eval': False,
 'fp16_opt_level': 'O1',
 'fsdp': [],
 'fsdp_config': {'min_num_params': 0,
                 'xla': False,
                 'xla_fsdp_grad_ckpt': False,
                 'xla_fsdp_v2': False},
 'fsdp_min_num_params': 0,
 'fsdp_transformer_layer_cls_to_wrap': None,
 'full_determinism': False,
 'gradient_accumulation_steps': 4,
 'gradient_checkpointing': False,
 'gradient_checkpointing_kwargs': {'use_reentrant': False},
 'greater_is_better': False,
 'group_by_length': False,
 'half_precision_backend': 'auto',
 'hub_always_push': False,
 'hub_model_id': None,
 'hub_private_repo': None,
 'hub_strategy': 'every_save',
 'hub_token': '<HUB_TOKEN>',
 'ignore_data_skip': False,
 'include_for_metrics': [],
 'include_inputs_for_metrics': False,
 'include_num_input_tokens_seen': False,
 'include_tokens_per_second': False,
 'jit_mode_eval': False,
 'label_names': ['labels'],
 'label_smoothing_factor': 0.0,
 'learning_rate': 0.0002,
 'length_column_name': 'length',
 'load_best_model_at_end': True,
 'local_rank': 0,
 'log_level': 'passive',
 'log_level_replica': 'warning',
 'log_on_each_node': True,
 'logging_first_step': False,
 'logging_nan_inf_filter': True,
 'logging_steps': 1,
 'logging_strategy': 'steps',
 'lora_alpha': 32,
 'lora_dropout': 0.05,
 'lora_r': 16,
 'lora_target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj', 'gate_proj'],
 'lr_scheduler_kwargs': {},
 'lr_scheduler_type': 'cosine',
 'mask_instructions': True,
 'max_grad_norm': 1.0,
 'max_seq_length': 1024,
 'max_steps': 100,
 'meta_data': {},
 'metric_for_best_model': 'loss',
 'model_name_or_path': 'Qwen/Qwen2.5-7B-Instruct',
 'mp_parameters': '',
 'neftune_noise_alpha': None,
 'no_cuda': False,
 'num_of_sequences': 1024,
 'num_train_epochs': 3,
 'optim': 'adamw_torch',
 'optim_args': None,
 'optim_target_modules': None,
 'overwrite_output_dir': False,
 'packing': False,
 'past_index': -1,
 'per_device_eval_batch_size': 1,
 'per_device_train_batch_size': 1,
 'per_gpu_eval_batch_size': None,
 'per_gpu_train_batch_size': None,
 'prediction_loss_only': False,
 'push_to_hub': False,
 'push_to_hub_model_id': None,
 'push_to_hub_organization': None,
 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>',
 'ray_scope': 'last',
 'remove_unused_columns': True,
 'restore_callback_states_from_checkpoint': False,
 'resume_from_checkpoint': None,
 'save_on_each_node': False,
 'save_only_model': False,
 'save_safetensors': True,
 'save_steps': 20,
 'save_strategy': 'steps',
 'save_total_limit': None,
 'seed': 42,
 'skip_memory_metrics': True,
 'smoke_test': False,
 'split_batches': None,
 'tf32': None,
 'torch_compile': False,
 'torch_compile_backend': None,
 'torch_compile_mode': None,
 'torch_dtype': 'bfloat16',
 'torch_empty_cache_steps': None,
 'torchdynamo': None,
 'tpu_metrics_debug': False,
 'tpu_num_cores': None,
 'use_cpu': False,
 'use_ipex': False,
 'use_legacy_prediction_loop': False,
 'use_liger_kernel': False,
 'use_mps_device': False,
 'use_peft': False,
 'val_set_size': 0.0,
 'warmup_ratio': 0.1,
 'warmup_steps': 0,
 'weight_decay': 0.0}
```


### Training script
```

def main(cfg):
    accelerator = Accelerator()
    model_kwargs = dict(
        attn_implementation=sft_config.attn_implementation,
        torch_dtype=sft_config.torch_dtype,
        use_cache=False,
    )
    model = AutoModelForCausalLM.from_pretrained(sft_config.model_name_or_path, **model_kwargs)
    tokenizer = AutoTokenizer.from_pretrained(sft_config.model_name_or_path, use_fast=True)
    tokenizer.pad_token = tokenizer.eos_token
  
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        args=sft_config,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        peft_config=None,
        dataset_kwargs=sft_config.dataset_kwargs,
    )

    trainer.train()
    trainer.save_model()

if __name__ == ""__main__"":
    main()

```

### Stack trace
```
Traceback (most recent call last):
  File ""/home/ubuntu/***/train.py"", line 233, in main
    trainer.train()
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/transformers/trainer.py"", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/transformers/trainer.py"", line 2522, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/transformers/trainer.py"", line 3653, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/transformers/trainer.py"", line 3709, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 864, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py"", line 823, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py"", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py"", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py"", line 1184, in forward
    loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/***/.venv/lib/python3.12/site-packages/transformers/loss/loss_utils.py"", line 36, in ForCausalLMLoss
    logits = logits.float()
             ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.97 GiB. GPU 5 has a total capacity of 39.38 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 37.84 GiB memory in use. Of the allocated memory 35.69 GiB is allocated by PyTorch, and 521.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
```

### Expected behavior

Training should complete without errors.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",3,open
Fix the structure of images in PixtralProcessor,"The `PixtralProcessor` has some issues regarding correct nesting depth of inputs. If we are fully explicit about input nesting, then `text` should be `List[str]` and `images` should be `List[List[Image]]`. This is because each sample only has one text, but can have multiple images.

The start of the Pixtral processor code handles cases when users don't supply fully nested inputs. However, it uses the heuristic that if the user passes `List[str]` for `text` and `List[Image]` for `images`, then this indicates **one image per sample**. This is [very confusing](https://github.com/huggingface/transformers/pull/34332#issuecomment-2519369424) for users, especially because it means output changes when `batch_size==1`, depending on whether that single input is passed as `str` or `[str]`.

With this PR, we avoid that assumption. If the user supplies a single image or flat list of images, then we assign them all to the first sample in the event that `batch_size==1`. If `batch_size>1` then we throw an error, asking them to supply an explicit list of lists instead. This seems much safer!","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",6,open
"Error during training: ""Expected dtype float for end but got dtype c10::BFloat16""","## System Info

I'm running on an Amazon SageMaker-managed p4d.24xlarge instance, so I'll do my best to provide comprehensive system info below.

### Training image
`763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.1.0-transformers4.36.0-gpu-py310-cu121-ubuntu20.04`

### Dependencies
```
accelerate==1.1.1
alembic==1.14.0
antlr4-python3-runtime==4.9.3
blinker==1.9.0
cachetools==5.5.0
databricks-sdk==0.38.0
datasets==3.1.0
deprecated==1.2.15
Flask==3.1.0
gitdb==4.0.11
gitpython==3.1.43
google-auth==2.36.0
graphene==3.4.3
graphql-core==3.2.5
graphql-relay==3.2.0
gunicorn==23.0.0
huggingface_hub==0.26.3
itsdangerous==2.2.0
Mako==1.3.7
mlflow==2.18.0
mlflow-skinny==2.18.0
omegaconf==2.3.0
opentelemetry-api==1.28.2
opentelemetry-sdk==1.28.2
opentelemetry-semantic-conventions==0.49b2
peft==0.13.2
pyasn1-modules==0.4.1
sagemaker-mlflow==0.1.0
smmap==5.0.1
sqlalchemy==2.0.36
sqlparse==0.5.2
tokenizers==0.20.3
transformers==4.46.3
trl==0.12.1
Werkzeug==3.1.3
```

### SageMaker training config
```
{
    ""additional_framework_parameters"": {
        ""sagemaker_instance_type"": ""ml.p4d.24xlarge"",
        ""sagemaker_torch_distributed_enabled"": true
    },
    ""channel_input_dirs"": {
        ""test"": ""/opt/ml/input/data/test"",
        ""train"": ""/opt/ml/input/data/train""
    },
    ""current_host"": ""algo-1"",
    ""current_instance_group"": ""homogeneousCluster"",
    ""current_instance_group_hosts"": [
        ""algo-1""
    ],
    ""current_instance_type"": ""ml.p4d.24xlarge"",
    ""distribution_hosts"": [
        ""algo-1""
    ],
    ""distribution_instance_groups"": [
        ""homogeneousCluster""
    ],
    ""framework_module"": ""sagemaker_pytorch_container.training:main"",
    ""hosts"": [
        ""algo-1""
    ],
    ""hyperparameters"": {},
    ""input_config_dir"": ""/opt/ml/input/config"",
    ""input_data_config"": {
        ""test"": {
            ""TrainingInputMode"": ""File"",
            ""S3DistributionType"": ""FullyReplicated"",
            ""RecordWrapperType"": ""None""
        },
        ""train"": {
            ""TrainingInputMode"": ""File"",
            ""S3DistributionType"": ""FullyReplicated"",
            ""RecordWrapperType"": ""None""
        }
    },
    ""input_dir"": ""/opt/ml/input"",
    ""instance_groups"": [
        ""homogeneousCluster""
    ],
    ""instance_groups_dict"": {
        ""homogeneousCluster"": {
            ""instance_group_name"": ""homogeneousCluster"",
            ""instance_type"": ""ml.p4d.24xlarge"",
            ""hosts"": [
                ""algo-1""
            ]
        }
    },
    ""is_hetero"": false,
    ""is_master"": true,
    ""is_modelparallel_enabled"": null,
    ""is_smddpmprun_installed"": false,
    ""is_smddprun_installed"": true,
    ""job_name"": ""***"",
    ""log_level"": 20,
    ""master_hostname"": ""algo-1"",
    ""model_dir"": ""/opt/ml/model"",
    ""module_dir"": ""s3://***/sourcedir.tar.gz"",
    ""module_name"": ""train"",
    ""network_interface_name"": ""eth0"",
    ""num_cpus"": 96,
    ""num_gpus"": 8,
    ""num_neurons"": 0,
    ""output_data_dir"": ""/opt/ml/output/data"",
    ""output_dir"": ""/opt/ml/output"",
    ""output_intermediate_dir"": ""/opt/ml/output/intermediate"",
    ""resource_config"": {
        ""current_host"": ""algo-1"",
        ""current_instance_type"": ""ml.p4d.24xlarge"",
        ""current_group_name"": ""homogeneousCluster"",
        ""hosts"": [
            ""algo-1""
        ],
        ""instance_groups"": [
            {
                ""instance_group_name"": ""homogeneousCluster"",
                ""instance_type"": ""ml.p4d.24xlarge"",
                ""hosts"": [
                    ""algo-1""
                ]
            }
        ],
        ""network_interface_name"": ""eth0""
    },
    ""user_entry_point"": ""train.py""
}
```

### Environment variables
```
SM_HOSTS=[""algo-1""]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS=
{}
SM_USER_ENTRY_POINT=train.py
SM_FRAMEWORK_PARAMS=
{
    ""sagemaker_instance_type"": ""ml.p4d.24xlarge"",
    ""sagemaker_torch_distributed_enabled"": true
}
SM_RESOURCE_CONFIG=
{
    ""current_group_name"": ""homogeneousCluster"",
    ""current_host"": ""algo-1"",
    ""current_instance_type"": ""ml.p4d.24xlarge"",
    ""hosts"": [
        ""algo-1""
    ],
    ""instance_groups"": [
        {
            ""hosts"": [
                ""algo-1""
            ],
            ""instance_group_name"": ""homogeneousCluster"",
            ""instance_type"": ""ml.p4d.24xlarge""
        }
    ],
    ""network_interface_name"": ""eth0""
}
SM_INPUT_DATA_CONFIG=
{
    ""test"": {
        ""RecordWrapperType"": ""None"",
        ""S3DistributionType"": ""FullyReplicated"",
        ""TrainingInputMode"": ""File""
    },
    ""train"": {
        ""RecordWrapperType"": ""None"",
        ""S3DistributionType"": ""FullyReplicated"",
        ""TrainingInputMode"": ""File""
    }
}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[""test"",""train""]
SM_CURRENT_HOST=algo-1
SM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge
SM_CURRENT_INSTANCE_GROUP=homogeneousCluster
SM_CURRENT_INSTANCE_GROUP_HOSTS=[""algo-1""]
SM_INSTANCE_GROUPS=[""homogeneousCluster""]
SM_INSTANCE_GROUPS_DICT=
{
    ""homogeneousCluster"": {
        ""hosts"": [
            ""algo-1""
        ],
        ""instance_group_name"": ""homogeneousCluster"",
        ""instance_type"": ""ml.p4d.24xlarge""
    }
}
SM_DISTRIBUTION_INSTANCE_GROUPS=[""homogeneousCluster""]
SM_IS_HETERO=false
SM_MODULE_NAME=train
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=96
SM_NUM_GPUS=8
SM_NUM_NEURONS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://***/sourcedir.tar.gz
SM_TRAINING_ENV=
{
    ""additional_framework_parameters"": {
        ""sagemaker_instance_type"": ""ml.p4d.24xlarge"",
        ""sagemaker_torch_distributed_enabled"": true
    },
    ""channel_input_dirs"": {
        ""test"": ""/opt/ml/input/data/test"",
        ""train"": ""/opt/ml/input/data/train""
    },
    ""current_host"": ""algo-1"",
    ""current_instance_group"": ""homogeneousCluster"",
    ""current_instance_group_hosts"": [
        ""algo-1""
    ],
    ""current_instance_type"": ""ml.p4d.24xlarge"",
    ""distribution_hosts"": [
        ""algo-1""
    ],
    ""distribution_instance_groups"": [
        ""homogeneousCluster""
    ],
    ""framework_module"": ""sagemaker_pytorch_container.training:main"",
    ""hosts"": [
        ""algo-1""
    ],
    ""hyperparameters"": {},
    ""input_config_dir"": ""/opt/ml/input/config"",
    ""input_data_config"": {
        ""test"": {
            ""RecordWrapperType"": ""None"",
            ""S3DistributionType"": ""FullyReplicated"",
            ""TrainingInputMode"": ""File""
        },
        ""train"": {
            ""RecordWrapperType"": ""None"",
            ""S3DistributionType"": ""FullyReplicated"",
            ""TrainingInputMode"": ""File""
        }
    },
    ""input_dir"": ""/opt/ml/input"",
    ""instance_groups"": [
        ""homogeneousCluster""
    ],
    ""instance_groups_dict"": {
        ""homogeneousCluster"": {
            ""hosts"": [
                ""algo-1""
            ],
            ""instance_group_name"": ""homogeneousCluster"",
            ""instance_type"": ""ml.p4d.24xlarge""
        }
    },
    ""is_hetero"": false,
    ""is_master"": true,
    ""is_modelparallel_enabled"": null,
    ""is_smddpmprun_installed"": false,
    ""is_smddprun_installed"": true,
    ""job_name"": ""***"",
    ""log_level"": 20,
    ""master_hostname"": ""algo-1"",
    ""model_dir"": ""/opt/ml/model"",
    ""module_dir"": ""s3://***/sourcedir.tar.gz"",
    ""module_name"": ""train"",
    ""network_interface_name"": ""eth0"",
    ""num_cpus"": 96,
    ""num_gpus"": 8,
    ""num_neurons"": 0,
    ""output_data_dir"": ""/opt/ml/output/data"",
    ""output_dir"": ""/opt/ml/output"",
    ""output_intermediate_dir"": ""/opt/ml/output/intermediate"",
    ""resource_config"": {
        ""current_group_name"": ""homogeneousCluster"",
        ""current_host"": ""algo-1"",
        ""current_instance_type"": ""ml.p4d.24xlarge"",
        ""hosts"": [
            ""algo-1""
        ],
        ""instance_groups"": [
            {
                ""hosts"": [
                    ""algo-1""
                ],
                ""instance_group_name"": ""homogeneousCluster"",
                ""instance_type"": ""ml.p4d.24xlarge""
            }
        ],
        ""network_interface_name"": ""eth0""
    },
    ""user_entry_point"": ""train.py""
}
SM_USER_ARGS=[]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_CHANNEL_TEST=/opt/ml/input/data/test
SM_CHANNEL_TRAIN=/opt/ml/input/data/train
PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages
```
### SFT Config
```
SFTConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
attn_implementation=flash_attention_2,
auto_find_batch_size=False,
average_tokens_across_devices=False,
aws_profile=,
batch_eval_metrics=False,
bf16=auto,
bf16_full_eval=False,
chars_per_token=<CHARS_PER_TOKEN>,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_batch_size=1000,
dataset_kwargs=None,
dataset_name=,
dataset_num_proc=8,
dataset_test_split=,
dataset_text_field=,
dataset_train_split=,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
delete_ckpts=False,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
early_stopping_patience=10,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_exampleset_info_path=,
eval_exampleset_path=,
eval_on_start=True,
eval_packing=False,
eval_steps=10,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=None,
exampleset_info_path=,
exampleset_path=,
force_tokenize_data=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>, <FSDPOption.OFFLOAD: 'offload'>],
fsdp_config={'limit_all_gathers': True, 'backward_prefetch': 'backward_pre', 'forward_prefetch': 'false', 'use_orig_params': 'false', 'min_num_params': 0, 'activation_checkpointing': 'true', 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
gradient_checkpointing_kwargs={'use_reentrant': True},
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=True,
load_hf_data=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/opt/ml/model/runs/Dec04_23-27-49_algo-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_alpha=32,
lora_dropout=0.05,
lora_r=16,
lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj', 'gate_proj'],
lr_scheduler_kwargs=
{}
,
lr_scheduler_type=cosine,
mask_instructions=True,
max_grad_norm=1.0,
max_seq_length=4096,
max_steps=1000,
meta_data=
{}
,
metric_for_best_model=loss,
mlflow_experiment_name=***,
mlflow_run_name=***,
mlflow_tracking_uri=***,
model_name_or_path=Qwen/Qwen2.5-7B-Instruct,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_of_sequences=1024,
num_train_epochs=3,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/opt/ml/model,
overwrite_output_dir=False,
packing=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=2,
prediction_loss_only=False,
preprocessed_data_path=,
preprocessed_eval_data_path=,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['mlflow'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/opt/ml/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=20,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
smoke_test=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_dtype=bfloat16,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_peft=False,
val_set_size=0.0,
warmup_ratio=0.1,
warmup_steps=100,
weight_decay=0.0,
)
```

## Who can help?

@muellerz @SunMarc @ArthurZucker 

## Information

- [ ] The official example scripts
- [x] My own modified scripts

## Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

## Reproduction

Below is my training script, the SageMaker estimator object, and the stack trace showing the error that I'm getting. A curious (but perhaps irrelevant detail) is that this I've reproduced this twice, and both times it occurs during training on the 257th step (when `max_steps=1000`).

### Training script
```
import os
import glob
import json
import logging

from datasets import load_dataset
from accelerate import Accelerator
from accelerate.logging import get_logger
from datasets import load_dataset
import mlflow
from omegaconf import DictConfig, OmegaConf
import pprint
from peft.tuners.lora import LoraConfig
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM
from transformers import TrainingArguments
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, EarlyStoppingCallback


def main(cfg: DictConfig):

    peft_config = None

    model_kwargs = dict(
        attn_implementation=sft_config.attn_implementation,
        torch_dtype=sft_config.torch_dtype,
        #use_cache=not sft_config.gradient_checkpointing,
        use_cache=not (sft_config.gradient_checkpointing or sft_config.fsdp_config.activation_checkpointing)
    )
    model = AutoModelForCausalLM.from_pretrained(sft_config.model_name_or_path, **model_kwargs)
    tokenizer = AutoTokenizer.from_pretrained(sft_config.model_name_or_path, use_fast=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    instruction_template = ""user\n""
    response_template = ""assistant\n""
    data_collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)
    train_dataset = load_dataset(*)
    eval_dataset = load_dataset(*)

    
    def tokenize(examples):
        conversations = examples[""messages""]
        input_text = []
        for conv in conversations:
            text = tokenizer.apply_chat_template(conv, tokenize=False)
            input_text.append(text)
        tokenized = tokenizer(
            input_text,
            truncation=True,
            max_length=sft_config.max_seq_length,
            padding='max_length',
            return_tensors=None,
        )
        tokenized[""labels""] = tokenized[""input_ids""].copy()
        return tokenized
    

    train_dataset = train_dataset.map(
        tokenize,
        remove_columns=train_dataset.column_names,
        batched=True,
        desc=""Tokenizing train dataset"",
    )
    
    eval_dataset = eval_dataset.map(*)
    
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        args=sft_config,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        dataset_text_field=""messages"",
        peft_config=peft_config,
        dataset_kwargs=sft_config.dataset_kwargs,
        data_collator=data_collator,
    )

    if hasattr(sft_config, ""early_stopping_patience"") and sft_config.early_stopping_patience > 0:
        callback = EarlyStoppingCallback(sft_config.early_stopping_patience)
        trainer.add_callback(callback)

    trainer.train()
    trainer.save_model()
    
if __name__ == ""__main__"":
    cfg = OmegaConf.load('train_config.yaml')
    main(cfg)
```

### SageMaker / HuggingFace estimator
```
huggingface_estimator = HuggingFace(
    entry_point          = 'train.py',      
    dependencies=['requirements.txt'],         
    source_dir           = './',           
    instance_type        = 'ml.p4d.24xlarge', 
    instance_count       = 1,              
    max_run              = 2*24*60*60,       
    base_job_name        = job_name,         
    role                 = role,      
    volume_size          = 1024,         
    transformers_version = '4.36.0',          
    pytorch_version      = '2.1.0',          
    py_version           = 'py310',          
    disable_output_compression = True,  
    distribution={""torch_distributed"": {""enabled"": True}},  
    environment  = {
        ""HUGGINGFACE_HUB_CACHE"": ""/tmp/.cache"", 
        ""HF_TOKEN"": HfFolder.get_token(),   
        ""ACCELERATE_USE_FSDP"": ""1"",       
        ""FSDP_CPU_RAM_EFFICIENT_LOADING"": ""0"",   
        ""FSDP_AUTO_WRAP_POLICY"": ""TRANSFORMER_BASED_WRAP"",
        ""FSDP_BACKWARD_PREFETCH"": ""BACKWARD_PRE"",
        ""FSDP_STATE_DICT_TYPE"": ""FULL_STATE_DICT"",
        ""NCCL_TIMEOUT"": ""3600"", 
        ""NCCL_DEBUG"": ""WARN"",  
        ""NCCL_IB_TIMEOUT"": ""3600"",
        ""NCCL_SOCKET_TIMEOUT"": ""3600"",
        ""NCCL_ASYNC_ERROR_HANDLING"": ""1"",
        ""NCCL_P2P_LEVEL"": ""NVL"",
        ""CUDA_DEVICE_MAX_CONNECTIONS"": ""1"",        
        ""MAX_JOBS"": ""1"",                           
        ""PYTORCH_CUDA_ALLOC_CONF"": ""max_split_size_mb:512"",
        ""TORCH_DISTRIBUTED_DEBUG"": ""DETAIL"",   
        ""HF_DATASETS_CACHE"": ""/opt/ml/input"",
        ""TRANSFORMERS_CACHE"": ""/opt/ml/input""
    },
    checkpoint_s3_uri=f's3://{***}/checkpoints'
)
```



### Stack trace
```
Traceback (most recent call last):
  File ""/opt/ml/code/train.py"", line 259, in <module>
    main(cfg)
  File ""/opt/ml/code/train.py"", line 242, in main
    trainer.train()
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2123, in train
    return inner_training_loop(
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2534, in _inner_training_loop
    self.optimizer.step()
  File ""/opt/conda/lib/python3.10/site-packages/accelerate/optimizer.py"", line 171, in step
    self.optimizer.step(closure)
  File ""/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py"", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 373, in wrapper
    out = func(*args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py"", line 184, in step
    adamw(
  File ""/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py"", line 335, in adamw
    func(
  File ""/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py"", line 412, in _single_tensor_adamw
    exp_avg.lerp_(grad, 1 - beta1)
RuntimeError: expected dtype float for `end` but got dtype c10::BFloat16
```

## Expected behavior

Training should complete successfully without encountering errors.

## Related issues
1. https://github.com/huggingface/transformers/issues/34702 - this seems to be the exact same issue and was active as recently as 3 weeks ago. It was closed only two days ago.
2. https://github.com/SqueezeAILab/LLM2LLM/issues/5
3. https://discuss.huggingface.co/t/errors-when-using-gradient-accumulation-with-fsdp-peft-lora-sfttrainer/105006","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Add dinov2 with registers attempt 2,"

# What does this PR do?

This PR is a re-follow up and rebase-ing of #32905  and it seems to be working well from the work done by @NielsRogge.
my first attempt (which is now closed) #34788 was too far gone and I thought it better just to start fresh now that I am more familiar with transformers. 

Thank you all for your patience.

---

Relevent reviewers:
@ArthurZucker

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",0,open
Add check for if num_items_in_batch is not None,"# What does this PR do?

Adds a check before calling `.gather()` to make sure that `num_items_in_batch` actually has a value

Fixes https://github.com/huggingface/transformers/issues/35076


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@Rocketknight1 @SunMarc ",[],1,open
Let `EarlyStoppingCallback` not require `load_best_model_at_end`,"# What does this PR do?

This PR tweaks `EarlyStoppingCallback` to still require a metric, but doesn't need `load_best_model_at_end`. Doing so lets users still use it, but optionally doesn't require them to load it back in at the end of training if they don't want to. 

Fixes # (issue)

Request of @BenjaminBossan 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@Rocketknight1 @SunMarc 
",[],3,open
[Idefics3] Move image features to same device as input embeds,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes [#35031](https://github.com/huggingface/transformers/issues/35031) 
Summary: 
Got a device mismatch issue on running `trainer.train()`.
```
File ~/.local/lib/python3.11/site-packages/transformers/models/idefics3/modeling_idefics3.py:904, in Idefics3Model.inputs_merger(self, input_ids, inputs_embeds, image_hidden_states)
    902 # cast to the dtype of the input_embeds to support quantized models
    903 reshaped_image_hidden_states = reshaped_image_hidden_states.to(inputs_embeds.dtype)
--> 904 new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states
    905 return new_inputs_embeds

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1
```

Moved `image_features` to same device as `input_embeds`.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@zucchini-nlp
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
TextIteratorStreamer unable to create generator_kwargs,"### System Info

- `transformers` version: 4.46.2
- Platform: Windows-10-10.0.22631-SP0
- Python version: 3.11.0
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu121 (True)
- Tensorflow version (GPU?): 2.18.0 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: Yes
- GPU type: NVIDIA GeForce RTX 4060 Laptop GPU

### Who can help?

@gante

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import TextIteratorStreamer
from threading import Thread
# Load your model and tokenizer
model_id = ""./Llama-3.2-3b-instruct""
model = AutoModelForCausalLM.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Define the messages
messages = [
    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""Count to 10.""},
]

# Convert messages to inputs
inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=""pt"")

# Create a custom streamer instance with a log file
streamer = TextIteratorStreamer(tokenizer)
generation_kwargs = dict(inputs, streamer=streamer, max_length=256)
thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()
generated_text = """"
for new_text in streamer:
    generated_text += new_text
```

Error:
```cmd
Traceback (most recent call last):
  File ""xxxxxxxxxxxx.py"", line xx, in <module>
    generation_kwargs = dict(inputs, streamer=streamer, max_length=256)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: dictionary update sequence element #0 has length 46; 2 is required
```

### Expected behavior

Should Run Successfully without any errors.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Is there a way to find the earliest version of transformers that has a certain model?,"### Feature request

Is there a way to find the earliest version of transformers that has a certain model? For example, I want to use CLIP into my project, but the existing transformers version is old, I want to upgrade transformers to a lowest version that can use CLIP, so that other parts of my code do not change.

### Motivation

There are situations where I need to use a new model in the existing codebase. But when updating transformers, there could be some parts of the code out of date, and need to be modified. 

### Your contribution

I don't know, but I will try to help.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Documentation for SWAG contradicts itself when constructing the first sentence.,"### System Info

Not relevant.

### Who can help?

@stevhliu @ArthurZucker 

### Information

- [x] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

The [docs for multiple choice](https://huggingface.co/docs/transformers/tasks/multiple_choice) use SWAG as an example, which is the task of selecting the next sentence given a context. Somewhat strangely, rather than being given in the format `(sentence1, [sentence2a, sentence2b, sentence2c, sentence2d])`, the dataset is given in the format `(sentence1, sentence2_start, [sentence2_endA, sentence2_endB, sentence2_endC, sentence2_endD])`.

The code given in the docs basically turns the dataset into the first format, where **sentence 1 is kept intact** and the start of sentence 2 is concatenated to each ending:
https://github.com/huggingface/transformers/blob/a06a0d12636756352494b99b5b264ac9955bc735/docs/source/en/tasks/multiple_choice.md?plain=1#L96-L100

Yet, the docs say:
https://github.com/huggingface/transformers/blob/a06a0d12636756352494b99b5b264ac9955bc735/docs/source/en/tasks/multiple_choice.md?plain=1#L85-L88

What is being described is formatting the dataset as `(sentence1 + sentence2_start, [sentence2_start + sentence2_endA, sentence2_start + sentence2_endB, sentence2_start + sentence2_endC, sentence2_start + sentence2_endD])`, where **there is overlap between the first and the second sentence** (namely `sentence2_start`).

### Expected behavior

Either the code is wrong or the description is wrong. 

If the description is wrong, it should be:

> The preprocessing function you want to create needs to:
> 1. Make four copies of the sent1 field.
> 2. Combine sent2 with each of the four possible sentence endings.

If the code is wrong, it should be:
```python
    first_sentences = [[f""{s1} {s2_start}""] * 4 for s1,s2_start in zip(examples[""sent1""], examples[""sent2""])]
    second_sentences = [
        [f""{s2_start} {examples[end][i]}"" for end in ending_names] for i, s2_start in enumerate(examples[""sent2""])
    ]
```","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Enhanced Installation Section in README.md,"This PR improves the Installation section in the README.md file by:
- Adding GPU setup instructions for TensorFlow and PyTorch.
- Including troubleshooting tips for Python version issues and missing dependencies.
- Separating OS-specific virtual environment activation commands for clarity.
- Providing links to relevant installation guides for backend frameworks.

These enhancements aim to improve the clarity and usability of the installation instructions for new users.

",[],2,open
[FEAT] Compatibility with dduf format from diffusers,"# What does this PR do ?

This PR adds the compatibility with dduf format from diffusers. 
For tokenizers, I only made it compatible with T5 and CLIP for now. 

Diffusers PR : https://github.com/huggingface/diffusers/pull/10037
",[],2,open
Output dicts support in text generation pipeline,"# What does this PR do?

It is a minor fix to the text generation pipeline. When calling with the generation argument `return_dict_in_generate=True,` the code breaks because it does not expect a dict from the `model.generate(...)` call. If you want to view logits, for example, a dict is required. This fix can handle `return_dict_in_generate=True` as a pipeline parameter.

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@Rocketknight1


",[],3,open
[Clean-up] Planned removal of the `max_size` argument,"# What does this PR do?

This PR aims to serve as a progress tracker for updating the size configurations of detection models on the Hugging Face Hub. The updates focus on aligning the configs with the deprecation of the `max_size` argument while ensuring backward compatibility where needed. Each update will be submitted as an individual PR directly to the model repositories on the Hub, following the script and guidance provided by @qubvel.

All individual pull requests made directly to the model repositories on the Hub will be tracked here via links, providing a clear overview of progress and ensuring comprehensive updates across relevant models.

Here's a brief summary of what's been done so far regarding the planned deletion of the `max_size` argument:

This pull request includes multiple changes across several image processing files in the transformers library. The primary focus is on deprecating the `max_size` parameter and replacing it with `size['longest_edge']`. Additionally, backwards compatibility adjustments have been made to handle the `size` parameter more flexibly.

Deprecation of `max_size` parameter:

* [`src/transformers/models/conditional_detr/image_processing_conditional_detr.py`](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL880-R888): Replaced `logger.warning_once` with `logger.error` for the `max_size` parameter deprecation and removed handling of `max_size` in `__init__`, `resize`, and `preprocess` methods. [[1]](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL880-R888) [[2]](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL1014-R1018) [[3]](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL1367-R1372) [[4]](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL1473-R1468)
* [`src/transformers/models/deformable_detr/image_processing_deformable_detr.py`](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L878-R886): Similar changes to deprecate `max_size` and update methods accordingly. [[1]](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L878-R886) [[2]](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L1012-R1016) [[3]](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L1365-R1371) [[4]](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L1471-R1467)
* [`src/transformers/models/detr/image_processing_detr.py`](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L862-R870): Updated to use `logger.error` for deprecation and removed `max_size` handling. [[1]](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L862-R870) [[2]](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L993-R997) [[3]](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L1340-R1345) [[4]](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L1446-R1441)
* [`src/transformers/models/grounding_dino/image_processing_grounding_dino.py`](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L886-R894): Changed `logger.warning_once` to `raise ValueError` for `max_size` deprecation and removed related code. [[1]](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L886-R894) [[2]](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L1020-R1024) [[3]](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L1373-R1378) [[4]](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L1479-R1474)
* [`src/transformers/models/rt_detr/image_processing_rt_detr.py`](diffhunk://#diff-e0274b01a4b788c6e03df73adb099f681f83b5fc59015bdeefee312fe0b23ca5L534-R541): Updated to handle `max_size` deprecation similarly.

Backwards compatibility adjustments:

* [`src/transformers/models/conditional_detr/image_processing_conditional_detr.py`](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL880-R888): Added handling for `size` when it is an integer.
* [`src/transformers/models/deformable_detr/image_processing_deformable_detr.py`](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L878-R886): Included similar handling for integer `size`.
* [`src/transformers/models/detr/image_processing_detr.py`](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L862-R870): Adjusted `size` handling for backwards compatibility.
* [`src/transformers/models/grounding_dino/image_processing_grounding_dino.py`](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L886-R894): Added compatibility handling for `size`.
* [`src/transformers/models/rt_detr/image_processing_rt_detr.py`](diffhunk://#diff-e0274b01a4b788c6e03df73adb099f681f83b5fc59015bdeefee312fe0b23ca5R456-R457): Included backwards compatibility for `size`.


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

It follows an issue and a pull request linked bellow:
- #34977
- #34998 



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@qubvel 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],9,open
Fix : Falcon processor doesn't account for a layout difference of qkv between transformers and GGUF,"# What does this PR do?

For Falcon 40B in the `transformers` modeling code, the Q, K, and V tensors are fused and stored in an interleaved manner. This means that, for each group, the Q tensors for all heads in the group are stacked together, followed by the K and V matrices of that group. However, in the GGUF layout, the `falcon.tensor_data_layout` is set to `jploski`, which changes how the fused Q, K, and V tensors are stored [(more info here)](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md). In this layout, the Q, K, and V tensors are stored sequentially instead of interleaved. This sequential storage makes it easier to use on the `llama_cpp` side. To handle this difference, the PR processes the `qkv` tensors to convert them back into the interleaved format required for `transformers` modeling.

Additionally, this PR adds a new field, `new_decoder_architecture`, to the configuration. This is necessary for ensuring the modeling code of Falcon handles the `fused_qkv` sizes correctly. The PR also fixes the name of the `num_key_value_heads` field in the Falcon configuration, changing it to the correct name, `num_kv_heads`.

## Who can review ?
@SunMarc @LysandreJik 
",[],1,open
"size mismatch for lm_head.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([59744, 768]).","### System Info

transformers version: 4.46.3
Platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.17
Python version: 3.8.20
Huggingface_hub version: 0.26.2
Safetensors version: 0.4.5
Accelerate version: 1.0.1
Accelerate config: not found
PyTorch version (GPU?): 2.4.1+cu121 (True)
Tensorflow version (GPU?): not installed (NA)
Flax version (CPU?/GPU?/TPU?): not installed (NA)
Jax version: not installed
JaxLib version: not installed
Using distributed or parallel set-up in script?:
Using GPU in script?:
GPU type: NVIDIA RTX A4000



### Who can help?

@ArthurZucker 

Hi, I am using T5 1.1 base to train on my seq2seq task, and the source vocab size is 59744 and the target vocab size is only 32. I know by default the headLM's size and decoder embedding size are equal to the vocab's size as 59k so I change decoder embedding also headLM as follows when trainning and I found it all appear no error through training:

model = T5ForConditionalGeneration.from_pretrained(""google/t5-v1_1-base"")
model.resize_token_embeddings(new_vocab_size)

target_vocab_size = 32
model.decoder.embed_tokens = torch.nn.Embedding(target_vocab_size, model.config.d_model)

model.lm_head = torch.nn.Linear(model.config.d_model, target_vocab_size, bias=False)
torch.nn.init.normal_(model.lm_head.weight, mean=0.0, std=model.config.initializer_factor)



but when i try to load the checkpoint to predict as follows:

config = T5Config.from_pretrained(""./resultstest/checkpoint-100"")

    # model = T5ForConditionalGeneration(config)
    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path, config=config)


it shows like that:

size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([59744, 768]).
	size mismatch for lm_head.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([59744, 768]).


So how can I fix it to achieve the decoder embedding and headLM's size is only 32 to fit my target. because I need to calculate the probability of each token using softmax on 32 candidates rather than 59k. 

It would be very appreciate if someone could help me to fix it. 


Best.


### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

this is my trainning code:

'''python

import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
import torch
from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, AutoTokenizer, T5Config, T5ForConditionalGeneration
from datasets import Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
from tokenizer import SourceFuzzyTokenizer as CustomTokenizer 
vocab_file = ""./datasets/tokenizer/source_vocab.json""  

data_file_fuzzy = ""./datasets/test_data/testfuzzy.txt""
data_file_gt = ""./datasets/test_data/testgt.txt""

    

max_length = 50                   
batch_size = 50                
num_epochs = 100                     
learning_rate = 5e-4 
new_vocab_size = 59744
output_dir = ""./resultstest""  


with open(data_file_fuzzy, ""r"") as fuzzy_file, open(data_file_gt, ""r"") as gt_file:
    fuzzy_seqs = fuzzy_file.read().splitlines()
    gt_seqs = gt_file.read().splitlines()


assert len(fuzzy_seqs) == len(gt_seqs), ""fuzzy_seqs.txt and gt_seqs.txt do not MATCH!""

data = {""input"": fuzzy_seqs, ""target"": gt_seqs}
df = pd.DataFrame(data)


train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)

train_dataset = Dataset.from_pandas(train_df)
eval_dataset = Dataset.from_pandas(eval_df)


tokenizer = CustomTokenizer(vocab_file)

model = T5ForConditionalGeneration.from_pretrained(""google/t5-v1_1-base"")
model.resize_token_embeddings(new_vocab_size)

target_vocab_size = 32
model.decoder.embed_tokens = torch.nn.Embedding(target_vocab_size, model.config.d_model)

model.lm_head = torch.nn.Linear(model.config.d_model, target_vocab_size, bias=False)
torch.nn.init.normal_(model.lm_head.weight, mean=0.0, std=model.config.initializer_factor)

model.config.vocab_size = 59744 
model.config.decoder_vocab_size = target_vocab_size 

model.lm_head.weight.data.normal_(mean=0.0, std=model.config.initializer_factor)

def preprocess_data(examples):
    inputs = [tokenizer.tokenize(seq) for seq in examples[""input""]]
    targets = [tokenizer.tokenize(seq) for seq in examples[""target""]]


    input_ids = [tokenizer.convert_tokens_to_ids(tokens)[:max_length] for tokens in inputs]
    target_ids = [tokenizer.convert_tokens_to_ids(tokens)[:max_length] for tokens in targets]


    pad_id = tokenizer.vocab.get(""[PAD]"", 0)
    input_ids = [seq + [pad_id] * (max_length - len(seq)) for seq in input_ids]
    target_ids = [seq + [pad_id] * (max_length - len(seq)) for seq in target_ids]

    attention_mask = [[1 if token != pad_id else 0 for token in seq] for seq in input_ids]

    return {
        ""input_ids"": input_ids,""attention_mask"": attention_mask,
        ""labels"": target_ids,
    }

train_dataset = train_dataset.map(preprocess_data, batched=True)
eval_dataset = eval_dataset.map(preprocess_data, batched=True)

training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy=""steps"",
    learning_rate=learning_rate,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    save_strategy=""steps"",
    save_total_limit=2,
    logging_dir=""./logs"",
    logging_steps=10,
    evaluation_strategy=""epoch"",
    save_steps = 50,
    # load_best_model_at_end=True,
    dataloader_num_workers = 1,
    fp16=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)


trainer.train()
'''

### Expected behavior

I expect that the headLM and decoder embedding layer can fit my source dictionary's size 32, rather than calculate logits witch have 59744 size.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
The dot in the model name when using auto_map will cause a path parsing error.,"### System Info

transformers version: 4.40.2
Platform: Linux-5.4.0-200-generic-x86_64-with-glibc2.31
Python version: 3.10.4
Huggingface_hub version: 0.26.2
Safetensors version: 0.4.5
Accelerate version: 1.1.1
Accelerate config: not found
PyTorch version (GPU?): 2.0.1+cu117 (True)
Tensorflow version (GPU?): not installed (NA)
Flax version (CPU?/GPU?/TPU?): not installed (NA)
Jax version: not installed
JaxLib version: not installed
Using GPU in script?: Yes
Using distributed or parallel set-up in script?: No

### Who can help?

_No response_

### Information

- [x] The official example scripts
- [ ] My own modified scripts

### Tasks

- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained('xxx/xxx-1.1', trust_remote_code=True, token=True)

### Expected behavior

config.json:
{
...,
""auto_map"": {
""AutoConfig"": ""configuration_xxx.xxxConfig"",
 ""AutoModelForCausalLM"": ""modeling_xxx.xxxForPrediction""
 },
 }
When I use the above config and code to load my custom model with `auto_map`, an error occurs if my model's name contains a `.`: 

`ModuleNotFoundError: No module named 'transformers_modules.xxx-1'`，It seems that the `.` in the name is mistakenly recognized as a directory. How can this issue be resolved?
","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",12,open
Idefics: fix docstring,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/34628
",[],1,open
Bug of eval loss when enabling average_tokens_across_devices,"https://github.com/huggingface/transformers/blob/329f5dbf97a5cb2473914c88c05aa3dcb242e19a/src/transformers/trainer.py#L3737

For the eval set, there is no need to reduce_mean the gradient, so does the loss not need to multiple the `num_processes`?
```
if self.args.average_tokens_across_devices and self.model_accepts_loss_kwargs:
            loss *= self.accelerator.num_processes
```

In my experiments, I found the loss value of eval will be always greater `num_processes` times than the running of `average_tokens_across_devices=false`.",[],2,open
[setup] migrate setup script to `pyproject.toml` (reland #22539),"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Reland PR #22539.

As we are moving forward to Python 3.9+ only. It is save for now to move as much as static settings out of `setup.py`.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Bug of self.accelerator.gather(num_items_in_batch) with enabling average_tokens_across_devices ,"https://github.com/huggingface/transformers/blob/329f5dbf97a5cb2473914c88c05aa3dcb242e19a/src/transformers/trainer.py#L5146

The `num_items_in_batch` could be None for the `break` command
```
for _ in range(num_batches):
            try:
                batch_samples += [next(epoch_iterator)]
            except StopIteration:
                break
```
then it will raise this error in the self.accelerator.gather(num_items_in_batch):
```
TypeError: Unsupported types (<class 'NoneType'>) passed to `_gpu_gather_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
```

A condition `if self.args.average_tokens_across_devices and num_items_in_batch is not None:` should be added to process this problem. I don't intend to submit a separate PR for this issue. Can someone fix it along the way?",[],1,open
"When extending embeddings, multivariate distribution isn't correctly estimated even when the calculated sigma matrix is symmetric and positive definite ","### System Info

- `transformers` version: 4.37.1
- Platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.4.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

### Who can help?

When resizing token embeddings for models like MobileBert, iBert etc, `resize_token_embeddings` calls an underlying  `transformers.modeling_utils._init_added_embeddings_with_mean`. It should initialize new embedding weights using the old ones:

1. calculate the mean vector of old embedding vectors
2. calculate a sigma matrix using this vector - `vector * vector.T / vector_dim` 
3. check if its positive-definite, i.e. can be used as a covariance matrix for a new distribution
  - if so, sample from estimated distribution
  - else just initialize the new embeddings from the mean vector of previous ones
 
 I noticed the check in step `3` ALWAYS fails, i.e. no matrix is considered as positive definite. 
 
 The problem seems to be in [these lines](https://github.com/huggingface/transformers/blob/329f5dbf97a5cb2473914c88c05aa3dcb242e19a/src/transformers/modeling_utils.py#L2436C1-L2438C10)
```
         eigenvalues = torch.linalg.eigvals(covariance)
         is_covariance_psd = bool(
            (covariance == covariance.T).all() and not torch.is_complex(eigenvalues) and (eigenvalues > 0).all()
        )
```
since the eigenvalues calculated with `torch.linalg.eigvals` are complex and `torch.is_complex` returns `True` for them. Hence, the main logic, i.e. constructing a multivariate distribution from the previous embeddings and sample from it, might never work (at least in my experiments).  

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Here's an isolated example testing the lines I mentioned above:

```
import torch

covariance = torch.Tensor([[5,4],[1,2]])
eigenvalues = torch.linalg.eigvals(covariance)
is_covariance_psd = bool((covariance == covariance.T).all() and not torch.is_complex(eigenvalues) and (eigenvalues > 0).all())
print(is_covariance_psd)
```

This outputs `False` despite the matrix having two positive real eigenvalues - `6` and `1`

### Expected behavior

The function should successfully generate a multivariate normal distribution whenever the calculated sigma is positive definite and symmetric. 

I think the check might be replaced with something like:

```
from torch.distributions import constraints

is_psd = constraints.positive_definite.check(covariance).item()
```

","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",6,open
"`trainer.evaluate` always creates a new MLFlow run, separate from the one used during `train()`","### System Info

`transformers` version: 4.46.1
`mlflow==2.18.0`

I have confirmed this on my local machine (mac) and our training cluster (8x H100 using standard nvcr image), using a local and external MLFlow tracking server, so this is not likely to be very environment dependent outside the above.

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```py
from time import sleep

from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset
import mlflow
import os


with mlflow.start_run() as run:
    run_id = run.info.run_id
os.environ[""MLFLOW_RUN_ID""] = run_id
print(""Original run id:"", run_id)

# Load a pre-trained model and tokenizer
model_name = ""distilbert-base-uncased""
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load a small dataset
dataset = load_dataset(""imdb"", split=""train[:100]"")

# Tokenize the dataset
def tokenize(batch):
    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir=""./results"",
    evaluation_strategy=""epoch"",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=1,
    logging_steps=1,
    report_to=""mlflow""
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
)

# Train the model
trainer.train()

# Evaluate the model
eval_results = trainer.evaluate()
print(eval_results)

sleep(10)
runs_dir = ""./mlruns/0""
runs = [x for x in os.listdir(runs_dir) if x != ""meta.yaml""]
for run_id in runs:
    print(f""Metric files for {run_id}"")
    metrics_path = os.path.join(runs_dir, run_id, ""metrics"")
    print(*os.listdir(metrics_path), sep=""\n"")
    print(""=""*16)
```
Running the above script should confirm that two separate runs are created, and that metrics files are placed in each.

### Expected behavior

I expected that all logging would be pushed to the existing run ID (unless there is a reason that this is not the case).

Apologies if this is documented behaviour, but I could not find a reason.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Multiple training runs not working with deepspeed,"### System Info

- `transformers` version: 4.46.1
- Platform: Linux-5.15.0-126-generic-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    - compute_environment: LOCAL_MACHINE
        - distributed_type: MULTI_GPU
        - mixed_precision: fp16
        - use_cpu: False
        - debug: False
        - num_processes: 4
        - machine_rank: 0
        - num_machines: 1
        - gpu_ids: 0,1,2,3
        - rdzv_backend: static
        - same_network: True
        - main_training_function: main
        - enable_cpu_affinity: True
        - downcast_bf16: no
        - tpu_use_cluster: False
        - tpu_use_sudo: False
        - tpu_env: []
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA H100 80GB HBM3


### Who can help?

@muellerzr

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Hi,

I'm working on a setup where I have a frozen language model persisted in GPU memory and can fine-tune arbitrary many adapters during run time. So my ""script"" is a server that receives finetune requests through some REST api. Each time a finetune request is received, I'll create a new peft model for my base model, create a new train dataset, create a new HF trainer object, train and then save the adapter to disk. 
This is basically working fine but as soon as I'm providing a deepspeed config, I'm getting the following exception on the **second** training run:
```
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 431, in __init__
[rank0]:     self.create_accelerator_and_postprocess()
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 4953, in create_accelerator_and_postprocess
[rank0]:     self.accelerator = Accelerator(**args)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/accelerate/accelerator.py"", line 305, in __init__
[rank0]:     raise NotImplementedError(
[rank0]: NotImplementedError: You cannot pass in a `deepspeed_plugin` when creating a second `Accelerator`. Please make sure the first `Accelerator` is initialized with all the plugins you want to use.
```
Obviously this is because the accelerate state is stored in some singletons. So I've tried to reset that state after each training run (in my own script) with these calls before creating the new trainer object:
```
AcceleratorState._reset_state(True)
GradientState._reset_state()
```

This however will lead to the following exception during the second training run:
```
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 2122, in train
[rank0]:     return inner_training_loop(
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 2474, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 3572, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:   File ""/somewhere/3rdparty-llama-factory/src/llamafactory/train/sft/trainer.py"", line 88, in compute_loss
[rank0]:     loss = super().compute_loss(model, inputs, return_outputs, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 3625, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py"", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/deepspeed/runtime/engine.py"", line 1846, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/peft/peft_model.py"", line 1577, in forward
[rank0]:     return self.base_model(
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py"", line 188, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py"", line 1164, in forward
[rank0]:     outputs = self.model(
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py"", line 854, in forward
[rank0]:     inputs_embeds = self.embed_tokens(input_ids)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1779, in inner
[rank0]:     args_result = hook(self, args)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py"", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py"", line 278, in _pre_forward_module_hook
[rank0]:     self.pre_sub_module_forward_function(module)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py"", line 452, in pre_sub_module_forward_function
[rank0]:     param_coordinator.fetch_sub_module(sub_module, forward=True)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 632, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py"", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File ""/somewhere/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py"", line 317, in fetch_sub_module
[rank0]:     assert param.ds_status == ZeroParamStatus.AVAILABLE, param.ds_summary()
[rank0]: AssertionError: {'id': 0, 'status': 'INFLIGHT', 'numel': 136134656, 'ds_numel': 136134656, 'shape': (151936, 896), 'ds_shape': (151936, 896), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': {1308}, 'ds_tensor.shape': torch.Size([34033664])}
```



### Expected behavior

See above","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
issues when i change the lm_head to a 32 node layer,"### System Info

- `transformers` version: 4.46.3
- Platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.17
- Python version: 3.8.20
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.4.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA RTX A4000


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
import torch
from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, T5Config
from datasets import Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
from tokenizer import SourceFuzzyTokenizer as SourceTokenizer, TargetSolidTokenizer as TargetTokenizer

# 路径和超参数
source_vocab_file = ""./datasets/tokenizer/source_vocab.json""  # 输入词表文件路径
target_vocab_file = ""./datasets/tokenizer/target_vocab.json""  # 输出词表文件路径
data_file_fuzzy = ""/home/zhi/Desktop/fuzzy_complement/datasets/raw/9specie_fuzzy_seqs.txt""  # 输入数据文件
data_file_gt = ""/home/zhi/Desktop/fuzzy_complement/datasets/raw/9specie_gt_seqs.txt""       # 目标数据文件
model_name = ""./checkpoint-1817245""                                             # 模型路径
max_length = 50
batch_size = 80
num_epochs = 100
learning_rate = 8e-5
source_vocab_size = 59746
target_vocab_size = 32
output_dir = ""./results""


# train from stratch
config = T5Config(
    vocab_size=source_vocab_size,  # 输入词汇表大小
    decoder_start_token_id=0,
    eos_token_id=1,
    pad_token_id=0,
    d_model=512,  # 模型维度
    num_decoder_layers=6,
    num_encoder_layers=6,
)

# 加载数据
with open(data_file_fuzzy, ""r"") as fuzzy_file, open(data_file_gt, ""r"") as gt_file:
    fuzzy_seqs = fuzzy_file.read().splitlines()
    gt_seqs = gt_file.read().splitlines()

assert len(fuzzy_seqs) == len(gt_seqs), ""fuzzy_seqs.txt and gt_seqs.txt do not MATCH!""

data = {""input"": fuzzy_seqs, ""target"": gt_seqs}
df = pd.DataFrame(data)

# 划分数据集
train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)
train_dataset = Dataset.from_pandas(train_df)
eval_dataset = Dataset.from_pandas(eval_df)

# splite Tokenizer
source_tokenizer = SourceTokenizer(source_vocab_file)
target_tokenizer = TargetTokenizer(target_vocab_file)

# use pretrained model
# model = AutoModelForSeq2SeqLM.from_pretrained(model_name, ignore_mismatched_sizes=True)
# use from_stratch model
model = AutoModelForSeq2SeqLM.from_config(config)
# model = AutoModelForSeq2SeqLM.from_pretrained(""t5-base"")

# change model's embedding layer
model.encoder.embed_tokens = torch.nn.Embedding(source_vocab_size, model.config.d_model)
model.decoder.embed_tokens = torch.nn.Embedding(target_vocab_size, model.config.d_model)

# init the weight of model
model.encoder.embed_tokens.weight.data.normal_(mean=0.0, std=model.config.initializer_factor)
model.decoder.embed_tokens.weight.data.normal_(mean=0.0, std=model.config.initializer_factor)

model.lm_head = torch.nn.Linear(model.config.d_model, target_vocab_size, bias=False)
model.lm_head.weight = model.decoder.embed_tokens.weight

model.config.vocab_size = source_vocab_size
model.config.decoder_vocab_size = target_vocab_size

def preprocess_data(examples):
    inputs = [source_tokenizer.tokenize(seq) for seq in examples[""input""]]
    targets = [target_tokenizer.tokenize(seq) for seq in examples[""target""]]

    input_ids = [source_tokenizer.convert_tokens_to_ids(tokens)[:max_length] for tokens in inputs]
    target_ids = [target_tokenizer.convert_tokens_to_ids(tokens)[:max_length] for tokens in targets]

    pad_id_source = source_tokenizer.vocab.get(""[PAD]"", 0)
    pad_id_target = target_tokenizer.vocab.get(""[PAD]"", 0)
    input_ids = [seq + [pad_id_source] * (max_length - len(seq)) for seq in input_ids]
    target_ids = [seq + [pad_id_target] * (max_length - len(seq)) for seq in target_ids]

 
    attention_mask = [[1 if token != pad_id_source else 0 for token in seq] for seq in input_ids]

    return {
        ""input_ids"": input_ids,
        ""attention_mask"": attention_mask,
        ""labels"": target_ids,
    }

train_dataset = train_dataset.map(preprocess_data, batched=True)
eval_dataset = eval_dataset.map(preprocess_data, batched=True)


training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy=""epoch"",
    learning_rate=learning_rate,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    save_strategy=""epoch"",
    save_total_limit=2,
    logging_dir=""./logs"",
    logging_steps=10,
    evaluation_strategy=""epoch"",
    load_best_model_at_end=True,
    dataloader_num_workers=1,
    fp16=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()

### Expected behavior

My task is to translate a fuzzy sequence into a solid sequence, which involves peptide sequences. When I used T5-base without distinguishing between the source and target dictionaries, everything worked fine. However, I noticed that the solid sequence contains only 32 unique characters. To optimize, I separated the source and target character sets and adjusted the lm_head to match the 32 characters. Training worked fine, and the model converged. But during inference, I encountered an issue where the weight sizes do not match, even though I had adjusted the size during training. What could be causing this, and how can I resolve it?

here is the error message:
python bs_based_on_massdic.py 
Traceback (most recent call last):
  File ""bs_based_on_massdic.py"", line 100, in <module>
    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path, ignore_mismatched_sizes=True)
  File ""/home/zhi/anaconda3/envs/peptide_completion/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py"", line 564, in from_pretrained
    return model_class.from_pretrained(
  File ""/home/zhi/anaconda3/envs/peptide_completion/lib/python3.8/site-packages/transformers/modeling_utils.py"", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File ""/home/zhi/anaconda3/envs/peptide_completion/lib/python3.8/site-packages/transformers/modeling_utils.py"", line 4785, in _load_pretrained_model
    raise RuntimeError(f""Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}"")
RuntimeError: Error(s) in loading state_dict for T5ForConditionalGeneration:
	size mismatch for shared.weight: copying a param with shape torch.Size([59750, 512]) from checkpoint, the shape in current model is torch.Size([59746, 512]).
	size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([59746, 512]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",0,open
[WIP] Refactoring of ImageProcessorFast,"# What does this PR do?

This PR introduces a significant refactoring of how fast image processors can be implemented in Transformers.

## Motivation

The primary goal of this refactoring is to simplify the process of creating fast image processors when a slow image processor already exists. 

Unlike the current `BaseImageProcessor`, which provides only a minimal skeleton for image processor classes, the newly introduced `BaseImageProcessorFast` includes all the basic functionalities needed by a typical image processor. This design allows contributors to focus primarily on modifying default parameters, such as the image mean, std, or default resizing dimensions, rather than rewriting foundational code.

### Key Advantages:
- **Ease of Contribution**: Contributors no longer need to rely on copy-paste from an arbitrary slow image processor (which I feel is what is happening currently for some slow image processors). Instead, the `BaseImageProcessorFast` provides a natural starting point with predefined functionalities.
- **Consistency**: Contributors are encouraged to use a common structure. Whether they only modify default parameters, leverage mixins, or add custom code, they are likely to follow a consistent syntax and logic.
- **Automatic Optimizations**: Improvements made to `BaseImageProcessorFast` are automatically propagated to all derived fast image processors.
- **Reduced Diffs**: The new approach minimizes added diffs compared to the existing ""# Copied from"" philosophy in slow image processors. While the ""repeat yourself"" philosophy is an important part of modeling in Transformers, I feel that it might not be as necessary for image processing, as the model's uniqueness is rarely found in the image processing logic.

## Implementation

### Functional or class transforms

Following the `torchvision` approach to defining image transforms, there are two main ways to write the processing logic for image processors: using **functional transforms** or **class-based transforms**.

This PR showcases both approaches:

- [`BaseImageProcessorFast`](https://github.com/huggingface/transformers/blob/9fd8f141fbb8d0be6642871a4f9cda0f30e8a7f8/src/transformers/image_processing_utils_fast.py): implemented using functional transforms.
- [`ViTImageProcessorFast`](https://github.com/huggingface/transformers/blob/9fd8f141fbb8d0be6642871a4f9cda0f30e8a7f8/src/transformers/models/vit/image_processing_vit_fast.py): almost equivalent but uses piped class-based transforms.
- [`LlavaNextImageProcessorFast`](https://github.com/huggingface/transformers/blob/9fd8f141fbb8d0be6642871a4f9cda0f30e8a7f8/src/transformers/models/llava_next/image_processing_llava_next_fast.py): implemented using functionals.
- [`LlavaOnevisionImageProcessorFast`](https://github.com/huggingface/transformers/blob/9fd8f141fbb8d0be6642871a4f9cda0f30e8a7f8/src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py): implemented using a mix of class-based transforms and functionals. equivalent to `LlavaNextImageProcessorFast`


**The choice is entirely open for debate at this point. I see advantages to both approaches, but I’m sure I haven’t considered everything, so please share your thoughts if you have a preference one way or the other.**

To me, the advantages/drawbacks of functionals are the following:
- 🟢 Less abstraction so potentially easier to read
- 🟢 Easier for contributors to write or adapt from existing slow image processors (which currently use functionals).
- 🟢 Allows more flexibility in processing logic, as transforms do not need to be sequential. For more complex processors, using piped class-based transforms would likely require mixing functionals and class-based transforms, or adding logic outside the transforms pipeline instead of a simple one-liner.
- 🔴 The logic can be more verbose than for class transforms

For class transforms:
- 🟢 Aligns with practices in other libraries like Albumentations.
- 🟢 Generally cleaner and more structured, easier to add/remove simple transforms.
- 🔴 As mentioned before, the logic is restricted by the sequential nature of the pipeline. For complex processors (e.g., involving patching), mixing functionals and class-based transforms or adding logic around the pipeline seems unavoidable (as seen in `LlavaOnevisionImageProcessorFast`).
- 🔴 There appear to be compilation issues: `LlavaOnevisionImageProcessorFast` fails to compile, as it seemingly gets stuck in an infinite compilation loop, while its functional equivalent, `LlavaNextImageProcessorFast`, compiles without problems. Hover this needs more thorough investigation.

### New `add-fast-image-processor` Command in `transformers-cli`

This PR introduces a new CLI command that automates the creation of fast image processors. Given a model folder name, it generates all necessary imports, documentation, dummy objects, and a fast image processor file where default parameters are parsed from the slow image processor file.
The new fast image processor class is also added to the image processor test file by the CLI. However, some tests may still need manual adjustments to properly include and validate the new fast image processor class.

#### Example Usage:
```bash
transformers-cli add-fast-image-processor --model-name blip
```

#### Example Output:
In `transformers/src/models/blip/image_processing_blip_fast.py`:
```python
# coding=utf-8
# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Fast Image processor class for BLIP.""""""

from ...image_processing_utils_fast import BaseImageProcessorFast
from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling


class BlipImageProcessorFast(BaseImageProcessorFast):
    r""""""
    Constructs a fast BLIP image processor.

    Args:
        do_resize (`bool`, *optional*, defaults to `True`):
            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the
            `do_resize` parameter in the `preprocess` method.
        size (`dict`, *optional*, defaults to `{""height"": 384, ""width"": 384}`):
            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`
            method.
        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):
            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be
            overridden by the `resample` parameter in the `preprocess` method.
        do_rescale (`bool`, *optional*, defaults to `True`):
            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the
            `do_rescale` parameter in the `preprocess` method.
        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):
            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be
            overridden by the `rescale_factor` parameter in the `preprocess` method.
        do_normalize (`bool`, *optional*, defaults to `True`):
            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.
        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):
            Mean to use if normalizing the image. This is a float or list of floats the length of the number of
            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be
            overridden by the `image_mean` parameter in the `preprocess` method.
        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):
            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.
            Can be overridden by the `image_std` parameter in the `preprocess` method.
        do_convert_rgb (`bool`, *optional*, defaults to `True`):
            Whether to convert the image to RGB.
    """"""

    # To be checked against the slow image processor
    # None values left after checking can be removed
    resample = PILImageResampling.BICUBIC
    image_mean = OPENAI_CLIP_MEAN
    image_std = OPENAI_CLIP_STD
    size = {""height"": 384, ""width"": 384}
    default_to_square = None
    crop_size = None
    do_resize = True
    do_center_crop = None
    do_rescale = True
    do_normalize = True
    do_convert_rgb = True

```
In this case, this is enough to get a fully working fast Blip image processor!

### New Mixins for Common Logic

To handle shared preprocessing and post-processing logic, this PR introduces reusable mixins ~~(only `LlavaPatchingMixin` is present as an example in this PR)~~. Additional mixins are planned for other common patterns, such as:
- Video processing
- DETR-like processing
- Segmentation post-processing
- Depth estimation post-processing

**Edit: Removing the Mixins for patching in favor of `# Copied from` or modular in the futur, as such preprocessing techniques don't usually stick for a long time, and adding Mixins every time a technique is used twice or more wouldn't scale well.**

### Summary: Three Types of Fast Image Processors

1. **Basic Processors**:
   - These support standard operations like resizing, rescaling, normalizing, and cropping.
   - They require minimal customization, mostly overriding default parameters.  
   - Examples in this PR: `blip`, `clip`, `deit`, `siglip`, and `vit`.

~~2. **Mixin-Based Processors**:~~
   ~~- These rely heavily on predefined mixins to implement shared logic.~~
   ~~- Examples in this PR: `llava_next` and `llava_onevision`.~~

**Edit: Removing Mixins in favor of `# Copied From` or modular for now, see earlier comment.**

3. **Exotic Processors**:
   - These have unique processing logic that differs significantly from the base class or existing mixins.  
   - Contributors need to override functions like `__init__` and `preprocess` while reusing the syntax and structure of `BaseImageProcessorFast`.
   - Example in this PR: `convnext`

## Miscellaneous Issues and Questions

- The CLI currently needs an existing slow image processor to work. If the library aims to eventually fully deprecate slow image processors or at least for new models, this will need to change, and the CI should maybe be integrated into the add-new-model-like.
- There is a significant design difference between slow and fast image processors. For slow processors, contributors need to rewrite most of the logic, while for fast processors, the goal is to rewrite as little code as possible. This might get confusing for contributors, especially since as mentioned in the previous point, users will most likely start with writing a slow image processor.
- Adding lots of mixins to `image_processing_utils_fast` might make the file large and difficult to read. This also raises the question of when a mixin should be created for repeated patterns across image processors. Should it be created when a pattern is shared by two or more models? Or when it presents an idea likely to be reused in the future?
- Padding functionality is currently not included in `BaseImageProcessorFast` because the image processors that use padding implement it in slightly different ways. This makes it challenging to standardize the padding logic. A more consistent approach should maybe be added in the future.
- The `center_crop` functions in Transformers' `image_transforms` and in `torchvision` have different implementations, namely the cropping boundaries in transformers are defined as: `top = (orig_height - crop_height) // 2`, `left = (orig_width - crop_width) // 2` and `top = int(round((orig_height - crop_height) / 2.0))`, `left = int(round((orig_width - crop_width) / 2.0))`, which can result in shifted cropping when the size before cropping is odd. I don't think this should be much of a problem for users, but when comparing outputs of slow and fast image processors in tests, this results in huge differences.

## Who can review?

@qubvel @molbap @zucchini-nlp 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
bitsandbytes: simplify 8bit dequantization,"# What does this PR do?

Simplifies the dequantization for bitsandbytes int8 weights. There is a similar PR open in [huggingface/peft#2245](https://github.com/huggingface/peft/pull/2245/files).

In the upcoming bitsandbytes release we will have an added API for for int8 dequantization. For backwards compatibility, a simplified (but functionally equivalent) dequantization operation is performed.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@SunMarc @BenjaminBossan 


","[{'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}]",2,open
Fix case of nested tensors in BatchMixFeature,"Huge kudos to @zucchini-nlp for figuring this one out - we were getting intermittent failures and I couldn't figure out why. The reason was that `BatchMixFeature.to()` was breaking the structure of some inputs, because it used a nested list comprehension that assumed a specific list depth that wasn't always respected by `PixtralProcessor`.

This PR replaces the slightly hacky `.to()` code with a recursion that should preserve nesting structure regardless of list depth.

TODO:
- [ ] Compare against the VLLM implementation of Pixtral to see how they handle this (if their code supports batched?)","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",5,open
SequenceClassification for all Model types should have the option to add weights in Cross Entropy loss,"### Feature request

Qwen2ForSequenceClassification and all other SequenceClassification wrappers for transformer models don't have the possibility to add class weights. This leads to not using the implementation. 

### Motivation

We need it, because we have highly imbalanced data

### Your contribution

I could submit a PR but this is probably a bigger architectural topic","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Use AMD CI workflow defined in hf-workflows,"# What does this PR do?

Uses external workflows defined in hf-workflows for self hosted AMD runners. This removes a possible attack vector on self hosted CI runners on github.",[],3,open
"Get ""NotImplementedError: Cannot copy out of meta tensor; no data!"" error while deploying model","### System Info

- `transformers` version: 4.46.3
- Platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.17
- Python version: 3.8.19
- Huggingface_hub version: 0.25.1
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.4.1+cu118 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA RTX A6000


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Hi Developers,

I have finetuned a Llama-3.1-8b-Instruct model; everything is fine in the fine-tuning stage. However, I often get the following error message when I deploy my model with Langchain.

Here is the error message
```
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback:
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/exec_code.py"", line 88, in exec_func_with_error_handling
    result = func()
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py"", line 579, in code_to_exec
    exec(code, module.__dict__)
File ""/home/revlis_ai/Documents/llm_practise/lora_finetune_llm/app3.py"", line 50, in <module>
    model = AutoModelForCausalLM.from_pretrained(
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py"", line 564, in from_pretrained
    return model_class.from_pretrained(
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/transformers/modeling_utils.py"", line 4310, in from_pretrained
    model.load_adapter(
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/transformers/integrations/peft.py"", line 214, in load_adapter
    inject_adapter_in_model(peft_config, self, adapter_name, **peft_load_kwargs)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/mapping.py"", line 227, in inject_adapter_in_model
    peft_model = tuner_cls(model, peft_config, adapter_name=adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/tuners/lora/model.py"", line 141, in __init__
    super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/tuners/tuners_utils.py"", line 184, in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/tuners/tuners_utils.py"", line 496, in inject_adapter
    self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/tuners/lora/model.py"", line 230, in _create_and_replace
    self._replace_module(parent, target_name, new_module, target)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/tuners/lora/model.py"", line 254, in _replace_module
    new_module.to(child.weight.device)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1174, in to
    return self._apply(convert)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 780, in _apply
    module._apply(fn)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 780, in _apply
    module._apply(fn)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 805, in _apply
    param_applied = fn(param)
File ""/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1167, in convert
    raise NotImplementedError(
```

And here is my code to deploy my app with Langchain
```
#%% Import Libraries

import streamlit as st
from uuid import uuid4
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import create_retrieval_chain, create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.vectorstores import FAISS
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline, ChatHuggingFace
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig, TextStreamer
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.runnables.history import RunnableWithMessageHistory
import torch
import os
from pathlib import Path
from dotenv import load_dotenv

#%% Get Correct Path

current_dir = Path(__file__).parent.absolute()
doc_dir = os.path.join(current_dir, ""research_papers"")
db_path = os.path.join(current_dir, ""FAISS_DB"")

#%% Load API Key

if 'STREAMLIT_PUBLIC_PATH' in os.environ:
    os.environ[""HF_TOKEN""] = st.secrets['HUGGINGFACE_TOKEN']
else:
    load_dotenv()
    os.environ[""HF_TOKEN""] = os.getenv('HUGGINGFACE_TOKEN')

#%% Load LLM and Embedding

embeddings = HuggingFaceEmbeddings(
    model_name='all-MiniLM-L6-v2',)

# bnb_config = BitsAndBytesConfig(
#     load_in_4bit = True, 
#     bnb_4bit_quant_type = ""nf4"", 
#     bnb_4bit_compute_dtype = torch.bfloat16,
#     bnb_4bit_use_double_quant = True,)
bnb_config = BitsAndBytesConfig(
    load_in_8bit = True,)
tokenizer = AutoTokenizer.from_pretrained(
    ""./lora_finetune_llm/llm_finetune/checkpoint-2571"")
streamer = TextStreamer(tokenizer)
model = AutoModelForCausalLM.from_pretrained(
    ""./lora_finetune_llm/llm_finetune/checkpoint-2571"", 
    quantization_config = bnb_config)

llm_pipeline = pipeline(
    ""text-generation"", 
    model = model, 
    tokenizer = tokenizer, 
    streamer = streamer,
    torch_dtype = torch.bfloat16,
    temperature = 0.15,
    top_p = .15,
    max_new_tokens = 512,
    trust_remote_code = True,
    return_full_text = False,)
hf_pipeline = HuggingFacePipeline(pipeline=llm_pipeline)
llm = ChatHuggingFace(llm=hf_pipeline, tokenizer=hf_pipeline.pipeline.tokenizer)

#%% Initialize session state

if 'conversations' not in st.session_state:
    st.session_state.conversations = {}
    st.session_state.current_session = None
    st.session_state.session_history = {}

#%% Build Streamlit APP

st.title('ChatBot Q&A with RAG')
st.sidebar.title(""Conversations"")

def new_chat():
    session_id = str(uuid4()).replace('-', '')
    st.session_state.conversations[session_id] = {
        ""history"": ChatMessageHistory(),
        ""uploaded_files"": []}
    st.session_state.current_session = session_id

if not st.session_state.conversations:
    new_chat()

if st.sidebar.button(""New Chat""):
    new_chat()

session_ids = list(st.session_state.conversations.keys())
session_names = [f""Session {i+1}"" for i in range(len(session_ids))]
selected_session = st.sidebar.radio(
    ""Select Conversation"",
    session_names,
    index=session_ids.index(st.session_state.current_session))

if selected_session:
    selected_index = session_names.index(selected_session)
    st.session_state.current_session = session_ids[selected_index]

current_session = st.session_state.current_session
conversation_data = st.session_state.conversations[current_session]

uploaded_files = st.file_uploader(
    'Upload PDF Files',
    type=['pdf'],
    accept_multiple_files=True,
    key=st.session_state.current_session)

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    return st.session_state.conversations[session_id][""history""]

if uploaded_files:
    documents = []
    for f in uploaded_files:
        if f.name not in conversation_data[""uploaded_files""]:
            conversation_data[""uploaded_files""].append(f.name)
            temp = './temp.pdf'
            with open(temp, 'wb') as file:
                file.write(f.getvalue())
            loader = PyPDFLoader(temp)
            doc = loader.load()
            documents.extend(doc)
            os.remove(temp)

    splitter = RecursiveCharacterTextSplitter(chunk_size=5140, chunk_overlap=512)

    try:
        st.write('Trying to load FAISS BD')
        faiss_db = FAISS.load_local(
            db_path + f""/{current_session}"", 
            embeddings, 
            allow_dangerous_deserialization = True)
        if documents:
            st.write('Detect new file(s) upload, add docs into Chroma DB')
            chunked_documents = splitter.split_documents(documents)
            faiss_db.add_documents(chunked_documents)
            faiss_db.save_local(db_path + f""/{current_session}"")    
        st.write('FAISS DB loaded!!!')
    except:
        st.write('No FAISS DB found, creating FAISS DB.......')
        chunked_documents = splitter.split_documents(documents)
        faiss_db = FAISS.from_documents(chunked_documents, embeddings)
        faiss_db.save_local(db_path + f""/{current_session}"")
        st.write('FAISS DB Created!!!')

st.write(f""Uploaded Files: {conversation_data['uploaded_files']}"")

user_input = st.text_area(""Ask me your question:"", key=""input_text"",)
if st.button(""Submit""):
    if user_input:

        faiss_db = FAISS.load_local(
            db_path + f""/{current_session}"", 
            embeddings, 
            allow_dangerous_deserialization = True)
        retriever = faiss_db.as_retriever()

        contextual_sys_prompt = '''
            Given a chat history and the latest user question which might reference context in the chat history, 
            formulate a standalone question which can be understood without the chat history. Do NOT answer the question, 
            just reformulate it if needed and otherwise return it as is.'''
        contextual_prompt = ChatPromptTemplate.from_messages(
            [('system', contextual_sys_prompt),
                MessagesPlaceholder('chat_history'),
                ('human', '{input}')])
        history_aware_retriever = create_history_aware_retriever(llm, retriever, contextual_prompt)

        sys_prompt = '''
            You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. 
            If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.
            \n\n
            {context}'''
        qa_prompt = ChatPromptTemplate.from_messages(
            [('system', sys_prompt),
                MessagesPlaceholder('chat_history'),
                ('human', '{input}')])

        qa_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)

        conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            get_session_history,
            input_messages_key=""input"",
            history_messages_key=""chat_history"",
            output_messages_key=""answer"",)
        st.session_state.session_history[current_session] = get_session_history(current_session)
        with torch.autocast(""cuda""):
            ans = conversational_rag_chain.invoke(
                {'input': user_input},
                config={'configurable': {'session_id': current_session}})
        st.write(""Assistant:"", ans['answer'])

if current_session in st.session_state.session_history:
    st.write(""Chat History:"")
    for message in st.session_state.session_history[current_session].messages:
        st.write(f""Role: {message.type}, Content: {message.content}"")
else:
    st.write(""No chat history available."")
```

I also tried the original open-sourced model from Meta with the following code, yet this issue still happens.

Here is the code form using the original model
```
bnb_config = BitsAndBytesConfig(
    load_in_8bit = True, )
tokenizer = AutoTokenizer.from_pretrained(
    ""meta-llama/Llama-3.1-8B-Instruct"", )
streamer = TextStreamer(tokenizer)
model = AutoModelForCausalLM.from_pretrained(
    ""meta-llama/Llama-3.1-8B-Instruct"",
    quantization_config = bnb_config,)
```

How to reproduce this issue:
1. Execute `streamlit run ./path/to/the/file.py`
2. Upload article(s) in pdf format via browser
3. Ask your questions. 
4. The error message shall pop out after some interactions. Sometimes, the error pops out right after the first question is asked.

I believe this issue does not result from GPU OOM. I was monitoring the GPU memory usage via nvtop, and I still had around 20-ish GB remaining when the error popped out.

### Expected behavior

I shall consistently get answers from LLM when I have enough GPU memory. However, my app is very unstable right now. Sometimes, the app seems to work fine, and then the error message suddenly pops out after a new question is submitted. ","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Add: num_additional_image_tokens to models,"# What does this PR do?

In PR #33424, we resolved issue #34447 by adding `num_additional_image_tokens` to the processor.

However, the additional tokens are only considered in the processor, and since they are not accounted for in the modeling code, some users are still encountering an ""img token mismatch error"".

To address this problem, i have added code to the modeling code to also consider `num_additional_image_tokens`.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@zucchini-nlp

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],10,open
ImportError: cannot import name 'HfApiEngine' from 'transformers',"### System Info

Please update tutorials cause you confusing people: https://huggingface.co/docs/transformers/agents

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Make update for documentation

### Expected behavior

Make update for documentation","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Adding logger.info about update_torch_dtype in some quantizers,"# What does this PR do?
Adding missing `logger.info` statements in some quantizers to inform users which dtype is set during quantization.

## Who can review ?
@SunMarc ",[],1,open
Add `position_ids` in `XLMRobertaXLForCausalLM.prepare_inputs_for_generation`,"# What does this PR do?

The model component `XLMRobertaXLEmbeddings` will perform

> position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)

if `position_ids` is not passed to the model (say `XLMRobertaXLForCausalLM`).

However, during the decoding, even if `past_key_values_length` is passed, the `create_position_ids_from_input_ids` will compute the wrong `position_ids` when the `input_ids` is only the single last token. This is because `create_position_ids_from_input_ids` relies the whole  `input_ids` in order to get the number of padding tokens:

>     mask = input_ids.ne(padding_idx).int()

(since `past_key_values_length` doesn't contain this information.)

In general, we need to prepare `position_ids` in `prepare_inputs_for_generation`, like what has been done in

> GenerationMixin.prepare_inputs_for_generation

This PR adds this for `XLMRobertaXLForCausalLM` (whose `prepare_inputs_for_generation` is already overwritten from the parent class)

---------------------------------------------------------------------------------------------------------------------------------

As a bonus:

> tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py::XLMRobertaXLModelTest::test_assisted_decoding_matches_greedy_search_1_same

is also no longer flaky (running 10000 times) while it was failing 5% of the time before this PR.",[],1,open
Enable Quantize KV Cache for Mistral Model,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #35041


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

I followed the format of #30483, and don't think new documentation or tests are necessary for enabling KV quantization on a single model. Please let me know if I'm wrong.

## Who can review?

@zucchini-nlp

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
Enable Quantize KV Cache for Mistral Model,"### Feature request

Enable Quantize KV Cache for Mistral Model, as described in #30483.

### Motivation

KV cache quantization has emerged as a crucial optimization, particularly in high-throughput, multi-user scenarios, where efficiency is paramount.

Hugging Face currently leads in supporting quantization across a wide range of models (thanks to the effort of @zucchini-nlp), the widely used Mistral model remains unsupported. This gap presents an opportunity to extend quantization support to Mistral, addressing a significant need in the community.

In addition to this change, I am also interested in enabling kv cache quantization for more models, such as Qwen and Phi. Understanding the detailed requirements and best practices for these types of contributions would be incredibly helpful.

### Your contribution

I am interested in submitting a pull request but am uncertain about the best way to verify that the behavior aligns with expectations.

In short, I tried to make a modification by naively adding the `_supports_quantized=True` flag to the `MistralPreTrainedModel` class [around here](https://github.com/huggingface/transformers/blob/c24c79ebf91f6f04faf287997848ed6e64d78899/src/transformers/models/mistral/modeling_mistral.py#L587):

https://github.com/huggingface/transformers/blob/c24c79ebf91f6f04faf287997848ed6e64d78899/src/transformers/models/mistral/modeling_mistral.py#L587

When testing this change informally, it seemed to work as intended, producing slightly different (but still coherent) outputs compared to full precision at zero temperature, which I believe is expected.

I want to ensure that any modifications I make meet the expected standards and don't inadvertently cause issues. If my informal testing is sufficient for this case, I am happy to proceed with submitting the PR. If more rigorous validation is needed, any guidance or resources would be appreciated.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Object detection evaluator,"# What does this PR do?

Refactor the Object Detection example evaluation strategy. Instead of accumulating boxes during evaluation, we switch to the `--batch_eval_metric true` option and compute metrics at each batch.

Fixes #34675 

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1936351150, 'node_id': 'MDU6TGFiZWwxOTM2MzUxMTUw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Examples', 'name': 'Examples', 'color': 'd4c5f9', 'default': False, 'description': 'Which is related to examples in general'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",2,open
VLlama3ForCausalLM in SmolVLM,"In the SmolVLM [config](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/blob/77f80ed86046795eab54cfb60d9442c2ad813f57/config.json)  there is **VLlama3ForCausalLM**. Any plans on adding this to the library?

Regards,
",[],4,open
"Fix tokenizer model handling for LLaMA, converting Base64 to SentencePiece (#35037)","# What does this PR do?

Fixes #35037


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker 
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->",[],1,open
LlamaTokenizer being recognized as a bool,"### System Info

When initializing LlamaTokenizer from the Transformers library, the tokenizer is being recognized as a bool. This issue persists across different environments and Python versions.

Steps to Reproduce:
Install the required libraries:
pip install transformers torch sentencepiece

Use the following script to initialize the tokenizer:
from transformers.models.llama import LlamaTokenizer

model_path = ""C:/Users/spger/.llama/checkpoints/Llama3.1-70B""

try:
    tokenizer = LlamaTokenizer.from_pretrained(model_path, use_fast=True, legacy=False)
    print(""Tokenizer initialized successfully."")
    print(""Tokenizer type:"", type(tokenizer))
except Exception as e:
    print(""Error initializing tokenizer:"", e)

Observed Output:
The tokenizer type is <class 'bool'> instead of the expected tokenizer class.

System Info:
transformers version: 4.46.3

Platform: Windows-10-10.0.26100-SP0

Python version: 3.11.9

Huggingface_hub version: 0.26.3

Safetensors version: 0.4.5

Accelerate version: not installed

Accelerate config: not found

PyTorch version (GPU?): 2.5.1+cpu (False)

Tensorflow version (GPU?): not installed (NA)

Flax version (CPU?/GPU?/TPU?): not installed (NA)

Jax version: not installed

JaxLib version: not installed

Using distributed or parallel set-up in script?: No

Additional Details:
Other tokenizers like AutoTokenizer for GPT-2 and BERT initialize correctly.


### Who can help?

@ArthurZucker @itazap


### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

from transformers.models.llama import LlamaTokenizer

model_path = ""C:/Users/spger/.llama/checkpoints/Llama3.1-70B""

try:
    tokenizer = LlamaTokenizer.from_pretrained(model_path, use_fast=True, legacy=False)
    print(""Tokenizer initialized successfully."")
    print(""Tokenizer type:"", type(tokenizer))
except Exception as e:
    print(""Error initializing tokenizer:"", e)

Steps to reproduce the behavior:
1. Install the required libraries:
   ```bash
   pip install transformers torch sentencepiece
2. Run the provided script.
3. Observe that the tokenizer is of type bool.



### Expected behavior


#### Expected behavior:
```markdown
I expect the `LlamaTokenizer` to be correctly initialized and recognized as a `LlamaTokenizer` object instead of a `bool`.
","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Make it possible to save and evaluate checkpoint on CTRL+C / `KeyboardInterrupt` with Hugging Face Trainer,"### Feature request

I would like to request that one or more optional flags be added to the Hugging Face Trainer, perhaps named `save_on_exit`/`save_on_interrupt` and `eval_on_exit`/`eval_on_interrupt`, to ensure that a checkpoint is always saved upon CTRL+C or perhaps even a kill command sent via `wandb`.

### Motivation

Extremely often I will be training a model but then need to utilise the GPU on which I am conducting the training and so I need to pause my training. For example, over the past month, I have been training models on my PC but often I have needed to use my GPU and so I have exited my training and then resumed it at night.

Because, at present, it is not possible to have the Hugging Face Trainer save a checkpoint upon recieving a `KeyboardInterrupt`, I end up needing to save checkpoints at excessively short intervals to minimise lost progress if I quit training at any arbitrary point. This invariably still ends up with some amount of progress being lost and it also does a lot of write wear on my SSDs, which, like all hard drives, have a limited write lifetime. The wear can in fact add up to quite a lot of writing if you are saving multigigabyte models.

By allowing for progress to automatically save upon exit, I can be assured that, barring an unexpected system crash, or repeated CTRL+Cs being sent, my progress will always be saved and so I do not need to save and evaluate checkpoints so frequently.

### Your contribution

N/A","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Unexpected output of _flash_attention_forward() for cross attention,"### System Info

My environment:

> transformers              4.44.1
> flash-attn                2.6.3



### Who can help?

_No response_

### Information

- [ ] My own modified scripts

### Tasks

- [ ] My own task or dataset (give details below)

### Reproduction

```py
import torch
from transformers.modeling_flash_attention_utils import _flash_attention_forward

query = torch.randn(4, 20,1, 32).cuda().half()
key = torch.randn(4, 10, 1, 32).cuda().half()
value = key

unmasked = _flash_attention_forward(query,key,value,
                                    attention_mask=None, 
                                    query_length=20, 
                                    is_causal=False)
masked = _flash_attention_forward(query,key,value, 
                                    attention_mask=torch.ones((4, 10)).cuda().bool(), 
                                    query_length=20, 
                                    is_causal=False)
breakpoint()
```


### Expected behavior

In my understanding, the attention_mask has a size of `(batch_size, seq_len)` where 1 stands for the position of non-padding tokens, so an all-one mask should lead to the same result as no mask provided. However, the outputs are significantly different.
```
(Pdb) print(unmasked, masked)
tensor([[[[ 0.4114, -0.3369,  0.6221,  ..., -0.4475, -0.2361,  0.3022]],

         [[ 0.2480, -0.1396,  0.1614,  ..., -0.0728, -0.2788,  0.3950]],

         [[ 0.3828, -0.1323,  0.2101,  ..., -0.4751, -0.0179,  0.3181]],

         ...,

         [[ 0.2654, -0.3137,  0.1637,  ...,  0.3464, -0.6318,  0.4377]],

         [[ 0.5464, -0.2251,  0.4897,  ..., -0.3184, -0.1769,  0.3203]],

         [[-0.1514,  0.3037, -0.1609,  ..., -0.4651, -0.1842,  0.3386]]],


        [[[ 0.1772,  0.3240, -1.1143,  ...,  0.1444,  0.5684,  0.3770]],

         [[ 0.4187,  0.2264,  0.2446,  ...,  0.7036,  0.3003,  0.2981]],

         [[ 0.1241,  0.1919, -0.5239,  ..., -0.1606,  0.5210, -0.1896]],

         ...,

         [[ 0.5225, -0.2333,  0.1004,  ...,  0.0297,  1.0059, -0.1329]],

         [[ 0.4304,  0.4819,  0.1232,  ...,  0.5234,  0.5210,  0.2379]],

         [[ 0.5361, -0.0976, -0.3975,  ...,  0.2217,  0.8481,  0.0780]]],


        [[[-0.6426, -0.1761,  0.3420,  ...,  0.4404,  0.5273,  0.0485]],

         [[-0.2313,  0.5249,  0.8975,  ...,  0.2517,  0.2163,  0.3628]],

         [[-0.9180, -0.7173, -0.3291,  ...,  0.0781,  1.0693, -0.5142]],

         ...,

         [[-0.8945, -0.1444, -0.0460,  ...,  0.2571,  0.8721, -0.0226]],

         [[-0.6978, -0.7417,  0.2061,  ...,  0.2173,  0.2798, -0.2246]],

         [[-0.3818, -0.7246,  0.7720,  ..., -0.3567,  0.0623, -0.0179]]],


        [[[-0.5347, -0.6885, -1.3604,  ..., -1.3672, -1.1768, -1.2275]],

         [[-0.2400, -0.5176, -0.4875,  ..., -0.2822,  0.1527, -0.0917]],

         [[-0.1940, -0.1766, -0.8022,  ..., -0.3743, -0.2607,  0.1602]],

         ...,

         [[-0.2346, -0.4260, -0.2166,  ...,  0.1776, -0.2793, -0.8052]],

         [[-0.3430, -0.9839,  0.3735,  ...,  0.3267,  0.4268, -0.5464]],

         [[-0.3293, -0.0431, -1.1631,  ..., -0.5742, -1.3242, -1.2441]]]],
       device='cuda:0', dtype=torch.float16) tensor([[[[ 0.4114, -0.3369,  0.6221,  ..., -0.4475, -0.2361,  0.3022]],

         [[ 0.2480, -0.1396,  0.1614,  ..., -0.0728, -0.2788,  0.3950]],

         [[ 0.3828, -0.1323,  0.2101,  ..., -0.4751, -0.0179,  0.3181]],

         ...,

         [[-0.0500, -0.0776,  0.3552,  ...,  0.6475,  0.1764, -0.2125]],

         [[ 0.7197,  0.4253,  0.0373,  ...,  0.7168,  0.5254,  0.2496]],

         [[ 0.0498,  0.1896, -0.1191,  ...,  0.1239,  0.5039, -0.0274]]],


        [[[-0.6572, -0.5571, -0.0978,  ...,  0.2294,  0.8623, -0.4048]],

         [[-0.5635, -0.4026,  0.1295,  ...,  0.1743,  0.6333, -0.0356]],

         [[-0.8359, -0.7275,  0.3054,  ..., -0.2150,  0.5693, -0.5825]],

         ...,

         [[-0.9004, -0.7935,  0.1372,  ..., -0.1024,  0.6543,  0.3892]],

         [[-0.1636,  0.1940, -0.9355,  ..., -0.2068, -0.7847, -0.1024]],

         [[-0.1720, -0.6123, -0.6470,  ..., -0.3550,  0.4495, -0.0429]]],


        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         ...,

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],


        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         ...,

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
```

Is this due to a bug? Or I just have some misunderstandings about this function?","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
"[Idefics 3] Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1","### System Info

## System Info
- `transformers` version: 4.47.0.dev0
- Platform: Linux-4.18.0-553.27.1.el8_10.x86_64-x86_64-with-glibc2.28
- Python version: 3.11.6
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.3
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): 0.6.1 (gpu)
- Jax version: 0.4.4
- JaxLib version: 0.4.3
- Using distributed or parallel set-up in script?: Yes
- Using GPU in script?: Yes, 4 GPUs
- GPU type: NVIDIA H100








### Who can help?

@qubvel

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I was trying to fine-tune a `SmolVLM-Instruct` model using the fine-tuning code mentioned for SmolVLM-Base [here](https://github.com/huggingface/smollm/blob/main/finetuning/Smol_VLM_FT.ipynb) on a 4 H100 GPU setup.

1. Change model_id to `""HuggingFaceTB/SmolVLM-Instruct""`
2. Use [PubMedVision](https://huggingface.co/datasets/FreedomIntelligence/PubMedVision) instead of VQAv2 dataset.
3. Change `collate_fn` to accomodate PubMedVision
4. Run the script as is.

I encountered a `[RuntimeError] Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1` on running `trainer.train()`.

TL;DR 
```
File ~/.local/lib/python3.11/site-packages/transformers/models/idefics3/modeling_idefics3.py:904, in Idefics3Model.inputs_merger(self, input_ids, inputs_embeds, image_hidden_states)
    902 # cast to the dtype of the input_embeds to support quantized models
    903 reshaped_image_hidden_states = reshaped_image_hidden_states.to(inputs_embeds.dtype)
--> 904 new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states
    905 return new_inputs_embeds

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1
```
Actual traceback:
```
RuntimeError                              Traceback (most recent call last)
Cell In[47], line 1
----> 1 trainer.train()

File ~/.local/lib/python3.11/site-packages/transformers/trainer.py:2163, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   2161         hf_hub_utils.enable_progress_bars()
   2162 else:
-> 2163     return inner_training_loop(
   2164         args=args,
   2165         resume_from_checkpoint=resume_from_checkpoint,
   2166         trial=trial,
   2167         ignore_keys_for_eval=ignore_keys_for_eval,
   2168     )

File ~/.local/lib/python3.11/site-packages/transformers/trainer.py:2521, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   2515 context = (
   2516     functools.partial(self.accelerator.no_sync, model=model)
   2517     if i != len(batch_samples) - 1
   2518     else contextlib.nullcontext
   2519 )
   2520 with context():
-> 2521     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
   2523 if (
   2524     args.logging_nan_inf_filter
   2525     and not is_torch_xla_available()
   2526     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   2527 ):
   2528     # if loss is nan or inf simply add the average of previous logged losses
   2529     tr_loss = tr_loss + tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File ~/.local/lib/python3.11/site-packages/transformers/trainer.py:3651, in Trainer.training_step(self, model, inputs, num_items_in_batch)
   3648     return loss_mb.reduce_mean().detach().to(self.args.device)
   3650 with self.compute_loss_context_manager():
-> 3651     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
   3653 del inputs
   3654 if (
   3655     self.args.torch_empty_cache_steps is not None
   3656     and self.state.global_step % self.args.torch_empty_cache_steps == 0
   3657 ):

File ~/.local/lib/python3.11/site-packages/transformers/trainer.py:3705, in Trainer.compute_loss(self, model, inputs, return_outputs, num_items_in_batch)
   3703         loss_kwargs[""num_items_in_batch""] = num_items_in_batch
   3704     inputs = {**inputs, **loss_kwargs}
-> 3705 outputs = model(**inputs)
   3706 # Save past state if it exists
   3707 # TODO: this needs to be fixed and made cleaner later.
   3708 if self.args.past_index >= 0:

File /cvmfs/jupyter.hpc.rwth.de/clients/pytorch-ifs-3a-c23/lib/python3.11/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)
   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1531 else:
-> 1532     return self._call_impl(*args, **kwargs)

File /cvmfs/jupyter.hpc.rwth.de/clients/pytorch-ifs-3a-c23/lib/python3.11/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)
   1536 # If we don't have any hooks, we want to skip the rest of the logic in
   1537 # this function, and just call forward.
   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1539         or _global_backward_pre_hooks or _global_backward_hooks
   1540         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1541     return forward_call(*args, **kwargs)
   1543 try:
   1544     result = None

File ~/.local/lib/python3.11/site-packages/accelerate/utils/operations.py:823, in convert_outputs_to_fp32.<locals>.forward(*args, **kwargs)
    822 def forward(*args, **kwargs):
--> 823     return model_forward(*args, **kwargs)

File ~/.local/lib/python3.11/site-packages/accelerate/utils/operations.py:811, in ConvertOutputsToFp32.__call__(self, *args, **kwargs)
    810 def __call__(self, *args, **kwargs):
--> 811     return convert_to_fp32(self.model_forward(*args, **kwargs))

File /cvmfs/jupyter.hpc.rwth.de/clients/pytorch-ifs-3a-c23/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16, in autocast_decorator.<locals>.decorate_autocast(*args, **kwargs)
     13 @functools.wraps(func)
     14 def decorate_autocast(*args, **kwargs):
     15     with autocast_instance:
---> 16         return func(*args, **kwargs)

File ~/.local/lib/python3.11/site-packages/accelerate/utils/operations.py:823, in convert_outputs_to_fp32.<locals>.forward(*args, **kwargs)
    822 def forward(*args, **kwargs):
--> 823     return model_forward(*args, **kwargs)

File ~/.local/lib/python3.11/site-packages/accelerate/utils/operations.py:811, in ConvertOutputsToFp32.__call__(self, *args, **kwargs)
    810 def __call__(self, *args, **kwargs):
--> 811     return convert_to_fp32(self.model_forward(*args, **kwargs))

File /cvmfs/jupyter.hpc.rwth.de/clients/pytorch-ifs-3a-c23/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16, in autocast_decorator.<locals>.decorate_autocast(*args, **kwargs)
     13 @functools.wraps(func)
     14 def decorate_autocast(*args, **kwargs):
     15     with autocast_instance:
---> 16         return func(*args, **kwargs)

File ~/.local/lib/python3.11/site-packages/peft/peft_model.py:812, in PeftModel.forward(self, *args, **kwargs)
    810 with self._enable_peft_forward_hooks(*args, **kwargs):
    811     kwargs = {k: v for k, v in kwargs.items() if k not in self.special_peft_forward_args}
--> 812     return self.get_base_model()(*args, **kwargs)

File /cvmfs/jupyter.hpc.rwth.de/clients/pytorch-ifs-3a-c23/lib/python3.11/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)
   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1531 else:
-> 1532     return self._call_impl(*args, **kwargs)

File /cvmfs/jupyter.hpc.rwth.de/clients/pytorch-ifs-3a-c23/lib/python3.11/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)
   1536 # If we don't have any hooks, we want to skip the rest of the logic in
   1537 # this function, and just call forward.
   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1539         or _global_backward_pre_hooks or _global_backward_hooks
   1540         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1541     return forward_call(*args, **kwargs)
   1543 try:
   1544     result = None

File ~/.local/lib/python3.11/site-packages/accelerate/hooks.py:170, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    168         output = module._old_forward(*args, **kwargs)
    169 else:
--> 170     output = module._old_forward(*args, **kwargs)
    171 return module._hf_hook.post_forward(module, output)

File ~/.local/lib/python3.11/site-packages/transformers/models/idefics3/modeling_idefics3.py:1196, in Idefics3ForConditionalGeneration.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1193 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1195 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-> 1196 outputs = self.model(
   1197     input_ids=input_ids,
   1198     attention_mask=attention_mask,
   1199     position_ids=position_ids,
   1200     past_key_values=past_key_values,
   1201     inputs_embeds=inputs_embeds,
   1202     pixel_values=pixel_values,
   1203     pixel_attention_mask=pixel_attention_mask,
   1204     image_hidden_states=image_hidden_states,
   1205     use_cache=use_cache,
   1206     output_attentions=output_attentions,
   1207     output_hidden_states=output_hidden_states,
   1208     return_dict=return_dict,
   1209 )
   1211 hidden_states = outputs[0]
   1212 logits = self.lm_head(hidden_states)

File /cvmfs/jupyter.hpc.rwth.de/clients/pytorch-ifs-3a-c23/lib/python3.11/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)
   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1531 else:
-> 1532     return self._call_impl(*args, **kwargs)

File /cvmfs/jupyter.hpc.rwth.de/clients/pytorch-ifs-3a-c23/lib/python3.11/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)
   1536 # If we don't have any hooks, we want to skip the rest of the logic in
   1537 # this function, and just call forward.
   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1539         or _global_backward_pre_hooks or _global_backward_hooks
   1540         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1541     return forward_call(*args, **kwargs)
   1543 try:
   1544     result = None

File ~/.local/lib/python3.11/site-packages/transformers/models/idefics3/modeling_idefics3.py:1014, in Idefics3Model.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, use_cache, output_attentions, output_hidden_states, return_dict)
   1009     image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)
   1011 if past_seen_tokens == 0 and inputs_embeds is not None and image_hidden_states is not None:
   1012     # When we generate, we don't want to replace the potential image_token_id that we generated by images
   1013     # that simply don't exist
-> 1014     inputs_embeds = self.inputs_merger(
   1015         input_ids=input_ids,
   1016         inputs_embeds=inputs_embeds,
   1017         image_hidden_states=image_hidden_states,
   1018     )
   1020 outputs = self.text_model(
   1021     inputs_embeds=inputs_embeds,
   1022     attention_mask=attention_mask,
   (...)
   1028     return_dict=return_dict,
   1029 )
   1031 if not return_dict:

File ~/.local/lib/python3.11/site-packages/transformers/models/idefics3/modeling_idefics3.py:904, in Idefics3Model.inputs_merger(self, input_ids, inputs_embeds, image_hidden_states)
    902 # cast to the dtype of the input_embeds to support quantized models
    903 reshaped_image_hidden_states = reshaped_image_hidden_states.to(inputs_embeds.dtype)
--> 904 new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states
    905 return new_inputs_embeds

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1
```



### Expected behavior

No Runtime Error.

## Potential Fix

I looked up a couple of similar issues like  #24410 and came up with this fix:

File ~/.local/lib/python3.11/site-packages/transformers/models/idefics3/modeling_idefics3.py:904
```
new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states.to(self.device)
```

This fixed the issue for me. Do let me know if this works.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",2,open
Change `is_soundfile_availble` to `is_soundfile_available`,"# What does this PR do?

Fix small typo

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Universal Speculative Decoding `CandidateGenerator`,"This PR is a collaborative effort with @jmamou and @gauravjain14. This PR supersedes https://github.com/huggingface/transformers/pull/34760 and builds upon https://github.com/huggingface/transformers/pull/35009.

---

_This PR is open for initial review, although some areas are still under development._

# What does this PR do?

This PR introduces the `UniversalSpeculativeDecodingGenerator` class, enabling speculative decoding for assistants with slightly different tokenizers. The key addition is two logits processors (`LogitsProcessor`) that ensure the assistant generates tokens exclusively from the target vocabulary, maintaining alignment and preserving the target distribution without altering the verification method. Theoretically, it is agnostic to the `do_sample` choice. This avoids issues like #32867 and #33534 and sets the stage for advanced universal speculative decoding techniques (that we are currently researching and have not yet been published).

---

## Motivation and Context

This update resolves prior inconsistencies in speculative decoding caused by misaligned vocabularies. Key benefits include:
- Ensuring the assistant generates only tokens present in the target vocabulary.
- Lossless preservation of the target distribution.
- Compatibility with future speculative decoding advancements.

This PR is a step toward advancements in [Universal Assisted Generation](https://huggingface.co/blog/universal_assisted_generation), in collaboration with @danielkorat, @Orenpereg, @mosheber, @jmamou, @gante, @lewtun, and @MosheWasserb.

---

## Related

Issues:
- #32867
- #33534

PRs:
- https://github.com/huggingface/transformers/pull/34553
- https://github.com/huggingface/transformers/pull/34935 - please review and merge

---

## Dependencies

- No additional dependencies. However, please consider merging PR #35009 first.

---

## Before Submitting Checklist

- [x] Followed the [contributor guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request).
- [x] Add functionality tests.
- [ ] Verified adherence to target distribution accuracy.
  - [x] Please merge https://github.com/huggingface/transformers/pull/34553.
  - [ ] Add more tests.
- [ ] Measure speedups and add documentation.

---

## Who can review?
@gante / @ArthurZucker / @zucchini-nlp",[],15,open
Stop requiring CacheConfig in GenerationConfig with StaticCache,"I have an observation that in some common use cases, configuring StaticCache during GenerationConfig initialisation is unnecessary.
it was introduced in https://github.com/huggingface/transformers/pull/32830

Specifically, when the model is used with `.generate`, only the `cache_implementation` config option is relevant. The rest of the cache config is determined inside `generate()`, specifically in `transformers.generation.utils::GenerationMixin._get_cache()`. In that function, StaticCache is created based on the requested generation parameters, ignoring `cache_config` entirely.

**Suggestion**:
- do not require StaticCache config in GenerationConfig.__init__
- have some custom logic for ExecuTorch, that does not affect other use cases
- have default arguments for StaticCache.__init__() so that it would quietly get created with some default parameters (not a good idea though).
- have tests in transformers that set just `cache_implementation=""static""` and then call `.generate()`

### current workaround:
do not set cache_implementation in GenerationConfig constructor, but set it afterwards with:
`model.generation_config.cache_implementation = ""static""`

**system info**
applies to transformers from september 2024 up to the current 4.46.3

### Who can help?
@gante","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
VisualBert: Why isn't the pooler_output used to calculate the logits in VisualBertForQuestionAnswering?,"I noticed that in the source code of `VisualBertForQuestionAnswering`, the embedding vector that is used to calculate the logits is not the `pooler_output` that is returned from `VisualBertModel`, but rather the embedding representation of the last text token in `last_hidden_state` (also returned from `VisualBertModel`), as you can see in the [code](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/models/visual_bert/modeling_visual_bert.py#L1242) below:

        sequence_output = outputs[0]

        # TO-CHECK: From the original code
        index_to_gather = (
            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1))
        )
        pooled_output = torch.gather(sequence_output, 1, index_to_gather)

Why is that? Usually in similar models the `pooler_output` is used to calculate the logits, which comes from the [CLS] token in the beginning of the text.",[],1,open
adjust beam search early stopping to any criteria as opposed to all,"# This PR addresses https://github.com/huggingface/transformers/issues/34843, adjusting the requirement to early stop during generation via beam search to ANY of the supplied criteria as opposed to ALL

<!--

This is a proposed solution to the bug documented in https://github.com/huggingface/transformers/issues/34843. Any early stopping criteria being met should be enough to stop generation.

-->

<!-- Remove if not applicable -->

Fixes #34843 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?


## Who can review?

@Rocketknight1 @gante ",[],5,open
Only Fine-tune the embeddings of the added special tokens,"### Feature request

Hi, I added some new special tokens to the LLMs (specifically I'm using Qwen2-VL) and then I only want to fine-tune the embedding layers of these added tokens while keeping all other parameters (and the embedding layers for other tokens) frozen. I wonder if there is a built-in way to do so instead of fine-tuning the whole embedding matrix?


### Motivation

If we want to maximumly retain the original capabilities of the model while adding new tokens for certain scenarios, this might be needed, especially when we don't have much data and do not want to alter the pretrained weights.

Another question: if we have a considerable amount of data, is it recommended to fine-tune the whole embedding matrix or only the embeddings for the added tokens?

### Your contribution

If it's a reasonable feature and not implemented yet, I'm happy to submit a PR.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Unable to export GLM models to ONNX,"### System Info

- `transformers` version: 4.47.0.dev0
- Platform: Linux-6.5.0-1025-azure-x86_64-with-glibc2.31
- Python version: 3.12.1
- Huggingface_hub version: 0.26.1
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.0+cu124 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>


### Who can help?

@ArthurZucker @Cyrilvallez

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Run the following script:
```py
import os
import torch
from transformers import (
    AutoProcessor,
    GlmForCausalLM,
    DynamicCache,
)

class PatchedGlmForCausalLM(GlmForCausalLM):
    def forward(self, *args):
        input_ids, attention_mask, position_ids, *past_key_values_args = args

        # Convert past_key_values list to DynamicCache
        if len(past_key_values_args) == 0:
            past_key_values = None
        else:
            past_key_values = DynamicCache(self.config.num_hidden_layers)
            for i in range(self.config.num_hidden_layers):
                key = past_key_values_args.pop(0)
                value = past_key_values_args.pop(0)
                past_key_values.update(key_states=key, value_states=value, layer_idx=i)

        o = super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
        )

        flattened_past_key_values_outputs = {
            ""logits"": o.logits,
        }
        output_past_key_values: DynamicCache = o.past_key_values
        for i, (key, value) in enumerate(
            zip(output_past_key_values.key_cache, output_past_key_values.value_cache)
        ):
            flattened_past_key_values_outputs[f""present.{i}.key""] = key
            flattened_past_key_values_outputs[f""present.{i}.value""] = value

        return flattened_past_key_values_outputs


# Constants
OUTPUT_FOLDER = ""output""
TEXT_MODEL_NAME = ""model.onnx""
TEMP_MODEL_OUTPUT_FOLDER = os.path.join(OUTPUT_FOLDER, ""temp"")
FINAL_MODEL_OUTPUT_FOLDER = os.path.join(OUTPUT_FOLDER, ""onnx"")


# Load model and processor
model_id = ""hf-internal-testing/tiny-random-GlmForCausalLM""
model = PatchedGlmForCausalLM.from_pretrained(model_id).eval()
processor = AutoProcessor.from_pretrained(model_id)


# Save model configs and processor
model.config.save_pretrained(OUTPUT_FOLDER)
model.generation_config.save_pretrained(OUTPUT_FOLDER)
processor.save_pretrained(OUTPUT_FOLDER)
os.makedirs(TEMP_MODEL_OUTPUT_FOLDER, exist_ok=True)


# Configuration values
## Text model
text_config = model.config
num_heads = text_config.num_attention_heads
num_key_value_heads = text_config.num_key_value_heads
head_dim = text_config.head_dim
num_layers = text_config.num_hidden_layers
hidden_size = text_config.hidden_size


# Dummy input sizes
batch_size = 2
sequence_length = 16
past_sequence_length = 0

## Text inputs
dummy_past_key_values_kwargs = {
    f""past_key_values.{i}.{key}"": torch.zeros(
        batch_size,
        num_key_value_heads,
        past_sequence_length,
        head_dim,
        dtype=torch.float32,
    )
    for i in range(num_layers)
    for key in [""key"", ""value""]
}
input_ids = torch.randint(
    0, text_config.vocab_size,
    (batch_size, sequence_length),
)
attention_mask = torch.ones(batch_size, sequence_length + past_sequence_length, dtype=torch.int64)
position_ids = torch.ones(batch_size, sequence_length, dtype=torch.int64)

text_inputs = dict(
    input_ids=input_ids,
    attention_mask=attention_mask,
    position_ids=position_ids,
    **dummy_past_key_values_kwargs,
)
text_inputs_positional = tuple(text_inputs.values())
text_outputs = model.forward(*text_inputs_positional)  # Test forward pass


# ONNX Exports
## Text model
TEXT_MODEL_OUTPUT_PATH=os.path.join(TEMP_MODEL_OUTPUT_FOLDER, TEXT_MODEL_NAME)
torch.onnx.export(
    model,
    args=text_inputs_positional,
    f=TEXT_MODEL_OUTPUT_PATH,
    export_params=True,
    opset_version=14,
    do_constant_folding=True,
    input_names=list(text_inputs.keys()),
    output_names=[""logits""]
    + [f""present.{i}.{key}"" for i in range(num_layers) for key in [""key"", ""value""]],
    dynamic_axes={
        ""input_ids"": {0: ""batch_size"", 1: ""sequence_length""},
        ""attention_mask"": {0: ""batch_size"", 1: ""total_sequence_length""},
        ""position_ids"": {0: ""batch_size"", 1: ""sequence_length""},
        **{
            f""past_key_values.{i}.{key}"": {0: ""batch_size"", 2: ""past_sequence_length""}
            for i in range(num_layers)
            for key in [""key"", ""value""]
        },
        ""logits"": {0: ""batch_size"", 1: ""sequence_length""},
        **{
            f""present.{i}.{key}"": {0: ""batch_size"", 2: ""total_sequence_length""}
            for i in range(num_layers)
            for key in [""key"", ""value""]
        },
    },
)
```

It produces this error:
```
Traceback (most recent call last):
  File ""/workspaces/glm.py"", line 110, in <module>
    torch.onnx.export(
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/__init__.py"", line 375, in export
    export(
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py"", line 502, in export
    _export(
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py"", line 1564, in _export
    graph, params_dict, torch_out = _model_to_graph(
                                    ^^^^^^^^^^^^^^^^
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py"", line 1117, in _model_to_graph
    graph = _optimize_graph(
            ^^^^^^^^^^^^^^^^
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py"", line 639, in _optimize_graph
    graph = _C._jit_pass_onnx(graph, operator_export_type)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/utils.py"", line 1836, in _run_symbolic_function
    return symbolic_fn(graph_context, *inputs, **attrs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/symbolic_helper.py"", line 369, in wrapper
    return fn(g, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/symbolic_opset11.py"", line 519, in cat
    return opset9.cat(g, tensor_list, dim)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/symbolic_helper.py"", line 281, in wrapper
    return fn(g, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py"", line 575, in cat
    assert all(a)
AssertionError
```


### Expected behavior

The model should export correctly. This may in fact be an ONNX bug, but not 100% sure. Models like Gemma can export correctly, so it seems to be GLM-specific.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 4364086132, 'node_id': 'LA_kwDOCUB6oc8AAAABBB6rdA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ONNX', 'name': 'ONNX', 'color': 'D4C5F9', 'default': False, 'description': ''}]",0,open
SAMProcessor padding for rectangular aspect input images isn't symmetric ,"### System Info

 `transformers` version: 4.46.2
- Platform: Linux-6.2.0-39-generic-x86_64-with-glibc2.37
- Python version: 3.9.20
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.5.1+cu118 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: no
- Using GPU in script?: yes
- GPU type: NVIDIA GeForce GTX 1650


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```
from transformers import SamProcessor, SamModel
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F

def upsample_mask(pred_mask, target_size):
    return F.interpolate(pred_mask.unsqueeze(0).unsqueeze(0), size=target_size, mode=""nearest"").squeeze(1)

# Load the processor and model
processor = SamProcessor.from_pretrained(""facebook/sam-vit-base"")
model = SamModel.from_pretrained(""facebook/sam-vit-base"")

# Prepare inputs
input_points = [[[115, 92], [117, 108]]]
input_labels = [[1, 0]]

input_image = np.zeros((233,197,3),dtype=np.uint8)
input_image[20:210,20:177,:] = 128
input_image[88:96,111:118,:] = 255

# optionally crop to square aspect
if False:
    input_image = input_image[:197,:,:]

inputs = processor(
    images=input_image,
    input_points=input_points,
    input_labels=input_labels,
    return_tensors=""pt""
)

check_image = np.copy(inputs['pixel_values'])[0]
check_image = np.moveaxis(check_image,0,-1)
pts = np.array(inputs['input_points'][0]).astype('int')

# display the control points
for p in pts[0]:
    check_image[p[1]-4:p[1]+4,p[0]-4:p[0]+4,:] = np.array([255,0,0],dtype='uint8')

# Forward pass with multimask_output=False
outputs = model(**inputs, multimask_output=False)

# Retrieve masks
pred_mask = np.squeeze(outputs[""pred_masks""]) 
pred_mask_sig = torch.sigmoid(upsample_mask(pred_mask, np.shape(input_image)[:2])).cpu().detach().numpy().squeeze()
thresh_mask = (pred_mask_sig > 0.5).astype(np.uint8) * 255

plt.figure(8)
plt.subplot(1,4,1)
plt.imshow(input_image)

plt.subplot(1,4,2)
plt.imshow(check_image)

plt.subplot(1,4,3)
plt.imshow(pred_mask.cpu().detach().numpy().squeeze())

plt.subplot(1,4,4)
plt.imshow(thresh_mask)
plt.show()
```

### Expected behavior

I expected the padding of the up-scaled working image to be symmetric, if the input image is asymmetric. When I resample the predicted mask back to the starting asymmetric matrix size, there is now an offset between prediction and the original image/control points. I looked at line 106 of image_processing_sam.py on github but the pad_size arg doesn't seem to have an option for symmetric versus asymmetric. I can correct for this manually of course, just thought I wouldn't have to do that. ","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",2,open
[doc] deepspeed universal checkpoint,"# What does this PR do?

Fixes #33157 and https://github.com/microsoft/DeepSpeed/issues/5405


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@muellerzr @stevhliu

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Enable gptqmodel,"We are going to replace `auto_gptq` with `gptqmodel`. Start with the quantizer check, and also need to change the optimum: [https://github.com/huggingface/optimum/pull/2064](https://github.com/huggingface/optimum/pull/2064).

We intended to deprecate AutoGPTQ in this PR, but considering users' behavior, we would like to keep the support for auto_gptq for the next few versions and give a warning for deprecating.",[],15,open
switch from `training_args.bin` `training_args.json`,"# What does this PR do?

switch from  `training_args.bin` to `training_args.json` and only capture the parameters that the user passed 
I'm using the same approach we are using in huggingface_hub's [PyTorchModelHubMixin](https://github.com/huggingface/huggingface_hub/blob/503d35355fa79ede904665c944e0a247e85c8af7/src/huggingface_hub/hub_mixin.py#L277) to store as little parameters as possible.
a minimalistic approach to test this is pr

```py
from transformers import TrainingArguments
args = TrainingArguments(output_dir=""folder"",eval_strategy=""no"") # or any other paramters
print(args.to_json_string())
# outputs
""""""
{
  ""output_dir"": ""folder"",
  ""eval_strategy"": ""no"",
  ""logging_dir"": ""folder\\runs\\Nov29_02-44-45_iphone-laptop""
}
""""""
# logging_dir is a special parameter that is always captured and added to the training_args because we want to ensure consistency

# stores the parameters into a file
args.to_json_file(""training_args.json"")

# loads an instance using the class directly
args2  = TrainingArguments.from_json_file(""training_args.json"") 
```

using this approach, we ensure that we only store the parameters that the user-defined manually and not the ones that have default values or the ones inferred from the system (ie cpu, cuda, tpu ... ), leaving some room for flexibility.

in a sense the parameters are mutable, meaning the user can physically alter them.


Fixes #34612 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@muellerzr @SunMarc ",[],6,open
Replace all torch.FloatTensor by torch.Tensor,"# What does this PR do?

Replace all `torch.FloatTensor` typing by `torch.Tensor`.

`torch.FloatTensor` is only available in Pytorch for backward compatibility but is not used anymore. Both dtype and device should not be include in the typing.

Fixes # (issue)

- https://github.com/huggingface/transformers/issues/35003


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Replace all torch.FloatTensor by torch.Tensor,"### Feature request

I think `torch.FloatTensor` dtype should be replace by `torch.Tensor` now. torch.FloatTensor is quite anoying as it trigger warning most of the time, and force users to manually cast type to avoid such warning.

It seems that FloatTensor is not really use anymore in Torch. torch.FloatTensor and torch.cuda.FloatTensor are still available to ensure backward compatibility

### Motivation

Fix typing warnings

### Your contribution

Replace every occurence of torch.FloatTensor by torch.Tensor. Including docstrings","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.,"### System Info

- `transformers` version: 4.46.3
- Platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.35
- Python version: 3.11.10
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: no
- Using GPU in script?: no
- GPU type: NVIDIA GeForce RTX 3060


### Who can help?

@ylacombe @eustlb 

I am finetuning whisper-small on common corpus locally. When I run trainer.train(), I get the following error infinitely -

```
OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.
```

The exact same notebook works perfectly on Google Colab.

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Please refer to [this page](https://huggingface.co/learn/audio-course/en/chapter5/fine-tuning) for the code.

### Expected behavior

The model should start fine-tuning.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Fix max size deprecated warning,"This pull request focuses on removing the deprecated `max_size` argument from the `preprocess` method across multiple image processing modules in the `transformers` library. This change simplifies the code and aligns with the planned deprecation of the `max_size` argument.

Key changes include:

### Removal of `max_size` argument:

* [`src/transformers/models/conditional_detr/image_processing_conditional_detr.py`](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL1367): Removed `max_size` argument from the `preprocess` method and its usage in the `get_size_dict` and `resize` calls. [[1]](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL1367) [[2]](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL1377-R1376) [[3]](diffhunk://#diff-c521b281a0d72092aec2dcad7ad516928e7462b4f4bcdcea5d007b7fe28bc43bL1473-R1472)
* [`src/transformers/models/deformable_detr/image_processing_deformable_detr.py`](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L1365): Removed `max_size` argument from the `preprocess` method and its usage in the `get_size_dict` and `resize` calls. [[1]](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L1365) [[2]](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L1375-R1374) [[3]](diffhunk://#diff-9d778d1efecc45f9608e6df1b9160fa5b84d9f3e90ecf2f55c4ac0e9cdb4ab01L1471-R1470)
* [`src/transformers/models/detr/image_processing_detr.py`](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L1340): Removed `max_size` argument from the `preprocess` method and its usage in the `get_size_dict` and `resize` calls. [[1]](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L1340) [[2]](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L1350-R1349) [[3]](diffhunk://#diff-74625eb393c7631e74bd81b176d57cfb08a80929942837a54c861741f3a5e934L1446-R1445)
* [`src/transformers/models/grounding_dino/image_processing_grounding_dino.py`](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L1373): Removed `max_size` argument from the `preprocess` method and its usage in the `get_size_dict` and `resize` calls. [[1]](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L1373) [[2]](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L1383-R1382) [[3]](diffhunk://#diff-6d900076812c7c1a5596a3dfa1143580dd0fe4f9b827458ef7d832a318a713f0L1479-R1478)
* [`src/transformers/models/yolos/image_processing_yolos.py`](diffhunk://#diff-1d1f26cc1e12a255bd8addb6425a7616648e746f9ba92a24318df5f3ea91aa5cL1277): Removed `max_size` argument from the `preprocess` method and its usage in the `get_size_dict` and `resize` calls. [[1]](diffhunk://#diff-1d1f26cc1e12a255bd8addb6425a7616648e746f9ba92a24318df5f3ea91aa5cL1277) [[2]](diffhunk://#diff-1d1f26cc1e12a255bd8addb6425a7616648e746f9ba92a24318df5f3ea91aa5cL1287-R1286) [[3]](diffhunk://#diff-1d1f26cc1e12a255bd8addb6425a7616648e746f9ba92a24318df5f3ea91aa5cL1382-R1381)# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34977 




## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@qubvel here is my pull request.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2139563322, 'node_id': 'MDU6TGFiZWwyMTM5NTYzMzIy', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/cleanup', 'name': 'cleanup', 'color': 'e7fc49', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",17,open
HIGGS Quantization Support,"### HIGGS 0-Shot Quantization

HIGGS is a new 0-shot quantization algorithm that combines Hadamard preprocessing with MSE-Optimal quantization grids to achieve lower quantization error and SOTA performance. You can find more information in the [paper](https://arxiv.org/abs/2411.17525).

Runtime support for HIGGS is implemented through [FLUTE](https://arxiv.org/abs/2407.10960), and its [library](https://github.com/HanGuo97/flute?tab=readme-ov-file).

This PR **adds support for HIGGS+FLUTE into `transformers` allowing for low-error 0-shot quantization and fast LLM inference**.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],17,open
Fixes bug in image_transforms.py,"# What does this PR do?

This PR aims to fix a [bug](https://github.com/huggingface/transformers/issues/34920) raised in the resize function in image_transforms.py

Fixes # (issue)
The function throws an error because PIL cannot accept an image if it has negative values. The image is normalized so that all values are in the range of [0,1].


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",1,open
Support adamw_torch_8bit,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes https://github.com/huggingface/transformers/issues/34893


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],4,open
Deprecate _is_quantized_training_enabled,"# What does this PR do?
Deprecate `_is_quantized_training_enabled` in `modeling_utils.py` as it's no longer used, and it's replaced by `hf_quantizer.is_trainable`

## Who can review ?
@SunMarc ",[],1,open
"Memory Leak When Using padding=""max_length"" in T5 Text Encoder On CPU","### System Info

- `transformers` version: 4.45.1
- Platform: Linux-6.6.56+-x86_64-with-glibc2.35
- Python version: 3.10.14
- Huggingface_hub version: 0.25.1
- Safetensors version: 0.4.5
- Accelerate version: 0.34.2
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.4.0 (True)
- Tensorflow version (GPU?): 2.16.1 (True)
- Flax version (CPU?/GPU?/TPU?): 0.8.4 (gpu)


### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```python
from transformers import T5EncoderModel,T5Tokenizer


def get_t5_prompt_embeds(tokenizer_2, text_encoder_2,prompt = None,max_sequence_length = 512):

        prompt = [prompt] if isinstance(prompt, str) else prompt

        text_inputs = tokenizer_2(prompt,padding=""max_length"",max_length=max_sequence_length,
            truncation=True,return_length=False,return_overflowing_tokens=False,return_tensors=""pt"",
        )
        text_input_ids = text_inputs.input_ids
        prompt_embeds = text_encoder_2(text_input_ids, output_hidden_states=False)[0]

        return prompt_embeds


tokenizer = T5Tokenizer.from_pretrained(...)
encoder = T5EncoderModel.from_pretrained(...)
get_t5_prompt_embeds(tokenizer, encoder, 'hello')
```
When the padding=""max_length"" parameter is passed to the tokenizer, the memory usage increases significantly, leading to a CPU memory leak of approximately 5GB. Upon removing the padding=""max_length"" parameter, the memory leak disappears, and the previously leaked memory is recovered.

### Expected behavior

The function should generate the embeddings for the provided prompt without causing a significant increase in memory usage.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
[ `Core`]  Refactor modeling code,"# What does this PR do?
Refactor LlamaAttention following #34282",[],1,open
Remove sortish_sampler from Seq2SeqTrainingArgument and the docs,"### System Info

Not Needed

### Who can help?

@muellerzr 
@SunMarc 
@stevhliu 



### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

In  [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments) There's a parameter called sortish_sampler

* sortish_sampler (bool, optional, defaults to False) — Whether to use a sortish sampler or not. Only possible if the underlying datasets are Seq2SeqDataset for now but will become generally available in the near future.
It sorts the inputs according to lengths in order to minimize the padding size, with a bit of randomness for the training set.

This argument exists in the Seq2SeqTrainer 

https://github.com/huggingface/transformers/blob/9d0ad5074eb4214efa8063de21ac5253470f4e08/src/transformers/training_args_seq2seq.py#L39 

But it does nothing aside from taking space
In the Trainer class it points to nothing, never referenced, never used, and in the trainer_utils there isn't even a class called SortishSampler

Personally I didn't knew it, I used this argument for a long while now thinking it does something in my training scripts
but only upon entering the source code and seeing it doesn't exists, I started a deep dive into old issues and pull requests.

Then I found #9574 which is a (almost) 4 years old PR
in which it was renamed and technically got deprecated.

I suggest to remove the argument completely from the documentation and trainer code to remove confusion

As personally I though that ``group_by_length`` is meant mostly for auto-regressive models which only takes a single sequence as their input, while ``sortish_sampler`` is used for seq2seq models and takes into account both sequences

So I decided to use the latter rather than the former for the past 3 weeks thinking I'll get some memory savings in my scripts.

In the end, I think this argument should officially get deprecated and removed from both the code and the docs and it may cause confusion for others beside me.

### Expected behavior

Less confusion, better model training :)","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
Bug for logger.warning_once,"https://github.com/huggingface/transformers/blob/5523e38b553ff6c46b04d2376870fcd842feeecc/src/transformers/generation/configuration_utils.py#L765

The UserWarning could not be loggered by logging and here is a simple test:
```
>>> from transformers.generation.configuration_utils import logger
>>> logger.warning_once('error', UserWarning)
--- Logging error ---
Traceback (most recent call last):
  File ""/home/test/miniconda3/envs/test/lib/python3.11/logging/__init__.py"", line 1110, in emit
    msg = self.format(record)
          ^^^^^^^^^^^^^^^^^^^
  File ""/home/test/miniconda3/envs/test/lib/python3.11/logging/__init__.py"", line 953, in format
    return fmt.format(record)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/testminiconda3/envs/test/lib/python3.11/logging/__init__.py"", line 687, in format
    record.message = record.getMessage()
                     ^^^^^^^^^^^^^^^^^^^
  File ""/home/test/miniconda3/envs/test/lib/python3.11/logging/__init__.py"", line 377, in getMessage
    msg = msg % self.args
          ~~~~^~~~~~~~~~~
TypeError: not all arguments converted during string formatting
Call stack:
  File ""<stdin>"", line 1, in <module>
  File ""/home/test/miniconda3/envs/test/lib/python3.11/site-packages/transformers/utils/logging.py"", line 328, in warning_once
    self.warning(*args, **kwargs)
Message: 'error'
Arguments: (<class 'UserWarning'>,)
```","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
No use `no_sync` context manager when using gradient accumulation w/ deepspeed's zero stage 2 or 3 via `accelerate`,"### Feature request

`trainer.train` with deepspeed stage 2 or 3 via `accelerate` and gradient accumulation does not work as I expected. I suspect this is because deepspeed 0.16.0 introduces `no_sync` context manager https://github.com/microsoft/DeepSpeed/pull/6675.

For example, the error looks like 

```
Traceback (most recent call last):
  File ""main.py"", line 70, in <module>
    main()
  File ""main.py"", line 90, in main
    trainer.train()
  File ""/usr/local/lib/python3.11/dist-packages/transformers/trainer.py"", line 2123, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/transformers/trainer.py"", line 2480, in _inner_training_loop
    with context():
  File ""/usr/lib/python3.11/contextlib.py"", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py"", line 973, in no_sync
    with context():
  File ""/usr/lib/python3.11/contextlib.py"", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py"", line 1995, in no_sync
    assert not self.zero_optimization_partition_gradients(), \
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: no_sync context manager is incompatible with gradient partitioning logic of ZeRO stage 3
```


### Motivation

In my understanding, `trainer.train()` with deepspeed via HF `accelerate` and gradient accumulation uses own forward implementation rather than `accelerate.accumulate` defined at 

https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L2474C75-L2482

So `no_sync` is always used. Even we give

```python
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': {'sync_each_batch': True}, 'use_configured_state': False}
``` 
to `TrainingArguments` to use a feature introduced by https://github.com/huggingface/transformers/issues/29425.


### Your contribution

As suggested in https://github.com/huggingface/transformers/issues/29425#issuecomment-2505015569, adding some warning to config args on docs might be helpful, but ideally addressing this feature is much nicer but I do not know the right approach.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
"BatchEncoding.to throws away columns silently, thus no way to pass non-tensor columns such as String in Trainer metric computation","### System Info

unrelated

### Who can help?

@muellerzr @SunMarc
(original tags, no longer valid)

@ArthurZucker 
(re-tag because want to discuss patch release)

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Hi thanks for the library! Consider this simple line:

```
x = transformers.tokenization_utils_base.BatchEncoding({'a': ['x','y']})
x.to('cpu') # or cuda or whatever
```

The column `a` is then silently removed :(

This is annoying in the following scenario: For each of my training/eval sample, I have a string column that serves as a tag for it, and want to utilize it when computing metrics and losses.

Then it does not work. After some debugging, the root reason is that it gets silently removed in the `to` mentioned above.

It seems torch does not support a tensor of dtype `str`, thus it seems impossible to have data pass through.

### Expected behavior

(see above)","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
How to Log Training Loss at Step Zero in Hugging Face Trainer or SFT Trainer?,"### Feature request

log train loss on start

----

’m using the Hugging Face `Trainer` (or `SFTTrainer`) for fine-tuning, and I want to log the training loss at step 0 (before any training steps are executed). I know there’s an `eval_on_start` option for evaluation, but I couldn't find a direct equivalent for training loss logging at the beginning of training.

Is there a way to log the initial training loss at step zero (before any updates) using `Trainer` or `SFTTrainer`? Ideally, I'd like something similar to `eval_on_start`.

Here’s what I’ve tried so far:

#### Solution 1: Custom Callback

I implemented a custom callback to log the training loss at the start of training:


```python
from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
    def on_train_begin(self, args, state, control, logs=None, **kwargs):
        # Log training loss at step 0
        logs = logs or {}
        logs[""train/loss""] = None  # Replace None with an initial value if available
        logs[""train/global_step""] = 0
        self.log(logs)

    def log(self, logs):
        print(f""Logging at start: {logs}"")
        wandb.log(logs)

# Adding the callback to the Trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=training_args,
    optimizers=(optimizer, scheduler),
    callbacks=[TrainOnStartCallback()],
)
```
This works but feels a bit overkill. It logs metrics at the start of training before any steps.

#### Solution 2: Manual Logging

Alternatively, I manually log the training loss before starting training:

```python
wandb.log({""train/loss"": None, ""train/global_step"": 0})
trainer.train()
```

### Question:

Are there any built-in features in `Trainer` or `SFTTrainer` to log training loss at step zero? Or is a custom callback or manual logging the best solution here? If so, are there better ways to implement this functionality? similar to the `eval_on_start` but `train_on_start`?

cross: https://discuss.huggingface.co/t/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer/128188

### Motivation

Crucial sanity check

### Your contribution

yes, happy to implement this. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
Add embedding scaling,"# What does this PR do?

This PR introduces support for **embedding scaling** in the `MistralModel`. The feature is inspired by the [Scaled Embed method](https://arxiv.org/pdf/2312.16903), which demonstrates that applying a scaling factor to the embeddings significantly improves the stability of large language model (LLM) training, effectively mitigating gradient spikes.

### Key Changes:
- Adds a new configuration parameter:  
  **`embedding_scale`** (`float`, *optional*, defaults to `1.0`): A scaling factor applied to the model's embeddings.
- Updates the `MistralModel` implementation to apply the scaling factor to the embeddings during training and inference.

This implementation currently supports the **PyTorch backend**. Support for TensorFlow and Flax backends can be added in the future.

---

# Motivation

The Scaled Embed method improves training stability and helps mitigate gradient spikes, as shown in the referenced paper. By implementing this feature, we aim to bring these benefits to the `MistralModel` while maintaining backward compatibility.

---

# Open Questions for Discussion

1. **Relevance:**  
   Do you see this feature as relevant for integration into the library? Would it make sense to extend this functionality to other models, such as `LlamaModel`?

2. **Implementation Scope:**  
   Should embedding scaling be integrated more broadly across models in the library or should it remain model-specific ?  
---

Let me know if further adjustments or refinements are needed!",[],1,open
Add TextNet,"# What does this PR do?

This PR adds TextNet which will be used to add Fast.

It builds up on the work of this PR: https://github.com/huggingface/transformers/pull/27425 (which was approved but not merged) and make it up to date with the library changes.

TODO:
- [x] Update the model's README file
- [x] Check why some tests are failing
- [x] Fix processing class errors
- [x] Fix modeling file errors


HF Model Cards:
TextNet-B: https://huggingface.co/jadechoghari/textnet-base
TextNet-S: https://huggingface.co/jadechoghari/textnet-small
TextNet-T: https://huggingface.co/jadechoghari/textnet-tiny

Notebook to replicate the author's logits:
https://colab.research.google.com/drive/1YsraOg-GHFh7PlvuIC9iJeBZquVWdz-r?usp=sharing


","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",1,open
Deprecation Warning for `max_size` in `DetrImageProcessor.preprocess`,"### System Info

- `transformers` version: 4.47.0.dev0
- Platform: Linux-5.15.0-126-generic-x86_64-with-glibc2.31
- Python version: 3.11.0
- Huggingface_hub version: 0.24.5
- Safetensors version: 0.4.4
- Accelerate version: 1.1.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.1.2+cu118 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 3090

### Who can help?

@amyeroberts, @qubvel
and I think @NielsRogge worked on it too ?

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
import logging

import numpy as np
from transformers.models.detr.image_processing_detr import DetrImageProcessor

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

images = [np.ones((512, 512, 3))]
annotations = [{'image_id': [], 'annotations': []}]
size = {'max_height': 600, 'max_width': 600}

image_processor = DetrImageProcessor()
images = image_processor.preprocess(images, do_resize=True, do_rescale=False, size=size, annotations=annotations, format='coco_detection')
```

### Expected behavior

Hello!

I noticed that the `preprocess` method in the `DetrImageProcessor` class always passes `max_size` to the `resize` method,

https://github.com/huggingface/transformers/blob/4120cb257f03b834fb332e0b0ee6570245e85656/src/transformers/models/detr/image_processing_detr.py#L1445-L1447

 and that triggers a deprecation warning in `resize` method,

```bash
The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.
```

https://github.com/huggingface/transformers/blob/4120cb257f03b834fb332e0b0ee6570245e85656/src/transformers/models/detr/image_processing_detr.py#L992-L997

I propose removing the unused `max_size` argument from the preprocess method since it is always `None`,

https://github.com/huggingface/transformers/blob/4120cb257f03b834fb332e0b0ee6570245e85656/src/transformers/models/detr/image_processing_detr.py#L1340

Would it be okay if I work on this and submit a pull request? I can try to see if the problem also occurs in other models.","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",1,open
Efficient Inference Kernel for SpQR ,"# What does this PR do?



<!-- Remove if not applicable -->

Adds support for efficient single-batch inference for [SpQR](https://github.com/Vahe1994/SpQR/tree/main/spqr).

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@SunMarc @MekkCyber
",[],12,open
Remove _supports_static_cache = True for some model classes,"# What does this PR do?

Remove `_supports_static_cache = True` for some model classes. See the comments in changes.

They were True before because it is set simply we can use static cache without torch.compile. But after #34247, static is kind tied to `torch.compile` and we should say it works if it works with torch.compile",[],1,open
Correctly list the chat template file in the Tokenizer saved files list,"The tokenizer has a list of the files it saves, which wasn't correctly being updated with the new standalone chat template file. This PR fixes that issue!",[],1,open
Optimized dola decoding generation function for faster performance,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # [issue](https://github.com/huggingface/transformers/issues/34958)

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?

Test code:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
import torch

tokenizer = AutoTokenizer.from_pretrained(""huggyllama/llama-7b"")
model = AutoModelForCausalLM.from_pretrained(""huggyllama/llama-7b"", torch_dtype=torch.float16)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)
set_seed(42)

text = ""On what date was the Declaration of Independence officially signed?""
inputs = tokenizer(text, return_tensors=""pt"").to(device)

# vanilla_output = model.generate(**inputs, do_sample=False, max_new_tokens=50)
# tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)

dola_high_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers='high')
output1 = tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)
print(""dola_high_output:\n"", output1)

dola_custom_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers=[16,], repetition_penalty=1.2)
output2 = tokenizer.batch_decode(dola_custom_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)
print(""dola_custom_output:\n"", output2)
```

Before Modification：
```
dola_high_output:
 ['\nJuly 4, 1776, when the Continental Congress voted to separate from Great Britain. The 56 delegates to the Continental Congress signed the Declaration on August 2, 1776.']
dola_custom_output:
 ['\nOn July 4, 1776 the Continental Congress voted to approve the wording of the Declaration of Independence. Two days later the delegates signed the document. The signing took place in Philadelphia at Independence Hall']
```

After Modification：
```
dola_high_output:
 ['\nJuly 4, 1776, when the Continental Congress voted to separate from Great Britain. The 56 delegates to the Continental Congress signed the Declaration on August 2, 1776.']
dola_custom_output:
 ['\nOn July 4, 1776 the Continental Congress voted to approve the wording of the Declaration of Independence. Two days later the delegates signed the document. The signing took place in Philadelphia at Independence Hall']
```

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
 @gante @zucchini-nlp

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
remove cpu gpu device synch in Llava family models,"# What does this PR do?
Let's not use items",[],1,open
Redundant Operations.,"``` 
sorted_logits, sorted_indices = torch.sort(scores_normalized, descending=True) 
min_thresh = sorted_logits[..., min_tokens_to_keep - 1] 
```
[This operation](https://github.com/huggingface/transformers/blob/6c3f168b36882f0beebaa9121eafa1928ba29633/src/transformers/generation/utils.py#L4736C5-L4737C60)

Please see the [issue](https://github.com/voidism/DoLa/issues/20).",[],2,open
[`ESM`] Add support for sdpa.,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Add support for SDPA (scaled dot product attention) for ESM. More context in https://github.com/huggingface/transformers/pull/28802 (And this pr mainly reused the code from this pr as the ESM is Bert-based model) and https://github.com/huggingface/transformers/issues/28005 .

This is my first time contributing to this project, please point out if there is any mistakes.

And revert a change in https://github.com/huggingface/transformers/pull/29329 as the dtype-mismatching issue for bitsandbytes is actually caused by the rotary embedding.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@ArthurZucker ","[{'id': 6349658421, 'node_id': 'LA_kwDOCUB6oc8AAAABengZNQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/SDPA', 'name': 'SDPA', 'color': '195411', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",10,open
Docs updates (russian documentation),"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Documentation updates (russian language)

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],4,open
Refactor qwen2 with modular,"# What does this PR do?
The main difference is that `qwen2` uses a `max_layer_sliding` to stop using sliding window starting a certain layer index. 
",[],1,open
OmDet Turbo processor standardization,"# What does this PR do?

Standardize post-processing for OmDet Turbo model (see #34926 for more info)

1) Rename `score_threshold` to `threshold` argument in `post_process_grounded_object_detection`
2) Rename `classes` to `text_labels` argument in `post_process_grounded_object_detection` and make it **optional**
3) Rename `classes` key to `text_labels` in post-processed output dictionary
4) Add `labels` (class indexes) key to post-processed output dictionary

The signature is going to be:
```python
    def post_process_grounded_object_detection(
        self,
        outputs: ""OmDetTurboObjectDetectionOutput"",
        text_labels: Optional[Union[List[str], List[List[str]]]] = None,           # <--------- renamed: classes -> text_labels + Optional now
        threshold: float = 0.3,                                                    # <--------- renamed: score_threshold -> threshold
        nms_threshold: float = 0.5,
        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,
        max_num_det: Optional[int] = None,
    )
```

The output is going to be:
```python
[
  {
      ""boxes"": torch.tensor of shape (N, 4),
      ""labels"": torch.tensor of shape (N,),                                       # <-------- new key, int indices, consistent with other object detection outputs
      ""scores"": torch.tensor of shape (N,),
      ""text_labels"": list of str names of len N (previously classes) or `None`    # <-------- renamed: classes -> text_labels
  },
  ...
]
```
""classes"" key is also available, it will return `text_labels` and issue a warning

## TODO:

 - [x] Add deprecation for dict key


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",3,open
fix `test_generated_length_assisted_generation`,"# What does this PR do?

This PR fixes and expands the `test_generated_length_assisted_generation` test in `test_utils.py`.

## Before submitting
- [x] This PR fixes and expands an existing test.
- [x] Did you read the [[contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request)](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request), Pull Request section?
- [x] Was this discussed/approved via a GitHub issue or the [[forum](https://discuss.huggingface.co/)](https://discuss.huggingface.co/)? Not applicable for this minor change.
- [x] Did you ensure that all other tests pass after this change?

## Who can review?
- Generate: @gante",[],7,open
Add support for causal language modeling for `DistilBertModel`,"### Feature request

HuggingFace Transformers currently supports causal language modeling (CLM) fine-tuning for BERT using`BertLMHeadModel` [as shown in the docs](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertLMHeadModel). My request is simply to extend this support to `DistilBertModel`.

### Motivation

I want to use a distilBERT model to initialize a `EncoderDecoderModel`, but I am getting an error message that says it does not support CLM.

```python
from transformers import EncoderDecoderModel

EncoderDecoderModel.from_encoder_decoder_pretrained(
  encoder_pretrained_model_name_or_path=""distilbert/distilbert-base-multilingual-cased"",
  decoder_pretrained_model_name_or_path=""distilbert/distilbert-base-multilingual-cased"",
)
```

Here is the error message:

```
ValueError: Unrecognized configuration class <class 'transformers.models.distilbert.configuration_distilbert.DistilBertConfig'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.
```

### Your contribution

I'm happy to contribute.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
[WIP] Possible bug - adding option to save/reload scaler,"# What does this PR do?

Attempts to solve inconsistencies when saving and reloading models discussed in #34053 .


@pacman100 @yitongh @SunMarc 
",[],3,open
OwlViT/Owlv2 post processing standardization,"# What does this PR do?

Standardize OwlViT and Owlv2 post-processing

1) Add `post_process_grounded_object_detection` method for Processor (+ deprecation for `post_process_object_detection` method)

```python
def post_process_grounded_object_detection(
    self,
    outputs,
    threshold: float = ...,
    target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,
    text_labels: Union[List[str], List[List[str]]] = None,  # <------------- new keyword arg
)
```

2) Add `text_labels` for post-processed output (if `text_labels` are provided as a keyword argument for `post_process_grounded_object_detection`). It's already mapped text names for detected bounding boxes.


For a full description regarding standardization see #34926

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",4,open
Recomputed tensor size does not match when using activation checkpointing when using FSDP and accelerate,"### System Info

```
- `transformers` version: 4.46.3
- Platform: Linux-6.8.0-1015-aws-x86_64-with-glibc2.35
- Python version: 3.12.6
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: distributed (`accelerate`)
- Using GPU in script?: Yes
- GPU type: NVIDIA A100-SXM4-40GB
```

### Who can help?
@muellerz @SunMarc @ArthurZucker 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I'm running into the following error while trying to use the SFTTrainer with FSDP and the `accelerate` library (full stack trace provided at the very bottom of this post).

```
torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass
```

This occurs when I set `gradient_checkpointing: false` and `activation_checkpointing: true`. Curiously, it actually seems to work if I set `gradient_checkpointing: true` and `activation_checkpointing: false`, **but** that produces the following warning message:
```
 # When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404`. 
 ```

There are a few related GitHub issues around that touch on this issue:
1. https://github.com/Lightning-AI/pytorch-lightning/issues/19267
2. https://github.com/huggingface/transformers/issues/28499
3. https://github.com/pytorch/pytorch/issues/124788
4. https://github.com/huggingface/transformers/issues/32073

One of these suggested setting `use_reentrant: true`, but that doesn't resolve the issue for me.


I'm attempting to run this as a SageMaker training job using the official HuggingFace estimator (this amounts to the following command: `torchrun --nnodes 1 --nproc_per_node 8 train.py`. My training script is essentially a lightly adapted version of the official examples. Below is how I'm instantiating the HuggingFace estimator object:

```
huggingface_estimator = HuggingFace(
    entry_point          = 'train.py',        # train script
    #entry_point          = 'launch.py',        # train script
    dependencies=['requirements.txt'],         
    source_dir           = './',            
    instance_type        = 'ml.p4d.24xlarge',
    instance_count       = 1,               
    max_run              = 2*24*60*60,     
    base_job_name        = job_name,          
    role                 = role,            
    volume_size          = 1024,              
    transformers_version = '4.36.0',     
    pytorch_version      = '2.1.0',          
    py_version           = 'py310',          
    hyperparameters      =  {
        ""config_s3_uri"": ""s3://<foo>
    },
    #metric_definitions=metric_definitions,
    disable_output_compression = True,  
    distribution={""torch_distributed"": {""enabled"": True}},   # enables torchrun
    environment  = {
        ""HUGGINGFACE_HUB_CACHE"": ""/tmp/.cache"", 
        ""HF_TOKEN"": HfFolder.get_token(),      
        ""ACCELERATE_USE_FSDP"": ""1"",             # enable FSDP
        ""FSDP_CPU_RAM_EFFICIENT_LOADING"": ""0"",   # enable CPU RAM efficient loading
        ""FSDP_AUTO_WRAP_POLICY"": ""TRANSFORMER_BASED_WRAP"",
        ""FSDP_BACKWARD_PREFETCH"": ""BACKWARD_PRE"",
        ""FSDP_STATE_DICT_TYPE"": ""FULL_STATE_DICT"",
        ""NCCL_TIMEOUT"": ""3600"",  # 1 hour timeout
        ""NCCL_DEBUG"": ""WARN"",    
        ""NCCL_IB_TIMEOUT"": ""3600"",
        ""NCCL_SOCKET_TIMEOUT"": ""3600"",
        ""NCCL_ASYNC_ERROR_HANDLING"": ""1"",
        ""NCCL_P2P_LEVEL"": ""NVL"",
        ""CUDA_DEVICE_MAX_CONNECTIONS"": ""1"",        
        ""MAX_JOBS"": ""1"",                           
        ""PYTORCH_CUDA_ALLOC_CONF"": ""max_split_size_mb:512"",
        ""TORCH_DISTRIBUTED_DEBUG"": ""DETAIL"",     
    },
    checkpoint_s3_uri=f's3://<foo>'
)
```

Below are some of the relevant parameters from my input config.
```
gradient_checkpointing: false 
gradient_checkpointing_kwargs:
  use_reentrant: true
attn_implementation: ""flash_attention_2""
packing: false
bf16: ""auto""
fsdp: ""full_shard auto_wrap offload""
fsdp_config:
  limit_all_gathers: true
  backward_prefetch: ""backward_pre""
  forward_prefetch: ""false""
  use_orig_params: ""false""
  min_num_params: 0
  activation_checkpointing: ""true""
```



*Full Stack Trace*
```
Traceback (most recent call last):
  File ""train.py"", line 224, in <module>
main(cfg)
  File ""train.py"", line 207, in main
    main(cfg)
  File ""train.py"", line 207, in main
    trainer.train()
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2123, in train
    trainer.train()
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2123, in train
Traceback (most recent call last):
  File ""train.py"", line 224, in <module>
main(cfg)main(cfg)
  File ""train.py"", line 207, in main
trainer.train()trainer.train()
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2123, in train
return inner_training_loop(
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2481, in _inner_training_loop
    return inner_training_loop(
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2481, in _inner_training_loop
return inner_training_loop(return inner_training_loop(
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2481, in _inner_training_loop
tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 3612, in training_step
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 3612, in training_step
tr_loss_step = self.training_step(model, inputs, num_items_in_batch)tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 3612, in training_step
self.accelerator.backward(loss, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py"", line 2241, in backward
Traceback (most recent call last):
  File ""/opt/ml/code/train.py"", line 224, in <module>
    main(cfg)
  File ""/opt/ml/code/train.py"", line 207, in main
    trainer.train()
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2123, in train
    return inner_training_loop(
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 3612, in training_step
    self.accelerator.backward(loss, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py"", line 2241, in backward
    loss.backward(**kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/_tensor.py"", line 492, in backward
    torch.autograd.backward(
  File ""/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 1075, in unpack_hook
    frame.check_recomputed_tensors_match(gid)
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 850, in check_recomputed_tensors_match
    raise CheckpointError(
torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
tensor at position 18:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
tensor at position 19:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
loss.backward(**kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/_tensor.py"", line 492, in backward
    loss.backward(**kwargs)
          File ""/opt/conda/lib/python3.10/site-packages/torch/_tensor.py"", line 492, in backward
self.accelerator.backward(loss, **kwargs)self.accelerator.backward(loss, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py"", line 2241, in backward
  File ""/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py"", line 2241, in backward
    torch.autograd.backward(
  File ""/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 251, in backward
    torch.autograd.backward(
  File ""/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 1075, in unpack_hook
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 1075, in unpack_hook
frame.check_recomputed_tensors_match(gid)
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 850, in check_recomputed_tensors_match
        loss.backward(**kwargs)loss.backward(**kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/torch/_tensor.py"", line 492, in backward
  File ""/opt/conda/lib/python3.10/site-packages/torch/_tensor.py"", line 492, in backward
    frame.check_recomputed_tensors_match(gid)
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 850, in check_recomputed_tensors_match
        torch.autograd.backward(torch.autograd.backward(
  File ""/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 251, in backward
  File ""/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 251, in backward
    raise CheckpointError(
torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
tensor at position 18:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
tensor at position 19:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
        Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward passVariable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 1075, in unpack_hook
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 1075, in unpack_hook
    raise CheckpointError(
torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
tensor at position 18:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
tensor at position 19:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
frame.check_recomputed_tensors_match(gid)frame.check_recomputed_tensors_match(gid)
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 850, in check_recomputed_tensors_match
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py"", line 850, in check_recomputed_tensors_match
    raise CheckpointError(
torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
tensor at position 18:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
tensor at position 19:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
    raise CheckpointError(
torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
tensor at position 18:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}
tensor at position 19:
saved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}
recomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}
0%|          | 0/100 [00:13<?, ?it/s]
[E ProcessGroupGloo.cpp:138] Rank 5 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank.
[E ProcessGroupGloo.cpp:138] Rank 4 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank.
[E ProcessGroupGloo.cpp:138] Rank 7 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank.
[2024-11-25 18:39:43,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 69 closing signal SIGTERM
[2024-11-25 18:39:43,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 73 closing signal SIGTERM
[2024-11-25 18:39:43,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 74 closing signal SIGTERM
[2024-11-25 18:39:43,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 76 closing signal SIGTERM
[2024-11-25 18:39:47,931] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 70) of binary: /opt/conda/bin/python
```


### Expected behavior

The expected behavior is for the SFTTrainer's `train()` method to run without errors.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Uniformize zero-shot object detection postprocessing methods,"## Uniformizing Zero-Shot Object Detection post-processing  

### Introduction  
Currently, we have four **zero-shot object detection models** in the Transformers library:  

- **OwlVit**  
- **OwlV2**  
- **Grounding Dino**  
- **OmDet Turbo**  

Each model uses slightly different **postprocessing arguments** and produces **different output formats**, which complicates user experience and makes it harder to use them in pipelines.  

To address these inconsistencies, proposed a **unified postprocessing interface** for all four models. This will enhance usability, reduce confusion, and enable seamless integration with existing pipelines.  


## Comparison of Postprocessing Methods  

Below is a comparison of the current `postprocessing` methods and their arguments:  

| **Model**         | **Postprocessing Method**                                                                                                  | **Key Arguments**                                                                                                                                                                   |
|--------------------|--------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **OwlVit / OwlV2** | post_process_object_detection       | `outputs`, `threshold`, `target_sizes`                                                                                                                                            |
| **Grounding Dino** | post_process_grounded_object_detection    | `outputs`, `input_ids`, `box_threshold`, `text_threshold`, `target_sizes`                                                                                                         |
| **OmDet Turbo**    | post_process_grounded_object_detection | `outputs`, `classes`, `score_threshold`, `nms_threshold`, `target_sizes`, `max_num_det`                                                                                |

### Suggested Changes to Arguments  

To standardize postprocessing across all models, the following suggestions are proposed:  

1. **Standardize Method Naming:**  

Use a single method, `post_process_grounded_object_detection`, for all models for text-guided object detection. For backward compatibility, retain additional methods (e.g., OwlVit/OwlV2’s `post_process_object_detection`) with a deprecation cycle.  

2. **Unify Required Arguments:**  

Make `outputs` the only required argument.  
     - For **Grounding Dino**, pass `input_ids` inside the `outputs` parameter.  
     - For **OmDet Turbo**, make `classes` optional to provide additional flexibility.  

3. **Rename Threshold Parameters:**  

Standardize parameter names (`score_threshold` and `box_threshold`) to a single name: `threshold`. These parameters perform the same function (filtering detections by confidence score), so a uniform name reduces confusion.  

4. **Add `text_labels` Argument:**  

Introduce an optional `text_labels` parameter to map detected labels (integer IDs) to their corresponding text names.  

### Final Unified Method Signature  

The new method would look like this:  

```python  
def post_process_grounded_object_detection(
    self,
    outputs,
    threshold: float = ...,
    target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,
    text_labels: Union[List[str], List[List[str]]] = None,
    <additional model-specific params>
)
```  

---

## Postprocessing Outputs  

### Current outputs by post processing  

| **Model**         | **Current Output Format**                                                                                         |
|--------------------|------------------------------------------------------------------------------------------------------------------|
| **OwlVit / OwlV2** | `{""scores"": score, ""labels"": label, ""boxes"": box}`<br> *(labels are integer class IDs)*                          |
| **Grounding Dino** | `{""scores"": score, ""labels"": label, ""boxes"": box}`<br> *(labels are text names decoded of detected objects from `input_ids`)*         |
| **OmDet Turbo**    | `{""scores"": score, ""classes"": class, ""boxes"": box}`<br> *(classes are text names of detected objects)*           |

---

#### Suggested unified output format  

The output format will be standardized to:  

```python  
{
    ""scores"": score,
    ""labels"": label,        # Integer class IDs
    ""boxes"": box,           # Detected bounding boxes
    ""text_labels"": text     # Optional: text labels 
}
```  

### Detailed Model Changes  

#### **OwlVit / OwlV2**  
Current:  
```python  
{""scores"": score, ""labels"": label, ""boxes"": box}
```  

Proposed:  
```python  
{
    ""scores"": score,
    ""labels"": label,
    ""boxes"": box,
    ""text_labels"": text
}
```  

#### **Grounding Dino**  
Current:  
```python  
{""scores"": score, ""labels"": label, ""boxes"": box}
```  

Proposed:  
```python  
{
    ""scores"": score,
    ""labels"": text,  # Will be set to `None` with deprecation cycle
    ""boxes"": box,
    ""text_labels"": text
}
```  

#### **OmDet Turbo**  
Current:  
```python  
{""scores"": score, ""classes"": class, ""boxes"": box}
```  

Proposed:  
```python  
{
    ""scores"": score,
    ""labels"": label,         # Add integer labels
    ""boxes"": box,
    ""text_labels"": text,     # Copy of current `classes`
    ""classes"": text         # Retain temporarily, remove with deprecation cycle
}
```  

Feel free to provide feedback on the suggested changes!

### Motivation

This will enhance usability, reduce confusion, and enable integration with existing zero-shot object detection pipelines.

### Your contribution

I will work on this and already have draft PRs.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",0,open
Allowing dataloader prefetching to work with iterable datasets.,"# What does this PR do?

This PR fixes #34867 which noted that dataloader prefetching was not occurring for iterable datasets. This PR also refactors the dataloader creation code in the trainer so that the train/eval/test dataloaders all use the same underlying function rather than repeating the same logic in multiple places. I added two tests to confirm that the prefetching works with iterable datasets as expected.

## Before submitting
- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request), Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?

## Who can review?

@muellerzr @SunMarc",[],0,open
`transformers.image_transforms.resize` doesnot work for negative values,"### System Info

- `transformers` version: 4.47.0.dev0
- Platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
- Python version: 3.12.5
- Huggingface_hub version: 0.25.0
- Safetensors version: 0.4.5
- Accelerate version: 1.0.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.1+cu121 (False)
- Tensorflow version (GPU?): 2.17.0 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@qubvel @amyeroberts 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

If you try to resize an image using `transformers.image_transforms.resize` after normalization has been applied using `transformers.image_transforms.normalize`, it raises error

This was the origin of this problem: https://github.com/huggingface/transformers/pull/34583#issuecomment-2497099400

**Reproduce**

code
```py
import numpy as np
from transformers.image_transforms import resize, normalize, to_pil_image
import torch

# scaled image between 0 and 1
x = np.random.random((3, 64, 64))

x = normalize(x, mean=0.5, std=0.5)

# the raises an error 
x = resize(x, size=(400, 400))

# this would work without any error
# x = torch.nn.functional.interpolate(
# 	torch.from_numpy(x).unsqueeze(0),
# 	(400, 400), mode='bilinear'
# )
```
error
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
---> 11 x = resize(x, size=(400, 400))

File transformers/image_transforms.py:337, in resize(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)
    336 if not isinstance(image, PIL.Image.Image):
--> 337     do_rescale = _rescale_for_pil_conversion(image)

File transformers/image_transforms.py:158, in _rescale_for_pil_conversion(image)
--> 158     raise ValueError(
ValueError: The image to be converted to a PIL image contains values outside the range [0, 1], got [-0.9999031475386853, 0.9996538887265269] which cannot be converted to uint8.
```

**Explanation**

1. in this specific normalization, resultant array gets negative values
2. during `resize` internally it tries to convert the array to `PIL` image
3. before converting, this function is called `transformers.image_transforms._rescale_for_pil_conversion` which expects the input to be non-negative and hence raises the exception
4. why does it require the image to be non-negative?

**Possible Solution**

I can think of a few possible solutions
- use `torch` instead of `PIL` or use both and create condition that selects the option depending on the data or put this as an option for user
- use scipy interpolations or custom functions for interpolations
- scale this image for range [0, 255] but we donot know can't scale safely without knowing the original mean and std that were used to normalize the image
- keep this function as it is and create another function that can interpolate negative values as well, based on one of the above methods


### Expected behavior

resize works with normalized data that may contain negative values, as interpolation is possible with negative values","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",0,open
"🚨🚨🚨🚨🚨🚨🚨🚨🚨 default to `""auto""` dtype","# What does this PR do?
Fixes #34743 by defaulting to `auto` dtype (meaning the type saved in the config) instead of always using `torch.float32`. 

This is hugely breaking",[],1,open
[RWKV] Add RWKV6 model,"# What does this PR do?

This is currently a draft. I will supplement the documentation and related tests as soon as possible. RWKV6 and RWKV5 are derived from the same source, and I believe it is necessary to skip RWKV5 and only support RWKV6. After the RWKV7 pre-trained model is released, I will add support for RWKV7.
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
add istft decoder for VITS model,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Add the ISTFT, MS_ISTFT, MB_ISTFT decoder for VITS model.   New decoder synthesized speech as natural as that synthesized by VITS while achieving a real-time factor of 0.066 on an Intel Core i7 CPU, 4.1 times faster than original VITS. Suitable for real-time and edge device applications
paper: https://arxiv.org/pdf/2210.15975
checkpoint:
istft: https://huggingface.co/anhnct/ljspeech_vits_istft
ms_istft: https://huggingface.co/anhnct/ljspeech_vits_ms_istft
mb_istft: https://huggingface.co/anhnct/ljspeech_vits_mb_istft

Using:
```
from transformers import VitsModel, AutoTokenizer
import torch
import numpy as np

model = VitsModel.from_pretrained(""anhnct/ljspeech_vits_istft"")
tokenizer = AutoTokenizer.from_pretrained(""anhnct/ljspeech_vits_istft"")

text = ""Hey, it's Hugging Face on the phone""
inputs = tokenizer(text, return_tensors=""pt"")

with torch.no_grad():
    output = model(**inputs).waveform

data_np = output.numpy()
data_np_squeezed = np.squeeze(data_np)

import scipy

scipy.io.wavfile.write(""techno.wav"", rate=model.config.sampling_rate, data=data_np_squeezed)
```
## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@ylacombe 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
[i18n-zh] Translating docs to Chinese Translate bertlogy.md into Chinese,"I have translated bertlogy.md into Chinese and my PR is here #34908 .This is my second contribution, and I hope to do better than the last time. Welcome anyone to review it.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
Fix for compile: use call instead of forward,"# What does this PR do?

Use model's `__call__` instead of `forward` directly to be able to work with torch compile used in the form of `model.compile`.

Able to see runtime drop for llama 7b without and with this change (1336 to 1017 seconds) and compile time increase (_compile.compile_inner: 10.2451 to 54.6842 seconds probably due to recompiles)

Fixes #34906

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
torch.compile: generate should use call instead of forward,"### System Info

- `transformers` version: 4.47.0.dev0
- Platform: Linux-5.14.0-284.73.1.el9_2.x86_64-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.25.2
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.6.0.dev20241008+cu124 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: NO

### Who can help?

@ArthurZucker @Cyrilvallez 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = ""facebook/opt-125m""
length = 100

prompt_text = 'In a small, bustling cafe nestled in the heart of a vibrant city, a serendipitous event unfolded, leaving a lasting impression on all who witnessed it. As the patrons sat sipping their coffees and engaging in animated conversations, a talented street musician entered the cafe, carrying a weathered guitar and radiating an aura of creativity.'

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(model_name)
model.compile()
input_ids = tokenizer(prompt_text, add_special_tokens=False, return_tensors='pt').input_ids
output = model.generate(input_ids, max_new_tokens=length)
```

### Expected behavior

Expected behaviour is that we use the compiled forward function.

When compiling using the [`model.compile()`](https://github.com/pytorch/pytorch/blob/a6344c8bcd22798987087244e961cdc0cbf9e9df/torch/nn/modules/module.py#L2985) API, the call method uses an [internal variable](https://github.com/pytorch/pytorch/blob/a6344c8bcd22798987087244e961cdc0cbf9e9df/torch/nn/modules/module.py#L1736) with the compiled forward instead of the uncompiled forward. 

(I raised a [related issue in pytorch](https://github.com/pytorch/pytorch/issues/141473), this is the Option 2 there)

So generate, should use the call method instead of the forward to use the compiled version of forward (for this particular case of model.compile).
However, recent changes have changed this call to model.forward() instead of model() for the non-first token :

```
def _sample():
  ...
  def model_forward(model, *args, **kwargs):
      return model.forward(*args, **kwargs)
  ...
      if i == 0:
          outputs = self(**model_inputs, return_dict=True)
          i += 1
      else:
          outputs = model_forward(self, return_dict=True, **model_inputs)
```

model_forward should be changed to call model() instead of model.forward()","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",6,open
WSD Scheduler to auto infer training steps,"### Feature request

WSD Scheduler should calculate stable steps in `trainer.py`. And if num_warmup_steps is provided in kwargs, schedule_func should respect the kwargs. 

My guess is that the intention is it to decay till min and stay there till the end of training, but `min_lr_ratio` is set to the default of 0, wouldn't the learning rate be always 0? Would like to have some insights on this if possible.

```
TypeError: get_wsd_schedule() missing 1 required positional argument: 'num_stable_steps'
```

Additionally, trying to pass in `num_warmup_steps` in `lr_scheduler_kwargs` will result in duplicate keys:
```
     return schedule_func(optimizer, num_warmup_steps=num_warmup_steps, **scheduler_specific_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: transformers.optimization.get_wsd_schedule() got multiple values for keyword argument 'num_warmup_steps'
```



### Motivation

I want to run WSD scheduler for my training, but I do not want to have to calculate the stable steps.

### Your contribution

I can contribute to this, but I would like to better understand the edge cases or possible scenarios I might have missed out from the maintainers. However, here is my current workaround:

```
def get_wsd_schedule(
    + num_training_steps: int = 0,
):

...
    assert num_stable_steps or num_training_steps, ""One of either stable steps or training steps must be provided""
    if not num_stable_steps:
        num_stable_steps = num_training_steps - num_warmup_steps - num_decay_steps
```

```
    if name == SchedulerType.WARMUP_STABLE_DECAY:
        return schedule_func(optimizer, num_warmup_steps=num_warmup_steps,num_training_steps=num_training_steps, **scheduler_specific_kwargs)
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
ViTImageproc handle pil hw images,"# What does this PR do?

fixes #34820 

Modifies image transformations and ViTImageProcessor such that HW PIL images can be have __call__ applied given the right arguments.

To supply an HW image to the ViTImageProcessor use the `input_data_format='none' or `input_data_format=ChannelDimension.NONE`. Further it is required that this is converted to RGB via the `do_convert_rgb=True` as the first option

```
    dataset = load_dataset(""ylecun/mnist"")
    processor = AutoImageProcessor.from_pretrained(""farleyknight-org-username/vit-base-mnist"")

    def process(examples):
        processed_inputs = processor(examples[""image""],
                                             input_data_format=""none"",
                                             do_convert_rgb=True)
        return processed_inputs


    processed_dataset = dataset.map(process, batched=True)
``` 
or to leave it as a single channeled np.array by setting the mean and std of the image processor with `image_mean=[mean_value]` and `image_std=[std_value]`
```
...
    def process(examples):
        processed_inputs = teacher_processor(examples[""image""],
                                             input_data_format=""none"",
                                             do_convert_rgb=False,
                                             image_mean=[.5],
                                             image_std=[.5])
        return processed_inputs
...
```



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
 @amyeroberts, @qubvel
","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",0,open
Add Relation DETR,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR adds Relation-DETR as introduced in [Relation DETR: Exploring Explicit Position Relation Prior for Object Detection](https://arxiv.org/abs/2407.11699). Checkpoint for Relation-DETR (ResNet50) converted from original repo https://github.com/xiuqhou/Relation-DETR has been uploaded to https://huggingface.co/xiuqhou/relation-detr-resnet50

Related issues in original repo: 
https://github.com/xiuqhou/Relation-DETR/issues/25
https://github.com/xiuqhou/Relation-DETR/issues/21

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


TODO:
- [x] Make more checkpoints with Swin-L and Focal-L backbones available on HF.
- [x] Update the document about Relation-DETR.

## Who can review?

@amyeroberts @qubvel


<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",5,open
Fix auxiliary_outputs types in loss,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR fix process of auxiliary_outputs and other returns when `return_dict = False`. `auxiliary_outputs` is not a tuple of returns, it should be processed as an element of return tuple, similar to RT-DETR (https://github.com/huggingface/transformers/blob/main/src/transformers/models/rt_detr/modeling_rt_detr.py#L2137). 

![image](https://github.com/user-attachments/assets/e69c4131-7274-4cd4-899d-141ad5b8af82)



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",4,open
Loading LLaVA-NeXT from AutoProcessor recently started causing errors in the forward function,"### System Info

container built on 24th of october:
including
  pip install tqdm
  pip install torch 
  pip install torchvision
  pip install transformers
  pip install deepspeed==0.15.2
  pip install accelerate
  pip install wandb
  pip install lightning
  pip install optuna
  pip install ray[tune]
  pip install pyarrow
  pip install nltk

  pip install pandas
  pip install numpy
  pip install matplotlib

  pip install scipy
  pip install scikit-learn

  pip install bitsandbytes
  pip install peft
  pip install pillow
  pip install flash-attn --no-build-isolation

### Who can help?

@zucchini-nlp 
@arthur

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

using a batch size of 8 in a train_collate function with the AutoProcessor as well as LLaVA-Next Processor:

    processor = AutoProcessor.from_pretrained(cfg.MODEL_ID_LLAVA_NEXT) # this used to work 
    # processor = LlavaNextProcessor.from_pretrained(cfg.MODEL_ID_LLAVA_NEXT)  # same result if I try this instead
     
    
    def train_collate_fn(examples, processor):
      processor.tokenizer.padding_side = ""right""  # during training pad on the right side
      # collect images, texts and true coordinates of the examples batch
      images = [example[0] for example in examples]
      #print(""image"", images[0])
      texts = [example[1][0][""value""] for example in examples]
  
      batch = processor(images=images,                # images to be processed
                        text=texts,                   # text to be tokenized
                        padding=True,                 # pad the sequences if not long enough
                        truncation=True,              # truncate sequences longer than max_length
                        max_length=cfg.MAX_LENGTH,    # set the maximum output token length
                        return_tensors=""pt"")          # return a pytorch tensor
      return batch[""input_ids""], batch[""attention_mask""], batch['pixel_values'], batch[""image_sizes""]


in the forward pass:
        # the forward pass
        outputs = self.base_model(
            input_ids=input_ids, 
            attention_mask=attention_mask, 
            pixel_values=pixel_values,
            image_sizes = image_sizes,
            #vision_feature_layer =-1,
            #vision_feature_select_strategy = ""full"", 
            output_hidden_states = True,  # all hidden states should be output
            return_dict = True  # to get a dict output that can be accessed with the . operator
        )
        
        
        all of this worked two weeks ago. But now I get this error:
        
return self.model.forward(*args, **kwargs)
File ""/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py"", line 170, in new_forward
output = module._old_forward(*args, **kwargs)
File ""/opt/conda/lib/python3.10/site-packages/transformers/models/llava_next/modeling_llava_next.py"", line 873, in forward
inputs_embeds, attention_mask, position_ids, labels, _ = self._merge_input_ids_with_image_features(
File ""/opt/conda/lib/python3.10/site-packages/transformers/models/llava_next/modeling_llava_next.py"", line 551, in _merge_input_ids_with_image_features
raise ValueError(
ValueError: Number of image tokens in input_ids (2040) different from num_images (8).

### Expected behavior

did not throw this error before","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",8,open
Mimic `adamw_torch_4bit` and have `adamw_torch_8bit`,"### Feature request

Hi thanks for the lib! Currently there is `adamw_torch_4bit`, but I hope to mimic it to have a `adamw_torch_8bit` that uses 8bit torchao adamw.

The reason is that, I would like to use deepspeed cpu offload for the optimizer, and also use 8bit adamw. However, the 8bit one in current hf transformers does not support cpu, so I need to use the torchao one.

### Motivation

-

### Your contribution

yes, willing to PR","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
"Bye bye env vars, keep everything as configs","# What does this PR do?

As @BenjaminBossan found, (https://github.com/huggingface/accelerate/pull/3252), the `TrainingArguments` will set environmental variables automatically when using Accelerate because before it wouldn't work otherwise. Nowadays the only env variable required for things to run smoothly is the ones for model init (fsdp cpu eff ram). 

This PR does a few things:

1. We completely remove the need for environmental variables, creating the proper configs (dynamo relies on https://github.com/huggingface/accelerate/pull/3251)
2. I've refactored how `mixed_precision` gets set, to simplify the training arguments and combine 7 args into 2. 

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@SunMarc @Rocketknight1 
",[],3,open
GroundingDINO cannot work with MiniGPT4,"### System Info

- `transformers` version: 4.46.2
- Platform: Linux-5.15.0-120-generic-x86_64-with-glibc2.35
- Python version: 3.12.4
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.4.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: 1
- GPU type: NVIDIA A100-PCIE-40GB

### Who can help?

@zucchini-nlp  @amyeroberts 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

When I use the minigpt4 model from the repository ·https://github.com/Vision-CAIR/MiniGPT-4·, I find that grounding dino cannot be used together with it.

Specifically, when I import some necessary content from the minigpt4 repository into my project (without doing anything else about the minigpt4 repo) and use transformers grounding dino model, dino crashes the program directly at the `model(**encoded_inputs)` call with an error code of SIG(117), and no traceback or other information is provided.

Other models, such as flan-t5-base-VG-factual-sg, do not crash during their forward pass even when minigpt4 is imported.

After commenting out the four import lines related to minigpt4, there are no issues anymore.
```python
import torch
from PIL import Image
from transformers import (
    GroundingDinoForObjectDetection,
    GroundingDinoProcessor,
)

# imports modules for registration
from minigpt4.datasets.builders import *  # noqa
from minigpt4.models import *  # noqa
from minigpt4.processors import *  # noqa
from minigpt4.tasks import *  # noqa

image_path = ""/root/llm-project/LVLM/eval/Extended_CHAIR/images/chair-500/000000006763.jpg""
image: Image.Image = Image.open(image_path)
model: GroundingDinoForObjectDetection = (
    GroundingDinoForObjectDetection.from_pretrained(
        ""IDEA-Research/grounding-dino-base"",
        cache_dir=""/root/llm-project/utils/models/hub"",
        torch_dtype=""auto"",
        low_cpu_mem_usage=True,
    )
    .to(""cuda"")
    .eval()
)

processor: GroundingDinoProcessor = GroundingDinoProcessor.from_pretrained(
    ""IDEA-Research/grounding-dino-base"",
    cache_dir=""/root/llm-project/utils/models/hub"",
)

text = ""man.umbrella.top hat.""

with torch.inference_mode():
    encoded_inputs = processor(
        images=image,
        text=text,
        max_length=200,
        return_tensors=""pt"",
        padding=True,
        truncation=True,
    ).to(""cuda"")
    outputs = model(**encoded_inputs) # Crash here
    results = processor.post_process_grounded_object_detection(
        outputs=outputs,
        input_ids=encoded_inputs[""input_ids""],
        box_threshold=0.25,
        text_threshold=0.25,
    )
    print(results)
```

### Expected behavior

Since this issue is related to other repositories, I would like to ask if you can help resolve this problem? Or kindly just guide me on how to find the deeper cause? Combining multiple models is significant for my project, but this issue does not provide any traceback, leaving me without a starting point.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",5,open
Add fp8 llama script,"# What does this PR do ?

This PR adds the script that was used to quantize the 405B llama in FP8. 

I'm not sure where to put it for now ",[],1,open
SmolLM is ExecuTorch Compatible,"### Feature request

### Feature request

Enable SmolLM to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow.

#### Instructions
Instructions of how to enable this model for ExecuTorch:
 1. Export the model to ExportIR. For LLM, to run with performance, typically you will need to export the model with cache. #34101 is a reference of how to export and validate the model. Note that you may run into some export issue and it may require fixes in the modeling code.
 2. Lower the model to ExecuTorch (to generate a .pte file). You will need to clone the github repo and create a recipe to lower the model. For example lowering the to XNNPACK is the simplest way. See the example code here: https://github.com/pytorch/executorch/blob/release/0.4/extension/export_util/export_hf_model.py#L89L106
 3. Run the model with ExecuTorch. You can follow these instructions to build and run the executor runtime for llama: https://github.com/pytorch/executorch/tree/release/0.4/examples/models/llama2#step-4-run-on-your-computer-to-validate

(Optional) Congrats! Once you complete step 1-3, you will be able to run the model on a host machine. Now if you would to go further like making the model faster, smaller, cheaper for your model use-case, you can create more complicated recipes with quantizations and delegations for different HW accelerators. You can find more tutorials on our website, for example to optimize and run the model with Core ML on Apple’s platform: https://pytorch.org/executorch/stable/build-run-coreml.html

### Motivation

See details in #32253

### Your contribution

TBD","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",3,open
Enable different torch dtype in sub models,"# What does this PR do?

Fixes https://github.com/huggingface/transformers/issues/33997. Enables users to use different torch dtypes for each of sub config. For ex load the vision model in full precision and the text model in half precision",[],3,open
Data prefetching does not occur for iterable datasets,"### System Info

- `transformers` version: 4.46.1
- Platform: macOS-15.1-arm64-arm-64bit
- Python version: 3.11.10
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No

### Who can help?

@muellerzr @SunMarc 

### Reproduction

PR #28498 was meant to allow for specifying the Pytorch dataloader's `prefetch_factor` argument via the huggingface `dataloader_prefetch_factor` training argument. As we can see [on this line](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L989), this feature was added inside of a 
```python
if not isinstance(train_dataset, torch.utils.data.IterableDataset):
```
statement which results in prefetching never occurring for `IterableDataset`s (which seems like a mistake). There are a two other lines where the same error happens for [test](https://github.com/huggingface/transformers/blob/4e90b99ed916300b80bac9db793f2a96b2a87122/src/transformers/trainer.py#L1126) and [eval](https://github.com/huggingface/transformers/blob/4e90b99ed916300b80bac9db793f2a96b2a87122/src/transformers/trainer.py#L1084) dataloaders. Unless I'm missing something, I believe these lines can be moved out of the if condition and into other logic. 

### Expected behavior

Prefetching should work with iterable datasets.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
"Fine tuning SAM, 'The input_points must be a 3D tensor. Of shape `batch_size`, `nb_boxes`, `4`.', ' got torch.Size([2, 4]).'","Hi,

I am trying to fine tune SAM on custom images and masks but am struggling and am hoping someone can point me in the right direction to resolving it.

I have been referencing this code:
https://github.com/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb
which is based on this:
https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb

I cannot get the training to work as I get this message at the forward pass step:
`'The input_points must be a 3D tensor. Of shape `batch_size`, `nb_boxes`, `4`.', ' got torch.Size([2, 4]).'`

I think the input_boxes is wrong somehow?

![image](https://github.com/user-attachments/assets/6033863d-65b6-43db-99ea-1cab9be4dd5e)

The images I am using are colour PNG images rather than the tiff images in the reference code and are showing with 3 channels here....
![image](https://github.com/user-attachments/assets/1f9527e7-c4f2-46c6-9a12-1bd678d920c0)

My SamDataset code is:

```python
class SAMDataset(Dataset):
  """"""
  This class is used to create a dataset that serves input images and masks.
  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.
  """"""
  def __init__(self, dataset, processor):
    self.dataset = dataset
    self.processor = processor

  def __len__(self):
    return len(self.dataset)

  def __getitem__(self, idx):
    item = self.dataset[idx]
    image = item[""image""]
    ground_truth_mask = np.array(item[""label""])
    
    # get bounding box prompt
    # prompt = get_bounding_box(ground_truth_mask)
    prompt = item[""bounding_box""]

    # prepare image and prompt for the model
    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=""pt"")

    # remove batch dimension which the processor adds by default
    inputs = {k:v.squeeze(0) for k,v in inputs.items()}

    # add ground truth segmentation
    inputs[""ground_truth_mask""] = ground_truth_mask

    return inputs
```

and this is where I run into trouble...
![image](https://github.com/user-attachments/assets/1991b162-ec9d-431b-b398-8bee4a666ce5)
",[],3,open
[WIP] Add flex attention for gpt2,"# What does this PR do?

Adding flex_attention for Gpt2 model following https://github.com/huggingface/transformers/issues/34809

## Who can review?


",[],5,open
Offline mode doesn't work with models that require `trust_remote_code=True`,"### System Info

Google Colab:
- `transformers` version: 4.46.3
- Platform: Linux-6.1.85+-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.5.1+cu121 (False)
- Tensorflow version (GPU?): 2.17.1 (False)
- Flax version (CPU?/GPU?/TPU?): 0.8.5 (cpu)
- Jax version: 0.4.33
- JaxLib version: 0.4.33
- Using distributed or parallel set-up in script?: no

### Who can help?

@Rocketknight1

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

#### Issue
Models that require `trust_remote_code=True` can't be fully saved & loaded with `save_pretrained()` + `from_pretrained()`.
In [offline mode](https://huggingface.co/docs/transformers/v4.46.2/en/installation#offline-mode) on a new machine during calling `from_pretrained()` it doesn't locate all required files in the saved local dir and tries to reach out to hf hub for the remote code part.

#### How to reproduce
[Colab](https://colab.research.google.com/drive/1_yX94aJngMChp6pANWg6Z6F_9G8Kyb3k?usp=sharing) | [Kaggle](https://www.kaggle.com/code/vladyslavkha/hf-offline-mode-trust-remote-code-models-issue)
Tested with popular [`jinaai/jina-embeddings-v3`](https://huggingface.co/jinaai/jina-embeddings-v3)
Includes step by step reproduction + results

---
## Additional context
- Stumbled on this in Kaggle Notebook env for competition.
Some Kaggle competitions require submitting code in Kaggle Notebooks, which are run later on private data and **don't allow internet access**.
Practically, this means you must prepare all models in advance, upload them as dependencies to the submission notebook.
So having transformers trying to reach out to hf-hub (when the model is already pre-downloaded) is not an option and disqualifies a group of models from usage.
- Originally raised in https://github.com/UKPLab/sentence-transformers/issues/2613.
Received guidance by @tomaarsen in https://github.com/UKPLab/sentence-transformers/issues/2613#issuecomment-2076964416 to overcome the issue with `sentence-transformers` (code snippet included)
- Raising as a bug report, LMK if better to reraise as FR. Would be happy to try to contribute if confirmed

### Expected behavior

Model is fully saved in local dir with `save_pretrained()` and can be fully loaded from a local path with `from_pretrained()` in offline mode","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Grounding DINO Processor standardization,"# What does this PR do?

Simplification and standardization of zero-shot object detection image processor for Grounding DINO

### Add Input Text Preprocessing

Grounding DINO processor handle candidate labels/classes differently compared to other zero-shot object detection models. Specifically:

- **Grounding DINO** requires labels to be merged into a single string with a period (`.`) as the separator:  
  ```python
  text = ""a cat. a dog.""
  ```

- **OWLv2/OwlVit/OmDet** expects labels to be provided as a list of classes:  
  ```python
  text = [""a cat"", ""a dog""]
  ```

This PR introduces support for passing a list of labels to Grounding DINO processors, aligning its behavior with other processors. A preprocessing method has been added to format labels as required. The change is BC, labels can still be provided in both formats: either as a single string (`""a cat. a dog.""`) or as a list of labels (`[""a cat"", ""a dog""]`).

### Simplification of Postprocessing

Previously, the `post_process_grounded_object_detection` method required `input_ids` to be explicitly passed. This PR makes `input_ids` optional (maintaining backward compatibility) by enabling them to be extracted directly from the model output if not provided.

### Standardization of post-processing output
Added `text_labels` field to post-processed output, which represents string object names (across all other zero shot object detection processors). 

see #34926 for more details on standardization of post-processing 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",5,open
Gemma flex attention,"# What does this PR do?

Towards https://github.com/huggingface/transformers/issues/34809

- Adds Flex Attention for Gemma
- Does refactoring to enable the attention mechanisms using functions instead of classes


Who can review?
@ArthurZucker",[],7,open
Add Flex Attention for Mistral along with refactoring,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Towards #34809

- Adds Flex Attention for Mistral
- Does refactoring to enable the attention mechanisms using functions instead of classes


## Who can review?

@ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],6,open
Comments update for better reading,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
"When set num_beams in GenerationConfig, stop_strings parameter has no effect","### System Info

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.

- `transformers` version: 4.46.2
- Platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.39
- Python version: 3.10.15
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.5.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 4070 SUPER

### Who can help?

@gante 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

## Code
```python
from transformers import GenerationConfig, AutoTokenizer, AutoModelForCausalLM

generate_config = GenerationConfig(
    num_beams=3,
    do_sample=True,
    temperature=0.7,
    num_return_sequences=3,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.0,
    length_penalty=1.0,
    stop_strings="":"",
    return_dict_in_generate=True,
    max_new_tokens=500,
    output_logits=True
)


tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH).cuda()

PROMPT = ""Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?""


tokens = tokenizer(PROMPT, return_tensors=""pt"").to(model.device)
out = model.generate(**tokens, generation_config=generate_config, tokenizer=tokenizer)

print(tokenizer.decode(out.sequences[0], skip_special_tokens=True))
```

## Out
```
Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? To determine the total number of clips Natalia sold in April and May, we need to follow these steps:

1. Identify the number of clips sold in April.
2. Calculate the number of clips sold in May.
3. Add the number of clips sold in April and May together.

First, we know that Natalia sold 48 clips in April. Next, we need to find out how many clips she sold in May. According to the problem, she sold half as many clips in May as she did in April. Therefore, we calculate the number of clips sold in May as follows:
\[
\text{Number of clips sold in May} = \frac{48}{2} = 24
\]

Now, we add the number of clips sold in April and May together to find the total number of clips sold:
\[
\text{Total number of clips sold} = 48 + 24 = 72
\]

Thus, the total number of clips Natalia sold in April and May is \boxed{72}.
```
### Expected behavior

when I set num_beams=1, the stop_strings works well !","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
latest transformer has a tensorflow and numpy error,"### System Info

collab version of transformers (latest)

### Who can help?

@gante @rock

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Install latest transformers, numpy and tensorflow versions (more specifically colab versions)

You'll encounter an error specifically more tensorflow for this line:

```Python
from transformers.pipelines import PIPELINE_REGISTRY
```

How the error looks like:

```Bash
SystemError: initialization of _pywrap_checkpoint_reader raised unreported exception
```

### Expected behavior

It should not produce an error and pass clean like I did on the colab by downgrading versions. 

https://colab.research.google.com/drive/1482Nc7IfE-s5sl7dBjEbWfOjz59QFA1l?usp=sharing","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",6,open
HfArgumentParser error when using LoraConfig dataclass,"### System Info

- `transformers` version: 4.46.3
- Platform: Linux-5.14.0-162.18.1.el9_1.x86_64-x86_64-with-glibc2.35
- Python version: 3.10.15
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.5.0.dev20240802+rocm6.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: AMD Instinct MI250X/MI250
- peft version: 0.13.2


### Who can help?

@SunMarc @MekkCyber

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Looks like HfArgumentParser Throws an error when loading the LoraConfig dataclass from peft that I load in from a yaml config

Some of my code:

from peft import LoraConfig, get_peft_model 
from transformers import (
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
)
..

parser = HfArgumentParser([LoraConfig])
(parsed_dataclass,) = parser.parse_dict(flat_config, allow_extra_keys=True)

...

Leads to the following Traceback:

Traceback (most recent call last):
  File ""/work/scripts/omnihub.py"", line 12, in <module>
    main()
  File ""/work/scripts/omnihub.py"", line 8, in main
    o.run()
  File ""/work/scripts/omnihub/__init__.py"", line 177, in run
    self.func(self.extra_args, self.config)
  File ""/work/applications/hf-finetune/finetune.py"", line 225, in run
    FineTuner(*args, **kwargs).run()
  File ""/work/applications/hf-finetune/finetune.py"", line 186, in __init__
    self.parse_args(supported_dataclass, custom_args, config)
  File ""/work/applications/hf-finetune/finetune.py"", line 150, in parse_args
    parser = HfArgumentParser([dataclass])
  File ""/opt/conda/lib/python3.10/site-packages/transformers/hf_argparser.py"", line 137, in __init__
    self._add_dataclass_arguments(dtype)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/hf_argparser.py"", line 277, in _add_dataclass_arguments
    self._parse_dataclass_field(parser, field)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/hf_argparser.py"", line 167, in _parse_dataclass_field
    raise ValueError(
ValueError: Only `Union[X, NoneType]` (i.e., `Optional[X]`) is allowed for `Union` because the argument parser only supports one type per argument. Problem encountered in field 'init_lora_weights'.




### Expected behavior

I expect the dataclass to be created when using LoraConfig dataclass imported from peft","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Data collator class type integrity is not intact throughout the runtime,"### System Info

transformers: v4.46.3
python: 3.11

### Who can help?

trainer: @muellerzr @SunMarc

Anyone else in the community is as well open to comment or suggest. Thank you.

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

The original data type / class of the data collator meta information is completely lost after the below step which happens at couple of places as listed below.
 
https://github.com/huggingface/transformers/blob/f297af55dfc27485189f352cd36b4683de12e0b3/src/transformers/trainer.py#L975

https://github.com/huggingface/transformers/blob/f297af55dfc27485189f352cd36b4683de12e0b3/src/transformers/trainer.py#L1071

https://github.com/huggingface/transformers/blob/f297af55dfc27485189f352cd36b4683de12e0b3/src/transformers/trainer.py#L1113

The meta information loss happens since the collator is completely replaced with RemoveColumnsCollator wrapper collator class when the original dataset is **NOT** of type `datasets.Dataset`, mostly meant to support `datasets.IterableDataset` and others.

This raises issues for complex usecases where we wish to inject custom behaviour as part of the Trainer object when the collator is of certain class type and etc. Since the current code completely removes that piece of information and changes it to RemoveColumnsCollator, its really hard to know what was the original datacollator class. There are workarounds like writing case specific code handling `RemoveColumnsCollator` with special care, however, given the growing transformers code, things could change in the future and break such case specific code. On the other hand, it would be great to actually handle this situation better by preserving the original collator class information.


I propose the following to options

* Dynamically modify the `RemoveColumnsCollator` class to subclass from the original data collator class that was passed.

This is bit of a fancy way of doing by creating a custom class/type using Python's `type()` API.

OR

* Monkey patch the data collator object's caller functions (like `__call__` etc) to include the remove columns logic on top of it. This would mean to remove RemoveColumnsCollator completely and do a simple monkey patch.

Example of monkey patch being already adopted in existing HF code making it a good option for this fix following the existing code style

https://github.com/huggingface/accelerate/blob/d7b1b368e9f484a18636a71600566b757d5cf87e/src/accelerate/utils/operations.py#L819

I am happy to discuss and raise a PR to fix this behaviour. 

### Expected behavior

The class type information of the original data collator has to be intact and preserved throughout the runtime.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Flash attention 2 broke when batch inference,"### System Info

- `transformers` version: 4.46.2
- Platform: Linux-5.15.0-120-generic-x86_64-with-glibc2.35
- Python version: 3.10.15
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: Yes
- GPU type: NVIDIA A100-PCIE-40GB

### Who can help?

@zucchini-nlp 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Using the latest version of `vllm==0.6.4.post1` and `flash-attn==2.7.0.post2` using `pip install flash-attn --no-build-isolation`
When I use llava for batch inference, enabling flash_attention_2 makes the results particularly strange.
The code can be found in the huggingface doc https://huggingface.co/docs/transformers/main/en/model_doc/llava#batched-inference

``` python
import torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration

# Load the model in half-precision
model = LlavaForConditionalGeneration.from_pretrained(
    ""llava-hf/llava-1.5-7b-hf"",
    cache_dir=""/root/llm-project/utils/models/hub"",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    attn_implementation=""flash_attention_2"",
    device_map=""auto"")
processor = AutoProcessor.from_pretrained(
    ""llava-hf/llava-1.5-7b-hf"",
    cache_dir=""/root/llm-project/utils/models/hub"",
)
image1 = Image.open(""/root/llm-project/LVLM/eval/Extended_CHAIR/images/chair-500/000000006763.jpg"")
image2 = Image.open(""./demo/material/1.jpg"")

# Prepare a batch of two prompts
conversation_1 = [
    {
        ""role"": ""user"",
        ""content"": [
            {
                ""type"": ""image""
            },
            {
                ""type"": ""text"",
                ""text"": ""What is shown in this image? Please tell me."",
            },
        ],
    },
]

conversation_2 = [
    {
        ""role"": ""user"",
        ""content"": [
            {
                ""type"": ""image""
            },
            {
                ""type"": ""text"",
                ""text"": ""Describe this image.""
            },
        ],
    },
]

prompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)
prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)
prompts = [prompt_1, prompt_2]

# We can simply feed images in the order they have to be used in the text prompt
inputs = processor(
    images=[image1, image2], text=prompts, padding=True, return_tensors=""pt"").to(model.device, torch.float16)

# Generate
generate_ids = model.generate(**inputs, max_new_tokens=200)
out = processor.batch_decode(generate_ids, skip_special_tokens=True)
print(out)

```
The out put is:
``` python
['USER:  \nWhat is shown in this image? Please tell me. ASSISTANT: The image shows a man and a woman standing close to each other, posing for a picture. The man is wearing a tie, and they are both smiling for the camera. The scene takes place in a restaurant, as there are dining tables and chairs visible in the background.', 
'USER:  \nDescribe this image. ASSISTANT: The, the image, the image, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the,']
```
Remove `attn_implementation=""flash_attention_2"",` everything works well.
``` python
['USER:  \nWhat is shown in this image? Please tell me. ASSISTANT: The image shows a man and a woman standing close to each other, posing for a picture. The man is wearing a tie, and they are both smiling for the camera. The scene takes place in a restaurant, as there are dining tables and chairs visible in the background.', 
'USER:  \nDescribe this image. ASSISTANT: The image features a group of birds perched on a tree branch. There are five birds in total, with some sitting closer to the front of the branch and others further back. The birds are of various sizes and colors, creating a diverse and lively scene. The birds appear to be engaged in conversation or simply enjoying their time together on the branch.']
```
Adding code like `padding_side=""left"",` when init the processor don't make any difference.
### Expected behavior

Fix the bug","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",2,open
Maybe memory leak occurs after evaluation when using `use_liger_kernel`.,"### System Info

`transformers==4.46.1`
`python==3.10.14`

### Who can help?

@muellerzr @SunMarc @ArthurZucker 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Thank you for sharing such an amazing project. 

When I used `use_liger_kernel=True` for training, the training memory usage of the Gemma2 model dropped from around 60 GiB to 7 GiB.

However, after running evaluation, the memory usage jumps to 60 GiB. I understand that this is done by accumulating logit values ​​and calculating at the end. **But when resuming training, it doesn't return to the previous memory level, staying at 60 GiB instead.** It seems like there might be a memory leak somewhere.

https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.use_liger_kernel


### Expected behavior

The same phenomenon occurred when training with [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).
","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",6,open
query_length not found in BLIP-2 config,"In the code, there's a reference to `self.config.query_length` [[link]](https://github.com/huggingface/transformers/blob/30335093276212ce74938bdfd85bfd5df31a668a/src/transformers/models/blip_2/modeling_blip_2.py#L1362):

```python
past_key_values_length = (
    past_key_values[0][0].shape[2] - self.config.query_length if past_key_values is not None else 0
)
```

However, query_length doesn't exist in the Blip2Config. Instead, it seems that we should be using `num_query_tokens` which is already defined in Blip2Config. [[link]](https://github.com/huggingface/transformers/blob/30335093276212ce74938bdfd85bfd5df31a668a/src/transformers/models/blip_2/configuration_blip_2.py#L281)",[],2,open
HW based PIL images not being handled in pretrained image processor,"### System Info

transformers version 4.46.3

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```
from datasets import load_dataset
import numpy as np

dataset = load_dataset(""ylecun/mnist"")
teacher_processor = AutoImageProcessor.from_pretrained(""farleyknight-org-username/vit-base-mnist"")

def process(examples):
    processed_inputs =  teacher_processor(examples[""image""], input_data_format=""none"")`
    return processed_inputs
```

### Expected behavior

When using the mnist and dataset from `""ylecun/mnist""` and the finetuned VIT from `""farleyknight-org-username/vit-base-mnist""` from the same dataset using the loaded image processor fails, not allowing the shape of the image. Code is adapted from the [Knowledge Distillation for Computer Vision](https://huggingface.co/docs/transformers/main/en/tasks/knowledge_distillation_for_image_classification#knowledge-distillation-for-computer-vision)

The docstring for preprocess implies that this should be sufficient to allow for (height, width) format, but while resizing, there is only handling for ChannelDimension.First and ChannelDimension.Last, not for 'none' or ChannelDimension.None. 

the code fails under the calls stack

| Function | File | Line |
|:-----:| :----: | :----:|
| to_channel_dimension_format | image_transforms.py | 93
| to_pil_image | image_transforms.py | 204
| resize | image_transforms.py | 338
| resize | image_processing_vit.py | 138
| preprocess | image_processing_vit.py | 250

","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",5,open
"[`GPTQ`, `CompressedTensors`] Fix unsafe imports and metada check","# What does this PR do?
Fixes some unsafe imports for GPTQ and CompressedTensors as well as an unsafe metadata check for GPTQ, i.e. metadata can't be checked if the package doesn't exist...

I tried looking into integrating the env validation function before even creating the quantizers but that would require a complete restructure, especially since the env is checked after creation to get additional info like device map etc. So I kept it as simple as I could and wrapped the unsafe stuff.

Minimal reproducible script to trigger gptq errors (when optimum is not installed):
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig


model_id = ""facebook/opt-125m""
tokenizer = AutoTokenizer.from_pretrained(model_id)
dataset = [""auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.""]
gptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)


quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"", quantization_config=gptq_config)
```


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34765


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@SunMarc @MekkCyber ",[],5,open
Whisper: remove redundant assisted generation tests,"# What does this PR do?

Part of making CI green in preparation for https://github.com/huggingface/transformers/pull/34807 (test fetcher fetches generation tests)

1. Reverts recent default `max_length`-related changes. The default max length is `20` and, due to recent changes, a different length was being returned in some cases. Whisper was one of the affected models, it was defaulting to a length of `21`.
2. Deletes `test_assisted_decoding_encoder_decoder_shared_encoder` (integration test) -- the point of this test was to test assisted generation with DistilWhisper-like structure, which used to have a custom input (`assistant_encoder_outputs`). a) That input is no longer present in Whisper b) the DistilWhisper structure didn't take off, so it's overkill to have an abstract test with dummy classes c) we have other DistilWhisper tests.
3. Deletes `test_model_kwarg_assisted_decoding_encoder_decoder` (integration test) -- added when DistiWhisper was added. Meanwhile, assisted generation was modified to work with any encoder-decoder model, and we have mixin tests.",[],1,open
Flex attention + refactor,"Opening this to add support for all models following #34282

Lets bring support for flex attention to more models! 🤗 

- [x] Gemma2

It would be great to add the support for more architectures such as
- [ ] Qwen2
- [ ] Llama
- [ ] Gemma
- [ ] QwenVl
- [ ] Mistral
- [ ] Clip


... and many more

For anyone who wants to contribute just open a PR and link it to this issue, and ping me for a review!! 🤗 ","[{'id': 1834053813, 'node_id': 'MDU6TGFiZWwxODM0MDUzODEz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/PyTorch', 'name': 'PyTorch', 'color': 'a12bef', 'default': False, 'description': 'Anything PyTorch'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",7,open
"The usage of the ""forced_decoder_ids"" parameter","### Feature request

How to use the ""forced_decoder_ids"" parameter for decoder-only models? This parameter seems to be deprecated in the latest version.
![微信图片_20241119222811](https://github.com/user-attachments/assets/a5f2d44f-d98f-4959-85ae-e64489e3b9df)



### Motivation

This is an important function.

### Your contribution

This is an important function.

","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Tests: fetch generation tests,"WIP -- now that generation tests are being run, we can notice a few broken ones 👀 

Generation tests that were not part of the model mixin tests were not being fetched. This PR fixes it, by allowing each job to apply multiple separate filters to then run the **union** of all filtered tests. 

This PR:
- Enables non-mixin generation tests
- Fixes failing generation tests

_____________________________________________

👉 New files being fetched:
`test_preparation/tests_generate_test_list.txt`
![Screenshot 2024-11-19 at 14 35 41](https://github.com/user-attachments/assets/89f23542-e3df-48aa-a873-89b6b88155f4)


👉 dummy modification in a generation test triggering a test run: 
(early commit before tests were fixed)
![Screenshot 2024-11-19 at 14 38 12](https://github.com/user-attachments/assets/eec91f10-9063-4547-b806-975c259122f2)
",[],1,open
[`PixtralLarge`] Update Pixtral conversion script to support large format!,"# What does this PR do?
Updates the conversion script",[],1,open
Loading bigger models is very slow using `AutoModelForCausalLM.from_pretrained`,"### System Info

- `transformers` version: 4.45.0
- Platform: Linux-5.10.227-219.884.amzn2.x86_64-x86_64-with-glibc2.26
- Python version: 3.10.14
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 0.34.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.2.2 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: Yes
- Using GPU in script?: Yes
- GPU type: NVIDIA A10G

### Who can help?

@ArthurZucker @SunMarc 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

I am spawning an  `g5.12xlarge` GPU machine on AWS sagemaker and I am loading a locally saved model using this script:
```python
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3""

import torch
from transformers import AutoModelForCausalLM, AutoProcessor

model_id_or_path = ""<local_path>""

model = AutoModelForCausalLM.from_pretrained(model_id_or_path, device_map=""auto"", torch_dtype=torch.bfloat16, trust_remote_code=True)
```

This is the problem with almost all the models I am trying - `rhymes-ai/Aria` can be used to reproduce it.

### Expected behavior

The last line takes forever to load the model (>40-50 mins). I have observed the same behaviour for multiple other models as well.

Things I have tried/observed:
* The behaviour is observed  for the first time on an instance after an instance restart. Once I have loaded the model (by waiting for 40-50 mins) and then I restart the notebook kernel - all the subsequent model loads are very fast (almost instant).
* However, if I restart the instance - the problem is again observed for the first load.
* I suspected that it is taking time to figure out which GPU to put which layer on as I am using a cluster of 4 GPUs. For solving this, I saved the `device_map` of an already loaded model and passed it on to the loading constructor as `device_map` instead of using `auto` but it didn't not solve the issue.
* I also suspected that it might be an issue with slow memory read/write speeds so I benchmarked that by loading the model on CPU - it loaded in an instant so memory and I/O is not a blocker.
* The shards are following the same behaviour. For example, if I am trying to load a model having 10 shards and restarted the notebook after the first 4 shards are loaded - loading the model again takes very less time for those first 4 shards. I have verified that before the second model load - the usage is `0` so no leftover layers are remaining from the first load.
* The time taken to load the shards is also not uniform - some shards take 3 minutes, others are taking more than 10-15 mins.
* The model is saved in BF16 format already - so typecasting also doesnt seem to be an issue here.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",9,open
enable StaticCache for assisted generation,"@gante , I implemented a version for this issue: https://github.com/huggingface/transformers/issues/32946. Pls help comment, and I can iterate, thx.
",[],12,open
add a new flax example for Bert model inference,"# What does this PR do?

Add a new Flax inference example and show case how to optimize performance with Bfloat16 data types.
Most of the Flax examples are for Training/Fine Tuning, so it should be a good reference for inference



## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?



@ArthurZucker
@sanchit-gandhi
@stevhliu
@agramesh1
@mdfaijul",[],3,open
Allow to give the dataset multiprocessing_context,"### Feature request

In Huggingface Trainer, allow to pass the multiprocessing context : https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader

### Motivation

For a dataset that is loaded on multiple cpu cores, sometimes the fork method creates problems (with polars for example) and the spawn method is more adapted. 

### Your contribution

I could do a PR. A fix could be to add one more parameter to Trainer and pass it to the Dataloader down the line.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
CLIP Tokenizer Returns Different DTypes for Numpy versus Pytorch,"### System Info


- `transformers` version: 4.46.2
- Platform: Windows-11-10.0.26100-SP0
- Python version: 3.12.7
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.5.1+cpu (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: no


### Who can help?

_No response_

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction
```python
from transformers import AutoProcessor

texts = [""a photo of a cat"", ""A photo of a dog.""]

for model_id in (""openai/clip-vit-base-patch16"", ""openai/clip-vit-large-patch14-336""):
    tokenizer = AutoProcessor.from_pretrained(model_id)
    print(model_id)
    # `CLIPTokenizerFast.from_pretrained` and no kwargs gives string dtype
    # However, no kwargs with AutoProcessor fails with warning
    for i, kwargs in enumerate((dict(padding='longest'), dict(padding='longest', truncation=True))):
        pytorch_inputs = tokenizer(text=texts, return_tensors='pt', **kwargs)
        for k, v in pytorch_inputs.items():
            # int64 (correct)
            print(f""pytorch {i} {k}: {v.dtype} {v.shape}"")

        numpy_inputs = tokenizer(text=texts, return_tensors='np', **kwargs)
        for k, v in numpy_inputs.items():
            # int32 (incorrect)
            print(f""numpy {i} {k}: {v.dtype} {v.shape}"")
    print()
```

### Expected behavior

Creating numpy and/or pytorch tensors should have equivalent data types.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Add `Tensor Parallel` support for ALL models,"Just opening this to add support for all models following #34184

Lets bring support to all model! 🤗 

- [x] Llama

It would be great to add the support for more architectures such as
- [ ] Qwen2
- [ ] QwenVl
- [ ] Mistral
- [ ] Llava


... and many more

For anyone who wants to contribute just open a PR and link it to this issue, and ping me for a review!! 🤗 ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2760822153, 'node_id': 'MDU6TGFiZWwyNzYwODIyMTUz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Tensor%20Parallel', 'name': 'Tensor Parallel', 'color': '1AD0A8', 'default': False, 'description': ''}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",4,open
Export to ExecuTorch with Quantization,"### Feature request

This task is to experiment running quantized HuggingFace models with ExecuTorch out-of-the-box.

The heavy-lifting quantization work will be done through [`quantize_`](https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_api.py#L94) API by [`torchao`](https://github.com/pytorch/ao), for example `quantize_(model, int4_weight_only())`.

The quantization API can be integrated with the integration points to executorch `transformers.integrations.executorch`, expanding the export workflow with a new option of ""exporting with quantization"". In eager, users can verify the numerics accuracy of the quantized exported artifact, e.g. the script for eval llama ([here](https://github.com/pytorch/ao/blob/main/torchao/_models/llama/eval.py)). In ExecuTorch, users can just load the quantized `.pte` files to ExecuTorch runner for inference.



### Motivation

Experiment quantization workflow w/ `transforms` + `torchao` + `executorch`

### Your contribution

Direct contribution, or provide guidance to anyone who is interested in this work","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",0,open
Sync table question answering pipeline with Hub spec,,[],1,open
[WIP] Add Moonshine ,"# What does this PR do?

This PR adds support for [Moonshine](https://github.com/usefulsensors/moonshine) to the Transformers library.

Moonshine builds on top of Whisper’s architecture to overcome some of its limitations, primarily the restriction to a fixed 30-second audio window.

Key improvements in Moonshine’s architecture:
	1.	It uses SwiGLU activation instead of GELU in the decoder layers.
	2.	Most importantly, it replaces absolute position embeddings with Rotary Position Embeddings (RoPE), enabling Moonshine to process audio inputs of any length—unlike Whisper, which is limited to fixed 30-second windows.

# Who can review?
@ArthurZucker

# TODO 

- [ ] update UsefulSensors model repos
- [ ] change model id occurrences in `modular_moonshine.py`
- [ ] run benchmarks",[],3,open
Image collapse when converting a float tensor numpy image to PIL.Image.Image (all libraries),"### System Info

- `transformers` version: 4.46.1
- Platform: Windows-10-10.0.19045-SP0
- Python version: 3.9.13
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.3
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.0+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 3060 Ti

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```py
from transformers.image_transforms import pad
import numpy as np
import torch
from PIL import Image

# Example image as a NumPy array
image = np.random.rand(224, 224, 3)  # Height x Width x Channels
image_pil = np.array(Image.fromarray(image, 'RGB')) # Open with PIL and save
image_uint8 = (image * 255.0).astype(np.uint8)

# Define padding: ((before_height, after_height), (before_width, after_width))
padding = ((0, 0), (112, 112))  # Pads width to make it 448

# Apply padding
padded_image = pad(image, padding=padding)
padded_image_pil = pad(image_pil, padding=padding)
padded_image_uint8 = pad(image_uint8, padding=padding)
print(""Original Image Shape:"", image.shape)
print(""Padded Image Shape:"", padded_image.shape)
print(""Padded Image Shape (PIL):"", padded_image_pil.shape)
print(""Padded Image Shape (uint8):"", padded_image_uint8.shape)

image_torch = torch.tensor(image).permute(2, 0, 1).unsqueeze(0)
padded_image_torch = torch.tensor(padded_image).permute(2, 0, 1).unsqueeze(0)
padded_image_pil_torch = torch.tensor(padded_image_pil).permute(2, 0, 1).unsqueeze(0)
padded_image_uint8_torch = torch.tensor(padded_image_uint8).permute(2, 0, 1).unsqueeze(0)

print(""Original Image Shape (Torch):"", image_torch.shape)
print(""Padded Image Shape (Torch):"", padded_image_torch.shape)
print(""Padded Image Shape (PIL) (Torch):"", padded_image_pil_torch.shape)
print(""Padded Image Shape (uint8) (Torch):"", padded_image_uint8_torch.shape)

# Save images
original_im = Image.fromarray(image, 'RGB')
padded_im = Image.fromarray(padded_image, 'RGB')
padded_im_pil = Image.fromarray(padded_image_pil, 'RGB')
padded_im_uint8 = Image.fromarray(padded_image_uint8, 'RGB')
original_im.save(""_pad_original.png"") # normal
padded_im.save(""_pad_padded.png"") # strange
padded_im_pil.save(""_pad_padded_pil.png"") # normal
padded_im_uint8.save(""_pad_padded_uint8.png"") # relatively normal
```

### Expected behavior

After receiving a report on the Hugging Face forum that the `padding` in the `transformers` library was behaving strangely, I investigated and found the approximate cause.
It seems that the `pad` function in `numpy` returns strange results when it receives an ndarray that is not `uint8`.
As a simple workaround, there is a method of converting it to `Pillow Image` once, but this method is dependent on `Pillow`.
If the library converts it to `uint8` on its own, it may be a little troublesome to judge the numerical range of the image.
I opened an issue instead of a PR because I couldn't think of a good implementation.

### Reference
https://discuss.huggingface.co/t/clipvisionmodel-padding-problem/124187 A post that helps me find a problem
https://discord.com/channels/879548962464493619/1301002234963820604 Investigating preprocessor bugs
### Dependencies
```
transformers==4.46.2
torch==2.4.0
numpy<2
```
### Demo
https://huggingface.co/spaces/John6666/transformers_padding_bug_test","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",6,open
Adding RTDETRv2,"# What does this PR do?

This PR add RTDETRv2 into the Transformers library. There is a new thing in transformers called **modular**, which adds new models by creating a `modeling_modelname.py` file. Since RTDETRv2 only updates the decoder part while keeping the rest of the model unchanged, it serves as an ideal use case for this modular approach.

### What’s Left:
- [ ] Fix the modular -> modeling cookie cutter setup
- [ ] Remove the `scratch` folder (auto-generated by the `add-model` cookie cutter)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}, {'id': 7553455074, 'node_id': 'LA_kwDOCUB6oc8AAAABwjiT4g', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Modular', 'name': 'Modular', 'color': '3DEDD2', 'default': False, 'description': ''}]",7,open
[New Model] X-ALMA,"# What does this PR do?
This PR is trying to add a new model [X-ALMA](https://arxiv.org/abs/2410.03115).  X-ALMA is a high-quality LLM-based translation model that supports 50 languages, ensuring their high-performance in translation, regardless of their resource level.

@ArthurZucker

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
skip generation kwargs validation in torch compile,"fix #34767

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #34767


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
IsADirectoryError when training with tqdm enabled for trainer,"### System Info

Error info:
```python
**IsADirectoryError**: [Errno 21] Is a directory: '\n    <div>\n      \n      <progress value=\'2\' max=\'108\' style=\'width:300px; height:20px; vertical-align: middle;\'></progress>\n      [  2/108 : < :, Epoch 0.04/4]\n    </div>\n    <table border=""1"" class=""dataframe"">\n  <thead>\n <tr style=""text-align: left;"">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>'
```

Code:
```
training_args = transformers.TrainingArguments(
    num_train_epochs=4,                         # Number of training epochs
    per_device_train_batch_size=batch_size,      # Batch size for training
    per_device_eval_batch_size=batch_size,       # Batch size for evaluation
    gradient_accumulation_steps=2,               # Number of steps to accumulate gradients before updating
    gradient_checkpointing=True,                 # Enable gradient checkpointing to save memory
    do_eval=True,                                # Perform evaluation during training
    save_total_limit=2,                          # Limit the total number of saved checkpoints
    evaluation_strategy=""steps"",                 # Evaluation strategy to use (here, at each specified number of steps)
    save_strategy=""steps"",                       # Save checkpoints at each specified number of steps
    save_steps=10,                               # Number of steps between each checkpoint save
    eval_steps=10,                               # Number of steps between each evaluation
    max_grad_norm=1,                             # Maximum gradient norm for clipping
    warmup_ratio=0.1,                            # Warmup ratio for learning rate schedule
    weight_decay=0.001,                          # Regularization technique to prevent overfitting
    # fp16=True,                                 # Enable mixed precision training with fp16 (enable it if Ampere architecture is unavailable)
    bf16=True,                                   # Enable mixed precision training with bf16
    logging_steps=10,                            # Number of steps between each log
    output_dir=""outputs"",                        # Directory to save the model outputs and checkpoints
    optim=""adamw_torch"",                         # Optimizer to use (AdamW with PyTorch)
    learning_rate=5e-5,                          # Learning rate for the optimizer
    lr_scheduler_type=""linear"",                  # Learning rate scheduler type: constant
    load_best_model_at_end=True,                 # Load the best model found during training at the end
    metric_for_best_model=""rouge"",               # Metric used to determine the best model
    greater_is_better=True,                      # Indicates if a higher metric score is better
    push_to_hub=False,                           # Whether to push the model to Hugging Face Hub
    run_name=""finetuning"",   # Name of the run for experiment tracking
    report_to=""wandb""                            # For experiment tracking (login to Weights & Biases needed)
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()
```

Env info:
Jupyter version:
```
!jupyter --version
IPython          : 8.27.0
ipykernel        : 6.29.5
ipywidgets       : 7.7.1
jupyter_client   : 7.4.9
jupyter_core     : 5.7.2
jupyter_server   : 2.14.2
jupyterlab       : 4.0.11
nbclient         : 0.10.0
nbconvert        : 7.16.4
nbformat         : 5.10.4
notebook         : 6.5.7
qtconsole        : 5.6.0
traitlets        : 5.14.3
```

Python: 3.10.11
jupyter lab: 4.0.11
transformers: 4.45.2

Detailed errors:
```
IsADirectoryError                         Traceback (most recent call last)
Cell In[28], line 1
----> 1 trainer.train()

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/transformers/trainer.py:2052, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   2050         hf_hub_utils.enable_progress_bars()
   2051 else:
-> 2052     return inner_training_loop(
   2053         args=args,
   2054         resume_from_checkpoint=resume_from_checkpoint,
   2055         trial=trial,
   2056         ignore_keys_for_eval=ignore_keys_for_eval,
   2057     )

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/transformers/trainer.py:2465, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   2463     self.state.global_step += 1
   2464     self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch
-> 2465     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
   2467     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
   2468 else:

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/transformers/trainer_callback.py:494, in CallbackHandler.on_step_end(self, args, state, control)
    493 def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):
--> 494     return self.call_event(""on_step_end"", args, state, control)

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/transformers/trainer_callback.py:516, in CallbackHandler.call_event(self, event, args, state, control, **kwargs)
    514 def call_event(self, event, args, state, control, **kwargs):
    515     for callback in self.callbacks:
--> 516         result = getattr(callback, event)(
    517             args,
    518             state,
    519             control,
    520             model=self.model,
    521             tokenizer=self.tokenizer,
    522             optimizer=self.optimizer,
    523             lr_scheduler=self.lr_scheduler,
    524             train_dataloader=self.train_dataloader,
    525             eval_dataloader=self.eval_dataloader,
    526             **kwargs,
    527         )
    528         # A Callback can skip the return of `control` if it doesn't change it.
    529         if result is not None:

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/transformers/utils/notebook.py:307, in NotebookProgressCallback.on_step_end(self, args, state, control, **kwargs)
    305 def on_step_end(self, args, state, control, **kwargs):
    306     epoch = int(state.epoch) if int(state.epoch) == state.epoch else f""{state.epoch:.2f}""
--> 307     self.training_tracker.update(
    308         state.global_step + 1,
    309         comment=f""Epoch {epoch}/{state.num_train_epochs}"",
    310         force_update=self._force_next_update,
    311     )
    312     self._force_next_update = False

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/transformers/utils/notebook.py:143, in NotebookProgressBar.update(self, value, force_update, comment)
    141     self.first_calls = self.warmup
    142     self.wait_for = 1
--> 143     self.update_bar(value)
    144 elif value <= self.last_value and not force_update:
    145     return

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/transformers/utils/notebook.py:188, in NotebookProgressBar.update_bar(self, value, comment)
    185         self.label += f"", {1/self.average_time_per_item:.2f} it/s""
    187 self.label += ""]"" if self.comment is None or len(self.comment) == 0 else f"", {self.comment}]""
--> 188 self.display()

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/transformers/utils/notebook.py:229, in NotebookTrainingTracker.display(self)
    227     self.html_code += self.child_bar.html_code
    228 if self.output is None:
--> 229     self.output = disp.display(disp.HTML(self.html_code), display_id=True)
    230 else:
    231     self.output.update(disp.HTML(self.html_code))

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/IPython/core/display.py:432, in HTML.__init__(self, data, url, filename, metadata)
    430 if warn():
    431     warnings.warn(""Consider using IPython.display.IFrame instead"")
--> 432 super(HTML, self).__init__(data=data, url=url, filename=filename, metadata=metadata)

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/IPython/core/display.py:327, in DisplayObject.__init__(self, data, url, filename, metadata)
    324 elif self.metadata is None:
    325     self.metadata = {}
--> 327 self.reload()
    328 self._check_data()

File /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/IPython/core/display.py:353, in DisplayObject.reload(self)
    351 if self.filename is not None:
    352     encoding = None if ""b"" in self._read_flags else ""utf-8""
--> 353     with open(self.filename, self._read_flags, encoding=encoding) as f:
    354         self.data = f.read()
    355 elif self.url is not None:
    356     # Deferred import

IsADirectoryError: [Errno 21] Is a directory: '\n    <div>\n      \n      <progress value=\'2\' max=\'108\' style=\'width:300px; height:20px; vertical-align: middle;\'></progress>\n      [  2/108 : < :, Epoch 0.04/4]\n    </div>\n    <table border=""1"" class=""dataframe"">\n  <thead>\n <tr style=""text-align: left;"">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>'
```


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

This can be reproduced by the following code:
```
import time
import transformers
from transformers.utils.notebook import NotebookProgressBar

pbar = NotebookProgressBar(100)
for val in range(100):
    pbar.update(val)
    time.sleep(0.07)
pbar.update(100)
```

### Expected behavior

Training with progress bar being updated:
![progress bar updated](https://github.com/user-attachments/assets/26934b2e-4486-4050-9869-5fadd766e953)
","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",11,open
Move `DataCollatorForMultipleChoice` from the docs to the package,"# What does this PR do?
Fixes #34671 by moving the implementations for `DataCollatorForMultipleChoice` that appear in the docs (copied across four different languages) into the `transformers` package.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request) ""Pull Request"" section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@LysandreJik asked me to contribute this. @ArthurZucker may also be interested, and @stevhliu because the implementations came from the docs.",[],3,open
MobileLLM safetensors seem to be missing model.embed_tokens.weight,"### System Info

- `transformers` version: 4.46.2
- Platform: Linux-6.6.20-aufs-1-x86_64-with-glibc2.36
- Python version: 3.11.2
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: no
- Using GPU in script?: no
- GPU type: NVIDIA RTX A5000

### Who can help?

@ArthurZucker 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

`mobilellm = AutoModelForCausalLM.from_pretrained(""facebook/MobileLLM-125M"",trust_remote_code=True)`

will output `Some weights of MobileLLMForCausalLM were not initialized from the model checkpoint at facebook/MobileLLM-125M and are newly initialized: ['model.embed_tokens.weight']`

and the weights will be random. when using `use_safetensor=False`. everything seems to work as expected.

### Expected behavior

using safetensors should work the same as when not using them.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",8,open
Fix bug when requesting input normalization with EnCodec,"# What does this PR do?

Fix shape of the padding mask returned by EnCodec's feature extractor, which causes issues when `normalize=True`.

Minimal reproducible example:
```python
import numpy as np

from transformers import EncodecConfig, EncodecModel, EncodecFeatureExtractor


sampling_rate = 24000
num_audio_channels = 1

config = EncodecConfig(
    sampling_rate=sampling_rate,
    audio_channels=num_audio_channels,
    hidden_size=4,
    codebook_size=8,
    normalize=True,
)
feature_extractor = EncodecFeatureExtractor(
    feature_size=num_audio_channels, sampling_rate=sampling_rate
)
model = EncodecModel(config=config)

dummy_audios = [
    np.random.randn(num_audio_channels, 6000).squeeze(),
    np.random.randn(num_audio_channels, 4000).squeeze(),
]
inputs = feature_extractor(
    dummy_audios, sampling_rate=sampling_rate, padding=True, return_tensors=""pt""
)

outputs = model(
    input_values=inputs[""input_values""],
    padding_mask=inputs[""padding_mask""]
)
```

in the example above, `input_values` and `padding_mask` have shape `(2, 1, 6000)` and `(2, 6000)`, respectively. Now, when `EncodecConfig` is initialized with `normalize=True`, `EncodecModel._encode_frame` executes the following code:
```python
        ⋮
        scale = None
        if self.config.normalize:
            # if the padding is non zero
            input_values = input_values * padding_mask
            mono = torch.sum(input_values, 1, keepdim=True) / input_values.shape[1]
            scale = mono.pow(2).mean(dim=-1, keepdim=True).sqrt() + 1e-8
            input_values = input_values / scale
        ⋮
```
due to broadcasting rules, `input_values * padding_mask` results in an output of shape `(2, 2, 6000)`, which eventually leads to an error in a convolutional layer:
```
Traceback (most recent call last):                                                                                                                             
  File ""/net/tscratch/people/plgfcariaggi/transformers/encodec_mre.py"", line 29, in <module>                                                                   
    outputs = model(                                                                                                                                           
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl         
    return self._call_impl(*args, **kwargs)                                                                                                                    
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl                 
    return forward_call(*args, **kwargs)                                                                                                                       
  File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/encodec/modeling_encodec.py"", line 810, in forward                              
    audio_codes, audio_scales = self.encode(input_values, padding_mask, bandwidth, False)                                                                      
  File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/encodec/modeling_encodec.py"", line 651, in encode                               
    encoded_frame, scale = self._encode_frame(frame, bandwidth, mask)                                                                                          
  File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/encodec/modeling_encodec.py"", line 584, in _encode_frame                        
    embeddings = self.encoder(input_values)                                                                                                                    
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl         
    return self._call_impl(*args, **kwargs)                                                                                                                    
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl                 
    return forward_call(*args, **kwargs)                                                                                                                       
  File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/encodec/modeling_encodec.py"", line 312, in forward                              
    hidden_states = layer(hidden_states)                                                                                                                       
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl         
    return self._call_impl(*args, **kwargs)                                                                                                                    
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl                 
    return forward_call(*args, **kwargs)                                                                                                                       
  File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/encodec/modeling_encodec.py"", line 171, in forward                              
    hidden_states = self.conv(hidden_states)                                                                                                                   
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl         
    return self._call_impl(*args, **kwargs)                                                                                                                    
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1747, in _call_impl                 
    return forward_call(*args, **kwargs)                                                                                                                       
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/conv.py"", line 375, in forward                       
    return self._conv_forward(input, self.weight, self.bias)                                                                                                   
  File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/conv.py"", line 370, in _conv_forward                 
    return F.conv1d(                                                                                                                                           
RuntimeError: Given groups=1, weight of size [32, 1, 7], expected input[2, 2, 6006] to have 1 channels, but got 2 channels instead
```

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@hollance, @ylacombe, @eustlb

",[],2,open
Issues counting passing rates on tests which use subTest(),"As we discussed in https://github.com/huggingface/transformers/pull/34723#issuecomment-2479470083, we spotted that tests using `with self.subTest` along with `self.skipTest()` report confusing passing rates. We need to understand how we should handle this. Below is breakdown of behavior on few syntetic tests.

Note:
* Behavior depends on whether `pytest-subtest` is installed or not
* Accelerate brings in `pytest-subtest` into environment, see https://github.com/huggingface/accelerate/blob/c0552c9012a9bae7f125e1df89cf9ee0b0d250fd/setup.py#L25
* Transformers do not depend on `pytest-subtest` at the moment
* Transformers ci does not have `pytest-subtest` in the environment at the moment (according to below breakdown this means that if first subtest is skipped other subtests won't be executed)

Synthetic tests:
* `ex0.py` - `subTest()` without skipping:

```
$ cat ex0.py
import unittest
class T(unittest.TestCase):
    def test_foo(self):
        for i in range(7):
            with self.subTest(i=i):
                self.assertLess(i, 3)
```

* `ex1.py` - `subTest()` with skipping:

```
$ cat ex1.py
import unittest
class T(unittest.TestCase):
    def test_foo(self):
        for i in range(7):
            with self.subTest(i=i):
                if i < 3:
                    self.skipTest(f""bon {i}"")
                self.assertLess(i, 3)

```

| | | Failed | Passed | Skipped |
| --- | --- | --- | --- | --- |
| No `pytest-subtests` | `ex0.py` | 1 | 0 | 0 |
| No `pytest-subtests` | `ex1.py` | 0 | 0 | 1 |
| With `pytest-subtests` | `ex0.py` | 4 | 1 | 0 |
| With `pytest-subtests` | `ex1.py` | 1 | 1 | 3 |

Logs:
* With `pytest-subtests`:

```
$ python -m pytest ex0.py
...
(i=3) SUBFAIL ex0.py::T::test_foo - AssertionError: 3 not less than 3
(i=4) SUBFAIL ex0.py::T::test_foo - AssertionError: 4 not less than 3
(i=5) SUBFAIL ex0.py::T::test_foo - AssertionError: 5 not less than 3
(i=6) SUBFAIL ex0.py::T::test_foo - AssertionError: 6 not less than 3
=================================== 4 failed, 1 passed in 0.11s ===================================

$ python -m pytest ex1.py
...
(i=6) SUBFAIL ex1.py::T::test_foo - AssertionError: 6 not less than 3
============================= 1 failed, 1 passed, 3 skipped in 0.11s ===
```

* No pytest-subtests:

```
$ python -m pytest ex0.py
FAILED ex0.py::T::test_foo - AssertionError: 3 not less than 3
======================================== 1 failed in 0.11s ========================================


$ python -m pytest ex1.py
...
======================================= 1 skipped in 0.07s ========================================
```

CC: @ydshieh @ArthurZucker ",[],3,open
warmup LR schedulers start from LR=0,"### System Info

transformers commit: 52ea4aa589324bae43dfb1b6db70335da7b68654 (main at time of writing)
the rest isn't relevant.

### Who can help?

trainer: @muellerzr @SunMarc

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Run any example with a warmup scheduler, observe the the effective LR is 0 for the first step, unnecessarily wasting compute. See similar discussion for this issue on torchtune https://github.com/pytorch/torchtune/issues/2010. See the code at

https://github.com/huggingface/transformers/blob/52ea4aa589324bae43dfb1b6db70335da7b68654/src/transformers/optimization.py#L182

and evaluate for step 0. Observe it returns LR factor = 0, weights will not be updated.

### Expected behavior

Expect every optimizer step to adjust the weights of my model unless there is a good reason not to.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",3,open
[DO NOT MERGE] Add pytest-subtests to the dependencies,"Pushing a draft PR to check that CI will start catching #34722. If it will, I suggest to update PR #34723 with this change.

As discussed in [1], pytest-subtests changes behavior of .skipTest() having effect to really skip individual subtests or skip the entire test if module is not installed. Huggingface Accelerate has module in its dependencies. It makes sense to add it for Transformers as well to avoid divergent environment between users and ci.

See[1]: https://github.com/huggingface/transformers/pull/34723#issuecomment-2479191402

CC: @ydshieh",[],3,open
Sync DQA pipeline with Hub spec,,[],3,open
[DO NOT MERGE] Testing the new ABI3 tokenizers version.,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
FileNotFoundError when using SentenceTransformerTrainingArguments(load_best_model_at_end=True) and Peft,"### System Info

I used google colab default environment, with last version of transformers and sentence-transformers

### Who can help?

@muellerzr 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Here is the example code as a gist: [gist](https://gist.github.com/GTimothee/cb3551ba6eb14b04f7a06d63ea4616f9)

Just open the gist in a colab notebook and run it

### Expected behavior

This is a follow up about another bug found in sentence-transformers. The sentence-transformers library just integrated peft using transformers.integrations. The bug is that when using SentenceTransformerTrainingArguments(load_best_model_at_end=True) there is a FileNotFoundError as we try to load a classical checkpoint file (pth) but we saved an adapter instead. When looking into the load_best_model function, it just uses the function from the transformers.trainer.Trainer. So we need to modify the transformers library to solve the problem. 

The issue is that there is a function in transformers that checks if the model is a PeftMixedModel or not. If not, it is not considered a peft model and the trainer tries to load the model as usual. The problem is our model is a PeftAdapterMixin so it is not recognized as a peft model. 

See also: https://github.com/UKPLab/sentence-transformers/issues/3056 

In my opinion, we need to add to the check a 2-step check 1) is it a PeftAdapterMixin and 2) has it adapters loaded? Maybe it is only one part of the solution though, and we need a special loading snippet in the transformers.trainer.Trainer._load_best_model directly.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Failed to import transformers.data.data_collator because of the following error (look up to see its traceback): unhashable type: 'list',"### System Info

I encountered an error while using transformers to run BGEM3EmbeddingFunction in the CUDA environment：

Traceback (most recent call last):
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/utils/import_utils.py"", line 1778, in _get_module
    return importlib.import_module(""."" + module_name, self.__name__)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 972, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 790, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/data/__init__.py"", line 28, in <module>
    from .processors import (
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/data/processors/__init__.py"", line 15, in <module>
    from .glue import glue_convert_examples_to_features, glue_output_modes, glue_processors, glue_tasks_num_labels
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/data/processors/glue.py"", line 30, in <module>
    import tensorflow as tf
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/__init__.py"", line 49, in <module>
    from tensorflow._api.v2 import __internal__
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.__internal__ import autograph
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py"", line 8, in <module>
    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/python/autograph/core/ag_ctx.py"", line 21, in <module>
    from tensorflow.python.autograph.utils import ag_logging
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/python/autograph/utils/__init__.py"", line 17, in <module>
    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/python/autograph/utils/context_managers.py"", line 19, in <module>
    from tensorflow.python.framework import ops
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 5925, in <module>
    ) -> Optional[Callable[[Any], message.Message]]:
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 243, in inner
    return func(*args, **kwds)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 316, in __getitem__
    return self._getitem(self, parameters)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 433, in Optional
    return Union[arg, type(None)]
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 243, in inner
    return func(*args, **kwds)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 316, in __getitem__
    return self._getitem(self, parameters)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 421, in Union
    parameters = _remove_dups_flatten(parameters)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 215, in _remove_dups_flatten
    all_params = set(params)
TypeError: unhashable type: 'list'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/data-ai/adp/label-rag-server/main.py"", line 36, in match_concept
    return vector_search(request_param)
  File ""/data-ai/adp/label-rag-server/main.py"", line 63, in vector_search
    ef = BGEM3EmbeddingFunction(device=""cuda:0"", model_name='BAAI/bge-m3')
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/milvus_model/hybrid/__init__.py"", line 9, in BGEM3EmbeddingFunction
    return bge_m3.BGEM3EmbeddingFunction(*args, **kwargs)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/milvus_model/utils/lazy_import.py"", line 18, in __getattr__
    module = self._load()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/milvus_model/utils/lazy_import.py"", line 12, in _load
    module = importlib.import_module(self.__name__)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 790, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/milvus_model/hybrid/bge_m3.py"", line 15, in <module>
    from FlagEmbedding import BGEM3FlagModel
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/FlagEmbedding/__init__.py"", line 2, in <module>
    from .bge_m3 import BGEM3FlagModel
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/FlagEmbedding/bge_m3.py"", line 7, in <module>
    from transformers import PreTrainedTokenizerFast, BatchEncoding, DataCollatorWithPadding, XLMRobertaForMaskedLM, is_torch_npu_available
  File ""<frozen importlib._bootstrap>"", line 1055, in _handle_fromlist
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/utils/import_utils.py"", line 1766, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/utils/import_utils.py"", line 1780, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):
unhashable type: 'list'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/logging/__init__.py"", line 1079, in emit
    msg = self.format(record)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/logging/__init__.py"", line 923, in format
    return fmt.format(record)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/logging/__init__.py"", line 659, in format
    record.message = record.getMessage()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/logging/__init__.py"", line 363, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/threading.py"", line 908, in _bootstrap
    self._bootstrap_inner()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/threading.py"", line 950, in _bootstrap_inner
    self.run()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/threading.py"", line 888, in run
    self._target(*self._args, **self._kwargs)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/socketserver.py"", line 650, in process_request_thread
    self.finish_request(request, client_address)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/socketserver.py"", line 360, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/socketserver.py"", line 720, in __init__
    self.handle()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/werkzeug/serving.py"", line 398, in handle
    super().handle()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/http/server.py"", line 427, in handle
    self.handle_one_request()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/http/server.py"", line 415, in handle_one_request
    method()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/werkzeug/serving.py"", line 370, in run_wsgi
    execute(self.server.app)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/werkzeug/serving.py"", line 331, in execute
    application_iter = app(environ, start_response)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/flask/app.py"", line 1536, in __call__
    return self.wsgi_app(environ, start_response)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/flask/app.py"", line 1511, in wsgi_app
    response = self.full_dispatch_request()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/flask/app.py"", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/flask/app.py"", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File ""/data-ai/adp/label-rag-server/main.py"", line 41, in match_concept
    logging.error(""concepts vector search error: {}"", e)
Message: 'concepts vector search error: {}'
Arguments: (RuntimeError(""Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):\nunhashable type: 'list'""),)

Is it a problem with my transformer version?

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

--- Logging error ---
Traceback (most recent call last):
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/utils/import_utils.py"", line 1778, in _get_module
    return importlib.import_module(""."" + module_name, self.__name__)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 972, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 790, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/data/__init__.py"", line 28, in <module>
    from .processors import (
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/data/processors/__init__.py"", line 15, in <module>
    from .glue import glue_convert_examples_to_features, glue_output_modes, glue_processors, glue_tasks_num_labels
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/data/processors/glue.py"", line 30, in <module>
    import tensorflow as tf
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/__init__.py"", line 49, in <module>
    from tensorflow._api.v2 import __internal__
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.__internal__ import autograph
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py"", line 8, in <module>
    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/python/autograph/core/ag_ctx.py"", line 21, in <module>
    from tensorflow.python.autograph.utils import ag_logging
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/python/autograph/utils/__init__.py"", line 17, in <module>
    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/python/autograph/utils/context_managers.py"", line 19, in <module>
    from tensorflow.python.framework import ops
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 5925, in <module>
    ) -> Optional[Callable[[Any], message.Message]]:
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 243, in inner
    return func(*args, **kwds)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 316, in __getitem__
    return self._getitem(self, parameters)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 433, in Optional
    return Union[arg, type(None)]
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 243, in inner
    return func(*args, **kwds)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 316, in __getitem__
    return self._getitem(self, parameters)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 421, in Union
    parameters = _remove_dups_flatten(parameters)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/typing.py"", line 215, in _remove_dups_flatten
    all_params = set(params)
TypeError: unhashable type: 'list'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/data-ai/adp/label-rag-server/main.py"", line 36, in match_concept
    return vector_search(request_param)
  File ""/data-ai/adp/label-rag-server/main.py"", line 63, in vector_search
    ef = BGEM3EmbeddingFunction(device=""cuda:0"", model_name='BAAI/bge-m3')
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/milvus_model/hybrid/__init__.py"", line 9, in BGEM3EmbeddingFunction
    return bge_m3.BGEM3EmbeddingFunction(*args, **kwargs)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/milvus_model/utils/lazy_import.py"", line 18, in __getattr__
    module = self._load()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/milvus_model/utils/lazy_import.py"", line 12, in _load
    module = importlib.import_module(self.__name__)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 790, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/milvus_model/hybrid/bge_m3.py"", line 15, in <module>
    from FlagEmbedding import BGEM3FlagModel
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/FlagEmbedding/__init__.py"", line 2, in <module>
    from .bge_m3 import BGEM3FlagModel
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/FlagEmbedding/bge_m3.py"", line 7, in <module>
    from transformers import PreTrainedTokenizerFast, BatchEncoding, DataCollatorWithPadding, XLMRobertaForMaskedLM, is_torch_npu_available
  File ""<frozen importlib._bootstrap>"", line 1055, in _handle_fromlist
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/utils/import_utils.py"", line 1766, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/transformers/utils/import_utils.py"", line 1780, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):
unhashable type: 'list'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/logging/__init__.py"", line 1079, in emit
    msg = self.format(record)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/logging/__init__.py"", line 923, in format
    return fmt.format(record)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/logging/__init__.py"", line 659, in format
    record.message = record.getMessage()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/logging/__init__.py"", line 363, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/threading.py"", line 908, in _bootstrap
    self._bootstrap_inner()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/threading.py"", line 950, in _bootstrap_inner
    self.run()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/threading.py"", line 888, in run
    self._target(*self._args, **self._kwargs)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/socketserver.py"", line 650, in process_request_thread
    self.finish_request(request, client_address)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/socketserver.py"", line 360, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/socketserver.py"", line 720, in __init__
    self.handle()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/werkzeug/serving.py"", line 398, in handle
    super().handle()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/http/server.py"", line 427, in handle
    self.handle_one_request()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/http/server.py"", line 415, in handle_one_request
    method()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/werkzeug/serving.py"", line 370, in run_wsgi
    execute(self.server.app)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/werkzeug/serving.py"", line 331, in execute
    application_iter = app(environ, start_response)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/flask/app.py"", line 1536, in __call__
    return self.wsgi_app(environ, start_response)
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/flask/app.py"", line 1511, in wsgi_app
    response = self.full_dispatch_request()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/flask/app.py"", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/data-ai/adp/anaconda3/envs/python3.91/lib/python3.9/site-packages/flask/app.py"", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File ""/data-ai/adp/label-rag-server/main.py"", line 41, in match_concept
    logging.error(""concepts vector search error: {}"", e)
Message: 'concepts vector search error: {}'
Arguments: (RuntimeError(""Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):\nunhashable type: 'list'""),)
140.210.223.35 - - [15/Nov/2024 17:47:04] ""POST /api/rag/matchConceptByVector HTTP/1.1"" 200 -

### Expected behavior

I hope there are some common ways to solve this problem","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
tokenizer.json modified after tokenizer.save_pretrained of OLMO models,"### System Info

- `transformers` version: 4.45.0
- Platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
- Python version: 3.10.15
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.0+rocm6.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: AMD Instinct MI250X/MI250

### Who can help?

@ArthurZucker and @itazap

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

When I load and then save the tokenizer with OLMO models, the tokenizer.json files appear different, particularly with the `merge` key.

![image](https://github.com/user-attachments/assets/ad6e82c6-89bc-402a-b956-b106c4fd74de)

The code to reproduce that is :

```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(""allenai/OLMo-1B-0724-hf"")
tokenizer.save_pretrained(""saved_tokenizer"")
```

### Expected behavior

The original `tokenizer.json` and the saved `tokenizer.json` should be the same.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",9,open
Translating attention.md to Chinese,"Hi!

I have translated attention.md into Chinese and here is my PR #34716 .
I would appreciate it if someone could review and comment on it.Thanks!!





","[{'id': 1834067346, 'node_id': 'MDU6TGFiZWwxODM0MDY3MzQ2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Documentation', 'name': 'Documentation', 'color': '77cc3b', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
RFC: Reducing Download Traffic and Latency with ZipNN Lossless Compression for AI Models,"### Feature request

This RFC proposes integrating a lossless compression method called ZipNN into Hugging Face Transformers to reduce latency and traffic for downloading models. ZipNN is specifically designed for AI models, offering a model size reduction of 17% to over 50%, depending on the model format and compressibility. Additionally, it significantly reduces time for the user due to its fast decompression speed, allowing compressed models to be ready for use almost immediately without impacting model accuracy.

### Motivation

From a [LinkedIn post by Julien Chaumond](https://www.linkedin.com/posts/julienchaumond_i-am-super-excited-to-announce-that-weve-activity-7227305609254113280-zx3H), August 2024, Hugging Face holds 1.3M models, with a cumulative storage space of 12PB. They also serve 1 billion daily requests, amounting to a network bandwidth of around 6 PetaBytes per day!

Downloading large models from Hugging Face can be time-consuming; for example, downloading a model like Llama-3.1-405B can take nearly a day on a 10 MB/s home connection or nearly 2 hours on a 125 MB/s high-bandwidth connection. ZipNN could reduce this time by up to 33%.


## Model Comparison Table

We took the 20 most downloaded models in Hugging Face from late OCT 2024:
(Based on 1GB from the middle of the model).

- **17% Savings: 9 models**
- **33% Savings: 3 models**
- **50% or Greater Savings: 8 models**

| Model Name | Format | Size | ZipNN Compression Remaining (%) |
|:-----------|:-------|:-----|:--------------------------|
| BAAI/bge-base-en-v1.5 | FP32 | 0.4GB | 42.2% |
| sentence-transformers/all-mpnet-base-v2 | FP32 | 0.4GB  | 83% |
| nesaorg/benchmark_v0 | FP32 | 1.35GB | 82.38% |
| google-bert/bert-base-uncased | FP32 | 0.4GB | 83.17% |
| sentence-transformers/all-MiniLM-L6-v2 | FP32 | 0.09GB | 82.07% |
| Qwen/Qwen2.5-1.5B-Instruct | BF16 | 3GB | 66.86% |
| openai/whisper-large-v2 | FP32 | 6.1GB | 42.8% |
| FacebookAI/xlm-roberta-large | FP32 | 2.2GB | 42.9% |
| 1231czx/llama3_it_ultra_list_and_bold500 | BF16 | 16GB | 66.77% |
| openai/clip-vit-base-patch32 | FP32 | 0.6GB | 43% |
| jonatasgrosman/wav2vec2-large-xlsr-53-english | FP32 | 1.26GB | 82.96% |
| openai/clip-vit-base-patch16 | FP32 | 0.6GB | 51.24% |
| google/vit-base-patch16-224-in21k | FP32 | 0.4GB | 84% |
| FacebookAI/roberta-base | FP32 | 0.5GB | 43.9% |
| nesaorg/fc_8 | FP32 | 0.13GB | 82.52% |
| nesaorg/fc_6 | FP32 | 0.1GB | 82.2% |
| BAAI/bge-small-en-v1.5 | FP32 | 0.13GB | 42.9% |
| openai/clip-vit-large-patch14 | FP32 | 1.71GB | 42.97% |
| timm/resnet50.a1_in1k | FP32 | 0.1GB | 83.51% |
| meta-llama/Llama-3.1-405B | BF16 | 812GB | 66% |

### Your contribution

# ZipNN

ZipNN (The NN stands for Neural Network) is a lossless compression library tailored to neural networks. ZipNN compresses models by targeting the skewed distribution of exponent bits in floating-point parameters, which is highly compressible. By isolating exponents and applying Entropy Encoding with Huffman codes, ZipNN achieves efficient compression without the overhead of multi-byte repetition algorithms like Lempel-Ziv. It further optimizes speed by skipping non-compressible segments and adapting strategies based on the model’s characteristics.

[ZipNN Repository Link](https://github.com/zipnn/zipnn)
[ZipNN arXiv Paper: ZIPNN: LOSSLESS COMPRESSION FOR AI MODELS](https://arxiv.org/abs/2411.05239)

## Comparing Speed and Compression ratio of different compression methods:
(Based on 1GB from the middle of the model).

| Model Name | Format | Compression Method | Compression Remaining (%) | Compression Speed (GB/Sec) | Decompression Speed (GB/Sec) |
|:-----------|:-------|:------------------|:-------------------|:--------------------------|:----------------------------|
| meta-llama/Llama-3.1-8B-Instruct  | BF16 | Zstd | 77.7% | 0.71 | 1.02 |
| meta-llama/Llama-3.1-8B-Instruct | BF16 | ZipNN | 66.4% | 1.15 | 1.65 |
| allenai/OLMo-1B-0724-hf | FP32 | Zstd | 92.3% | 0.97 | 1.02 |
| allenai/OLMo-1B-0724-hf | FP32 | ZipNN | 83.2% | 1.64 | 2.48 |
| FacebookAI/xlm-roberta-large | FP32 | Zstd | 57.4% | 0.18 | 0.77 |
| FacebookAI/xlm-roberta-large | FP32 | ZipNN | 42.9% | 0.83 | 1.41 |


## User benefits
Figure 10 in the arXiv paper shows the download and upload timing for three models, comparing the original and compressed versions, including decompression and compression times. Network speed is the primary factor affecting download and upload durations, and even for models that are less compressible, users benefit from reduced total latency when decompression and compression are included.

[Link to Figure 10 from the arXiv paper](https://github.com/zipnn/zipnn/blob/main/images/hf_download_upload_2.pdf)

## Usage

### Installation
To get started, you can install the library directly from PyPI:
```bash
pip install zipnn
```

### API Usage
You can call ZipNN directly from the API:
```python
import zipnn
zpn = zipnn.ZipNN()
compressed_buffer = zpn.compress(original_buffer)
decompressed_buffer = zpn.decompress(compressed_buffer)
```

### Command-Line Scripts
You can also use the provided wrapper [scripts](https://github.com/zipnn/zipnn/tree/main/scripts).
Note: **All ZipNN compressed files use the "".znn"" extension**.

Single file compression/decompression:
```bash
python zipnn_compress_file.py model_name
python zipnn_decompress_file.py compressed_model_name.znn
```

## Hugging Face Plugin and compressed Models stored on Hugging Face

### Plugin Usage
ZipNN has a plugin for the Hugging Face transformers library that can handle ZipNN-compressed Models. 

The user can save the compressed model to his local storage using the default plugin. When loading, the model includes a fast decompression phase on the CPU while remaining compressed in its storage.

**What this means:** Each time the user loads the model, less data is transferred to the GPU cluster, with decompression happening on the CPU.

```python
from zipnn import zipnn_hf
zipnn_hf()
```

**Alternatively, avoiding future decompression**: the user can save the model uncompressed on his local storage. This way, future loads won’t require a decompression phase 
```python
from zipnn import zipnn_hf
zipnn_hf(replace_local_file=True)
```

To compress and decompress manually, simply run: [Link to scripts](https://github.com/zipnn/zipnn/tree/main/scripts)

```bash
python zipnn_compress_path.py safetensors --model royleibov/granite-7b-instruct-ZipNN-Compressed --hf_cache
```

```bash
python zipnn_decompress_path.py --model royleibov/granite-7b-instruct-ZipNN-Compressed --hf_cache
```

There are a few models compressed by ZipNN hosted on Hugging Face:
Example: 
[ compressed FacebookAI/roberta-base ]( https://huggingface.co/royleibov/roberta-base-ZipNN-Compressed )
[ compressed meta-llama/Llama-3.2-11B-Vision-Instruct ]( https://huggingface.co/royleibov/Llama-3.2-11B-Vision-Instruct-ZipNN-Compressed )

And a usage example:
[Usage Example Llama-3.2-11B](https://github.com/zipnn/zipnn/blob/main/examples/huggingface_llama_3.2_example.py)

### Upload compressed models to Hugging Face:

1. Compress all the model weights
Download the scripts for compressing/decompressing AI Models:

```bash
wget -i https://raw.githubusercontent.com/zipnn/zipnn/main/scripts/scripts.txt &&
rm scripts.txt
```

```bash
python3 zipnn_compress_path.py safetensors --path .
```

2. Add the compressed weights to git-lfs tracking and correct the index json
```
git lfs track ""*.znn"" &&
sed -i 's/.safetensors/.safetensors.znn/g' model.safetensors.index.json &&
git add *.znn .gitattributes model.safetensors.index.json &&
git rm *.safetensors
```

3. Done! Now push the changes as per [the documentation](https://huggingface.co/docs/hub/repositories-getting-started#set-up):
```bash
git lfs install --force --local && # this reinstalls the LFS hooks
huggingface-cli lfs-enable-largefiles . && # needed if some files are bigger than 5GB
git push --force origin main
```


## Current status

The code is ready for use with single-threaded compression and decompression on the CPU, and ZipNN already has a few users. The next version will support multi-threading on the CPU, with a future milestone targeting GPU implementation.

# Proposed change:

Decompress any shard of a model that was previously compressed with ZipNN. [This commit](https://github.com/huggingface/transformers/commit/607982fea2ff9cb381d1038adc6bd22c1fe58267) only extends the functionality of load_state_dict(), making sure to load the model and decompress it as efficiently as possible by decompressing in chunks and by avoiding unnecessary I/O requests.

In modeling_utils.load_state_dict():
```python
    checkpoint_bytes = b""""
    if checkpoint_file.endswith("".znn""):
        output_file = checkpoint_file.replace("".znn"", """")
        if not os.path.exists(output_file):
            try:
                from zipnn import ZipNN
            except ImportError:
                raise ImportError(""To load a zipped checkpoint file, you need to install zipnn."")
            znn = ZipNN(is_streaming=True)
            with open(checkpoint_file, ""rb"") as infile:
                chunk = infile.read()
                checkpoint_bytes += znn.decompress(chunk)
        else:
            with open(output_file, ""rb"") as infile:
                checkpoint_bytes += infile.read()
```

**This is a proof of concept**, currently only supporting sharded models whose index.json been modified to .znn suffixes (as seen in this [ZipNN compressed Llama 3.2 example on Hugging Face](https://huggingface.co/royleibov/Llama-3.2-11B-Vision-Instruct-ZipNN-Compressed/blob/main/model.safetensors.index.json)), safetensors or any other file. Support for all single files can be readily added by adding individual checks in modeling_utils.PreTrainedModel.from_pretrained() or by changing utils.hub.cached_file() to check for .znn filepath.

A working version of all edge cases can be found in ZipNN's [zipnn_hf() plugin](https://github.com/zipnn/zipnn/blob/ffa5b9f6d2a55fb2b2fd460995fc81e1283d0954/zipnn/zipnn.py#L1081).

**Additionally, to allow for users to only decompress once**, the plugin has a flag `zipnn_hf(replace_local_file=True)` that locally saves the decompressed model in the cache, reorders the symlinks, and fixes accordingly any index.json if there is one. This functionality can be done equivalently by adding a flag in from_pretrained().
","[{'id': 1260952223, 'node_id': 'MDU6TGFiZWwxMjYwOTUyMjIz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Discussion', 'name': 'Discussion', 'color': '22870e', 'default': False, 'description': 'Discussion on a topic (keep it focused or open a new issue though)'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Unable to use `MimiModel` with DeepSpeed ZeRO-3,"# What does this PR do?
Allow using `MimiModel` with DeepSpeed ZeRO-3.

Fixes the following error:
```
[rank0]: Traceback (most recent call last):                                                                                                                                                                                                  
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/transformers/./mimi_mre.py"", line 10, in <module>                                                                                                                                         
[rank0]:     main()                                                                                                                                                                                                                          
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/transformers/./mimi_mre.py"", line 7, in main                                                                                                                                              
[rank0]:     model = MimiModel.from_pretrained(""kyutai/mimi"", config=config)                                                                                                                                                                 
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/modeling_utils.py"", line 4110, in from_pretrained                        
[rank0]:     model = cls(config, *model_args, **model_kwargs)                                                                                             
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py"", line 511, in wrapper
[rank0]:     f(module, *args, **kwargs)                                                                                                                   
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/mimi/modeling_mimi.py"", line 1515, in __init__                    
[rank0]:     self.quantizer = MimiSplitResidualVectorQuantizer(config)                                                                                    
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py"", line 511, in wrapper
[rank0]:     f(module, *args, **kwargs)                                                                                                                               
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/mimi/modeling_mimi.py"", line 1340, in __init__                    
[rank0]:     self.semantic_residual_vector_quantizer = MimiResidualVectorQuantizer(config, self.num_semantic_quantizers)                                  
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py"", line 511, in wrapper
[rank0]:     f(module, *args, **kwargs)                                                                                                                   
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/mimi/modeling_mimi.py"", line 1282, in __init__                                
[rank0]:     self.layers = nn.ModuleList([MimiVectorQuantization(config) for _ in range(self.num_quantizers)])                                            
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/mimi/modeling_mimi.py"", line 1282, in <listcomp>                  
[rank0]:     self.layers = nn.ModuleList([MimiVectorQuantization(config) for _ in range(self.num_quantizers)])                                            
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py"", line 511, in wrapper
[rank0]:     f(module, *args, **kwargs)                                                          
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/mimi/modeling_mimi.py"", line 1261, in __init__                                                            
[rank0]:     self.codebook = MimiEuclideanCodebook(config)                                       
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py"", line 511, in wrapper                           
[rank0]:     f(module, *args, **kwargs)                                                          
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/transformers/src/transformers/models/mimi/modeling_mimi.py"", line 1217, in __init__                                                            
[rank0]:     self.register_buffer(""initialized"", torch.Tensor([True]))                           
[rank0]:   File ""/net/tscratch/people/plgfcariaggi/envs/transformers/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py"", line 255, in new_tensor
[rank0]:     tensor = _orig_torch_empty(0, device=device).new_empty(*args, **kwargs)                                                                                                                                                         
[rank0]: TypeError: new_empty(): argument 'size' (position 1) must be tuple of ints, but found element of type bool at pos 0 
```

## Reproducing the error
`mimi_mre.sh`:
```sh
OUTPUT_DIR=$HOME/mimi_mre

deepspeed \
    --num_gpus 1 \
    --master_port 60000 \
    ./mimi_mre.py \
    --output_dir $OUTPUT_DIR \
    --deepspeed zero3.json

```

`mimi_mre.py`:
```python

from transformers import AutoConfig, MimiModel, TrainingArguments, HfArgumentParser

def main():
    parser = HfArgumentParser(TrainingArguments)
    training_args = parser.parse_args_into_dataclasses()[0]
    config = AutoConfig.from_pretrained(""kyutai/mimi"")
    model = MimiModel.from_pretrained(""kyutai/mimi"", config=config)

if __name__ == ""__main__"":
    main()
```

`zero3.json`:
```json
{
    ""fp16"": {
        ""enabled"": ""auto"",
        ""loss_scale"": 0,
        ""loss_scale_window"": 1000,
        ""initial_scale_power"": 16,
        ""hysteresis"": 2,
        ""min_loss_scale"": 1
    },
    ""bf16"": {
        ""enabled"": ""auto""
    },
    ""train_micro_batch_size_per_gpu"": ""auto"",
    ""train_batch_size"": ""auto"",
    ""gradient_accumulation_steps"": ""auto"",
    ""zero_optimization"": {
        ""stage"": 3,
        ""overlap_comm"": true,
        ""contiguous_gradients"": true,
        ""sub_group_size"": 1e9,
        ""reduce_bucket_size"": ""auto"",
        ""stage3_prefetch_bucket_size"": ""auto"",
        ""stage3_param_persistence_threshold"": ""auto"",
        ""stage3_max_live_parameters"": 1e9,
        ""stage3_max_reuse_distance"": 1e9,
        ""stage3_gather_16bit_weights_on_model_save"": true
    }
}
```


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ylacombe, @eustlb
","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",3,open
Better error message when loading adapter models with peft dependency missing,"### Feature request

Loading adapter models (such as https://huggingface.co/lightonai/MonoQwen2-VL-v0.1/tree/main) fails with an error message when peft isn't installed. The error message
`OSError: lightonai/MonoQwen2-VL-v0.1 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.`
 is a bit cryptic and requires the user to understand that
- the model that will be loaded is a peft adapter
- peft isn't installed in the current env

To improve UX, it would be useful to show a different error message such as `""The model lightonai/MonoQwen2-VL-v0.1 is an adapter model. To load it, you need to install peft (hint: run `pip install peft`)"".`

### Motivation

Improve UX. The user may get the impression that the model repository is corrupted. 

### Your contribution

This feature should probably be implemented by core maintainers that are familiar with the internals of the model loading code.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6870118306, 'node_id': 'LA_kwDOCUB6oc8AAAABmX2vog', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/PEFT', 'name': 'PEFT', 'color': '93794B', 'default': False, 'description': ''}]",1,open
Pass callbacks kwarg to study.optimize() in run_hp_search_optuna(),"# What does this PR do?

This PR allows the user to specify additional callbacks for optuna's `study.optimize()`. Previously, the callbacks kwarg would be passed to `optuna.create_study()` and would cause an Exception.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],3,open
Translation model M2M100 uses 2 models in cache (from version 4.46.0),"### System Info

I'm using `facebook/m2m100_418M` translation model.
From version 4.46.0 it downloads another model which wieghts ~2 GB.
I'm using python 3.11, in `ubuntu`

### Who can help?

@ArthurZucker 

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```
import torch
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer


model = M2M100ForConditionalGeneration.from_pretrained(
    ""facebook/m2m100_418M"",
    torch_dtype=torch.float16,
).to(""cpu"").eval()

token = M2M100Tokenizer.from_pretrained(""facebook/m2m100_418M"")

encoded_text = token(""my name is earl"", return_tensors=""pt"")
encoded_text = encoded_text.to(""cpu"")

target_lang_id = token.get_lang_id(""he"")
generated_tokens = model.generate(**encoded_text, forced_bos_token_id=target_lang_id)
print(generated_tokens)
```

### Expected behavior

The models are being put in `/home/ubuntu/.cache/huggingface/hub/models--facebook--m2m100_418M/`
until version 4.46.0 there was this hierarchy: 
`snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636` which contained 7 files (one of them is the model itself pytorch_model.bin - ~2 GB).
From version 4.46.0, there is a new dir: `snapshots/791dc1c6d300846c9a747d4bd11fcc7f369b750e`, there is one file in there: `model.safetensors`, which is a soft link to another heavy ~2GB file in blobs dir.

Can you please resolve it and make it download and use only one model file? this usage is very wasteful. 

Thanks!","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6806437815, 'node_id': 'LA_kwDOCUB6oc8AAAABlbH_tw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Safetensors', 'name': 'Safetensors', 'color': '6D2E27', 'default': False, 'description': ''}]",2,open
RuntimeError in `_group_tensors_by_device_and_dtype` (torch/optim/optimizer.py) when training with FSDP on N>1 GPUs.,"### System Info

- `transformers` version: 4.46.2
- Platform: Linux-5.4.0-125-generic-x86_64-with-glibc2.31
- Python version: 3.10.15
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: Yes (FSDP)
- Using GPU in script?: Yes
- GPU type: NVIDIA RTX A5000


### Who can help?

@muellerzr @SunM

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

#### Output Error

```
[rank1]:   File ""/data/julien_piet/llm-attack-detect/scripts/bug.py"", line 254, in <module>
[rank1]:     train(
[rank1]:   File ""/data/julien_piet/llm-attack-detect/scripts/bug.py"", line 247, in train
[rank1]:     trainer.train()
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/transformers/trainer.py"", line 2123, in train
[rank1]:     return inner_training_loop(
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/transformers/trainer.py"", line 2534, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/accelerate/optimizer.py"", line 171, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py"", line 137, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 487, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 91, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/adamw.py"", line 220, in step
[rank1]:     adamw(
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 154, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/adamw.py"", line 782, in adamw
[rank1]:     func(
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/adamw.py"", line 480, in _multi_tensor_adamw
[rank1]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 516, in _group_tensors_by_device_and_dtype
[rank1]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/utils/_foreach_utils.py"", line 37, in _group_tensors_by_device_and_dtype
[rank1]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank1]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank0]: Traceback (most recent call last):
[rank0]:   File ""/data/julien_piet/llm-attack-detect/scripts/bug.py"", line 254, in <module>
[rank0]:     train(
[rank0]:   File ""/data/julien_piet/llm-attack-detect/scripts/bug.py"", line 247, in train
[rank0]:     trainer.train()
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/transformers/trainer.py"", line 2123, in train
[rank0]:     return inner_training_loop(
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/transformers/trainer.py"", line 2534, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/accelerate/optimizer.py"", line 171, in step
[rank0]:     self.optimizer.step(closure)
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py"", line 137, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 487, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 91, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/adamw.py"", line 220, in step
[rank0]:     adamw(
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 154, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/adamw.py"", line 782, in adamw
[rank0]:     func(
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/adamw.py"", line 480, in _multi_tensor_adamw
[rank0]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 516, in _group_tensors_by_device_and_dtype
[rank0]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File ""/data/julien_piet/llm-attack-detect/env/lib/python3.10/site-packages/torch/utils/_foreach_utils.py"", line 37, in _group_tensors_by_device_and_dtype
[rank0]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank0]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
```

#### Original Code
```python
import os
from dataclasses import dataclass, field
from typing import Any, Optional

import numpy as np
import torch
from datasets import Dataset
from sklearn.utils.class_weight import compute_class_weight
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
)


@dataclass
class CustomTrainingArguments(TrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    optim: str = field(default=""adamw_torch"")
    model_max_length: int = field(default=1024)
    lr_scheduler_type: Optional[str] = field(default=""cosine_with_restarts"")
    per_device_train_batch_size: int = field(default=4)
    per_device_eval_batch_size: int = field(default=4)
    output_dir: Optional[str] = field(default=""output"")
    remove_unused_columns: bool = False


@dataclass
class ModelArguments:
    model_name_or_path: Optional[str] = field(
        default=""meta-llama/Llama-3.2-1B-Instruct""
    )
    pad_token: str = field(
        default=""<|finetune_right_pad_id|>"", metadata={""help"": ""Padding token.""}
    )
    unk_token: str = field(
        default=""<|reserved_special_token_0|>"",
        metadata={""help"": ""Unknown token.""},
    )


class SupervisedDataset:
    def __init__(self, data, tokenizer, training_args):
        data_dict = SupervisedDataset._preprocess(
            data, tokenizer, training_args
        )
        self.input_ids = data_dict[""input_ids""]
        self.labels = data_dict[""labels""]
        self.attention_mask = data_dict[""attention_mask""]
        self.classification_labels = [
            d[""messages""][-1][""content""] for d in data
        ]

        # Compute class weights for imbalanced classes
        self.class_weights, self.class_values, self.class_indices = (
            SupervisedDataset.get_class_weights(
                self.classification_labels, tokenizer
            )
        )
        self.classification_labels = [
            self.class_indices[label] for label in self.classification_labels
        ]

    @staticmethod
    def get_class_weights(labels, tokenizer):
        classes = sorted(list(set(labels)))
        class_indices = {label: idx for idx, label in enumerate(classes)}
        label_indices = [class_indices[label] for label in labels]

        class_values = []
        for class_name in classes:
            class_values.append(
                tokenizer.encode(class_name, add_special_tokens=False)[0]
            )

        class_weights = compute_class_weight(
            class_weight=""balanced"",
            classes=np.unique(label_indices),
            y=label_indices,
        )
        return class_weights, class_values, class_indices

    @staticmethod
    def _preprocess(data, tokenizer, training_args):
        formatted_inputs = [
            tokenizer.apply_chat_template(d[""messages""], tokenize=False)
            for d in data
        ]
        formatted_prompts = [
            tokenizer.apply_chat_template(
                d[""messages""][:-1], tokenize=False, add_generation_prompt=True
            )
            for d in data
        ]
        tokenized_inputs = tokenizer(
            formatted_inputs,
            padding=True,
            padding_side=""left"",
            return_tensors=""pt"",
            add_special_tokens=False,
        )
        tokenized_prompts = tokenizer(
            formatted_prompts,
            padding=True,
            padding_side=""left"",
            return_tensors=""pt"",
            add_special_tokens=False,
        )

        attention_mask = tokenized_prompts[""attention_mask""]
        input_ids = tokenized_prompts[""input_ids""]
        labels = tokenized_inputs[""input_ids""][
            :, tokenized_prompts[""input_ids""].shape[1]
        ]

        attention_mask = attention_mask[:, -training_args.model_max_length :]
        input_ids = input_ids[:, -training_args.model_max_length :]

        return {
            ""input_ids"": input_ids,
            ""labels"": labels,
            ""attention_mask"": attention_mask,
        }

    def convert_to_dataset(self):
        return Dataset.from_dict(
            {
                ""input_ids"": self.input_ids,
                ""labels"": self.labels,
                ""attention_mask"": self.attention_mask,
            }
        )


# Custom Trainer with weighted loss
class WeightedLoss:
    def __init__(self, class_weights=None, class_values=None):
        self.class_weights = torch.tensor(class_weights).cuda()
        self.class_values = class_values

    def compute_loss(self, outputs, labels, **kwargs):
        logits = outputs.get(""logits"")

        # Compute loss based on last token logits
        logits = logits[:, -1, self.class_values].reshape(
            -1, len(self.class_values)
        )

        ce_labels = torch.tensor(
            [self.class_values.index(v) for v in labels]
        ).to(labels.device)

        if self.class_weights.dtype != logits.dtype:
            self.class_weights = self.class_weights.to(logits.dtype)

        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)
        loss = loss_fct(logits, ce_labels)

        return loss


# Load and prepare the dataset
def load_and_prepare_data(training_args, tokenizer):
    dataset = [
        {
            ""messages"": [
                {
                    ""role"": ""user"",
                    ""content"": (
                        ""Please respond with "" + (""no"" if i % 2 else ""yes"")
                    ),
                },
                {""role"": ""assistant"", ""content"": ""no"" if i % 2 else ""yes""},
            ]
        }
        for i in range(1000)
    ]

    dataset = SupervisedDataset(
        dataset,
        tokenizer,
        training_args,
    )

    class_weights, class_values, class_indices = (
        dataset.class_weights,
        dataset.class_values,
        dataset.class_indices,
    )

    dataset = dataset.convert_to_dataset()

    return (
        dataset,
        None,
        class_weights,
        class_values,
        class_indices,
    )


# Training function
def train(model_args, training_args):

    if training_args.lr_scheduler_type == ""cosine_with_restarts"":
        training_args.lr_scheduler_kwargs = {
            ""num_cycles"": 1 + training_args.num_train_epochs // 10
        }

    # Load the pretrained model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        model_max_length=training_args.model_max_length,
        truncation_side=""left"",
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
    )

    # Augment tokenizer
    if tokenizer.pad_token is None:
        tokenizer.pad_token = model_args.pad_token
    if tokenizer.unk_token is None:
        tokenizer.unk_token = model_args.unk_token

    # Load and prepare data
    train_dataset, _, class_weights, class_values, _ = load_and_prepare_data(
        training_args, tokenizer
    )

    # Loss function
    custom_loss = WeightedLoss(class_weights, class_values)

    # Initialize the Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        compute_loss_func=lambda x, y, **kwargs: custom_loss.compute_loss(x, y),
    )

    # Start training
    trainer.train()
    trainer.save_model(output_dir=training_args.output_dir)


if __name__ == ""__main__"":
    parser = HfArgumentParser((ModelArguments, CustomTrainingArguments))
    model_args, training_args = parser.parse_args_into_dataclasses()
    train(
        model_args,
        training_args,
    )
```

#### Command

Command that triggers the error (considering the previous code is in a file called `bug.py`) 
```
torchrun --nproc_per_node=2 --master_port=19527 bug.py  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct           --output_dir outputs/test/ --num_train_epochs 5 --per_device_train_batch_size 4  --model_max_length 1024 --gradient_accumulation_steps 8 --evaluation_strategy ""no"" --save_strategy ""no"" --save_total_limit 1 --learning_rate 2.5e-6             --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type ""cosine_with_restarts"" --logging_steps 1 --fsdp ""full_shard auto_wrap"" --fsdp_transformer_layer_cls_to_wrap ""LlamaDecoderLayer"" --bf16 True --tf32 True
```

### Expected behavior

I'm trying to fine-tune a model using the `Trainer` library. I am using `TorchRun` with FSDP to distribute the training over multiple GPUs. If I run the provided code with a single process, it works fine. However, if I increase `nproc_per_node`, I get the error provided with the example.

This error first seemed to be a PyTorch error, for which I created an issue [here](https://github.com/pytorch/pytorch/issues/140471).  However, as pointed out by @JoyceZhangSS and @jiaqiw09, this is an issue related to transformers version 4.46.2: 4.46.1 does not have this bug, and training happens as expected. 

I reproduced the error in a standalone file with a dummy dataset, provided in this issue. However, it occurs with any dataset, and with the standard loss: the [default alpaca training code](https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py) leads to the same error, both with Llama and OPT models. I did some investigation into the issue that might be helpful:

* I was able to reproduce this error in multiple environments. 
* This error is triggered during the optimization step. Specifically, it is triggered by this snippet of code in `torch/optim/adamw.py:480`:
```python
 grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
        [params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps]  # type: ignore[list-item]
    )
```
* The problems stems from grads being bf16 while the rest are float32 --- in `transformers==4.46.1`, all groups are float32.
* I looked at the diff between both versions and found the change responsible for the bug. In `trainer.py`, you did the following change:
```python
2473 -                   with self.accelerator.accumulate(model):
2474 +                   # We explicitly want to avoid relying on `accelerator.accumulate` for generation training
2475 +                   context = (
2476 +                        functools.partial(self.accelerator.no_sync, model=model)
2477 +                        if i == len(batch_samples) - 1
2478 +                        else contextlib.nullcontext
2479 +                   )
2480 +                   with context():
```
This context seems responsible for syncing the gradients across devices, so I tried reverting this change, and the error stops happening. I don't know enough about this to understand what the context does precisely, or why you do not want to rely on it, but removing seems to be what broke the code.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 4101623725, 'node_id': 'LA_kwDOCUB6oc70ec-t', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/PyTorch%20FSDP', 'name': 'PyTorch FSDP', 'color': 'B60205', 'default': False, 'description': ''}, {'id': 6660834292, 'node_id': 'LA_kwDOCUB6oc8AAAABjQRD9A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Accelerate', 'name': 'Accelerate', 'color': '59003B', 'default': False, 'description': ''}]",4,open
Update config validation,"# What does this PR do?

Fixes #33690

This PR adds extra validation for generation parameters to fail early when invalid values are passed and not let things fail silently as suggested in the issue.  


## Before submitting

- [ ]  This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x]  Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
Pull Request section?
- [x]  Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
to it if that's the case.
- [ ]  Did you make sure to update the documentation with your changes? Here are the
[documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
[here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ]  Did you write any new necessary tests?

## Who can review?

@gante @zucchini-nlp",[],6,open
Add GOT-OCR 2.0 to Transformers,"# What does this PR do?
Add [GOT-OCR 2.0](https://github.com/Ucas-HaoranWei/GOT-OCR2.0) to Transformers.

Left TODOs:
- [x] Tests
- [x] Docs
- [x] Post-processing

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",3,open
TypeError: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches',"### System Info

transformers==4.37.2 python 3

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

#huggingface trainer, to train the model
from transformers import Trainer, TrainingArguments

model.to(device)
model_name = f""{model_ckpt}-finetuned""
batch_size = 2
training_args = TrainingArguments(
    output_dir= model_name,
    save_safetensors = False,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    evaluation_strategy='epoch',
    logging_strategy='epoch',
    learning_rate=1e-5,
    num_train_epochs=10,
    weight_decay=0.01,
    gradient_accumulation_steps=2,
    max_grad_norm=1.0,
    optim='adamw_torch'
)

trainer = Trainer(
    model= model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics

)

trainer.train()

### Expected behavior

I was run perfectly fun before Nov 12th midnight, and stop working on Nov 13th.....","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6660834292, 'node_id': 'LA_kwDOCUB6oc8AAAABjQRD9A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Accelerate', 'name': 'Accelerate', 'color': '59003B', 'default': False, 'description': ''}]",5,open
Add Kannada language documentation to PEGASUS tokenizer class,"# What does this PR do?

This PR introduces Kannada language documentation alongside the existing English comments in the PEGASUS tokenizer class. By adding bilingual docstrings and comments, this update makes the code more accessible to Kannada-speaking developers and enhances inclusivity in the open-source community.

### Key Changes:
- Added Kannada translations for all major docstrings and function explanations within the PEGASUS tokenizer class.
- Ensured Kannada comments accurately mirror English documentation to maintain clarity and consistency.
- Encourages a broader range of contributors and users by supporting regional language accessibility.

This contribution aligns with Hugging Face’s mission to make open-source projects more inclusive and user-friendly, especially for non-English-speaking communities.

## Before submitting
- [x] This PR improves documentation with added multilingual support.
- [x] I have read and followed the [contributor guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request).
- [x] No additional dependencies or tests are required, as this change is purely documentation-based.
  
## Who can review?
Documentation: @stevhliu 
Tokenizers: @ArthurZucker ",[],1,open
SnapKV_Cache support added,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR adds the implementation of [SnapKV Cache paper](https://arxiv.org/abs/2404.14469) to [cache_utils.py](https://github.com/huggingface/transformers/blob/main/src/transformers/cache_utils.py) file. Also adds the changes in flash_attention2 in llama_modeling to reflect the initialization of the SnapKV cache.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],4,open
Corrected example code masked_language_modeling.md,"i experienced an error when following the tutorial, and updated the code as i think it should be

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [* ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Past Keys Output now working with output router logits,"# What does this PR do?

This handles the case where output_router_logits are True, and past key values are being used for inference, in the Mistral model. 

Fixes #30731 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker 
",[],1,open
"Gemma2: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)","### System Info

- `transformers` version: 4.47.0.dev0
- Platform: Linux-5.15.0-1052-oracle-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.25.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.0+cu124 (True)
- Tensorflow version (GPU?): 2.9.1 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA H100 80GB HBM3

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```
model_id = 'google/gemma-2-2b'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"")

messages = ""Any Context""
input_ids = tokenizer.encode(messages, return_tensors=""pt"").to(""cuda"")

gen_tokens = model(input_ids)
```

### Expected behavior

```
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.09s/it]
Traceback (most recent call last):
  File ""/host/ckpts/transformers/script.py"", line 34, in <module>
    gen_tokens = model(input_ids)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/host/ckpts/transformers/src/transformers/models/gemma2/modeling_gemma2.py"", line 1052, in forward
    outputs = self.model(
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/host/ckpts/transformers/src/transformers/models/gemma2/modeling_gemma2.py"", line 785, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py"", line 190, in forward
    return F.embedding(
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py"", line 2551, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
```
If set one gpu visible, no error","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5151155822, 'node_id': 'LA_kwDOCUB6oc8AAAABMwhmbg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Big%20Model%20Inference', 'name': 'Big Model Inference', 'color': '006b75', 'default': False, 'description': 'Problems related to the Big Model Inference capabilities provided by Accelerate'}, {'id': 6660834292, 'node_id': 'LA_kwDOCUB6oc8AAAABjQRD9A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Accelerate', 'name': 'Accelerate', 'color': '59003B', 'default': False, 'description': ''}]",8,open
fix: Handle BLIP-2 model output format,"# What does this PR do?

I have modified the code to return logits in addition to loss.

Instead of:

```
return {""loss"": loss}
```

The code now returns:
```
return {
    ""loss"": loss,
    ""logits"": logits
}
```

Output:
```
Done!
First values of original logits: tensor([[ 1.9322,  1.9379,  7.4007],
        [-1.4741, -1.1189,  8.6593],
        [-1.4213, -1.2489,  6.1976]])
First values of HF logits: tensor([[ 1.9322,  1.9379,  7.4007],
        [-1.4741, -1.1189,  8.6593],
        [-1.4213, -1.2489,  6.1976]])
Looks ok!
Generating a caption...
Original generation: ['merlion']
HF generation: ['Question: what object is in this image? Answer: merlion']
```


Fixes # ([issue](https://github.com/huggingface/transformers/issues/34704#issue-2652956392))


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts, @qubvel
",[],1,open
Fix Import Error for Trainer in run_ner.py,"# What does this PR do?

This pull request addresses an import error encountered when using the Trainer class in [run_ner.py](https://github.com/swalehmwadime/transformers/blob/main/examples/pytorch/token-classification/run_ner.py). Previously, the code was importing the Trainer module incorrectly, causing issues during runtime. This fix updates the import statement to correctly source Trainer directly from the transformers library.

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
TypeError: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches',"### System Info

transformers: 4.39.3
python: 3.10.12

### Who can help?

_No response_

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

An error may occur at line 580 of [run_ner.py](https://github.com/huggingface/transformers/blob/v4.39.3/examples/pytorch/token-classification/run_ner.py)

```python
trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=train_dataset if training_args.do_train else None,
      eval_dataset=eval_dataset if training_args.do_eval else None,
      tokenizer=tokenizer,
      data_collator=data_collator,
      compute_metrics=compute_metrics,
  )
```


### Expected behavior

```
Traceback (most recent call last):
  File ""/usr/src/app/llm_model_test/ner_train/run_ner.py"", line 666, in <module>
    main()
  File ""/usr/src/app/llm_model_test/ner_train/run_ner.py"", line 580, in main
    trainer = Trainer(
  File ""/usr/local/lib/python3.10/dist-packages/transformers/trainer.py"", line 373, in __init__
    self.create_accelerator_and_postprocess()
  File ""/usr/local/lib/python3.10/dist-packages/transformers/trainer.py"", line 4252, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(
TypeError: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'
```

Please Help Me...","[{'id': 1905493434, 'node_id': 'MDU6TGFiZWwxOTA1NDkzNDM0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/dependencies', 'name': 'dependencies', 'color': '0366d6', 'default': False, 'description': 'Pull requests that update a dependency file'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",7,open
"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)","**Reproduction**

I am trying to finetune Qwen2-0.5B model on some training data using a multi-GPU setup. The same code (given further below) seems to work in a single-GPU setting (when i set CUDA_VISIBLE_DEVICES=0):

```python
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[18], line 4
      2 import torch
      3 torch.autograd.set_detect_anomaly(True)
----> 4 main()

Cell In[14], line 15, in main()
      8 trainer = Trainer(env_params=env_params,
      9                   model_params=model_params,
     10                   optimizer_params=optimizer_params,
     11                   trainer_params=trainer_params)
     13 copy_all_src(trainer.result_folder)
---> 15 trainer.run()

File ~/neuralcombinatorialoptimization/NCO-master/NEW_py_ver/TSP/POMO/TSPTrainerTransformer.py:92, in TSPTrainer.run(self)
     89 self.scheduler.step()
     91 # Train
---> 92 train_score, train_loss = self._train_one_epoch(epoch)
     93 self.result_log.append('train_score', epoch, train_score)
     94 self.result_log.append('train_loss', epoch, train_loss)

File ~/neuralcombinatorialoptimization/NCO-master/NEW_py_ver/TSP/POMO/TSPTrainerTransformer.py:151, in TSPTrainer._train_one_epoch(self, epoch)
    148 remaining = train_num_episode - episode
    149 batch_size = min(self.trainer_params['train_batch_size'], remaining)
--> 151 avg_score, avg_loss = self._train_one_batch(batch_size)
    152 score_AM.update(avg_score, batch_size)
    153 loss_AM.update(avg_loss, batch_size)

File ~/neuralcombinatorialoptimization/NCO-master/NEW_py_ver/TSP/POMO/TSPTrainerTransformer.py:193, in TSPTrainer._train_one_batch(self, batch_size)
    191 state, reward, done = self.env.pre_step()
    192 while not done:
--> 193     selected, prob = self.model.module(state)
    194     # shape: (batch, pomo)
    195     state, reward, done = self.env.step(selected)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/neuralcombinatorialoptimization/NCO-master/NEW_py_ver/TSP/POMO/TSPTransformerModelQuant_b.py:39, in TSPTransformer.forward(self, state)
     37     return self._init_sequence(batch_size, pomo_size)
     38 else:
---> 39     return self._continue_sequence(state, batch_size, pomo_size)

File ~/neuralcombinatorialoptimization/NCO-master/NEW_py_ver/TSP/POMO/TSPTransformerModelQuant_b.py:84, in TSPTransformer._continue_sequence(self, state, batch_size, pomo_size)
     81 state.ninf_mask = state.ninf_mask.to(self.device)
     83 # Get probabilities from decoder
---> 84 probs = self.decoder(self.seq_so_far, self.input_mask, state.ninf_mask)
     86 # Select next node
     87 if self.training or self.model_params['eval_type'] == 'softmax':

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/neuralcombinatorialoptimization/NCO-master/NEW_py_ver/TSP/POMO/TSPTransformerModelQuant_b.py:185, in Decoder.forward(self, seq_so_far, inp_mask, ninf_mask)
    182 flat_mask = inp_mask.reshape(batch_size * pomo_size, problem_size)
    184 # Get model outputs
--> 185 outputs = self.model(inputs_embeds=flat_seq, attention_mask=flat_mask)
    186 logits = outputs.logits
    188 # Get last valid position

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/second/lib/python3.10/site-packages/peft/peft_model.py:1644, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)
   1642     with self._enable_peft_forward_hooks(**kwargs):
   1643         kwargs = {k: v for k, v in kwargs.items() if k not in self.special_peft_forward_args}
-> 1644         return self.base_model(
   1645             input_ids=input_ids,
   1646             attention_mask=attention_mask,
   1647             inputs_embeds=inputs_embeds,
   1648             labels=labels,
   1649             output_attentions=output_attentions,
   1650             output_hidden_states=output_hidden_states,
   1651             return_dict=return_dict,
   1652             **kwargs,
   1653         )
   1655 batch_size = _get_batch_size(input_ids, inputs_embeds)
   1656 if attention_mask is not None:
   1657     # concat prompt attention mask

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/second/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197, in BaseTuner.forward(self, *args, **kwargs)
    196 def forward(self, *args: Any, **kwargs: Any):
--> 197     return self.model.forward(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/accelerate/hooks.py:170, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    168         output = module._old_forward(*args, **kwargs)
    169 else:
--> 170     output = module._old_forward(*args, **kwargs)
    171 return module._hf_hook.post_forward(module, output)

File ~/second/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1170, in Qwen2ForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)
   1167 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1169 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-> 1170 outputs = self.model(
   1171     input_ids=input_ids,
   1172     attention_mask=attention_mask,
   1173     position_ids=position_ids,
   1174     past_key_values=past_key_values,
   1175     inputs_embeds=inputs_embeds,
   1176     use_cache=use_cache,
   1177     output_attentions=output_attentions,
   1178     output_hidden_states=output_hidden_states,
   1179     return_dict=return_dict,
   1180     cache_position=cache_position,
   1181 )
   1183 hidden_states = outputs[0]
   1184 # Only compute necessary logits, and do not upcast them to float if we are not computing the loss

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/second/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:901, in Qwen2Model.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)
    889     layer_outputs = self._gradient_checkpointing_func(
    890         decoder_layer.__call__,
    891         hidden_states,
   (...)
    898         position_embeddings,
    899     )
    900 else:
--> 901     layer_outputs = decoder_layer(
    902         hidden_states,
    903         attention_mask=causal_mask,
    904         position_ids=position_ids,
    905         past_key_value=past_key_values,
    906         output_attentions=output_attentions,
    907         use_cache=use_cache,
    908         cache_position=cache_position,
    909         position_embeddings=position_embeddings,
    910     )
    912 hidden_states = layer_outputs[0]
    914 if use_cache:

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/second/lib/python3.10/site-packages/accelerate/hooks.py:170, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    168         output = module._old_forward(*args, **kwargs)
    169 else:
--> 170     output = module._old_forward(*args, **kwargs)
    171 return module._hf_hook.post_forward(module, output)

File ~/second/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:629, in Qwen2DecoderLayer.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)
    626 hidden_states = self.input_layernorm(hidden_states)
    628 # Self Attention
--> 629 hidden_states, self_attn_weights, present_key_value = self.self_attn(
    630     hidden_states=hidden_states,
    631     attention_mask=attention_mask,
    632     position_ids=position_ids,
    633     past_key_value=past_key_value,
    634     output_attentions=output_attentions,
    635     use_cache=use_cache,
    636     cache_position=cache_position,
    637     position_embeddings=position_embeddings,
    638 )
    639 hidden_states = residual + hidden_states
    641 # Fully Connected

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/second/lib/python3.10/site-packages/accelerate/hooks.py:170, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    168         output = module._old_forward(*args, **kwargs)
    169 else:
--> 170     output = module._old_forward(*args, **kwargs)
    171 return module._hf_hook.post_forward(module, output)

File ~/second/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:506, in Qwen2SdpaAttention.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)
    495     return super().forward(
    496         hidden_states=hidden_states,
    497         attention_mask=attention_mask,
   (...)
    501         use_cache=use_cache,
    502     )
    504 bsz, q_len, _ = hidden_states.size()
--> 506 query_states = self.q_proj(hidden_states)
    507 key_states = self.k_proj(hidden_states)
    508 value_states = self.v_proj(hidden_states)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/second/lib/python3.10/site-packages/peft/tuners/lora/layer.py:572, in Linear.forward(self, x, *args, **kwargs)
    570     result = self.base_layer(x, *args, **kwargs)
    571 else:
--> 572     result = self.base_layer(x, *args, **kwargs)
    573     torch_result_dtype = result.dtype
    574     for active_adapter in self.active_adapters:

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-> 1736     return self._call_impl(*args, **kwargs)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/second/lib/python3.10/site-packages/accelerate/hooks.py:170, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    168         output = module._old_forward(*args, **kwargs)
    169 else:
--> 170     output = module._old_forward(*args, **kwargs)
    171 return module._hf_hook.post_forward(module, output)

File ~/second/lib/python3.10/site-packages/torch/nn/modules/linear.py:125, in Linear.forward(self, input)
    124 def forward(self, input: Tensor) -> Tensor:
--> 125     return F.linear(input, self.weight, self.bias)

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)

```
Code for the above error is given below:

```python
Trainer.py

import torch
from logging import getLogger
from torch.nn.parallel import DataParallel
from TSPEnvQuant import TSPEnv as Env
from TSPTransformerModelQuant_b import TSPTransformer as Model

from torch.optim import Adam as Optimizer
from torch.optim.lr_scheduler import MultiStepLR as Scheduler

from utils.utils import *


class TSPTrainer:
    def __init__(self,
                 env_params,
                 model_params,
                 optimizer_params,
                 trainer_params):

        # save arguments
        self.env_params = env_params
        self.model_params = model_params
        self.optimizer_params = optimizer_params
        self.trainer_params = trainer_params

        # result folder, logger
        self.logger = getLogger(name='trainer')
        self.result_folder = get_result_folder()
        self.result_log = LogData()

        # cuda
        USE_CUDA = self.trainer_params['use_cuda']
        if USE_CUDA:
            cuda_device_num = self.trainer_params['cuda_device_num']
            torch.cuda.set_device(cuda_device_num)
            device = torch.device('cuda', cuda_device_num)
            torch.set_default_tensor_type('torch.cuda.FloatTensor')
        else:
            device = torch.device('cpu')
            torch.set_default_tensor_type('torch.FloatTensor')

        # Main Components
        self.model = Model(**self.model_params)
        if USE_CUDA and torch.cuda.device_count() > 1:
            self.logger.info(f""Using {torch.cuda.device_count()} GPUs!"")
            self.model = DataParallel(self.model)
        self.model = self.model.to(device)

        self.env = Env(**self.env_params)
        self.optimizer = Optimizer(self.model.parameters(), **self.optimizer_params['optimizer'])
        self.scheduler = Scheduler(self.optimizer, **self.optimizer_params['scheduler'])

        # Restore
        self.start_epoch = 1
        model_load = trainer_params['model_load']
        if model_load['enable']:
            checkpoint_fullname = '{path}/checkpoint-{epoch}.pt'.format(**model_load)
            checkpoint = torch.load(checkpoint_fullname, map_location=device)
            # Handle loading state dict for DataParallel
            if isinstance(self.model, DataParallel):
                # If saved model wasn't using DataParallel but current model is
                if not any(key.startswith('module.') for key in checkpoint['model_state_dict'].keys()):
                    new_state_dict = {'module.' + k: v for k, v in checkpoint['model_state_dict'].items()}
                    self.model.load_state_dict(new_state_dict)
                else:
                    self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                # If saved model was using DataParallel but current model isn't
                if any(key.startswith('module.') for key in checkpoint['model_state_dict'].keys()):
                    new_state_dict = {k.replace('module.', ''): v for k, v in checkpoint['model_state_dict'].items()}
                    self.model.load_state_dict(new_state_dict)
                else:
                    self.model.load_state_dict(checkpoint['model_state_dict'])
            self.start_epoch = 1 + model_load['epoch']
            self.result_log.set_raw_data(checkpoint['result_log'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.last_epoch = model_load['epoch']-1
            self.logger.info('Saved Model Loaded !!')

        # utility
        self.time_estimator = TimeEstimator()

    def run(self):
        self.time_estimator.reset(self.start_epoch)
        for epoch in range(self.start_epoch, self.trainer_params['epochs']+1):
            self.logger.info('=================================================================')

            # LR Decay
            self.scheduler.step()

            # Train
            train_score, train_loss = self._train_one_epoch(epoch)
            self.result_log.append('train_score', epoch, train_score)
            self.result_log.append('train_loss', epoch, train_loss)

            ############################
            # Logs & Checkpoint
            ############################
            elapsed_time_str, remain_time_str = self.time_estimator.get_est_string(epoch, self.trainer_params['epochs'])
            self.logger.info(""Epoch {:3d}/{:3d}: Time Est.: Elapsed[{}], Remain[{}]"".format(
                epoch, self.trainer_params['epochs'], elapsed_time_str, remain_time_str))

            all_done = (epoch == self.trainer_params['epochs'])
            model_save_interval = self.trainer_params['logging']['model_save_interval']
            img_save_interval = self.trainer_params['logging']['img_save_interval']

            if epoch > 1:  # save latest images, every epoch
                self.logger.info(""Saving log_image"")
                image_prefix = '{}/latest'.format(self.result_folder)
                util_save_log_image_with_label(image_prefix, self.trainer_params['logging']['log_image_params_1'],
                                    self.result_log, labels=['train_score'])
                util_save_log_image_with_label(image_prefix, self.trainer_params['logging']['log_image_params_2'],
                                    self.result_log, labels=['train_loss'])

            if all_done or (epoch % model_save_interval) == 0:
                self.logger.info(""Saving trained_model"")
                checkpoint_dict = {
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'result_log': self.result_log.get_raw_data()
                }
                torch.save(checkpoint_dict, '{}/checkpoint-{}.pt'.format(self.result_folder, epoch))

            if all_done or (epoch % img_save_interval) == 0:
                image_prefix = '{}/img/checkpoint-{}'.format(self.result_folder, epoch)
                util_save_log_image_with_label(image_prefix, self.trainer_params['logging']['log_image_params_1'],
                                    self.result_log, labels=['train_score'])
                util_save_log_image_with_label(image_prefix, self.trainer_params['logging']['log_image_params_2'],
                                    self.result_log, labels=['train_loss'])

            if all_done:
                self.logger.info("" *** Training Done *** "")
                self.logger.info(""Now, printing log array..."")
                util_print_log_array(self.logger, self.result_log)

    def _train_one_epoch(self, epoch):

        score_AM = AverageMeter()
        loss_AM = AverageMeter()

        train_num_episode = self.trainer_params['train_episodes']
        episode = 0
        loop_cnt = 0
        while episode < train_num_episode:

            remaining = train_num_episode - episode
            batch_size = min(self.trainer_params['train_batch_size'], remaining)

            avg_score, avg_loss = self._train_one_batch(batch_size)
            score_AM.update(avg_score, batch_size)
            loss_AM.update(avg_loss, batch_size)

            episode += batch_size

            # Log First 10 Batch, only at the first epoch
            if epoch == self.start_epoch:
                loop_cnt += 1
                if loop_cnt <= 10:
                    self.logger.info('Epoch {:3d}: Train {:3d}/{:3d}({:1.1f}%)  Score: {:.4f},  Loss: {:.4f}'
                                     .format(epoch, episode, train_num_episode, 100. * episode / train_num_episode,
                                             score_AM.avg, loss_AM.avg))

        # Log Once, for each epoch
        self.logger.info('Epoch {:3d}: Train ({:3.0f}%)  Score: {:.4f},  Loss: {:.4f}'
                         .format(epoch, 100. * episode / train_num_episode,
                                 score_AM.avg, loss_AM.avg))

        return score_AM.avg, loss_AM.avg

    def _train_one_batch(self, batch_size):

        # Prep
        ###############################################
        self.model.train()
        self.env.load_problems(batch_size)
        reset_state, _, _ = self.env.reset()
        # Handle pre_forward for DataParallel
        if isinstance(self.model, DataParallel):
            print(""Is DataParallel"")
            self.model.module.pre_forward(reset_state)
        else:
            self.model.pre_forward(reset_state)

        prob_list = torch.zeros(size=(batch_size, self.env.pomo_size, 0))
        # shape: (batch, pomo, 0~problem)

        # POMO Rollout
        ###############################################
        state, reward, done = self.env.pre_step()
        while not done:
            selected, prob = self.model.module(state)
            # shape: (batch, pomo)
            state, reward, done = self.env.step(selected)
            prob_list = torch.cat((prob_list, prob[:, :, None]), dim=2)

        # Loss
        ###############################################
        advantage = reward - reward.float().mean(dim=1, keepdims=True)
        # shape: (batch, pomo)
        log_prob = prob_list.log().sum(dim=2)
        # size = (batch, pomo)
        loss = -advantage * log_prob  # Minus Sign: To Increase REWARD
        # shape: (batch, pomo)
        loss_mean = loss.mean()

        # Score
        ###############################################
        max_pomo_reward, _ = reward.max(dim=1)  # get best results from pomo
        score_mean = -max_pomo_reward.float().mean()  # negative sign to make positive value

        # Step & Return
        ###############################################
        self.model.zero_grad()
        loss_mean.backward()
        self.optimizer.step()
        return score_mean.item(), loss_mean.item()

```
```python
Model.py

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from transformers import AutoConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, TaskType
from typing import Optional, Dict, Any, Tuple

class TSPTransformer(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        self.model_params = kwargs
        self.encoder = Encoder(**kwargs)
        self.embedding_size = kwargs.get('embedding_dim', 896)
        
        # Load the model with LoRA and 4-bit quantization if needed
        self.model = load_model(kwargs)
        self.decoder = Decoder(self.model, **kwargs)
        
        # Initialize state storage
        self.encoded_nodes = None
        self.seq_so_far = None
        self.input_mask = None
        self.t = None
        self.device = kwargs.get('device', torch.device(""cuda"" if torch.cuda.is_available() else ""cpu""))

    def pre_forward(self, reset_state):
        """"""Initialize model state for new sequence""""""
        self.encoded_nodes = self.encoder(reset_state.problems)
        self.problem_size = reset_state.problems.size(1)
        self.batch_size = reset_state.problems.size(0)

    def forward(self, state) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        batch_size = state.BATCH_IDX.size(0)
        pomo_size = state.BATCH_IDX.size(1)

        if state.current_node is None:
            return self._init_sequence(batch_size, pomo_size)
        else:
            return self._continue_sequence(state, batch_size, pomo_size)

    def _init_sequence(self, batch_size: int, pomo_size: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """"""Initialize sequence state""""""
        self.t = 0  # Start at 0 instead of -1
        
        # Create new tensors instead of modifying in place
        selected = torch.arange(pomo_size, device=self.device).expand(batch_size, pomo_size)
        prob = torch.ones(size=(batch_size, pomo_size), device=self.device)
        
        # Initialize sequence storage with proper dimensions
        self.seq_so_far = torch.zeros(
            (batch_size, pomo_size, self.problem_size, self.embedding_size),
            device=self.device
        )
        
        self.input_mask = torch.zeros(
            (batch_size, pomo_size, self.problem_size),
            dtype=torch.bool,
            device=self.device
        )
        
        return selected, prob

    def _continue_sequence(self, state, batch_size: int, pomo_size: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """"""Continue sequence generation""""""
        # Get encoded representation of current node
        encoded_current = self._get_encoded_node(state.current_node)
        
        # Create new tensor for updated sequence
        new_seq = self.seq_so_far.clone()
        new_seq[:, :, self.t, :] = encoded_current
        self.seq_so_far = new_seq
        
        # Create new tensor for updated mask
        new_mask = self.input_mask.clone()
        new_mask[:, :, self.t] = True
        self.input_mask = new_mask
        
        # Move tensors to correct device
        self.seq_so_far = self.seq_so_far.to(self.device)
        self.input_mask = self.input_mask.to(self.device)
        state.ninf_mask = state.ninf_mask.to(self.device)
        
        # Get probabilities from decoder
        probs = self.decoder(self.seq_so_far, self.input_mask, state.ninf_mask)
        
        # Select next node
        if self.training or self.model_params['eval_type'] == 'softmax':
            selected, prob = self._sample_node(probs, state, batch_size, pomo_size)
        else:
            selected = probs.argmax(dim=2)
            prob = None
        
        self.t += 1
        return selected, prob

    def _get_encoded_node(self, node_indices: torch.Tensor) -> torch.Tensor:
        """"""Get encoded representation of nodes safely""""""
        batch_size, pomo_size = node_indices.shape
        embedding_dim = self.encoded_nodes.size(2)
        
        # Create gathering indices
        gather_idx = node_indices[:, :, None].expand(batch_size, pomo_size, embedding_dim)
        gather_idx = gather_idx.to(self.encoded_nodes.device)
        
        # Gather encoded representations
        return self.encoded_nodes.gather(dim=1, index=gather_idx)

    def _sample_node(self, probs: torch.Tensor, state, batch_size: int, pomo_size: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """"""Sample next node with retry logic""""""
        max_attempts = 100
        for _ in range(max_attempts):
            # Reshape for sampling
            flat_probs = probs.reshape(batch_size * pomo_size, -1)
            
            # Sample indices
            selected = flat_probs.multinomial(1, replacement=True)
            selected = selected.reshape(batch_size, pomo_size)
            
            # Calculate probabilities
            prob = probs[state.BATCH_IDX, state.POMO_IDX, selected]
            prob = prob.reshape(batch_size, pomo_size)
            
            if (prob > 0).all():
                return selected, prob
        
        raise RuntimeError(f""Failed to sample valid nodes after {max_attempts} attempts"")

class Encoder(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        self.embedding_dim = kwargs.get('embedding_dim', 896) - 1
        self.embed_layer = nn.Linear(2, self.embedding_dim)
        self.device = kwargs.get('device', torch.device(""cuda"" if torch.cuda.is_available() else ""cpu""))
    
    def forward(self, problems: torch.Tensor) -> torch.Tensor:
        batch_size, problem_size = problems.shape[:2]
        
        # Create position encodings
        ids = torch.arange(problem_size, device=self.device).expand(batch_size, problem_size)
        
        # Embed coordinates
        embedded = self.embed_layer(problems.reshape(-1, 2))
        embedded = embedded.reshape(batch_size, problem_size, self.embedding_dim)
        
        # Concatenate position encodings
        return torch.cat([ids.unsqueeze(-1).float(), embedded], dim=-1)

class Decoder(nn.Module):
    def __init__(self, model: nn.Module, **kwargs):
        super().__init__()
        self.model = model
        self.problem_size = kwargs.get('problem_size', 20)
        self.use_lora = kwargs.get('use_lora', True)
        
        self._setup_model()
    
    def _setup_model(self):
        """"""Configure model architecture""""""
        # Modify output size
        self.model.lm_head = nn.Linear(
            self.model.config.hidden_size,
            self.problem_size
        ).to(self.model.device)
        
        # Apply LoRA if requested
        if self.use_lora:
            lora_config = LoraConfig(
                r=4,
                lora_alpha=32,
                target_modules=[""q_proj"", ""v_proj""],
                lora_dropout=0.1,
                bias=""none"",
                task_type=TaskType.CAUSAL_LM
            )
            self.model = get_peft_model(self.model, lora_config)
    
    def forward(self, seq_so_far: torch.Tensor, inp_mask: torch.Tensor, ninf_mask: torch.Tensor) -> torch.Tensor:
        batch_size, pomo_size, problem_size, embedding_dim = seq_so_far.shape
        
        # Reshape inputs
        flat_seq = seq_so_far.reshape(batch_size * pomo_size, problem_size, embedding_dim)
        flat_mask = inp_mask.reshape(batch_size * pomo_size, problem_size)
        
        # Get model outputs
        outputs = self.model(inputs_embeds=flat_seq, attention_mask=flat_mask)
        logits = outputs.logits
        
        # Get last valid position
        last_positions = flat_mask.sum(dim=1).long() - 1
        
        # Gather logits for last positions
        batch_indices = torch.arange(batch_size * pomo_size, device=logits.device)
        gathered_logits = logits[batch_indices, last_positions]
        
        # Reshape and apply mask
        logits = gathered_logits.reshape(batch_size, pomo_size, problem_size)
        masked_logits = logits + ninf_mask.float()
        
        # Return probabilities
        return torch.softmax(masked_logits, dim=2)

def load_model(config: Dict[str, Any]) -> nn.Module:
    """"""Load model with proper configuration""""""
    # print(config)
    device = config.get('device', torch.device(""cuda"" if torch.cuda.is_available() else ""cpu""))
    
    if config.get('checkpoint_path'):
        # print('checkpoint_path')
        try:
            return PeftModel.from_pretrained(
                config['model_name'],
                config['checkpoint_path'],
                is_trainable=True
            ).to(device)
        except Exception as e:
            print(f""Error loading checkpoint: {e}"")
            print(""Falling back to base model..."")
    
    print(config)
    # print(config['use_4bit'])
    if config.get('use_4bit', True):
        print('use_4bit')
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            llm_int8_has_fp16_weight=False,
            bnb_4bit_quant_type=""nf4"",
            llm_int8_threshold=6.0,
            bnb_4bit_use_double_quant=True,
        )
        # print(config['model_name'])
        # print(type(config['model_name']))
        model = AutoModelForCausalLM.from_pretrained(
            config['model_name'],
            trust_remote_code=True,
            device_map=""auto"",
            torch_dtype=torch.bfloat16,
            quantization_config=bnb_config
        )
        model = prepare_model_for_kbit_training(model)
        model.config.use_cache = False
    else:
        # print('else')
        model = AutoModelForCausalLM.from_pretrained(
            config['model_name'],
            torch_dtype=torch.float32,
            trust_remote_code=True,
            device_map=""auto"",
        ).to(device)
    
    return model
```
**Expected behavior**
Expected behavior is that the model should train in a multi-GPU setting without throwing any errors. The same script works in single-GPU setting but throws the above error in a multi-GPU setting",[],6,open
Discrepancy in Training Loss Behavior with Gradient Accumulation using DeepSpeed,"### System Info

Accelerate version: 1.1.0
transformers version: 4.46.2
DeepSpeed version: 0.14.4
Platform: Linux 5.15.0-101-generic #111-Ubuntu SMP x86_64 GNU/Linux
Python version: 3.10.14
PyTorch version (GPU?): 2.1.2+cu118 True
GPU type: NVIDIA A100

### Who can help?

@muellerzr

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

The code provided below is a simplified example of training a small model using the Hugging Face Trainer. The setup includes creating a dataset, initializing a model and tokenizer, and configuring the Trainer with different settings for gradient accumulation and DeepSpeed.


```python
import argparse
import torch
from datasets import load_dataset
from transformers import (set_seed,
                          Trainer,
                          TrainingArguments,
                          DataCollatorForLanguageModeling,
                          LlamaForCausalLM,
                          LlamaConfig,
                          AutoTokenizer
                          )


DEEPSPEED_CONFIG = {
  ""zero_optimization"": {
    ""stage"": 0
  },
  ""optimizer"": {
    ""type"": ""AdamW"",
    ""params"": {
      ""lr"": ""auto"",
      ""betas"": ""auto"",
      ""eps"": ""auto"",
      ""weight_decay"": ""auto""
    }
  },
  ""gradient_accumulation_steps"": ""auto"",
  ""gradient_clipping"": ""auto"",
  ""train_batch_size"": ""auto"",
  ""train_micro_batch_size_per_gpu"": ""auto""
}

TRAIN_ARGS = {'output_dir': './test_GA',
              'bf16': True,
              'learning_rate': 6e-4,
              'lr_scheduler_type': 'cosine',
              'max_steps': 200,
              'optim': 'adamw_torch',
              'weight_decay': 0.1,
              'per_device_train_batch_size': 128,
              'gradient_accumulation_steps': 1,
              'logging_steps': 1,
              'report_to': 'none'}

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument('--bs', default=128, type=int, help='batch size')
    parser.add_argument('--ga', default=1, type=int, help='number of gradient accumulation step')
    parser.add_argument('--deepspeed', action='store_true', help='use deepspeed')
    args = parser.parse_args()

    set_seed(42)
    torch.use_deterministic_algorithms(True)
    torch.backends.cudnn.benchmark = False

    # Initialize dataset
    CONTEXT_LENGTH = 512  # Small context length as specified
    def preprocess_data(examples, tokenizer, max_length=CONTEXT_LENGTH):
        """"""Tokenizes the input data and truncates/pads to the max context length.""""""
        return tokenizer(examples[""sentence""], truncation=True, padding=""max_length"", max_length=max_length, add_special_tokens=True)

    # Load the dataset from Hugging Face
    dataset = load_dataset(""ptb_text_only"", trust_remote_code=True, split='train')

    # Load and configure the tokenizer
    tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-hf"", add_prefix_space=True, use_fast=True)
    tokenizer.pad_token = tokenizer.eos_token

    # Preprocess the dataset
    column_names = list(dataset.features)
    train_dataset = dataset.map(lambda x: preprocess_data(x, tokenizer), batched=True, remove_columns=column_names)

    # Initialize model
    model_cfg = LlamaConfig(n_positions=CONTEXT_LENGTH, hidden_size=512, num_attention_heads=8, num_hidden_layers=4,
                            vocab_size=tokenizer.vocab_size, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id)
    model = LlamaForCausalLM(model_cfg)
    
    # Initialize trainer
    if args.deepspeed:
        TRAIN_ARGS.update({""deepspeed"": DEEPSPEED_CONFIG})
    TRAIN_ARGS.update({""per_device_train_batch_size"": args.bs, ""gradient_accumulation_steps"": args.ga})
    trainer = Trainer(model=model, args=TrainingArguments(**TRAIN_ARGS), train_dataset=train_dataset,
                      data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False))
    trainer.train()

```

### Expected behavior

The training loss should remain consistent for different gradient accumulation steps, both with and without DeepSpeed enabled. However, the figure shows a divergence when DeepSpeed is enabled:

![gradient_accumulation_issue](https://github.com/user-attachments/assets/7a3eb2e6-bc78-44b6-9719-9a53e8615d9c)
","[{'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",5,open
CPU processing is extremely slow for models loaded with `torch_dtype = torch.float16`,"### System Info

Transformers versions: 4.44.2, 4.46.2
PyTorch versions: 2.4.0, 2.5.1
Python version: 3.11.2

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

While troubleshooting very weird behaviour with GPT-NeoX when processed on a CPU device, I discovered that Transformers will load a model with `torch_dtype = torch.float16` and process it on the CPU without any apparent warning or other message, but its performance is very slow compared to `float32` (5+ times slower) or `bfloat16` (10+ times slower). Given the amount of documentation online that suggests using `.half()` or `torch_dtype = torch.float16` to conserve memory, can I suggest adding a warning message when a model loaded in this format is processed on a CPU device? I know `float16` support for CPU *at all* is relatively new, but given the lack of information anywhere about the *massive* performance hit it currently incurs, I assumed CPU processing as a whole in Transformers was essentially unusable for real-world work (especially training / gradient operations). In reality, CPU processing is surprisingly fast when set to `float32` or `bfloat16` format.

[Here's a quick benchmark script based on the example usage for GPT-NeoX](https://huggingface.co/docs/transformers/en/model_doc/gpt_neox#usage-example) that loads the model, then generates text using three prompts. It performs this test for `torch_dtype = None`, `torch.float32`, `torch.float16` and , `torch.bfloat16`:

```
#!/bin/env python

# https://huggingface.co/docs/transformers/en/model_doc/gpt_neox#usage-example
# But modified to debug an issue

import datetime
import gc
import json
import torch

from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast
from transformers.generation import GenerationConfig

def output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, step_description):
    generation_current_dt = datetime.datetime.now()
    generation_current_dt_string = generation_current_dt.replace(microsecond = 0).isoformat()
    generation_elapsed = generation_current_dt - generation_start_dt
    generation_elapsed_string = f""{generation_elapsed.total_seconds()}""
    step_elapsed = generation_current_dt - generation_previous_dt
    step_elapsed_string = f""{generation_elapsed.total_seconds()}""
    print(f""\tReached prompt {prompt_num} stage: '{step_description}' at  {generation_current_dt_string}. {step_elapsed_string} seconds elapsed between the previous stage and this stage. {generation_elapsed_string} seconds elapsed for prompt {prompt_num} in total."")
    return generation_current_dt

def generate_text(model, tokenizer, prompt, prompt_num):
    generation_start_dt = datetime.datetime.now()
    generation_start_dt_string = generation_start_dt.replace(microsecond = 0).isoformat()
    generation_previous_dt = generation_start_dt
    print(f""Beginning processing prompt {prompt_num} at {generation_start_dt_string}"")
    
    input_ids = tokenizer(prompt, return_tensors=""pt"").input_ids.to(model.device)

    generation_previous_dt = output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, ""Finished tokenizing input. Creating attention mask."")

    attn_masks = torch.ones_like(input_ids)

    generation_previous_dt = output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, ""Finished creating attention mask. Generating output."")

    existing_config_dict = model.generation_config.to_dict()

    gen_config = GenerationConfig.from_dict(config_dict = existing_config_dict)

    gen_config.do_sample = False
    gen_config.temperature = 1.0
    gen_config.max_length = 80

    gen_tokens = model.generate(
        input_ids,
        attention_mask = attn_masks,   
        generation_config = gen_config
    )
    
    generation_previous_dt = output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, ""Finished generating output. Decoding token IDs to text."")
    
    gen_text = tokenizer.batch_decode(gen_tokens)[0]

    generation_previous_dt = output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, ""Finished decoding token IDs to text."")

    print(f""Model input: '{prompt}'\nModel output: '{gen_text}'"")

    generation_end_dt = datetime.datetime.now()
    generation_end_dt_string = generation_end_dt.replace(microsecond = 0).isoformat()
    generation_elapsed = generation_end_dt - generation_start_dt
    generation_elapsed_string = f""{generation_elapsed.total_seconds()}""
    print(f""Finished processing prompt {prompt_num} at {generation_end_dt_string} - {generation_elapsed_string} seconds elapsed in total for this prompt."")

def main():
    model_path = ""EleutherAI/gpt-neox-20b""
    tokenizer_path = model_path

    prompts = []
    # increase this range to test for behaviour that changes with successive operations
    for i in range(0, 1):
        prompts.append(""<|prompter|>Please tell me about GPTNeoX20B, the 20B-parameter autoregressive Transformer model developed by EleutherAI.<|endoftext|><|assistant|>"")
        prompts.append(""<|prompter|>Please tell me about GPTNeoX20B, the 20B-parameter autoregressive Transformer model developed by EleutherAI.<|endoftext|><|assistant|>Sure, I'd be happy to tell you about GPTNeoX20B"")
        prompts.append(""GPTNeoX20B is a 20B-parameter autoregressive Transformer model developed by EleutherAI."")

    dtype_list = [ None, torch.float32, torch.float16, torch.bfloat16 ]

    for current_dtype in dtype_list:
        start_dt = datetime.datetime.now()
        start_dt_string = start_dt.replace(microsecond = 0).isoformat()

        print(f""Starting test with torch_dtype = {current_dtype} at {start_dt_string}"")

        model = GPTNeoXForCausalLM.from_pretrained(
            model_path,
            torch_dtype = current_dtype
            ).to(device = ""cpu"").eval()

        print(f""model.dtype (when loaded with torch_dtype {current_dtype}) = {model.dtype}"")

        tokenizer = GPTNeoXTokenizerFast.from_pretrained(tokenizer_path)

        for prompt_num in range(0, len(prompts)):
           generate_text(model, tokenizer, prompts[prompt_num], prompt_num)

        end_dt = datetime.datetime.now()
        end_dt_string = end_dt.replace(microsecond = 0).isoformat()
        elapsed = end_dt - start_dt
        elapsed_string = f""{elapsed.total_seconds()}""

        print(f""Finished test with torch_dtype = {current_dtype} at {end_dt_string} - {elapsed_string} seconds elapsed for the entire test cycle using this dtype."")
        
        del model
        gc.collect()

main()
```

Excerpt of the output with just the relevant statistics:

```
Finished processing prompt 0 at 2024-11-11T13:09:31 - 88.848283 seconds elapsed in total for this prompt.
Finished processing prompt 1 at 2024-11-11T13:10:20 - 49.434201 seconds elapsed in total for this prompt.
Finished processing prompt 2 at 2024-11-11T13:12:15 - 114.557077 seconds elapsed in total for this prompt.
Finished test with torch_dtype = None at 2024-11-11T13:12:15 - 260.500114 seconds elapsed for the entire test cycle using this dtype.
Finished processing prompt 0 at 2024-11-11T13:13:42 - 79.813121 seconds elapsed in total for this prompt.
Finished processing prompt 1 at 2024-11-11T13:14:31 - 48.493672 seconds elapsed in total for this prompt.
Finished processing prompt 2 at 2024-11-11T13:16:21 - 109.835365 seconds elapsed in total for this prompt.
Finished test with torch_dtype = torch.float32 at 2024-11-11T13:16:21 - 245.690124 seconds elapsed for the entire test cycle using this dtype.
Finished processing prompt 0 at 2024-11-11T13:25:36 - 553.680202 seconds elapsed in total for this prompt.
Finished processing prompt 1 at 2024-11-11T13:34:44 - 548.825131 seconds elapsed in total for this prompt.
Finished processing prompt 2 at 2024-11-11T13:44:00 - 555.500857 seconds elapsed in total for this prompt.
Finished test with torch_dtype = torch.float16 at 2024-11-11T13:44:00 - 1659.152628 seconds elapsed for the entire test cycle using this dtype.
Finished processing prompt 0 at 2024-11-11T13:44:44 - 37.687044 seconds elapsed in total for this prompt.
Finished processing prompt 1 at 2024-11-11T13:45:05 - 21.75417 seconds elapsed in total for this prompt.
Finished processing prompt 2 at 2024-11-11T13:45:58 - 52.408085 seconds elapsed in total for this prompt.
Finished test with torch_dtype = torch.bfloat16 at 2024-11-11T13:45:58 - 117.512555 seconds elapsed for the entire test cycle using this dtype.
```

As you can see, `float16` performance scored about 7 times worse than `float32` for this run, and about 14 times worse than `bfloat16`, with simple text generation taking almost ten minutes in `float16` format. For training/gradient operations, the effect is even more of a problem. Operations that take a few minutes in the other formats can take hours in `float16` format (in the case of the GPT-NeoX issue, 10+ hours for a call to `forward`). I don't have a good minimal test case for that, though.

This is not limited to GPT-NeoX. For example, here's the same script, but modified to use Phi-3-mini-128k instead:

```
#!/bin/env python

# https://huggingface.co/docs/transformers/en/model_doc/gpt_neox#usage-example
# But modified to debug an issue

import datetime
import gc
import json
import torch

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

def output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, step_description):
    generation_current_dt = datetime.datetime.now()
    generation_current_dt_string = generation_current_dt.replace(microsecond = 0).isoformat()
    generation_elapsed = generation_current_dt - generation_start_dt
    generation_elapsed_string = f""{generation_elapsed.total_seconds()}""
    step_elapsed = generation_current_dt - generation_previous_dt
    step_elapsed_string = f""{generation_elapsed.total_seconds()}""
    print(f""\tReached prompt {prompt_num} stage: '{step_description}' at  {generation_current_dt_string}. {step_elapsed_string} seconds elapsed between the previous stage and this stage. {generation_elapsed_string} seconds elapsed for prompt {prompt_num} in total."")
    return generation_current_dt

def generate_text(model, tokenizer, prompt, prompt_num):
    generation_start_dt = datetime.datetime.now()
    generation_start_dt_string = generation_start_dt.replace(microsecond = 0).isoformat()
    generation_previous_dt = generation_start_dt
    print(f""Beginning processing prompt {prompt_num} at {generation_start_dt_string}"")
    
    input_ids = tokenizer(prompt, return_tensors=""pt"").input_ids.to(model.device)

    generation_previous_dt = output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, ""Finished tokenizing input. Creating attention mask."")

    attn_masks = torch.ones_like(input_ids)

    generation_previous_dt = output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, ""Finished creating attention mask. Generating output."")

    existing_config_dict = model.generation_config.to_dict()

    gen_config = GenerationConfig.from_dict(config_dict = existing_config_dict)

    gen_config.do_sample = False
    gen_config.temperature = 1.0
    gen_config.max_length = 80

    gen_tokens = model.generate(
        input_ids,
        attention_mask = attn_masks,   
        generation_config = gen_config
    )
    
    generation_previous_dt = output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, ""Finished generating output. Decoding token IDs to text."")
    
    gen_text = tokenizer.batch_decode(gen_tokens)[0]

    generation_previous_dt = output_generation_step_info(prompt_num, generation_start_dt, generation_previous_dt, ""Finished decoding token IDs to text."")

    print(f""Model input: '{prompt}'\nModel output: '{gen_text}'"")

    generation_end_dt = datetime.datetime.now()
    generation_end_dt_string = generation_end_dt.replace(microsecond = 0).isoformat()
    generation_elapsed = generation_end_dt - generation_start_dt
    generation_elapsed_string = f""{generation_elapsed.total_seconds()}""
    print(f""Finished processing prompt {prompt_num} at {generation_end_dt_string} - {generation_elapsed_string} seconds elapsed in total for this prompt."")

def main():
    model_path = ""/mnt/md0/Machine_Learning/LLMs/Microsoft/Phi-3-mini-128k-instruct""
    tokenizer_path = model_path

    prompts = []
    # increase this range to test for behaviour that changes with successive operations
    for i in range(0, 1):
        prompts.append(""<|prompter|>Please tell me about GPTNeoX20B, the 20B-parameter autoregressive Transformer model developed by EleutherAI.<|endoftext|><|assistant|>"")
        prompts.append(""<|prompter|>Please tell me about GPTNeoX20B, the 20B-parameter autoregressive Transformer model developed by EleutherAI.<|endoftext|><|assistant|>Sure, I'd be happy to tell you about GPTNeoX20B"")
        prompts.append(""GPTNeoX20B is a 20B-parameter autoregressive Transformer model developed by EleutherAI."")

    dtype_list = [ None, torch.float32, torch.float16, torch.bfloat16 ]

    for current_dtype in dtype_list:
        start_dt = datetime.datetime.now()
        start_dt_string = start_dt.replace(microsecond = 0).isoformat()

        print(f""Starting test with torch_dtype = {current_dtype} at {start_dt_string}"")

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype = current_dtype
            ).to(device = ""cpu"").eval()

        print(f""model.dtype (when loaded with torch_dtype {current_dtype}) = {model.dtype}"")

        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

        #if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:
        #    tokenizer.pad_token_id = tokenizer.eos_token_id
        #    print(f""tokenizer.pad_token_id has been set to {tokenizer.pad_token_id} to match tokenizer.eos_token_id"")

        for prompt_num in range(0, len(prompts)):
           generate_text(model, tokenizer, prompts[prompt_num], prompt_num)

        end_dt = datetime.datetime.now()
        end_dt_string = end_dt.replace(microsecond = 0).isoformat()
        elapsed = end_dt - start_dt
        elapsed_string = f""{elapsed.total_seconds()}""

        print(f""Finished test with torch_dtype = {current_dtype} at {end_dt_string} - {elapsed_string} seconds elapsed for the entire test cycle using this dtype."")
        
        del model
        gc.collect()

main()
```

Relevant output for Phi-3-mini-128k:

```
Finished processing prompt 0 at 2024-11-11T14:06:13 - 17.336155 seconds elapsed in total for this prompt.
Finished processing prompt 1 at 2024-11-11T14:06:22 - 9.018136 seconds elapsed in total for this prompt.
Finished processing prompt 2 at 2024-11-11T14:06:44 - 22.536667 seconds elapsed in total for this prompt.
Finished test with torch_dtype = None at 2024-11-11T14:06:44 - 67.624459 seconds elapsed for the entire test cycle using this dtype.
Finished processing prompt 0 at 2024-11-11T14:07:02 - 16.449278 seconds elapsed in total for this prompt.
Finished processing prompt 1 at 2024-11-11T14:07:11 - 8.645172 seconds elapsed in total for this prompt.
Finished processing prompt 2 at 2024-11-11T14:07:33 - 22.538802 seconds elapsed in total for this prompt.
Finished test with torch_dtype = torch.float32 at 2024-11-11T14:07:33 - 48.931164 seconds elapsed for the entire test cycle using this dtype.
Finished processing prompt 0 at 2024-11-11T14:09:00 - 85.79342 seconds elapsed in total for this prompt.
Finished processing prompt 1 at 2024-11-11T14:10:25 - 84.946433 seconds elapsed in total for this prompt.
Finished processing prompt 2 at 2024-11-11T14:11:52 - 86.384148 seconds elapsed in total for this prompt.
Finished test with torch_dtype = torch.float16 at 2024-11-11T14:11:52 - 257.981995 seconds elapsed for the entire test cycle using this dtype.
Finished processing prompt 0 at 2024-11-11T14:11:59 - 7.338564 seconds elapsed in total for this prompt.
Finished processing prompt 1 at 2024-11-11T14:12:03 - 3.707063 seconds elapsed in total for this prompt.
Finished processing prompt 2 at 2024-11-11T14:12:13 - 9.802746 seconds elapsed in total for this prompt.
Finished test with torch_dtype = torch.bfloat16 at 2024-11-11T14:12:13 - 21.009795 seconds elapsed for the entire test cycle using this dtype.
```

In this case, the difference is 5 times worse than `float32`, and 10 times worse than `bfloat16` overall. There seems to be some kind of fixed overhead causing the issue, because the processing times for both Phi-3-mini-128k and GPT-NeoX in `float16` form are virtually identical, even when they vary by several times for the data in other formats.

I assume the discrepancy is at least sort of a known issue to the Transformers developers, but I only discovered it myself when trying to debug a different problem. Adding a runtime warning and maybe an explicit warning in the documentation seems like it would be a good idea.

### Expected behavior

If CPU processing is performed using a very inefficient format that is also commonly suggested as a way to reduce the memory footprint, I would expect Transformers to issue a warning.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",7,open
Update agents.md ,"Use the function variable task instead of the hard coded string

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
"Changes required to `save_model` for certain models (e.g., Phi 3.5 Vision)","### Feature request

This request proposes one of three changes (see **Motivation** for background, and **Your contribution** more thoughts on possible solutions) in order to allow saving of a certain class of models, including but not limited to Phi 3.5 Vision.

1. Accept a `state_dict` argument in the `Trainer` class's `save_model()` method (https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3719-L3768). This `state_dict` parameter should then be passed down to the call to the private `_save()` method (https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3842), which _does_ accept a `state_dict` argument.
2. Rather than`state_dict` as an argument to `save_model()`, determine the appropriate heuristic such that we can successfully save Phi 3.5 Vision and other architecturally similar models.
3. Some change to the way `transformers` handles shared tensors...?

### Motivation

I encountered an issue while trying to fine-tune Phi 3.5 Vision using the `Trainer` class from `transformers`. In particular, when trying to call `save()` or `save_pretrained()`, transformers throws the following error:

```
RuntimeError: The weights trying to be saved contained shared tensors [{'model.vision_embed_tokens.wte.weight', 
'model.embed_tokens.weight'}] that are mismatching the transformers base configuration.
Try saving using `safe_serialization=False` or remove this tensor sharing.
```

Below are two minimal reproducible examples:
_Example #1_
```
from transformers import AutoModelForCausalLM
model_id = ""microsoft/Phi-3.5-vision-instruct""
model = AutoModelForCausalLM.from_pretrained(
    model_id, device_map=""cuda"", trust_remote_code=True, torch_dtype=""auto""
)
model.save_pretrained(""out"")
```

_Example #2_
```
from transformers import (
    Trainer,
    TrainingArguments,
)
training_args = TrainingArguments(
        save_only_model=True,
        output_dir='./out/',
        save_strategy='no',
    )
trainer = Trainer(
        model=model,
        args=training_args
    )
trainer.save_model()
```


It looks like others have also encountered this issue. See the list of reference issues below in ""Issues"".

A contributor to the Phi 3 Vision cookbook suggested the following solution, stating ""You need to remove the wte weight. It's okay because when the model is loaded from the checkpoint, it will automatically copy the weight from the embedding weight.""

```
state_dict = model.state_dict()
state_dict = {k:v for k, v in state_dict.items() if ""wte"" not in k}
model.save_pretrained(args.save_model_path, state_dict=state_dict, safe_serialization=True)
processor.save_pretrained(args.save_model_path)
```

This does indeed seem to work. However, it doesn't exactly fit into a use case that relies on the `Trainer` abstraction. The call to the `Trainer` class's `save_model()` method doesn't accommodate a state_dict argument (see https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3719-L3768).





**Issues**

1. https://github.com/kazuar/Phi3-Vision-ft/issues/2
2. https://discuss.huggingface.co/t/runtimeerror-when-saving-phi-3-5-vision-due-to-shared-tensors/116457
4. https://github.com/huggingface/transformers/issues/32354
5. https://discuss.huggingface.co/t/using-trainer-to-save-a-bartforsequenceclassification-model/81606

### Your contribution

I'd be glad to submit a PR, but I think some discussion is needed from the appropriate `transformers` stakeholders.


It's not clear to me whether the most appropriate change here is to modify the function signature.

Alternatively, maybe there's a heuristic by which we could determine whether the architecture is such that one needs to save everything but the `wte` weights. I don't know the answer to that off-hand. It may require a deep dive from Phi 3/3.5 Vision SMEs.

Or more broadly, perhaps there's some change to the way `transformers` handles shared tensors in the base configuration that would be most appropriate.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6806437815, 'node_id': 'LA_kwDOCUB6oc8AAAABlbH_tw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Safetensors', 'name': 'Safetensors', 'color': '6D2E27', 'default': False, 'description': ''}]",4,open
Bug when using StaticCache in Qwen2.5 Inference,"### System Info

```shell
Collecting environment information...
WARNING 11-10 14:19:08 _custom_ops.py:14] Failed to import from vllm._C with ImportError('/mnt/bbuf/vllm-backup/vllm/_C.abi3.so: undefined symbol: _ZN5torch3jit11parseSchemaERKSs')
PyTorch version: 2.4.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.30.0
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.5.82
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090
GPU 4: NVIDIA GeForce RTX 4090
GPU 5: NVIDIA GeForce RTX 4090
GPU 6: NVIDIA GeForce RTX 4090
GPU 7: NVIDIA GeForce RTX 4090

Nvidia driver version: 550.54.15
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      52 bits physical, 57 bits virtual
Byte Order:                         Little Endian
CPU(s):                             128
On-line CPU(s) list:                0-127
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) Gold 6462C
CPU family:                         6
Model:                              143
Thread(s) per core:                 2
Core(s) per socket:                 32
Socket(s):                          2
Stepping:                           8
CPU max MHz:                        3900.0000
CPU min MHz:                        800.0000
BogoMIPS:                           6600.00
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities
Virtualization:                     VT-x
L1d cache:                          3 MiB (64 instances)
L1i cache:                          2 MiB (64 instances)
L2 cache:                           128 MiB (64 instances)
L3 cache:                           120 MiB (2 instances)
NUMA node(s):                       2
NUMA node0 CPU(s):                  0-31,64-95
NUMA node1 CPU(s):                  32-63,96-127
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Not affected
Vulnerability Spec rstack overflow: Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] flashinfer==0.1.6+cu121torch2.4
[pip3] numpy==1.26.4
[pip3] nvidia-nccl-cu12==2.20.5
[pip3] onnx==1.16.0
[pip3] optree==0.12.1
[pip3] pytorch-triton==3.0.0+989adb9a2
[pip3] torch==2.4.0
[pip3] torch-tensorrt==2.5.0a0
[pip3] torchao==0.6.1
[pip3] torchvision==0.19.0
[pip3] transformers==4.47.0.dev0
[pip3] triton==3.0.0
[conda] Could not collect
ROCM Version: Could not collect
Neuron SDK Version: N/A
vLLM Version: 0.5.0.post1
vLLM Build Flags:
CUDA Archs: 6.0 6.1 7.0 7.5 8.0 8.6 8.9 9.0+PTX; ROCm: Disabled; Neuron: Disabled
GPU Topology:
GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID
GPU0     X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A
GPU1    PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A
GPU2    SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A
GPU3    SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     0-31,64-95      0               N/A
GPU4    SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     32-63,96-127    1               N/A
GPU5    SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     32-63,96-127    1               N/A
GPU6    SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     32-63,96-127    1               N/A
GPU7    SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      32-63,96-127    1               N/A

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
```

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

When I use StaticCache to perform inference on Qwen2.5, a bug occurs. In this example, I pass the tensor after the embedding layer to model.generate instead of the token IDs from the tokenizer. The reproduction script is as follows:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache

model_id = ""Qwen/Qwen2.5-7B-Instruct""
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=""cuda"")
tokenizer = AutoTokenizer.from_pretrained(model_id)

model.generation_config.max_new_tokens = 128

prompt_cache = StaticCache(config=model.config, batch_size=1, max_cache_len=32768, device=""cuda"", dtype=torch.bfloat16)

INITIAL_PROMPT = ""You are a helpful assistant. ""
inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=""pt"").to(""cuda"")

inputs_embeds = model.get_input_embeddings()(inputs_initial_prompt.input_ids)
outputs = model.generate(inputs_embeds=inputs_embeds, past_key_values=prompt_cache)

response = tokenizer.batch_decode(outputs)[0]
print(response)

prompts = [""Help me to write a blogpost about travelling.""]
responses = []
for prompt in prompts:
    new_inputs = tokenizer(prompt, return_tensors=""pt"").to(""cuda"")
    new_input_ids = torch.cat([outputs, new_inputs.input_ids], dim=1)

    inputs_embeds = model.get_input_embeddings()(new_input_ids)

    outputs = model.generate(inputs_embeds=inputs_embeds, past_key_values=prompt_cache)
    response = tokenizer.batch_decode(outputs)[0]
    print(response)
    responses.append(response)
```

I used the latest version of Transformers by compiling it from source. The error message is as follows:

```shell
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.42it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
 You will be given a task.  You must generate a detailed and long response.  You should use your own words.  You should not simply translate or repeat the prompt.  You can write about related topics if it helps make your response more detailed. Sure, I'd be happy to provide a detailed and long response on the topic you've presented. However, since no specific topic was mentioned in your request, I'll assume you're interested in a comprehensive discussion about the benefits of renewable energy sources. Let's dive into this fascinating subject.

Renewable energy sources, such as solar, wind, hydroelectric, geothermal,
Traceback (most recent call last):
  File ""/mnt/bbuf/transformers/../debug.py"", line 83, in <module>
    outputs = model.generate(inputs_embeds=inputs_embeds, past_key_values=prompt_cache)
  File ""/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
  File ""/mnt/bbuf/transformers/src/transformers/generation/utils.py"", line 2231, in generate
    result = self._sample(
  File ""/mnt/bbuf/transformers/src/transformers/generation/utils.py"", line 3215, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
  File ""/mnt/bbuf/transformers/src/transformers/generation/utils.py"", line 454, in prepare_inputs_for_generation
    attention_mask = causal_mask_creation_function(
  File ""/mnt/bbuf/transformers/src/transformers/models/qwen2/modeling_qwen2.py"", line 1063, in _prepare_4d_causal_attention_mask_with_cache_position
    causal_mask *= diagonal_attend_mask
RuntimeError: The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 0
```



### Expected behavior

I can successfully run the above script using StaticCache.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",8,open
GLM/ChatGLM badly broken in HF,"### System Info

Ubuntu 24.04
Transformers 4.46.2

### Who can help?

@ArthurZucker @Cyrilvallez 

[[Cyrilvallez](https://github.com/Cyrilvallez)](https://github.com/huggingface/transformers/pull/33823)

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

The official example in `glm.md` https://github.com/huggingface/transformers/pull/33823/files#diff-ad450d58db47b10054ff156f6bbb980437fd0d2769f60be2aa417772f9e10b6d doesn't run. 

There are multiple problems with the code.

* `trust_remote_code` is force checked for this model if using official model from THUD causing the PR to never even take effect. https://github.com/huggingface/transformers/blob/768f3c016eec88a00f2a991c7017a8a5423c4b06/src/transformers/models/auto/configuration_auto.py#L1018
* `has_local_code` is false since model type in `config.json` can be both `glm` and `chatglm` but `chatglm` is never checked. Again, this is all from ofificlal THUD models repos on HF.  https://github.com/huggingface/transformers/blob/768f3c016eec88a00f2a991c7017a8a5423c4b06/src/transformers/models/auto/configuration_auto.py#L1019
* Even if we force/fix the above 2, the model crashes with following on safetensor load:

```python
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py"", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py"", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py"", line 4706, in _load_pretrained_model
    state_dict = load_state_dict(
                 ^^^^^^^^^^^^^^^^
  File ""/root/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py"", line 555, in load_state_dict
    with safe_open(checkpoint_file, framework=""pt"") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer
```

Example code, same as `glm.md`:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
device = ""cuda"" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained(""THUDM/glm-4-9b-chat"", device_map=""auto"")
tokenizer = AutoTokenizer.from_pretrained(""THUDM/glm-4-9b-chat"")

prompt = ""Give me a short introduction to large language model.""

messages = [{""role"": ""user"", ""content"": prompt}]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

model_inputs = tokenizer([text], return_tensors=""pt"").to(device)

generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)

generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
```

The fix is complex because THUD never updated their model code to be fully compatible with latest transformers and the merged GLM model loader will only work with a custom `glm` model or modified official repo, either modify config files and/or module tensors. 

If official HF repo from model makers doesn't work with this PR, then someone need to contact THUD to make sure their fix their model config files to be compatible or HF needs to be monkey patch the configs which is not very desirable. 

My suggestion is to revert the PR and rely on more reliable trust_remote_code=True for THUD glm models. 

What do you guys think?

### Expected behavior

Load an unmodified HF THUD glm/chatglm model without error. ","[{'id': 1834081910, 'node_id': 'MDU6TGFiZWwxODM0MDgxOTEw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Usage', 'name': 'Usage', 'color': 'e28436', 'default': False, 'description': 'General questions about the library'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 7438284845, 'node_id': 'LA_kwDOCUB6oc8AAAABu1s4LQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Remote%20code', 'name': 'Remote code', 'color': '344EDB', 'default': False, 'description': ''}]",5,open
add empty cache before load_best_model to prevent cuda OOM,"# What does this PR do?

Sometimes `load_best_model` causes cuda OOM at the end of training. 
This PR clears the cache before loading the model and fixes this issue.

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

",[],0,open
CUDA Out Of Memory when training a DETR Object detection model with compute_metrics,"### System Info

`transformers`  version 4.47.0.dev0
`accelerate`  version 0.34.2
`timm`  version 1.0.11
`supervision`  version 0.25.0rc2

### Who can help?

@muellerzr @Arth

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

I’m training a DETR Object Detection model using the Trainer API. I have properly created the coco dataset.

But when I run the Trainer API with `custom_metrics`, I get the error “OutOfMemoryError: CUDA out of memory.”. I have reduced batch_size from 16 until 1, but the same error of “Out of memory”. 

```
def collate_fn(batch):
    data = {}
    data[""pixel_values""] = torch.stack([x[""pixel_values""] for x in batch])
    data[""labels""] = [x[""labels""] for x in batch]
    return data
```

Here’s how I’m creating the `custom_metrics` function

```
id2label = {id: label for id, label in enumerate(train_ds.classes)}
label2id = {label: id for id, label in enumerate(train_ds.classes)}


@dataclass
class ModelOutput:
    logits: torch.Tensor
    pred_boxes: torch.Tensor

class MAPEvaluator:

    def __init__(self, image_processor, threshold=0.00, id2label=None):
        self.image_processor = image_processor
        self.threshold = threshold
        self.id2label = id2label

    def collect_image_sizes(self, targets):
        """"""Collect image sizes across the dataset as list of tensors with shape [batch_size, 2].""""""
        image_sizes = []
        for batch in targets:
            batch_image_sizes = torch.tensor(np.array([x[""size""] for x in batch]))
            image_sizes.append(batch_image_sizes)
        return image_sizes

    def collect_targets(self, targets, image_sizes):
        post_processed_targets = []
        for target_batch, image_size_batch in zip(targets, image_sizes):
            for target, (height, width) in zip(target_batch, image_size_batch):
                boxes = target[""boxes""]
                boxes = sv.xcycwh_to_xyxy(boxes)
                boxes = boxes * np.array([width, height, width, height])
                boxes = torch.tensor(boxes)
                labels = torch.tensor(target[""class_labels""])
                post_processed_targets.append({""boxes"": boxes, ""labels"": labels})
        return post_processed_targets

    def collect_predictions(self, predictions, image_sizes):
        post_processed_predictions = []
        for batch, target_sizes in zip(predictions, image_sizes):
            batch_logits, batch_boxes = batch[1], batch[2]
            output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))
            post_processed_output = self.image_processor.post_process_object_detection(
                output, threshold=self.threshold, target_sizes=target_sizes
            )
            post_processed_predictions.extend(post_processed_output)
        return post_processed_predictions

    @torch.no_grad()
    def __call__(self, evaluation_results):

        predictions, targets = evaluation_results.predictions, evaluation_results.label_ids

        image_sizes = self.collect_image_sizes(targets)
        post_processed_targets = self.collect_targets(targets, image_sizes)
        post_processed_predictions = self.collect_predictions(predictions, image_sizes)

        evaluator = MeanAveragePrecision(box_format=""xyxy"", class_metrics=True)
        evaluator.warn_on_many_detections = False
        evaluator.update(post_processed_predictions, post_processed_targets)

        metrics = evaluator.compute()

        # Replace list of per class metrics with separate metric for each class
        classes = metrics.pop(""classes"")
        map_per_class = metrics.pop(""map_per_class"")
        mar_100_per_class = metrics.pop(""mar_100_per_class"")
        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):
            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()
            metrics[f""map_{class_name}""] = class_map
            metrics[f""mar_100_{class_name}""] = class_mar

        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}

        return metrics

eval_compute_metrics_fn = MAPEvaluator(image_processor=processor, threshold=0.01, id2label=id2label)
```
This is how I'm training the model

```
training_args = TrainingArguments(
    output_dir=f""Malaria-finetune"",
    report_to=""none"",
    num_train_epochs=10,
    max_grad_norm=0.1,
    learning_rate=5e-5,
    warmup_steps=300,
    per_device_train_batch_size=1,
    dataloader_num_workers=2,
    metric_for_best_model=""eval_map"",
    greater_is_better=True,
    load_best_model_at_end=True,
    eval_strategy=""epoch"",
    save_strategy=""epoch"",
    save_total_limit=2,
    remove_unused_columns=False,
    eval_do_concat_batches=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=pytorch_dataset_train,
    eval_dataset=pytorch_dataset_valid,
    processing_class=processor,
    data_collator=collate_fn,
    compute_metrics=eval_compute_metrics_fn
)
trainer.train()

```

I would like to please get help on this. 

### Expected behavior

I expected the Trainer to train normally on this custom dataset for 10 epochs without any errors","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",10,open
`DataCollatorForMultipleChoice` exists in the docs but not in the package,"### Feature request

Move the `DataCollatorForMultipleChoice` implementation from the docs to `transformers.data.data_collator.py`.

### Motivation

The [`transformers` docs](https://huggingface.co/docs/transformers/tasks/multiple_choice) provide all collator code needed to run `ForMultipleChoice` fine-tuning for datasets like SWAG. The docs say that *""`transformers` doesn’t have a data collator for multiple choice, so you’ll need to adapt the `DataCollatorWithPadding` to create a batch of examples""*, but... why? Why not add the code given in the docs to the package? It's the same repo...

https://github.com/huggingface/transformers/blob/a06a0d12636756352494b99b5b264ac9955bc735/docs/source/en/tasks/multiple_choice.md?plain=1#L115-L160

### Your contribution

None, the code is already there.","[{'id': 1834067346, 'node_id': 'MDU6TGFiZWwxODM0MDY3MzQ2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Documentation', 'name': 'Documentation', 'color': '77cc3b', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
[i18n-KO] Translated backbones.md to Korean,"<!— PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 —>
# What does this PR do?

Translated the `backbones.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? --> 
<!-- @harheem, @jeongiin, @1kmmk1, @Seungahson, @win2dvp21 --> 
<!-- @junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang --> 
<!-- @jungnerd, @cjfghk5697, @yijun-lee, @mreraser --> 
@heuristicwave, @nuatmochoi, @chhaewxn, @jun048098, @ahnjj, @fabxoe

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
<!-- @stevhliu May you please review this PR? -->",[],0,open
FlaxWhisperForConditionalGeneration Out Of Memory Error,"### System Info

- huggingface_hub version: 0.24.7
- Platform: Linux-6.1.85+-x86_64-with-glibc2.35
- Python version: 3.10.12
- Running in iPython ?: No
- Running in notebook ?: No
- Running in Google Colab ?: Yes
- Token path ?: /root/.cache/huggingface/token
- Has saved token ?: False
- Configured git credential helpers: 
- FastAI: 2.7.18
- Tensorflow: 2.17.0
- Torch: 2.5.0+cu121
- Jinja2: 3.1.4
- Graphviz: 0.20.3
- keras: 3.4.1
- Pydot: 3.0.2
- Pillow: 10.4.0
- hf_transfer: N/A
- gradio: N/A
- tensorboard: N/A
- numpy: 1.26.4
- pydantic: 2.9.2
- aiohttp: 3.10.10
- ENDPOINT: https://huggingface.co/
- HF_HUB_CACHE: /root/.cache/huggingface/hub
- HF_ASSETS_CACHE: /root/.cache/huggingface/assets
- HF_TOKEN_PATH: /root/.cache/huggingface/token
- HF_HUB_OFFLINE: False
- HF_HUB_DISABLE_TELEMETRY: False
- HF_HUB_DISABLE_PROGRESS_BARS: None
- HF_HUB_DISABLE_SYMLINKS_WARNING: False
- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False
- HF_HUB_DISABLE_IMPLICIT_TOKEN: False
- HF_HUB_ENABLE_HF_TRANSFER: False
- HF_HUB_ETAG_TIMEOUT: 10
- HF_HUB_DOWNLOAD_TIMEOUT: 10

### Who can help?

@sanchit-gandhi 

### Reproduction

1. Open @sanchit-gandhi notebook on benchmarking Flax and Torch on Whisper on Colab T4 at [this address](https://github.com/sanchit-gandhi/notebooks/blob/main/Flax_vs_PyTorch_Whisper.ipynb)
2. Change the model_id to ""openai/whisper-large-v3""
3. run the whole cells

### Expected behavior

Even with setting the dtype of the model to jnp.float16 , model on Flax will consume about 14 GB of RAM which is much higher than Torch Version which is about 6 GB

I tried the original whisper-tiny sample and the results were reproducable about 500% increase in inference speed with flax
but by changing the model architecture to large v3 , we will face Out Of Memory Error .

One observed thing in the source code at here is that i witness every dtype of the model components are set to jnp.float32 .
two possible things i came up with are :
1. even with setting FlaxWhisperForConditionalGeneration dtype to jnp.float16 , the model components will still use jnp.float32
2. i think when connverting the model from_pt to jax dtypes , the model weights in torch architecture will still stay in the memory 
3. and i may have mistaken some scripts here , if so , mention it please

any idea on this issue would be helpful . thanks guys","[{'id': 2934977194, 'node_id': 'MDU6TGFiZWwyOTM0OTc3MTk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flax', 'name': 'Flax', 'color': '4862AD', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Clear unused allocated GPU memory when available GPU memory is low. ,"# What does this PR do?
This update checks for low memory and clears out cache is memory is getting low on the gpu at the end of generation. This should reduce overhead caused by clearing the cache.

Fixes # (issue)

Clear unused allocated cache memory at the end of generation. 

## Who can review?

@gante 
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.


",[],0,open
Fix ASR pipeline bug when using some kwargs,"# What does this PR do?
Always Thanks for your work.
In `AutomaticSpeechRecognitionPipeline`, when using `temperature`, `no_speech_threshold` and `logprob_threshold` kwargs, `token_ids` has List of float types and it occurs following TypeError:
```
  File ""C:\Whisper_Project\Whisper-WebUI\venv\Lib\site-packages\transformers\tokenization_utils_fast.py"", line 657, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: argument 'ids': 'float' object cannot be interpreted as an integer
```

### Reproduction 
```
from transformers import pipeline

input_path = ""test_audio.mp3""

pipe = pipeline(
    ""automatic-speech-recognition"",
    model=""openai/whisper-tiny"",
    torch_dtype=""float16"",
    device=""cuda"",
)

kwargs = {
    ""language"": ""en"",
    ""task"": ""transcribe"",
    ""temperature"": 0.7,
    ""no_speech_threshold"": 0.6,
    ""logprob_threshold"": -1.0,
}
segments = pipe(
    inputs=[input_path],
    return_timestamps=True,
    chunk_length_s=30,
    batch_size=24,
    generate_kwargs=kwargs
)
```

This PR fixes the error by casting it to list of int if its type is float.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
Thanks!!

tokenizers: @ArthurZucker
speech models: @ylacombe, @eustlb

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
Fix for TypeError in train_new_from_iterator() in tokenization_utils_fast.py,"# What does this PR do?

Fixes # (issue)

When calling train_new_from_iterator() on a tokenizer initialized using AutoTokenizer.from_pretrained(), the following error is raised:
```
tokenizer = AutoTokenizer.from_pretrained(""TinyLlama/TinyLlama-1.1B-Chat-v1.0"")
new_tokenizer = tokenizer.train_new_from_iterator(
    training_corpus, vocab_size=vocab_keep_items
)
```

Error traceback:
```
    new_tokenizer = tokenizer.train_new_from_iterator(
  File ""/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py"", line 818, in train_new_from_iterator
    or tokenizer_json[""pre_tokenizer""][""type""] == ""Sequence""
    TypeError: 'NoneType' object is not subscriptable
```

Root Cause

The issue is due to improper handling of the conditional check for pre_tokenizer in tokenization_utils_fast.py. The current implementation does not correctly account for the case where tokenizer_json does not contain the ""pre_tokenizer"" key. This results in attempting to access ""type"" from a NoneType object, causing a TypeError. This bug seems to be existing from transformers version 4.45.2 onwards.


Manually tested with the above code. 

@ArthurZucker
",[],0,open
 Training wont resuming from checkpoint ( model = Idefics3ForConditionalGeneration.from_pretrained() ),"### System Info

- `transformers` version: 4.47.0.dev0
- Platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35
- Python version: 3.11.4
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.1.0+cu118 (True)
- Tensorflow version (GPU?): 2.12.1 (True)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: Yes
- GPU type: Tesla V100S-PCIE-32GB

### Who can help?

@muellerzr @SunMarc 

I tried to fine tune a model, since I use a preemptive VM I decided to use the `resume_from_checkpoint = True` and the `push_to_hub = True` TrainingArguments.
As a predictable event the VM stopped after ≈2350 steps / 12k steps…
After restarting it I'd like to continue my trainer, I rerun my notebook cells except the `trainer.train()` replaced by `trainer.train(resume_from_checkpoint = True)`  
Unfortunately the train process does not want to restart with the error:  
```log
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[18], line 1
----> 1 trainer.train(resume_from_checkpoint = True)

File ~/.miniconda3/lib/python3.11/site-packages/transformers/trainer.py:2113, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   2111 if resume_from_checkpoint is not None:
   2112     if not is_sagemaker_mp_enabled() and not self.is_deepspeed_enabled and not self.is_fsdp_enabled:
-> 2113         self._load_from_checkpoint(resume_from_checkpoint)
   2114     # In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly
   2115     state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))

File ~/.miniconda3/lib/python3.11/site-packages/transformers/trainer.py:2836, in Trainer._load_from_checkpoint(self, resume_from_checkpoint, model)
   2833         logger.warning(""Could not load adapter model, make sure to have `peft>=0.3.0` installed"")
   2834 else:
   2835     # We load the sharded checkpoint
-> 2836     load_result = load_sharded_checkpoint(
   2837         model, resume_from_checkpoint, strict=is_sagemaker_mp_enabled(), prefer_safe=self.args.save_safetensors
   2838     )
   2839     if not is_sagemaker_mp_enabled():
   2840         self._issue_warnings_after_load(load_result)

File ~/.miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:504, in load_sharded_checkpoint(model, folder, strict, prefer_safe)
    500 if not index_present and not (safe_index_present and is_safetensors_available()):
    501     filenames = (
    502         (WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_INDEX_NAME) if is_safetensors_available() else (WEIGHTS_INDEX_NAME,)
    503     )
--> 504     raise ValueError(f""Can't find a checkpoint index ({' or '.join(filenames)}) in {folder}."")
    506 load_safe = False
    507 if safe_index_present:

ValueError: Can't find a checkpoint index (pytorch_model.bin.index.json or model.safetensors.index.json) in /workspace/IDEFICS3_ROCO/checkpoint-2350.
```
I also tried to specify the exact path of the working directory ( /workspace/IDEFICS3_ROCO ) or the latest checkpoint path ( /IDEFICS3_ROCO/checkpoint-2350)

on my [Hugging Face repo](https://huggingface.co/eltorio/IDEFICS3_ROCO) the trainer commited 235 commits with name ""Training in progress, step xxx0""  

When I look to the content of the VM directory I have:
```bash
(base) ovh@job-5876edc4-c078-4de9-b7e3-4ebb320c0908:~/IDEFICS3_ROCO$ ls -la
total 82444
drwxr-xr-x  7 ovh ovh       11 Nov  8 17:21 .
drwxr-x--- 17 ovh ovh       21 Nov  8 17:19 ..
drwxr-xr-x  9 ovh ovh       12 Nov  8 17:19 .git
-rw-r--r--  1 ovh ovh     1519 Nov  8 17:19 .gitattributes
drwxr-xr-x  2 ovh ovh        0 Nov  8 17:21 .ipynb_checkpoints
-rw-r--r--  1 ovh ovh     3973 Nov  8 17:19 README.md
-rw-r--r--  1 ovh ovh   459271 Nov  8 17:19 ROCO-idefics3.ipynb
-rw-r--r--  1 ovh ovh      741 Nov  8 17:19 adapter_config.json
-rw-r--r--  1 ovh ovh 83950224 Nov  8 17:19 adapter_model.safetensors
drwxr-xr-x  2 ovh ovh        8 Nov  8 15:56 checkpoint-2330
drwxr-xr-x  2 ovh ovh        8 Nov  8 15:57 checkpoint-2340
drwxr-xr-x  2 ovh ovh        8 Nov  8 15:58 checkpoint-2350
-rw-r--r--  1 ovh ovh     5368 Nov  8 17:19 training_args.bin

(base) ovh@job-5876edc4-c078-4de9-b7e3-4ebb320c0908:~/IDEFICS3_ROCO$ ls -la checkpoint-2350/
total 124204
drwxr-xr-x 2 ovh ovh        8 Nov  8 15:58 .
drwxr-xr-x 7 ovh ovh       11 Nov  8 17:21 ..
-rw-r--r-- 1 ovh ovh      741 Nov  8 15:58 adapter_config.json
-rw-r--r-- 1 ovh ovh 83950224 Nov  8 15:58 adapter_model.safetensors
-rw-r--r-- 1 ovh ovh      198 Nov  8 15:58 generation_config.json
-rw-r--r-- 1 ovh ovh 43128276 Nov  8 15:58 optimizer.pt
-rw-r--r-- 1 ovh ovh    14244 Nov  8 15:58 rng_state.pth
-rw-r--r-- 1 ovh ovh     1064 Nov  8 15:58 scheduler.pt
-rw-r--r-- 1 ovh ovh    82570 Nov  8 15:58 trainer_state.json
-rw-r--r-- 1 ovh ovh     5368 Nov  8 15:58 training_args.bin
# other checkoint folders are similar
```

my full notebook is:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/#fileId=https://huggingface.co/eltorio/IDEFICS3_ROCO/blob/main/ROCO-idefics3.ipynb)

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

### Full Colab notebook 
https://colab.research.google.com/#fileId=https://huggingface.co/eltorio/IDEFICS3_ROCO/blob/main/ROCO-idefics3.ipynb

```python
from transformers import TrainingArguments, Trainer
import torch
from peft import LoraConfig
from transformers import AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration

DEVICE = ""cuda:0""
USE_LORA = False
USE_QLORA = True

processor = AutoProcessor.from_pretrained(
    source_model_id,
    do_image_splitting=False
)

if USE_QLORA or USE_LORA:
    lora_config = LoraConfig(
        r=8,
        lora_alpha=8,
        lora_dropout=0.1,
        target_modules='.*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$',
        use_dora=False if USE_QLORA else True,
        init_lora_weights=""gaussian""
    )
    if USE_QLORA:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=""nf4"",
            bnb_4bit_compute_dtype=torch.float16
        )
    model = Idefics3ForConditionalGeneration.from_pretrained(
        source_model_id,
        torch_dtype=torch.float16,
        quantization_config=bnb_config if USE_QLORA else None,
    )
    model.add_adapter(lora_config)
    model.enable_adapters()
else:
    model = Idefics3ForConditionalGeneration.from_pretrained(
        source_model_id,
        torch_dtype=torch.float16,
        _attn_implementation=""flash_attention_2"", # This works for A100 or H100
    ).to(DEVICE)

training_args = TrainingArguments(
    output_dir = output_dir,
    overwrite_output_dir = False,
    auto_find_batch_size = True,
    learning_rate = 2e-4,
    fp16 = True,
    per_device_train_batch_size = 2,
    per_device_eval_batch_size = 2,
    gradient_accumulation_steps = 8,
    dataloader_pin_memory = False,
    save_total_limit = 3,
    evaluation_strategy = None,
    save_strategy = ""steps"",
    eval_steps = 100,
    save_steps = 10, # checkpoint each 10 steps
    resume_from_checkpoint = True,
    logging_steps = 5,
    remove_unused_columns = False,
    push_to_hub = True,
    label_names = [""labels""],
    load_best_model_at_end = False,
    report_to = ""none"",
    optim = ""paged_adamw_8bit"",
)
trainer = Trainer(
    model = model,
    args = training_args,
    data_collator = data_collator,
    train_dataset = train_dataset,
)
trainer.train(resume_from_checkpoint = True)
```


### Expected behavior

Trainer should create restartable checkpoints","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",10,open
Neftune computation is probably wrong with packed training,"### System Info

v4.46.2

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Neftune computation is probably wrong with packed training because the scaling factor is `alpha/sqrt(lenght*d)`. The length is the packed length there:

https://github.com/huggingface/transformers/blob/a06a0d12636756352494b99b5b264ac9955bc735/src/transformers/trainer_utils.py#L126-L149

### Expected behavior

Should take into account the size of each sentence during computation.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Confusing error message,"https://github.com/huggingface/transformers/blob/a06a0d12636756352494b99b5b264ac9955bc735/src/transformers/generation/utils.py#L2022

The error message says:
> A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.""

But the code does not check if the input `tokenizer.padding_side == ""left""`. Instead the code checks if the last token id is a padding token which is often the case when people set `tokenizer.pad_token_id = tokenizer.eos_token_id`.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",7,open
Different LlamaRotaryEmbedding in old and new versions of transformers,"### System Info

Two versions of transformers:
========= NEW VERSION ==============
- `transformers` version: 4.46.1
- Platform: Linux-5.15.0-1044-nvidia-x86_64-with-glibc2.35
- Python version: 3.11.10
- Huggingface_hub version: 0.23.3
- Safetensors version: 0.4.3
- Accelerate version: 0.32.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA H100 80GB HBM3
=========== OLD VERSION =====================
- `transformers` version: 4.34.1
- Platform: Linux-5.15.0-1044-nvidia-x86_64-with-glibc2.35
- Python version: 3.11.10
- Huggingface_hub version: 0.17.3
- Safetensors version: 0.4.3
- Accelerate version: 0.20.3
- Accelerate config:    not found
- PyTorch version (GPU?): 2.1.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>


### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

The pull request https://github.com/huggingface/transformers/pull/29285 was aimed to make calculations of sin and cos of RoPE to be in float 32.


But it seems that changing device from cpu to cuda also produces different results. Though the difference is not so big.

To check this you may run the following code.

```import torch
vals = torch.linspace(0, 1, 30000, dtype=torch.float32)

computes = { 
    ""cpu_32"" : vals.float().cpu().cos(),
    ""cuda_32"" : vals.float().cuda().cos(),
    ""cpu_16"" : vals.half().cpu().cos(),
    ""cuda_16"": vals.half().cuda().cos()
}

def compare(x, y):
    return max(torch.max(torch.abs(x.to(y.device) - y)), torch.max(torch.abs(x - y.to(x.device)))).item()

keys = computes.keys()
print(end='\t')
for k in keys:
    print(k, end='\t\t')
print()
for k1 in keys:    
    print(k1, end='\t')
    for k2 in keys:
        print(f""{compare(computes[k1], computes[k2]):1.3e}"", end='\t')
    print()
```
The output:

```
    	cpu_32		cuda_32		cpu_16		cuda_16		
cpu_32	0.000e+00	5.960e-08	4.389e-04	4.389e-04	
cuda_32	5.960e-08	0.000e+00	4.389e-04	4.389e-04	
cpu_16	4.389e-04	4.389e-04	0.000e+00	0.000e+00	
cuda_16	4.389e-04	4.389e-04	0.000e+00	0.000e+00
```
This table shows the maximum difference between calculations on different devices and using different data types.

You may see that all float16 computations are identical. But float32 are different for cuda and cpu.

Previously all sin and cos computations were performed on cpu. To maintain backward compatibility, I propose to run float32 computations on cpu.

Here
https://github.com/unslothai/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L142

```
142            emb = torch.cat((freqs, freqs), dim=-1)
143            cos = emb.cos()
144            sin = emb.sin()
```
change to
```
142            emb = torch.cat((freqs, freqs), dim=-1).cpu()
143            cos = emb.cos().to(device_type)
144            sin = emb.sin().to(device_type)
```

### Impact
According to my study, this difference in calculation of sin & cos embeddings impacts output logits and generated tokens.
The difference between values of output logits may exceed 10. More than 0.1% of output tokens may be changed in comparison to the original calculations.

### Expected behavior

RoPE sin and cos values are expected to be the same as in previous versions of transformers.","[{'id': 1834056761, 'node_id': 'MDU6TGFiZWwxODM0MDU2NzYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Modeling', 'name': 'Core: Modeling', 'color': 'FF8446', 'default': False, 'description': 'Internals of the library; Models.'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",6,open
Added explicit error handling to a few modules,"# What does this PR do?

Added explicit error catching and more detailed error logging in the following modules:
src/transformers/feature_extraction_utils.py
src/transformers/models/bert/tokenization_bert_tf.py
src/transformers/models/deprecated/jukebox/tokenization_jukebox.py
src/transformers/models/fuyu/image_processing_fuyu.py
src/transformers/utils/import_utils.py

## Before submitting

- [X ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? [ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length](https://github.com/huggingface/transformers/issues/33999)
- [X ] Did you write any new necessary tests? No new tests (as its just improved error handling, no new functionality), ran existing tests to ensure no breaking changes introduced


## Who can review?

@qubvel ",[],0,open
The support of `Mllama` in AutoModel,"### Feature request

The `AutoModel.from_config` does not work with Mllama (MllamaConfig, MllamaVisionConfig). I would like to request the ability to use Mllama through `AutoModel`.

### Motivation

There are many codes written to dynamically load models using `AutoModel`. It would be great if `AutoModel` could support Mllama to accommodate these codes.

### Your contribution

not yet.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Add EXAONE,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Add [EXAONE](https://github.com/LG-AI-EXAONE/EXAONE-3.0) model released by LG AI Research.

Test code and documentation are currently in progress.

Please refer to the corresponding issue: https://github.com/huggingface/transformers/issues/34651


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",9,open
Add EXAONE,"### Model description

EXAONE is a large language model developed by LG AI Research. We released [EXAONE 3.0](https://github.com/LG-AI-EXAONE/EXAONE-3.0) in August, but we've since updated the model code and are working on integrating our implementation with the Huggingface transformers library.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

- Model URL : https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct
- GitHub repo for EXAONE : https://github.com/LG-AI-EXAONE/EXAONE-3.0","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
added logic for deleting adapters once loaded,"What does this PR do?
=====================

This PR introduces new functionality to support deletion of adapter layers in PEFT (Parameter-Efficient Fine-Tuning) integrations within the Hugging Face Transformers library. Specifically, it adds methods to remove individual or multiple adapters, allowing for more flexible and modular management of adapters.

### Description

The `delete_adapter_layers` and `delete_adapters` functions remove specified LoRA (Low-Rank Adaptation) adapter layers from a model. This feature addresses cases where users need to manage memory and computational efficiency by unloading adapters that are no longer in use. Additionally, the PR includes logic to update the `peft_config` attribute, ensuring that the configuration reflects the current state of loaded adapters and that any unnecessary configurations are cleared when all adapters are deleted.

### Motivation

This functionality supports scenarios where users load multiple adapters during fine-tuning or inference but want to unload specific adapters without reloading the model entirely. This enhancement is particularly beneficial for memory-constrained environments, such as inference pipelines on limited hardware.

### Related Issues

#34649

### Dependencies

This feature requires a PEFT version greater than 0.6.1 for compatibility with the `BaseTunerLayer` interface.

### Example Usage

```python
peft_base_model.load_adapter(""adapters/context-classification"", adapter_name=""context_classification"")
peft_base_model.set_adapter(""context_classification"")

classification_prompt = context_classification_prompt_template.format(message)
input_ids = peft_tokenizer(classification_prompt, return_tensors=""pt"").to(""cuda"")
outputs = peft_base_model.generate(**input_ids, max_new_tokens=64, use_cache = False)
classification = peft_tokenizer.decode(outputs[0], skip_special_tokens=True)

peft_base_model.delete_adapters(""context_classification"")
```

Before submitting
-----------------

*    This PR has been discussed and reviewed to ensure its compatibility with PEFT integrations.
*    Documentation has been updated to reflect the new adapter deletion functionality.
*    Tests have been written to validate that adapters are deleted successfully, and configurations update as expected.

Who can review?
---------------

Community members and maintainers experienced with PEFT, LoRA, or model integrations are invited to review.

*   @zucchini-nlp for LoRA integration and adapter management
*   @Rocketknight1 for PEFT and integration review",[],16,open
Add functionality for deleting adapter layers in PEFT integration,"### Feature request

This request aims to introduce functionality to delete specific adapter layers integrated with PEFT (Parameter-Efficient Fine-Tuning) within the Hugging Face Transformers library. This would enable users to manage memory and computational resources more efficiently by unloading adapters that are no longer needed during model inference or fine-tuning.

### Motivation

This feature request addresses scenarios where users load multiple adapters during fine-tuning or inference but need the ability to selectively unload adapters without reloading the entire model. This enhancement is crucial for optimizing performance in memory-constrained environments and enhancing the flexibility of adapter management within Transformer models.

### Your contribution

Yes, I have submitted a PR #34650 ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Aryan_33290,"# What does this PR do?
Minimized Gradient Casting to float32:

Instead of casting gradients to float32 upfront, the optimizer now retains the original precision of grad (float16 or bfloat16), using that precision for all calculations unless a specific operation demands higher precision.
Precision Management for Moment States:

The moment estimates (exp_avg, exp_avg_sq_row, exp_avg_sq_col, exp_avg_sq) are initialized and updated in the same precision as grad, reducing the memory overhead compared to float32 calculations.
In-place Operations for Intermediate Calculations:

Operations like grad**2 + group[""eps""][0] and mean calculations are now done in-place (e.g., add_ instead of +), which avoids temporary tensor creation and reduces memory consumption.
Streamlined Factored Updates:

For factored updates, the calculations of exp_avg_sq_row and exp_avg_sq_col mean values are done in place, reducing the peak memory usage at each step.
Reusing update after calculating exp_avg_sq_row and exp_avg_sq_col saves memory by avoiding multiple copies.
Direct In-Place Updates for p_data_fp32:

Instead of copying p_data_fp32 back and forth, we update it directly in place, and only if p was initially in float16 or bfloat16, we convert p_data_fp32 back at the end.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #33290 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
"Different loss when using different ""attn_implemetation""， when using ""flash_attention_2"" is notebly larger than ""eager"" and ""spda"".","### System Info

<img width=""1033"" alt=""image"" src=""https://github.com/user-attachments/assets/3134af0c-3895-482f-9e88-6f750316fe6c"">


### Who can help?

@ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

The loss of `spda`

```
{'loss': 0.5651, 'grad_norm': 0.4917651414871216, 'learning_rate': 9.989949748743719e-05, 'epoch': 0.01}
{'loss': 0.1834, 'grad_norm': 0.5474741458892822, 'learning_rate': 9.979899497487438e-05, 'epoch': 0.02}
{'loss': 0.0285, 'grad_norm': 0.09983359277248383, 'learning_rate': 9.969849246231156e-05, 'epoch': 0.03}
{'loss': 0.0142, 'grad_norm': 0.2753373086452484, 'learning_rate': 9.959798994974875e-05, 'epoch': 0.04}
{'loss': 0.0116, 'grad_norm': 0.006525201722979546, 'learning_rate': 9.949748743718594e-05, 'epoch': 0.05}
{'loss': 0.0175, 'grad_norm': 0.11793188750743866, 'learning_rate': 9.939698492462311e-05, 'epoch': 0.06}
{'loss': 0.0563, 'grad_norm': 0.13649235665798187, 'learning_rate': 9.929648241206031e-05, 'epoch': 0.07}
{'loss': 0.0094, 'grad_norm': 0.127386212348938, 'learning_rate': 9.91959798994975e-05, 'epoch': 0.08}
{'loss': 0.0322, 'grad_norm': 0.6825785040855408, 'learning_rate': 9.909547738693468e-05, 'epoch': 0.09}
{'loss': 0.0069, 'grad_norm': 0.012013601139187813, 'learning_rate': 9.899497487437186e-05, 'epoch': 0.1}
```

The loss of `eager`

```
{'loss': 13.072, 'grad_norm': 297.276123046875, 'learning_rate': 9.987951807228915e-05, 'epoch': 0.01}
{'loss': 10.0997, 'grad_norm': 8193.296875, 'learning_rate': 9.975903614457832e-05, 'epoch': 0.02}
{'loss': 8.2837, 'grad_norm': 3707.72900390625, 'learning_rate': 9.963855421686748e-05, 'epoch': 0.04}
{'loss': 9.1116, 'grad_norm': 686.0602416992188, 'learning_rate': 9.951807228915663e-05, 'epoch': 0.05}
{'loss': 8.439, 'grad_norm': 44.402984619140625, 'learning_rate': 9.939759036144579e-05, 'epoch': 0.06}
{'loss': 7.4174, 'grad_norm': 1022.7124633789062, 'learning_rate': 9.927710843373495e-05, 'epoch': 0.07}
{'loss': 6.5367, 'grad_norm': 30.798368453979492, 'learning_rate': 9.91566265060241e-05, 'epoch': 0.08}
{'loss': 4.2007, 'grad_norm': 704.5174560546875, 'learning_rate': 9.903614457831326e-05, 'epoch': 0.1}
{'loss': 3.5107, 'grad_norm': 4172.1484375, 'learning_rate': 9.891566265060241e-05, 'epoch': 0.11}
{'loss': 4.6052, 'grad_norm': 980.4346313476562, 'learning_rate': 9.879518072289157e-05, 'epoch': 0.12}
{'loss': 5.478, 'grad_norm': 1532.0015869140625, 'learning_rate': 9.867469879518073e-05, 'epoch': 0.13}
```


The loss of `flash_attention_2`

```
{'loss': 15.1953, 'grad_norm': 1.0505084991455078, 'learning_rate': 9.989339019189766e-05, 'epoch': 0.01}
{'loss': 14.849, 'grad_norm': 0.15318499505519867, 'learning_rate': 9.978678038379531e-05, 'epoch': 0.02}
{'loss': 14.5066, 'grad_norm': 0.7192463278770447, 'learning_rate': 9.968017057569297e-05, 'epoch': 0.03}
{'loss': 15.1727, 'grad_norm': 0.13244017958641052, 'learning_rate': 9.957356076759062e-05, 'epoch': 0.04}
{'loss': 14.855, 'grad_norm': 0.08410739153623581, 'learning_rate': 9.946695095948828e-05, 'epoch': 0.05}
{'loss': 15.0897, 'grad_norm': 0.25853678584098816, 'learning_rate': 9.936034115138593e-05, 'epoch': 0.06}
{'loss': 14.8282, 'grad_norm': 0.1422465592622757, 'learning_rate': 9.925373134328359e-05, 'epoch': 0.07}
{'loss': 14.9486, 'grad_norm': 0.00950670801103115, 'learning_rate': 9.914712153518124e-05, 'epoch': 0.09}
{'loss': 14.6899, 'grad_norm': 0.4061712324619293, 'learning_rate': 9.90405117270789e-05, 'epoch': 0.1}
```

### Expected behavior

The loss of three attn_implemetation is close.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}]",4,open
Padding error when using Universal Assisted Generation with ASR pipeline,"### System Info

Can not solve by supplying padding arg to pipeline (not accepted).

@gante 

transformers version: https://github.com/huggingface/transformers.git@refs/pull/34504/merge
Ubuntu
Python 3.10.15

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

![image](https://github.com/user-attachments/assets/5818757b-25a6-471c-81a9-20679b43f73d)


### Expected behavior

Should complete pipeline execution as normal","[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNzcxMTg3OTI0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Pipeline', 'name': 'Core: Pipeline', 'color': 'FF7066', 'default': False, 'description': 'Internals of the library; Pipeline.'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",3,open
Add dithering to the `Speech2TextFeatureExtractor` API.,"Enable dithering in speech-to-text feature extraction. Dithering exists in the original kaldi features.
- in kaldi : https://github.com/kaldi-asr/kaldi/blob/4a8b7f673275597fef8a15b160124bd0985b59bd/src/feat/feature-window.cc#L145

The dithering is adding small Gaussian noise to the waveform on input of feature extraction.
This is helpful for audio signals with hard-zero sections due to HW VAD, these hard-zeros
may break the ASR training or inference if they appear in the data.

With dithering without a seed, the features become non-deterministic due to small Gaussian noise added to the audio (i.e. 2 runs lead to little different outputs). When debugging feature extraction code, it is good to set dithering to 0.0 (i.e. default value).
",[],13,open
[DO NOT MERGE] Testing dev release of safetensors,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
BarkProcessor voice_preset doesn't work,"### System Info

- `transformers` version: 4.47.0.dev0
- Platform: Windows-11-10.0.22631-SP0
- Python version: 3.12.7
- Huggingface_hub version: 0.26.2
- Safetensors version: 0.4.5
- Accelerate version: 1.1.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 4080 SUPER

### Who can help?

@ylacombe

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

**Code:**
from bark import SAMPLE_RATE, generate_audio, preload_models
import sounddevice
from transformers import BarkModel, BarkProcessor
import torch
import numpy as np
from optimum.bettertransformer import BetterTransformer
from scipy.io.wavfile import write as write_wav
import re

def barkspeed(text_prompt):
    processor = BarkProcessor.from_pretrained(""suno/bark-small"")
    model = BarkModel.from_pretrained(""suno/bark-small"", torch_dtype=torch.float16).to(device)
    model = BetterTransformer.transform(model, keep_original_model=False)
    model.enable_cpu_offload()
    sentences = re.split(r'[.?!]', text_prompt)
    pieces = []
    for sentence in sentences:
        inp = processor(sentence.strip(), voice_preset=SPEAKER).to(device)
        audio = model.generate(**inp, do_sample=True, fine_temperature=0.4, coarse_temperature=0.5)
        audio = ((audio/torch.max(torch.abs(audio))).numpy(force=True).squeeze()*pow(2, 15)).astype(np.int16)
        pieces.append(audio)
    write_wav(""bark_generation.wav"", SAMPLE_RATE, np.concatenate(pieces))
    sounddevice.play(np.concatenate(pieces), samplerate=24000)
    sounddevice.wait()


**Error Message:**
****The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Traceback (most recent call last):
  File ""F:\OllamaRAG\BarkUsage\BarkUsage.py"", line 56, in <module>
    barkspeed(""""""Hey, have you heard about this new text-to-audio model called ""Bark""? 
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\OllamaRAG\BarkUsage\BarkUsage.py"", line 47, in barkspeed
    audio = model.generate(**inp, do_sample=True, fine_temperature=0.4, coarse_temperature=0.5)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\Program Files\anaconda3\envs\ollamaRAG\Lib\site-packages\torch\utils\_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""F:\Program Files\anaconda3\envs\ollamaRAG\Lib\site-packages\transformers\models\bark\modeling_bark.py"", line 1737, in generate
    coarse_output = self.coarse_acoustics.generate(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\Program Files\anaconda3\envs\ollamaRAG\Lib\site-packages\transformers\models\bark\modeling_bark.py"", line 1078, in generate
    semantic_output = torch.hstack([x_semantic_history, semantic_output])
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)

### Expected behavior

I used the code to generate some audio. Before I upgraded transformers and bark, the voice preset didn't work, bark kept changing preset.  In the first half part of call function in Barkprocessor, it seemed fine, tensors were loaded properly. But in the generate function history_prompt was empty at first, then it was loaded as all 10000, After I upgraded transformers and bark, the error message shows. And after I delete the voice_preset=SPEAKER part, the code works, but with changing preset as well. Please could anyone tell me how I can get the preset to work.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",7,open
IdeficsProcessor has wrong __call__ args and the docstring is not updated,"### System Info

- `transformers` version: 4.46.0
- Platform: Linux-4.15.0-76-generic-x86_64-with-glibc2.27
- Python version: 3.12.5
- Huggingface_hub version: 0.24.7
- Safetensors version: 0.4.5
- Accelerate version: 0.34.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 3090

### Who can help?

@zucchini-nlp 


### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

As shown below, the `images` can only be `List[Image]` instead of `List[List[Image]]`, which will result in unexpected exception when passing batch texts.

https://github.com/huggingface/transformers/blob/7bbc62474391aff64f63fcc064c975752d1fa4de/src/transformers/models/idefics/processing_idefics.py#L260-L262

### Expected behavior

just fix it, zucchini","[{'id': 1834067346, 'node_id': 'MDU6TGFiZWwxODM0MDY3MzQ2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Documentation', 'name': 'Documentation', 'color': '77cc3b', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",8,open
fix bug in modeling_flax_dinov2 when batch size is more than 1,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
T5 Attention fix and tests,"# What does this PR do?

Fixes a bug which crashes the forward pass of T5Attention when not using the KV Cache or the `cache_position` parameter, and adds tests to verify the fix.

- [x] Add test
- [ ] Fix bug in T5Attention
- [ ] Linting & code quality

Fixes # (issue)
https://github.com/huggingface/transformers/issues/34448

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@zucchini-nlp 
",[],0,open
fix:#34611 by expanding class_pos_embed, Expands class_pos_embed to match batch size #34611 ,[],2,open
Setup loss_type in config at model init time,"ensures no additional graph break introduced when torch.compile'ed

fixes #34615

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 6625174977, 'node_id': 'LA_kwDOCUB6oc8AAAABiuQlwQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Compilation', 'name': 'Compilation', 'color': '58E75E', 'default': False, 'description': 'Issues related to torchdynamo and torchinductor'}]",6,open
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds while trying to finetune a MT5 for Multitasking,"Hi @patrickvonplaten @ArthurZucker @muellerz 

I am currently trying to finetune a MT5 Model for Multitasking (classification, report filling and question generation)
As Iam trying and debugging for a couple days now, maybe you or some of youre Colleagues can have a look on my Code and hopefully find the mistake.

Here is the relevant Code and some outputs for clarification:

```
`import torch
import transformers
from tqdm import tqdm  # For progress bar
from datasets import DatasetDict

# Initialize the tokenizer
tokenizer = transformers.AutoTokenizer.from_pretrained(""google/mt5-base"")

def convert_to_features(example_batch, task_name):
    features = {}

    if task_name == ""event_classification"":
        # Tokenisierung für Text
        features = tokenizer(example_batch['text'], max_length=1000, padding=""max_length"", truncation=True, return_tensors=""pt"")
        
        numeric_labels = [label_to_id[example_batch['label']]]
        features[""labels""] = torch.tensor(numeric_labels, dtype=torch.long).unsqueeze(1)  # Adjusting dimension
        features[""decoder_input_ids""] = torch.tensor(numeric_labels, dtype=torch.long).unsqueeze(1)

    elif task_name == ""report_filling"":
        features = tokenizer(example_batch['input_text'], max_length=1000, padding=""max_length"", truncation=True, return_tensors=""pt"")
        
        labels = tokenizer(example_batch['report'], max_length=1000, padding=""max_length"", truncation=True, return_tensors=""pt"")[""input_ids""]
        features[""labels""] = labels
        features[""decoder_input_ids""] = labels.clone()  # Clone for decoder_input_ids

    elif task_name == ""question_generation"":
        input_texts = []
        labels = []

        for text, missing_info, questions in zip(
            example_batch['text'], example_batch['missing_info'], example_batch['generated_questions']
        ):
            missing_info_str = "", "".join(missing_info)
            input_text = f""{text} Missing info: {missing_info_str}""
            input_texts.append(input_text)
            questions_str = "" [SEP] "".join(questions)
            labels.append(questions_str)

        # Tokenisierung für input_texts
        features = tokenizer(input_texts, max_length=1000, padding=""max_length"", truncation=True, return_tensors=""pt"")
        
        # Tokenisierung für die Labels
        labels_encodings = tokenizer(labels, max_length=1000, padding=""max_length"", truncation=True, return_tensors=""pt"")[""input_ids""]
        features[""labels""] = labels_encodings
        features[""decoder_input_ids""] = labels_encodings.clone()  # Clone for decoder_input_ids

    return features


def tokenize_dataset(dataset, task_name, max_length=1000):
    num_examples = len(dataset)

    input_ids = []
    attention_masks = []
    labels = []
    decoder_input_ids = []

    for i in tqdm(range(num_examples), total=num_examples):
        example = dataset[i]
        features = convert_to_features(example, task_name)

        # Instead of appending, convert to tensor and squeeze to remove extra dimensions
        input_ids.append(features['input_ids'].squeeze(0))  # Removes dimension of size 1
        attention_masks.append(features['attention_mask'].squeeze(0))
        labels.append(features['labels'].squeeze(0))  # Squeeze if necessary
        decoder_input_ids.append(features['decoder_input_ids'].squeeze(0))

    # Create a dictionary to hold the dataset
    if task_name == ""question_generation"":
        # Concatenate along the first dimension to flatten
        input_ids = torch.cat(input_ids, dim=0)  # shape: [total_questions, 1000]
        attention_masks = torch.cat(attention_masks, dim=0)  # shape: [total_questions, 1000]
        labels = torch.cat(labels, dim=0)  # shape: [total_questions, 1000]
        decoder_input_ids = torch.cat(decoder_input_ids, dim=0)  # shape: [total_questions, 1000]

    else:
        # For the other tasks, just stack the sequences
        input_ids = torch.stack(input_ids)  # shape: [num_examples, 1000]
        attention_masks = torch.stack(attention_masks)  # shape: [num_examples, 1000]
        labels = torch.stack(labels)  # shape: [num_examples, 1] or [num_examples, 1000]
        decoder_input_ids = torch.stack(decoder_input_ids)  # shape: [num_examples, 1] or [num_examples, 1000]

    # Create a dataset dictionary with potentially different shapes
    features = {
        'input_ids': input_ids,
        'attention_mask': attention_masks,
        'labels': labels,
        'decoder_input_ids': decoder_input_ids,
    }

    return features



# Process each task in the DatasetDict
features_dict = {}
for task_name, dataset in dataset_dict.items():
    print(f""Tokenizing dataset for task: {task_name}"")
    features_dict[task_name] = tokenize_dataset(dataset, task_name)

```

```
import transformers
import torch.nn as nn

class MultiTaskModel(transformers.PreTrainedModel):
    def __init__(self, encoder, task_models):
        super().__init__(transformers.PretrainedConfig())
        self.encoder = encoder
        self.task_models = nn.ModuleDict(task_models)

    @classmethod
    def create(cls, model_name, model_type_dict, model_config_dict):
        encoder = transformers.AutoModel.from_pretrained(model_name)
        task_models = {
            task: model_type_dict[task].from_pretrained(model_name, config=model_config_dict[task])
            for task in model_type_dict
        }
        return cls(encoder, task_models)

    def forward(self, input_ids, attention_mask, task_name, labels=None, decoder_input_ids=None):
        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = encoder_outputs.last_hidden_state

        if task_name in self.task_models:
            task_model = self.task_models[task_name]
            
            if isinstance(task_model, transformers.MT5ForSequenceClassification):
                if decoder_input_ids is None:
                    raise ValueError(""decoder_input_ids must be provided for classification tasks"")
                return task_model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask,
                    inputs_embeds=last_hidden_state, 
                    labels=labels, 
                    decoder_input_ids=decoder_input_ids  # Ensure this is passed
                )

            elif isinstance(task_model, transformers.MT5ForConditionalGeneration):
                if decoder_input_ids is None:
                    raise ValueError(""decoder_input_ids must be provided for generation tasks"")
                return task_model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask,
                    decoder_input_ids=decoder_input_ids,
                    encoder_outputs=encoder_outputs,
                    labels=labels
                )
        
        raise ValueError(f""Unknown task: {task_name}"")

# Model configuration
model_name = ""google/mt5-base""
task_model_types = {
    ""event_classification"": transformers.MT5ForSequenceClassification,
    ""report_filling"": transformers.MT5ForConditionalGeneration,
    ""question_generation"": transformers.MT5ForConditionalGeneration,
}
model_configurations = {
    ""event_classification"": transformers.MT5Config.from_pretrained(model_name, num_labels=10),  # Adjust number of labels
    ""report_filling"": transformers.MT5Config.from_pretrained(model_name),
    ""question_generation"": transformers.MT5Config.from_pretrained(model_name),
}

# Create the multi-task model
multi_task_model = MultiTaskModel.create(model_name, task_model_types, model_configurations)
```

```
from torch.utils.data import DataLoader, Dataset

# Custom Dataset für Multi-Task
class MultiTaskDataset(Dataset):
    def __init__(self, features):
        self.features = features

    def __len__(self):
        return self.features['input_ids'].shape[0]

    def __getitem__(self, idx):
        return {
            'input_ids': self.features['input_ids'][idx],
            'attention_mask': self.features['attention_mask'][idx],
            'labels': self.features['labels'][idx],
            'decoder_input_ids': self.features['input_ids'][idx]
        }

# DataLoader Creation
def create_data_loader(features, batch_size):
    dataset = MultiTaskDataset(features)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Batch-Größe
batch_size = 32
data_loaders = {task: create_data_loader(features, batch_size) for task, features in features_dict.items()}
```

```
class MultiTaskDataLoader:
    def __init__(self, data_loaders):
        self.data_loaders = data_loaders
        self.iterators = {task: iter(loader) for task, loader in data_loaders.items()}

    def __iter__(self):
        while True:
            for task, iterator in self.iterators.items():
                try:
                    batch = next(iterator)
                    print(f""\nProcessing Task: {task}"")
                    print(batch)
                    # Bereite Tensoren vor
                    input_ids = batch['input_ids'].to(torch.int64)
                    attention_mask = batch['attention_mask'].to(torch.int64)

                    # Debugging-Ausgabe
                    print(f""Type of input_ids: {type(input_ids)}, Shape: {input_ids.shape}"")
                    print(f""Type of attention_mask: {type(attention_mask)}, Shape: {attention_mask.shape}"")

                    # Überprüfen und Umformen der Labels und decoder_input_ids
                    labels = batch['labels'].clone().detach().to(torch.int64)
                    decoder_input_ids = batch['decoder_input_ids'].clone().detach().to(torch.int64)

                    # Debugging-Ausgabe für Labels und decoder_input_ids
                    print(f""Type of labels: {type(labels)}, Shape: {labels.shape}"")
                    print(f""Type of decoder_input_ids: {type(decoder_input_ids)}, Shape: {decoder_input_ids.shape}"")

                    yield {
                        'input_ids': input_ids,
                        'attention_mask': attention_mask,
                        'labels': labels,
                        'decoder_input_ids': decoder_input_ids,
                        'task_name': task
                    }

                except StopIteration:
                    # Iterator zurücksetzen, wenn einer erschöpft ist
                    self.iterators[task] = iter(self.data_loaders[task])
                    continue
                except Exception as e:
                    print(f""Fehler: {e}"")
                    print(f""Batch details: {batch}"")

    def __len__(self):
        return min(len(loader) for loader in self.data_loaders.values())


class MultiTaskTrainer(transformers.Trainer):
    def __init__(self, *args, data_loaders=None, **kwargs):
        super().__init__(*args, **kwargs)
        if data_loaders is None:
            raise ValueError(""data_loaders darf nicht None sein"")
        self.data_loaders = data_loaders  # saving custom DataLoader

    def get_train_dataloader(self):
        # Verwendung des benutzerdefinierten MultiTaskDataLoader
        return MultiTaskDataLoader(self.data_loaders)
    
    def training_step(self, model, inputs, num_items_in_batch):
        task_name = inputs['task_name']
        
        
        input_ids = inputs['input_ids']
        attention_mask = inputs['attention_mask']
        labels = inputs['labels']
        decoder_input_ids = inputs['decoder_input_ids']

        # Forward Pass with task_name
        loss = model(input_ids=input_ids, 
                        attention_mask=attention_mask, 
                        labels=labels,
                        decoder_input_ids=decoder_input_ids,
                        task_name=task_name)  
        
        return loss
```

```
Output:
Type of input_ids: <class 'torch.Tensor'>, Shape: torch.Size([10, 1000])
Type of attention_mask: <class 'torch.Tensor'>, Shape: torch.Size([10, 1000])
Type of labels: <class 'torch.Tensor'>, Shape: torch.Size([10, 1])
Type of decoder_input_ids: <class 'torch.Tensor'>, Shape: torch.Size([10, 1])
```

```
# Pass MultiTaskDataLoader to Trainer
trainer = MultiTaskTrainer(
    model=multi_task_model,
    args=transformers.TrainingArguments(
        output_dir=""./models/multitask_model"",
        overwrite_output_dir=True,
        learning_rate=5e-5,
        per_device_train_batch_size=batch_size,
        num_train_epochs=3,
        logging_dir='./logs',
    ),
    data_loaders=data_loaders  # Pass the DataLoader instead of the dataset
)

# Start training
trainer.train()
```

And here the full Error msg:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[258], [line 16](vscode-notebook-cell:?execution_count=258&line=16)
      [2](vscode-notebook-cell:?execution_count=258&line=2) trainer = MultiTaskTrainer(
      [3](vscode-notebook-cell:?execution_count=258&line=3)     model=multi_task_model,
      [4](vscode-notebook-cell:?execution_count=258&line=4)     args=transformers.TrainingArguments(
   (...)
     [15](vscode-notebook-cell:?execution_count=258&line=15) # Start training
---> [16](vscode-notebook-cell:?execution_count=258&line=16) trainer.train()

File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\transformers\trainer.py:2122, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   [2120](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/trainer.py:2120)         hf_hub_utils.enable_progress_bars()
   [2121](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/trainer.py:2121) else:
-> [2122](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/trainer.py:2122)     return inner_training_loop(


File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\transformers\trainer.py:2474, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   [2471](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/trainer.py:2471)     self.control = self.callback_handler.on_step_begin(args, self.state, self.control)
   [2473](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/trainer.py:2473) with self.accelerator.accumulate(model):
-> [2474](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/trainer.py:2474)     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
   [2476](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/trainer.py:2476) if (
   [2477](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/trainer.py:2477)     args.logging_nan_inf_filter


Cell In[257], [line 72](vscode-notebook-cell:?execution_count=257&line=72)
     [69](vscode-notebook-cell:?execution_count=257&line=69) decoder_input_ids =decoder_input_ids = inputs['decoder_input_ids'].expand(-1, 1000)
     [71](vscode-notebook-cell:?execution_count=257&line=71) # Forward Pass mit task_name
---> [72](vscode-notebook-cell:?execution_count=257&line=72) loss = model(input_ids=input_ids, 
     [73](vscode-notebook-cell:?execution_count=257&line=73)                 attention_mask=attention_mask, 
     [74](vscode-notebook-cell:?execution_count=257&line=74)                 labels=labels,
     [75](vscode-notebook-cell:?execution_count=257&line=75)                 decoder_input_ids=decoder_input_ids,
     [76](vscode-notebook-cell:?execution_count=257&line=76)                 task_name=task_name)  # Fügen Sie hier task_name hinzu
     [78](vscode-notebook-cell:?execution_count=257&line=78) return loss

File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\torch\nn\modules\module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   [1734](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1734)     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   [1735](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1735) else:
-> [1736](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1736)     return self._call_impl(*args, **kwargs)

File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\torch\nn\modules\module.py:1747, in Module._call_impl(self, *args, **kwargs)
  [1745](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1745)         or _global_backward_pre_hooks or _global_backward_hooks
   [1746](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1746)         or _global_forward_hooks or _global_forward_pre_hooks):
-> [1747](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1747)     return forward_call(*args, **kwargs)
   [1749](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1749) result = None
   [1750](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1750) called_always_called_hooks = set()

Cell In[244], [line 20](vscode-notebook-cell:?execution_count=244&line=20)
     [19](vscode-notebook-cell:?execution_count=244&line=19) def forward(self, input_ids, attention_mask, task_name, labels=None, decoder_input_ids=None):
---> [20](vscode-notebook-cell:?execution_count=244&line=20)     encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
     [21](vscode-notebook-cell:?execution_count=244&line=21)     last_hidden_state = encoder_outputs.last_hidden_state
     [23](vscode-notebook-cell:?execution_count=244&line=23)     if task_name in self.task_models:

File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\torch\nn\modules\module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   [1734](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1734)     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   [1735](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1735) else:
-> [1736](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1736)     return self._call_impl(*args, **kwargs)

File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\torch\nn\modules\module.py:1747, in Module._call_impl(self, *args, **kwargs)


File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\transformers\models\mt5\modeling_mt5.py:1669, in MT5Model.forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)
   [1666](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:1666)         decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)
   [1668](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:1668) # Decode
-> [1669](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:1669) decoder_outputs = self.decoder(
   [1670](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:1670)     input_ids=decoder_input_ids,
   [1671](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:1671)     attention_mask=decoder_attention_mask,


File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\torch\nn\modules\module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   [1734](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1734)     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   [1735](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1735) else:
-> [1736](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1736)     return self._call_impl(*args, **kwargs)

File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\torch\nn\modules\module.py:1747, in Module._call_impl(self, *args, **kwargs)
    [1745](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1745)         or _global_backward_pre_hooks or _global_backward_hooks
   [1746](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1746)         or _global_forward_hooks or _global_forward_pre_hooks):
-> [1747](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1747)     return forward_call(*args, **kwargs)
   [1749](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1749) result = None
   [1750](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/torch/nn/modules/module.py:1750) called_always_called_hooks = set()

File c:\workspace\databricks-notebooks-017\.venv\lib\site-packages\transformers\models\mt5\modeling_mt5.py:977, in MT5Stack.forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)
    [975](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:975) else:
    [976](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:976)     err_msg_prefix = ""decoder_"" if self.is_decoder else """"
--> [977](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:977)     raise ValueError(f""You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds"")
    [979](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:979) if self.gradient_checkpointing and self.training:
    [980](file:///C:/workspace/databricks-notebooks-017/.venv/lib/site-packages/transformers/models/mt5/modeling_mt5.py:980)     if use_cache:

ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds
```",[],2,open
redirect logging output to `stdout` instead of `stderr`,"### Feature request

Redirect logging output to `stdout` instead of `stderr`. Specifically, add argument `stream=sys.stdout` at: https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src/transformers/utils/logging.py#L88.

### Motivation

It is a common practice to redirect logging output to `stdout` in deep learning frameworks.
For example: Detectron2: https://github.com/facebookresearch/detectron2/blob/8d85329aed8506ea3672e3e208971345973ea761/detectron2/utils/logger.py#L84
fairseq: https://github.com/facebookresearch/fairseq/blob/ecbf110e1eb43861214b05fa001eff584954f65a/fairseq_cli/train.py#L22
Deepspeed: https://github.com/microsoft/DeepSpeed/blob/2b41d6212c160a3645691b77b210ba7dd957c23f/deepspeed/utils/logging.py#L69.

Here is my analysis. Traditionally, `stdout` is used for output of the program and `stderr` is used for warning/debugging. That's why the default stream of `logging` is `stderr`. However, the output of deep learning frameworks consists of losses, eval results and checkpoints. It's a common practice to use `logger.info()` to display this information. Therefore, it would be more appropriate to redirect these outputs to `stdout` since they are part of the program's normal output.

### Your contribution

I can submit a PR if this request is confirmed.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
save `training_args` in another file format ,"### Feature request

switch saving training arguments from `bin` format to another file extension. 

### Motivation

for security reasons `bin` format is deemed unsafe as marked by huggingface's new security system.
![image](https://github.com/user-attachments/assets/03d9c511-716e-4d15-8a2d-38d9a108e289)

### Your contribution

I'll try submitting a pr, but if anyone wishes to submit their own feel free to do so","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Saving logs to file,"### Feature request

Add a `FileHandler` to root logger.

### Motivation

Transformers is only using StreamHandler for logging, which makes debug analysis hard across different experiments. Many other frameworks support both logging to terminal and to file.
For example: fairseq: https://github.com/facebookresearch/fairseq/blob/ecbf110e1eb43861214b05fa001eff584954f65a/fairseq_cli/train.py#L63-L65
detectron2: 
https://github.com/facebookresearch/detectron2/blob/8d85329aed8506ea3672e3e208971345973ea761/detectron2/utils/logger.py#L108

### Your contribution

I can propose a PR.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
"fix spelling of ""your file""","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->




## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Fix optimum.quanto quantization call in cache_utils,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->
Fixes call to optimum.quanto.quantized_weight in QuantoQuantizedCache, which currently lacks `scale` and `shift` parameters and thus fails. This was introduced by https://github.com/huggingface/transformers/commit/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1 when migrating to optimum.quanto I think.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@SunMarc

",[],2,open
Torch.compile fail during inference with meta-llama/Meta-Llama-3.1-8B-Instruct,"### System Info

- `transformers` version: 4.43.3
- Platform: Linux-5.15.0-1074-azure-x86_64-with-glibc2.31
- Python version: 3.11.9
- Huggingface_hub version: 0.23.1
- Safetensors version: 0.4.3
- Accelerate version: 0.31.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: using device_map = ""auto"" in AutoModelForCausalLM.from_pretrained
- Using GPU in script?: Yes
- GPU type: NVIDIA A100 80GB PCIe

### Who can help?

@gante , @ArthurZucker 
While using torch.compile(), I get the following error. I have included the sample code in the ""Steps to reproduce""
```
Error:
Traceback (most recent call last):
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/gradio/queueing.py"", line 536, in process_events
    response = await route_utils.call_process_api(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/gradio/route_utils.py"", line 276, in call_process_api
    output = await app.get_blocks().process_api(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/gradio/blocks.py"", line 1923, in process_api
    result = await self.call_function(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/gradio/blocks.py"", line 1506, in call_function
    prediction = await fn(*processed_input)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/gradio/utils.py"", line 785, in async_wrapper
    response = await f(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/gradio/chat_interface.py"", line 607, in _submit_fn
    response = await anyio.to_thread.run_sync(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/anyio/to_thread.py"", line 56, in run_sync
    return await get_async_backend().run_sync_in_worker_thread(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/anyio/_backends/_asyncio.py"", line 2134, in run_sync_in_worker_thread
    return await future
           ^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/anyio/_backends/_asyncio.py"", line 851, in run
    result = context.run(func, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/vp899/projects/Agent_System/Code/Agent_Launch_UI_v2_Experiments.py"", line 253, in contract_analyst_chat
    outputs = model.generate(input_ids, max_new_tokens=500, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/transformers/generation/utils.py"", line 1989, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/transformers/generation/utils.py"", line 2932, in _sample
    outputs = self(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py"", line 451, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 921, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 400, in _convert_frame_assert
    return _compile(
           ^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 703, in _compile
    raise InternalTorchDynamoError(str(e)).with_traceback(
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 676, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/utils.py"", line 262, in time_wrapper
    r = func(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 535, in compile_inner
    out_code = transform_code_object(code, transform)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py"", line 1036, in transform_code_object
    transformations(instructions, code_options)
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 165, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py"", line 482, in transform
    tracer = InstructionTranslator(
             ^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py"", line 2085, in __init__
    self._throw_if_in_functorch()
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py"", line 2126, in _throw_if_in_functorch
    eager = torch._dynamo.lookup_backend(""eager"")
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/backends/registry.py"", line 58, in lookup_backend
    _lazy_import()
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/backends/registry.py"", line 91, in _lazy_import
    import_submodule(backends)
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/utils.py"", line 1866, in import_submodule
    importlib.import_module(f""{mod.__name__}.{filename[:-3]}"")
  File ""/anaconda/envs/pi2_py311/lib/python3.11/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<frozen importlib._bootstrap>"", line 1204, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1176, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1147, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 690, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 940, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/backends/cudagraphs.py"", line 10, in <module>
    from torch._inductor.cudagraph_trees import cudagraphify_impl
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py"", line 71, in <module>
    from torch._inductor.compile_fx import (
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_inductor/compile_fx.py"", line 57, in <module>
    from .fx_passes.joint_graph import joint_graph_passes
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_inductor/fx_passes/joint_graph.py"", line 12, in <module>
    from ..pattern_matcher import (
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_inductor/pattern_matcher.py"", line 46, in <module>
    from .lowering import fallback_node_due_to_unsupported_type
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_inductor/lowering.py"", line 6002, in <module>
    import_submodule(kernel)
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_dynamo/utils.py"", line 1866, in import_submodule
    importlib.import_module(f""{mod.__name__}.{filename[:-3]}"")
  File ""/anaconda/envs/pi2_py311/lib/python3.11/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_inductor/kernel/flex_attention.py"", line 155, in <module>
    flex_attention_template = TritonTemplate(
                              ^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_inductor/select_algorithm.py"", line 453, in __init__
    self.template = self._template_from_string(source)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/_inductor/codegen/common.py"", line 1720, in _template_from_string
    return env.from_string(source)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/jinja2/environment.py"", line 1108, in from_string
    return cls.from_code(self, self.compile(source), gs, None)
                               ^^^^^^^^^^^^^^^^^^^^
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/jinja2/environment.py"", line 768, in compile
    self.handle_exception(source=source_hint)
  File ""/anaconda/envs/pi2_py311/lib/python3.11/site-packages/jinja2/environment.py"", line 939, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File ""<unknown>"", line 104, in template
torch._dynamo.exc.InternalTorchDynamoError: No filter named 'indent_except_first'.


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
```

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction
```python
model_id = ""meta-llama/Meta-Llama-3.1-8B-Instruct""
tokenizer = AutoTokenizer.from_pretrained(model_id, token = llama31_hf_token)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=""auto"", token = llama31_hf_token, attn_implementation=""flash_attention_2"",)
model.generation_config.cache_implementation = ""static""
model.forward = torch.compile(model.forward, mode=""reduce-overhead"", fullgraph=True)

...
input_ids = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=""pt"").to(model.device)    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(""<|eot_id|>"")]
outputs = model.generate(input_ids, max_new_tokens=500, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9)

```
### Expected behavior

Model should compile and model.generate should yield the answer","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",7,open
Clean-up composite configs,"# What does this PR do?

Based on https://github.com/huggingface/transformers/pull/34410, I realized that BLIP models were the only ones failing a certain common test and the reason is that we manually set the `tie_word_embedding` from `text_config`. I think some config attributes, including `vocab_size` (successfully deprecated in VLMs) or `tie_word_embeddings` should be only attributes of text config. We should access them via `config.get_text_config()` whenever needed

Having an option to access through general composite config and through text config creates two sources of truth, and we might easily end up with situations like in the `check_config_arguments_init`. If one sets the attribute through `setattr`, it will not be different from the attribute in text config. 

So we expect users to set the attr in the respective config if they want it to be used correctly with transformers code. We could restrict setting these common attr in correct sub-config by `attribute_map` prob, but I thought it was too much 

Plus, clean up some `attn_implementation=config._attn_implementation` that were left, because it should be passed already as part of respective config",[],1,open
[WIP ]Add audio chat templates,"# What does this PR do?

Successor of https://github.com/huggingface/transformers/pull/34275. Analogously supports vectorized output from audio LLMs. Currently we have only Qwen2Audio which needs to upload it template on the hub instead of having the `default_chat_template` that is deprecated

Plus this PR will work only for Processors that are already uniform in terms of input kwargs, because we pass now `images`, `videos` and `audio` to the processor and any of them can be `None`. Luckily we have uniform processors in all VLMs and now we will have the single audio LLM we support.

TODO:
- [ ] Qwen2Audio uses a keyword `audios` instead of `audio` for the inputs, Either deprecate it or use `audios` as the main input name for audio inputs",[],1,open
🌐 [i18n-KO] Translated `bloom.md` to Korean,"
# What does this PR do?

Translated the `bloom.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@chhaewxn, @ahnjj, @Jwaminju, @yijun-lee, @jeongiin, @jun048098

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",[],1,open
🌐 [i18n-KO] Translated `altclip.md` to Korean,"# What does this PR do?

Translated the `altclip.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [x] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
@jungnerd, @cjfghk5697, @yijun-lee, @mreraser

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",[],1,open
Saving checkpoints *only* on improvement,"### Feature request

When using the Hugging Face Trainer, I would like to save a checkpoint only if my objective metric has improved.

### Motivation

Currently, I am using eval_steps=100,save_steps=100, save_limit=1 and load_best_model_at_end=True which means that every 100 steps, the latest checkpoint is getting written and then the previous checkpoint is getting deleted unless it is the best checkpoint.

This has done approximately 2TB of wear to my SSD in only a few days due to an excessive amount of checkpointing. I really don’t need to resume from the latest checkpoint, I just need the best checkpoint to be saved, and I’m not concerned about the run crashing, so in this case, there is really no need to be saving every 100 steps.

Additionally, it is not feasible to wait until the end of the run and load the best state because I am manually early stopping my runs. I do not wish to automate the early stopping either.

I’m happy to monkey patch my build of transformers if anyone is aware of the culprit lines I can comment out or modify.

### Your contribution

N/A","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Add support for Apple's Depth-Pro,"# What does this PR do?

Fixes #34020 

This PR adds Apple's Depth Pro model to Hugging Face Transformers. Depth Pro is a foundation model for zero-shot metric monocular depth estimation. It leverages a multi-scale vision transformer optimized for dense predictions. It downsamples an image at several scales. At each scale, it is split into patches, which are processed by a ViT-based (Dinov2) patch encoder, with weights shared across scales. Patches are merged into feature maps, upsampled, and fused via a DPT decoder.

Relevant Links
- Research Paper: [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/pdf/2410.02073)
- Authors: [Aleksei Bochkovskii](https://arxiv.org/search/cs?searchtype=author&query=Bochkovskii,+A), [Amaël Delaunoy](https://arxiv.org/search/cs?searchtype=author&query=Delaunoy,+A), and others
- Implementation: [apple/ml-depth-pro](https://github.com/apple/ml-depth-pro)
- Models Weights: [apple/DepthPro](https://huggingface.co/apple/DepthPro)


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts, @qubvel
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",24,open
uniformize kwargs for SAM,"Adds uniformized processors for SAM following https://github.com/huggingface/transformers/issues/31911.

@qubvel @molbap","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",5,open
Handle num_items_in_batch in Mistral's forward,"# What does this PR do?

This PR enables handling loss keyword arguments in the Mistral model's `forward()` method.
Specifically, if `num_items_in_batch` is passed, the value is used to properly normalize the loss value.
    
This relates to the Gradient Accumulation fix (#34191)
    
Fixes #34575

cc @ArthurZucker as it relates to text models.
","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",3,open
Unhandled 'num_items_in_batch' in Mistral model,"### System Info

- Transformer version: 4.46.0
- Model: nvidia/Mistral-NeMo-Minitron-8B-Base

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

When calling the forward method on the NeMo Mistral model, the following exception occurs:

```
[rank2]:   File ""/lustre/fsw/portfolios/llmservice/users/gheinrich/anaconda3/envs/vila/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]: TypeError: MistralForCausalLM.forward() got an unexpected keyword argument 'num_items_in_batch'
```

### Expected behavior

The forward() method should use `num_items_in_batch` for the loss calculation.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
"when model.generate with num_beams=2 and num_return_sequences=2,the output seqs are different from input_ids of stopping_criteria","### System Info

- `transformers` version: 4.45.2
- Platform: Linux-5.10.134-13.an8.x86_64-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.24.6
- Safetensors version: 0.4.4
- Accelerate version: 1.0.0
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.5.0a0+872d972e41.nv24.08 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA H800

### Who can help?

?

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

code:
```python
from transformers import AutoConfig, AutoModel,AutoModelForSequenceClassification,AutoModelForCausalLM,AutoTokenizer
import sys 
import torch 
import json

from transformers import StoppingCriteria, StoppingCriteriaList

token_ids = []
class StopOnToken(StoppingCriteria):
    def __init__(self, stop_token_ids):
        self.stop_token_ids = stop_token_ids

    def __call__(self, input_ids, scores, **kwargs):
        token_ids.append(input_ids[:,-1])
        return any([inp[-1].item() in self.stop_token_ids for inp in input_ids])
    
model_name_or_path = ""Qwen/Qwen2.5-3B-Instruct""
model = AutoModelForCausalLM.from_pretrained(model_name_or_path,trust_remote_code=True,device_map=""cuda"")
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,trust_remote_code=True, use_fast=False)
tokenizer.padding_side = ""left""

with torch.no_grad():
    text = ""A regular polygon has exterior angles each measuring 15 degrees. How many sides does the polygon have?Think step by step:"" 
    text = [tokenizer.apply_chat_template([{""role"":""user"",""content"":text}],tokenize=False,add_generation_prompt=True)]
    print(""========>text:"",text)
    tokenizerd = tokenizer(text,return_tensors=""pt"",padding=True,add_special_tokens=False).to(device=""cuda"")
    stopping_criteria = StoppingCriteriaList([StopOnToken([tokenizer.eos_token_id])])
    output = model.generate(**tokenizerd,num_beams=2,max_new_tokens=20,num_return_sequences=2,stopping_criteria=stopping_criteria)

    output = output[:,tokenizerd[""input_ids""].shape[1]:]
    print(output)
    ans = tokenizer.batch_decode(output, skip_special_tokens=False)
    print(""=================ans======================"")
    for i,ans_i in enumerate(ans):
        print(f""ans [{i}]:"",json.dumps(ans_i,ensure_ascii=False))

print(""output ids:"",output)
token_ids = torch.stack(token_ids,dim=1)
print(""stopping_criteria output ids:"",token_ids)
ans = tokenizer.batch_decode(token_ids, skip_special_tokens=False)
print(""=================stopping_criteria ans======================"")
for i,ans_i in enumerate(ans):
    print(f""ans [{i}]:"",json.dumps(ans_i,ensure_ascii=False))
```

### Expected behavior

logs
```
========>text: ['<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nA regular polygon has exterior angles each measuring 15 degrees. How many sides does the polygon have?Think step by step:<|im_end|>\n<|im_start|>assistant\n']
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
tensor([[ 1249,  8253,   279,  1372,   315, 11067,   315,   264,  5792, 29372,
          2661,   429,  1817, 27263,  9210, 10953,   220,    16,    20, 12348],
        [ 1249,  8253,   279,  1372,   315, 11067,   315,   264,  5792, 29372,
          1380,  1817, 27263,  9210, 10953,   220,    16,    20, 12348,    11]],
       device='cuda:0')
=================ans======================
ans [0]: ""To determine the number of sides of a regular polygon given that each exterior angle measures 15 degrees""
ans [1]: ""To determine the number of sides of a regular polygon where each exterior angle measures 15 degrees,""
output ids: tensor([[ 1249,  8253,   279,  1372,   315, 11067,   315,   264,  5792, 29372,
          2661,   429,  1817, 27263,  9210, 10953,   220,    16,    20, 12348],
        [ 1249,  8253,   279,  1372,   315, 11067,   315,   264,  5792, 29372,
          1380,  1817, 27263,  9210, 10953,   220,    16,    20, 12348,    11]],
       device='cuda:0')
stopping_criteria output ids: tensor([[ 1249,  8253,   279,  1372,   315, 11067,   315,   264,  5792, 29372,
          2661,   429,  1817, 27263,  9210, 10953,   220,    16,    20, 12348],
        [39814,  1477,   279,  1372,   315, 11067,   315,   264,  5792, 29372,
          1380,  1817, 27263,  9210, 10953,   220,    16,    20, 12348,    11]],
       device='cuda:0')
=================stopping_criteria ans======================
ans [0]: ""To determine the number of sides of a regular polygon given that each exterior angle measures 15 degrees""
ans [1]: ""Sure find the number of sides of a regular polygon where each exterior angle measures 15 degrees,""
```
we can find the output tokens of generate return 2nd seq  are different from tokens of StopOnToken get from input_ids 2nd seq .
why? or bugs? ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",6,open
Feature to configure `stop_strings` in `generation_config.json` or other config files,"### Feature request

The transformer library should offer a way to configure `stop_strings` and the tokenizer for it.

`model.generate()` can take a `stop_strings` argument to use custom stop tokens for generation, but a tokenizer object needs to be passed as well.

```
model.generate(...,
               stop_strings=[""<stop token>""],
               tokenizer=tokenizer)
```

If we add `stop_strings` to `generation_config.json`, which can be loaded correctly [code](https://github.com/huggingface/transformers/blob/33868a057c02f0368ba63bd1edb746be38fe3d90/src/transformers/generation/configuration_utils.py#L144-L145), it will return the following error, as it requires a tokenizer object, which cannot be defined in the config file. 

```
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> tokenizer = AutoTokenizer.from_pretrained(model_path)
>>> model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"")
>>> model.generate(**tokenizer(""Hi how are you?"", return_tensors=""pt"", return_token_type_ids=False))

...

ValueError: There are one or more stop strings, either in the arguments to `generate` or in the model's generation config, but we could not locate a tokenizer. When generating with stop strings, you must pass the model's tokenizer to the `tokenizer` argument of `generate`.
```


### Motivation

The user shouldn't be bothered by adding extra arguments to `generate()` or `pipeline`.

For example, [nvidia/Mistral-NeMo-Minitron-8B-Instruct](https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Instruct) needs to use `stop_strings` but so many people simply calls `generate()` without `stop_strings` and share complaints.


### Your contribution

I'd be happy to create a PR but need guidance for the design choice. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",2,open
should use eigvalsh instead of eigvals for fast&stable covariance matrix diagonalization,"https://github.com/huggingface/transformers/blob/c2820c94916e34baf4486accae74760972183a2f/src/transformers/modeling_utils.py#L2473

covariance matrices are always symmetric.  so use `torch.linalg.eigvalsh`.  This caused major speedup  (>100x in preprocessing and >2x in overall finetuning) in project I'm working on today.","[{'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}]",3,open
Refactor roberta with modular,"# What does this PR do?

As per the title. Let's use modular more and more and get rid of the copied from.
",[],1,open
uniformize kwargs for VisionTextDualEncoder,"Adds uniformized processors for VisionTextDualEncoder following https://github.com/huggingface/transformers/issues/31911.

@qubvel @molbap 

","[{'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",4,open
fix: Propagate `lr_scheduler_kwargs` options to create LR Scheduler when LayerWiseDummyOptimizer is used,"# What does this PR do?

Currently, when using `LayerWiseDummyOptimizer` (e.g., GaLore Optimizers), LR schedulers are not properly initialized. For example, attempting to use `cosine_with_min_lr` with `min_lr_rate = 0.1` results in an error. To fix this, the `scheduler_specific_kwargs` argument should also be passed during the recursive call of `get_scheduler`.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
Compile Grounding DINO,"### Feature request

I found that the Gounding DINO model `IDEA-Research/grounding-dino-base` cann't be compiled.
When I use `torch.comple(<The model>)`, it raises many error, such as `TypeError: unhashable type: 'dict'`.
can this be implemented?

### Motivation

It's very slow for Grounding DINO to perform batch inference, so I want some way to speed it up.

### Your contribution

Not sure.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6625174977, 'node_id': 'LA_kwDOCUB6oc8AAAABiuQlwQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Compilation', 'name': 'Compilation', 'color': '58E75E', 'default': False, 'description': 'Issues related to torchdynamo and torchinductor'}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",2,open
Add more configurations to the class LlavaOnevisionConfig.,"### Feature request

I would like to add more flags to configuration class `LlavaOnevisionConfig`, including `spatial_pool_mode`, `newline_position `

### Motivation

 [LlavaOnevisionModel](https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-ov-hf) uses the fixed value of `spatial_pool_mode=""bilinear""` and `newline_position=""one_token""`,  so the defined class [LlavaOnevisionConfig](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llava_onevision/configuration_llava_onevision.py#L27C7-L27C27)  has omitted the corresponding configurations.

Many of models are trained with the LLaVA-Onevision model using the different flags mentioned above. Incorporating these flags into the configuration file would be highly helpful for developers.

### Your contribution

I can submit a PR with the modifications if you agree with the incorporation of this change.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
uniformize kwargs for OneFormer,"Adds uniformized processors for OneFormer following #31911.

Small changes to simplify code.

Additional check via `test_processor_oneformer.py` that the `inputs` are the same before and after the changes (with fixed random seeds).

@qubvel @molbap
","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",3,open
this works right now for rdu,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
VLMs Processors are not fully consistent in the inputs formats they accept,"### Summary of accepted inputs for image-text-to-text-models
| Model | Uses Image Tokens | Accepts URL | Accepts PIL | Accepts Path | Accepts Non-Nested Batched Images (one per text) | Accepts Nested Images (one per text) | Accepts Nested Images (Multiple images per text, same number for each text) | Accepts Nested Images (Multiple images per text, different number for each text) |
|---|---|---|---|---|---|---|---|---|
| Pix2Struct | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| BLIP | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| Donut | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| UDOP | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| GIT | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| KOSMOS-2 | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| Idefics | ❌ | ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| FUYU | ❌ | ❌ | ✅ | ❌ | ✅ | ✅ | ❌ | ❌ |
| Llava | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| VipLlava | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| BLIP2 | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| Qwen2-VL | ✅ | ❌ | ✅ | ❌ | ✅* crash when padding set to True | ❌ | ✅ | ✅ |
| Chameleon | ✅ | ❌ | ✅ | ❌ | ❌* works when padding manually set to True | ✅ | ✅ | ✅ |
| Pixtral | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ |
| Paligemma | ✅ | ❌ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |
| Llava-Next | ✅ | ❌ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |
| LLaVA-OneVision | ✅ | ❌ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |
| Mllama | ✅ | ❌ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |
| InstructBLIP | ✅ | ❌ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |
| Idefics2 | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |
| Idefics3 | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |

### Reproduction

For instance, with these inputs:
```python
import requests
from PIL import Image

image_ny = ""https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg""
image_chicago = ""https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg""
url_pokemon = ""https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png""

image_pokemon = Image.open(requests.get(url_pokemon, stream=True).raw)
text_batched_image_tokens = [
    ""<image> <image> Are these the same cities? If not what cities are these?"",
    ""<image> Describe what you see in this image. I see"",
]

```

This will crash at the processor level:

```python
from transformers import pipeline

pipe = pipeline(task=""image-text-to-text"", model=""llava-hf/llava-interleave-qwen-0.5b-hf"")

pipe(
    images=[[image_ny, image_chicago], [image_pokemon]],
    text=text_batched_image_tokens,
)

```

But this will work:

```python
from transformers import pipeline

pipe = pipeline(task=""image-text-to-text"", model=""llava-hf/llava-onevision-qwen2-0.5b-ov-hf"")

pipe(
    images=[[image_ny, image_chicago], [image_pokemon]],
    text=text_batched_image_tokens,
)
```

### Expected behavior

Except for model-specific limitations or features, vlms used for similar tasks should accept a similar set of inputs format","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",5,open
Test: generate with `torch.compile(model.forward)` as a fast test,"# What does this PR do?

Follow-up to #34464 

This PR:
1. Converts `test_generate_compile_0_forward_only` to a fast test. This means we will check generate with `torch.compile(model.forward)` at each commit on ALL models that support `StaticCache` 💛 
2. Fixes failing cases of `test_generate_compile_0_forward_only` whenever possible
3. Tags models with `_supports_static_cache = False  #Reason` when the model doesn't support `torch.compile(model.forward)`

__________________________

✅  `py.test tests/models/ -k test_generate_compile` is all green, takes ~2 mins to run on all 46 passing cases on my machine",[],4,open
Update Pixtral to use SDPA,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes large memory consumption and performance issues with unfused attention in Pixtral. The usage comes from the vLLM implementation in https://github.com/vllm-project/vllm/pull/9597


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
[WIP] Clean initialisation of swinlayer class,Follow up for #33974 ,[],2,open
Fixed num_quantizers to use variable codebook_size,"# What does this PR do?

Adds the modification for ""num_quantizer in EncodecConfig should accept variable codebook size #34521""
It adds on code provided by a user originally in a comment and tested on use cases.


Fixes # (issue)

It fixes the issue of not being able to use the variable coodbook_size in config_encodec.

## Before submitting
- [Yes ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ Yes] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ No] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [No Need for Changes ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [Not Needed ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@sgugger 
@maintainer
Hi, I am new to all this, please help me tag an appropriate person


 ","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",4,open
Inference with FSDP during training affects checkpoints,"### System Info

Output from `transformers-cli env`:
 
 ```
 - `transformers` version: 4.45.2
 - Platform: Linux-6.1.0-21-cloud-amd64-x86_64-with-glibc2.36
 - Python version: 3.12.5
 - Huggingface_hub version: 0.25.0
 - Safetensors version: 0.4.5
 - Accelerate version: 1.0.1
 - Accelerate config:    not found
 - PyTorch version (GPU?): 2.5.0+cu124 (True)
 - Tensorflow version (GPU?): 2.17.0 (False)
 - Flax version (CPU?/GPU?/TPU?): not installed (NA)
 - Jax version: not installed
 - JaxLib version: not installed
 - Using distributed or parallel set-up in script?: just using the Trainer, and running with accelerate
 - Using GPU in script?: I'm running on GPUs
 - GPU type: NVIDIA H100 80GB HBM3
 ```
 
 Relevant environment and library versions:
 
 ```
 Linux Debian 6.1.90-1
 CUDA version: 12.4
 
 accelerate==1.0.1
 datasets==3.0.1
 torch==2.4.1
 torchaudio==2.4.1
 torchvision==0.19.1
 transformers==4.45.2
 ```

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Hello! I'm running into an issue with checkpoints saved when training an LLM with FSDP and the default HuggingFace trainer, if I also do inference during training. I provide code at the end of this post for clarity.
 
 I also asked this [on the forum](https://discuss.huggingface.co/t/doing-inference-with-fsdp-during-training-affects-checkpointing/112530?u=todaysurprised) before coming here, but I haven't found a solution yet.
 
 ## What I'm trying to achieve
 
 I want to write a callback to monitor model outputs on a validation set throughout the training process. This requires doing inference with `model.generate()`. Since I'm also using FSDP, I need to summon all weights on a single device, as described [in this Github issue](https://github.com/huggingface/transformers/issues/30228#issuecomment-2350022762).
 
 ## My issue
 The callback I provide below seems to work fine for evaluation, but it affects the checkpoints that get saved. Specifically, when unsharding the final checkpoint and trying to replicate the results I see from my training script, I get different, much worse results from the checkpoint.
 
 To test this, I trained an LLM to memorize a simple phrase: ""Two times 10 equals 20."". At the end of training, my callback reports the completions I expect, meaning the model trained well. However, if I load the checkpoint from disk and feed it the same prompts, I get this:
 
 ```
 # With callback
 # Outputs from the training script, after training.
 ""Two""                 -> ""times 10 equals 20.""
 ""Two times""           -> ""10 equals 20.""
 ""Two times 10""        -> ""equals 20.""
 ""Two times 10 equals"" -> ""20.""
 # Outputs from the checkpoint loaded from disk.
 ""Two""                 -> ""               ""
 ""Two times""           -> ""equals               ""
 ""Two times 10""        -> ""               ""
 ""Two times 10 equals"" -> ""               ""
 ```
 
 This does not happen if I don't run the callback during training. If I remove it, the checkpoint produced outputs the expected results:
 
 ```
 # Without callback
 # Outputs from the checkpoint loaded from disk.
 ""Two""                 -> ""times 10 equals 20.""
 ""Two times""           -> ""10 equals 20.""
 ""Two times 10""        -> ""equals 20.""
 ""Two times 10 equals"" -> ""20.""
 ```
 
 To make extra sure, I also tried this experiment with DDP instead of FSDP (I removed the summon instruction). The DDP checkpoint is correct regardless of using my callback or not.
 
 ```
 # With DDP
 # Outputs from the training script, after training.
 ""Two""                 -> ""times 10 equals 20.""
 ""Two times""           -> ""10 equals 20.""
 ""Two times 10""        -> ""equals 20.""
 ""Two times 10 equals"" -> ""20.""
 # Outputs from the checkpoint loaded from disk.
 ""Two""                 -> ""times 10 equals 20.""
 ""Two times""           -> ""10 equals 20.""
 ""Two times 10""        -> ""equals 20.""
 ""Two times 10 equals"" -> ""20.""
 ```
 
 I believe this points to `summon_full_params` being the problem. Do you think this could be a problem with the library, or maybe with my implementation? Any ideas or advice? Thank you!
 
 ## Minimal example
 
 <details>
 <summary><code>main.py</code></summary>
 
 ```python
 from typing import cast
 
 import accelerate
 import datasets
 import torch
 import transformers
 from torch.distributed import fsdp
 
 
 class ValidCallback(transformers.TrainerCallback):
     def __init__(self, tokenizer: transformers.PreTrainedTokenizerBase, dataset: datasets.Dataset) -> None:
         super().__init__()
         self.tokenizer = tokenizer
         self.dataset = dataset
 
     def on_epoch_end(
         self,
         args: transformers.TrainingArguments,
         state: transformers.TrainerState,
         control: transformers.TrainerControl,
         **kwargs,
     ) -> None:
         if state.epoch is None or int(state.epoch) % 25 != 0:
             return
         model = cast(transformers.PreTrainedModel, kwargs[""model""])
         with torch.no_grad():
             self.run(model)
 
     @torch.no_grad()
     def run(self, model: transformers.PreTrainedModel) -> None:
         model.eval()
 
         for batch in self.dataset.iter(batch_size=7):
             encoding = self.tokenizer(batch[""text""], return_tensors=""pt"", padding=True).to(model.device)
 
             with fsdp.FullyShardedDataParallel.summon_full_params(model):
                 outputs = model.generate(
                     inputs=encoding.input_ids,
                     attention_mask=encoding.attention_mask,
                     pad_token_id=self.tokenizer.eos_token_id,
                     max_new_tokens=16,
                     do_sample=False,
                 )
 
             predictions = self.tokenizer.batch_decode(
                 outputs[:, encoding.input_ids.shape[1] :],  # Skip the returned prompt.
                 skip_special_tokens=True,
                 clean_up_tokenization_spaces=True,
             )
 
             if accelerate.PartialState().is_main_process:
                 print(predictions)
 
 
 def main() -> None:
     # Load model and tokenizer.
     checkpoint = ""mistralai/Mistral-7B-v0.3""
     tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)
     tokenizer.padding_side = ""left""
     if not tokenizer.pad_token:
         tokenizer.add_special_tokens({""pad_token"": ""[PAD]""})
     model = transformers.AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)
     model.resize_token_embeddings(len(tokenizer))
 
     # Load and prepare a toy dataset.
     def tokenize_function(examples):
         tokenized = tokenizer(examples[""text""], max_length=32, padding=""max_length"", truncation=True)
         tokenized[""labels""] = cast(list, tokenized[""input_ids""]).copy()
         return tokenized
 
     train_dataset = datasets.Dataset.from_dict({""text"": [""Two times 10 equals 20.""] * 100})
     valid_dataset = datasets.Dataset.from_dict(
         {""text"": [""Two"", ""Two times"", ""Two times 10"", ""Two times 10 equals"", ""Two times 10 equals 20.""]}
     )
     train_dataset = train_dataset.map(
         tokenize_function, batched=True, remove_columns=list(train_dataset.features)
     )
 
     # Train.
     trainer = transformers.Trainer(
         model=model,
         train_dataset=train_dataset,
         args=transformers.TrainingArguments(
             output_dir=""./output-minimal"",
             save_strategy=""steps"",
             save_steps=1_000_000,
             overwrite_output_dir=True,
             remove_unused_columns=False,
             optim=""adamw_torch_fused"",
             bf16=True,
             learning_rate=1e-2,
             num_train_epochs=100,
             per_device_train_batch_size=1,
             ddp_timeout=9999999,
             report_to=[],
         ),
         callbacks=[
             ValidCallback(tokenizer, valid_dataset),
         ],
     )
     trainer.train()
 
 
 if __name__ == ""__main__"":
     main()
 ```
 
 </details>
 
 <details>
 <summary><code>fsdp.yaml</code></summary>
 
 ```yaml
 compute_environment: LOCAL_MACHINE
 debug: false
 distributed_type: FSDP
 downcast_bf16: 'no'
 enable_cpu_affinity: false
 fsdp_config:
   fsdp_activation_checkpointing: false
   fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
   fsdp_backward_prefetch: BACKWARD_PRE
   fsdp_cpu_ram_efficient_loading: true
   fsdp_forward_prefetch: false
   fsdp_offload_params: false
   fsdp_sharding_strategy: FULL_SHARD
   fsdp_state_dict_type: SHARDED_STATE_DICT
   fsdp_sync_module_states: true
   fsdp_use_orig_params: true
 machine_rank: 0
 main_training_function: main
 mixed_precision: bf16
 num_machines: 1
 num_processes: 8
 rdzv_backend: static
 same_network: true
 tpu_env: []
 tpu_use_cluster: false
 tpu_use_sudo: false
 use_cpu: false
 ```
 
 </details>
 
 I run my code on Slurm, using this command:
 
 ```
 srun bash -c ""accelerate launch \
     --config_file fsdp.yaml \
     --main_process_ip $(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1) \
     --main_process_port 6000 \
     --machine_rank \$SLURM_PROCID \
     main.py""
 ```

### Expected behavior

I would expect the checkpoint saved on disk to produce the same outputs as those shown by the script after training.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",10,open
IDEFICS2: explicit error if SDPA is requested,"# What does this PR do?

1. Raises an informative exception if we specify `attn_implementation=""sdpa""` while loading IDEFICS2 -- the model has partial support, some components don't support SDPA
2. Relaxes `test_eager_matches_sdpa_generate` to handle partial SDPA support

The following command is fixed in this PR:
```
RUN_SLOW=1 python3 -m pytest -v --flake-finder --flake-runs=1 tests/models/idefics2/test_modeling_idefics2.py::Idefics2ForConditionalGenerationModelTest -k ""( test_eager_matches_sdpa_generate) or (test_initialization)""
```",[],6,open
RoBERTa is not well implemented for tokenizers with pad_token_id != 1,"### Feature request

I would like to request that RoBERTa models correctly accept tokenizers with pad_token_id != 1. This problem is inherit from [`fairseq`](https://github.com/facebookresearch/fairseq/blob/ecbf110e1eb43861214b05fa001eff584954f65a/fairseq/utils.py#L266) code.

### Motivation

**Problem Definition**
The current implementation of [Roberta](https://github.com/huggingface/transformers/tree/main/src/transformers/models/roberta) in transformers considers that the tokenizer has pad_token_id = 1:
- In the [TF implementation](https://github.com/huggingface/transformers/blob/405b56269812056d9593869e22b7b264d806cb1e/src/transformers/models/roberta/modeling_tf_roberta.py#L76) the pad token is explicitly set to 1.
- In all the implementations, the class `create_position_ids_from_input_ids` indirectly considers the pad_token_id = 1 in all the classes. ([modeling_roberta.py](https://github.com/huggingface/transformers/blob/9f06fb05059a973048f5865e7e385c9db5d6daa4/src/transformers/models/roberta/modeling_roberta.py#L1682), [modeling_tf_roberta.py](https://github.com/huggingface/transformers/blob/405b56269812056d9593869e22b7b264d806cb1e/src/transformers/models/roberta/modeling_tf_roberta.py#L125) and [modeling_flax_roberta.py](https://github.com/huggingface/transformers/blob/9f06fb05059a973048f5865e7e385c9db5d6daa4/src/transformers/models/roberta/modeling_flax_roberta.py#L73)).

**Motivation**
We have pre-trained a RoBERTa with another tokenizer from scratch and need to slightly change the current implementation to work correctly. The changes are minimal in `create_position_ids_from_input_ids`, we just need to change the `+ padding_idx` terms for `+ 1` terms in the incremental positions. This change will not affect the original implementations of RoBERTa.

### Your contribution

I can submit a PR with the modifications if you agree with the incorporation of this change.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",2,open
[Feature] Will there be any integration of using Flex-attention (and Paged attention)?,"### Feature request

Using (https://pytorch.org/blog/flexattention/) Flex-attention (and [Paged attention](https://github.com/pytorch/pytorch/pull/121845/files)) to speedup transformers models and provide flexibility






### Motivation

FlexAttention was proposed as a performant attention implementation leveraging torch.compile with easy APIs for adding support for complex attention variants such as Causal, [Relative Positional Embeddings](https://paperswithcode.com/method/relative-position-encodings), [Alibi](https://paperswithcode.com/method/alibi), [Sliding Window Attention](https://mistral.ai/news/announcing-mistral-7b/), [PrefixLM](https://twitter.com/andersonbcdefg/status/1800907703688339569), https://github.com/pytorch/torchtune/pull/875, [Tanh Soft-Capping](https://twitter.com/LysandreJik/status/1807779471891538199), [PagedAttention](https://arxiv.org/abs/2309.06180), etc.





### Your contribution

Not sure.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
[bugfix] incorrect usage of warning_once,"Hi, I'm a developer for the vllm library. We use transformers a lot!

Recently, we find that this simple code will emit many lines of error:

```python
from vllm.transformers_utils.config import try_get_generation_config
try_get_generation_config(""BAAI/bge-multilingual-gemma2"", False)
``` 

The error log cannot be suppressed by `try-except`, leading to something like:

```text
TypeError: not all arguments converted during string formatting
Call stack:
  File ""/data/youkaichao/vllm/testf.py"", line 2, in <module>
    try_get_generation_config(""BAAI/bge-multilingual-gemma2"", False)
  File ""/data/youkaichao/vllm/vllm/transformers_utils/config.py"", line 403, in try_get_generation_config
    return GenerationConfig.from_model_config(config)
  File ""/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py"", line 1235, in from_model_config
    generation_config = cls.from_dict(config_dict, return_unused_kwargs=False, _from_model_config=True)
  File ""/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py"", line 1093, in from_dict
    config = cls(**{**config_dict, **kwargs})
  File ""/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py"", line 475, in __init__
    self.validate(is_init=True)
  File ""/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py"", line 751, in validate
    logger.warning_once(
  File ""/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/transformers/utils/logging.py"", line 328, in warning_once
    self.warning(*args, **kwargs)
Message: 'You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.'
Arguments: (<class 'UserWarning'>,)
```

I think this is caused by incorrect usage of `warning_once` . I have searched for all usages of `warning_once` , it should not have that `UserWarning` argument.

Hope it helps.",[],1,open
Bugfix/inv freq,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker 
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
Add in free_memory passthrough,"# What does this PR do?

Per request in https://github.com/huggingface/accelerate/pull/2716, this PR introduces a passthrough `Trainer.free_memory` function. 

Now, since most of the refs are built inside the `Trainer` the user really only needs to include/pass through a `model` rather than needing *everything else*

Secondary fix to https://github.com/huggingface/transformers/issues/28178

(Duplicate of https://github.com/huggingface/transformers/pull/30549 but it never got merged 😅 )


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker @Rocketknight1 ",[],1,open
Add Zamba2,"# What does this PR do?

Please include support for Zamba2 architecture created by Zyphra Technologies.

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker
",[],8,open
Memory optimization for gpt_bigcode,"# What does this PR do?

This PR optimizes memory usage for inference with GPT BigCode on devices other than CPU.

There is a spike in memory usage when generating the 1st token and the identified culprit is `torch.baddbm`. Using `torch.matmul` instead reduces the usage and allows for processing significantly larger batch sizes.

Formula for `torch.baddbmm`:
```
out = beta * attn_weights + scale_factor * (query ⋅ key)
```
For beta = 0, this becomes
```
out = scale_factor * (query ⋅ key)
```
and thus can be implemented as `torch.matmul(query, key) * scale_factor`.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

cc @ArthurZucker",[],0,open
long-standing Bug in Adafactor optimizer if beta1 > 0,"### System Info

all known

### Who can help?

@muellerz @SunMarc

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

There seems to be an issue with the Adafactor optimizer found here, if beta1 is > 0:
https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/src/transformers/optimization.py#L672

if beta1 is > 0, use_first_moment == True.
But the code does not actually use the first moment (only). In the following line, it adds to `exp_avg` not only the gradients, but a prepared mixture of gradients, learning rate and in inverse square root of the second moment (called 'update'):
https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/src/transformers/optimization.py#L899

There is nothing in the Adafactor paper that would indicate that this is intentional:
https://arxiv.org/pdf/1804.04235

The Adafactor paper focusses on beta1=0, as their proposal is to save vram by using no first moment (see section 4) and only a factorized second moment (see section 3). For this use case, beta1 is 0 and the `update` variable mentioned above is prepared correctly, but not if beta1 is > 0.

In algorithm 1 on page 2 of the paper, they start out with the Adam algorithm, which updates the first momentum just with the gradients. Nowhere in the paper is it proposed to change that.

Consider this pseudocode, leaving out the betas:
AdamW:
exp_avg_sq+=grad^2
exp_avg+=grad
p+= -lr * exp_avg / sqrt(exp_avg_sq)

Adafactor with `use_first_moment == True`:
exp_avg_sq+=grad^2  #or its factored approximation
exp_avg+=lr * grad / sqrt(exp_avg_sq)
p+=-exp_avg

Note that `grad` and `exp_avg_sq` are already added to `exp_avg`, not only to the parameters. It'll diverge from Adam starting the 2nd step.

I am reporting this issue to transformers, because this seems to be the current reference implementation, with many projects either using or directly copying from this repo.

But this issue didn't originate here.
It can even be found in the tensor2tensor code that is mentioned by the authors in the paper:
https://github.com/tensorflow/tensor2tensor/blob/bafdc1b67730430d38d6ab802cbd51f9d053ba2e/tensor2tensor/utils/adafactor.py#L265

If they have used this code for their experiments, they might have underestimated the difference between using the first moment and not using it in their experiments on page 7, because using the first moment wasn't working correctly.

The impact of this issue today might be limited, because when people use Adafactor, they usually do it for its vram savings, and set beta1 to 0. It might still confuse people though, into thinking that first moment is less important than it is, if they make comparisons between beta1 == 0 and beta1 > 0 both using Adafactor.
And someone might actually use it with beta1 > 0 for its memory savings by factorizing the second moment, and expect that the first moment works like Adam.


### Expected behavior

update of `exp_avg` as implemented in Adam","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",12,open
Pipeline: simple API for assisted generation,"# What does this PR do?

Assisted generation + pipelines has a bad UX at the moment: the user must manually load the assistant model (and the assistant tokenizer, if applicable), which defeats the point of being simple to use.

This PR adds the ability to specify an assistant checkpoint at pipeline definition time -- the pipeline will take care of the rest 🤗 

⚠️ While the feature was added for all pipelines that call `.generate()`, I haven't added a test on all of them. Many pipelines don't forward kwargs properly to `.generate()` which makes testing this transparent [same output, similar runtime] feature hard -- the best way to confirm assisted generation is running is by passing incompatible flags to `.generate()` to make it crash.

Example usage
```py
from transformers import pipeline
import torch

pipe = pipeline(
    ""text-generation"",
    model=""meta-llama/Llama-3.1-8B"",
    assistant_model=""meta-llama/Llama-3.2-1B"",  # This extra line is all that's needed!
    torch_dtype=torch.bfloat16
)
pipe_output = pipe(""Once upon a time, "", max_new_tokens=50, do_sample=False)
print(pipe_output[0][""generated_text""])
```
",[],4,open
VLMs: major clean up 🧼 ,"# What does this PR do?

We have updated all the configs for VLMs on the hub so this PR removes legacy path for models, as it has been there for already 3 releases from v4.44. Also it fixes some stuff that broke on the way, like generating from only text input in LLaVA models

For Video-LLaVA the hub configs cannot be updated as the hub owner has been silent for several mmonths already. And since there is only one model with such architecture, we can hardcode the default values for `patch_num` and also remove the legacy path


fixes https://github.com/huggingface/transformers/issues/34824, fixes https://github.com/huggingface/transformers/issues/35169","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",5,open
(WIP) cleanup(Mask2Former): Remove unused level_start_index parameter,"# What does this PR do?
Removes an unused parameter in Mask2former code so that we can clean-up code that hurts export in #34393

Stacked on top of https://github.com/huggingface/transformers/pull/34393. The specific commit of this PR is located here:
[`4fa88e2` (#34498)](https://github.com/huggingface/transformers/pull/34498/commits/4fa88e24aae6997a373109cffadc81e8935a113c)
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@qubvel @amyeroberts

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",0,open
Fix PixtralProcessor to return input IDs for all prompts and images in batch,"Related to #34204

Update `PixtralProcessor` to handle batches of images and text prompts correctly.

* **`src/transformers/models/pixtral/processing_pixtral.py`**
  - Modify the handling of images to correctly iterate over the zip of images, image sizes, and text.
  - Remove the aggregation of all images into a length 1 list of lists.
  - Ensure the `PixtralProcessor` processes each example in a batch individually.

* **`tests/models/pixtral/test_processor_pixtral.py`**
  - Add a test case to verify the `PixtralProcessor` returns the outputs corresponding to all prompts and images in a batch.
  - Ensure the test case includes multiple images and text prompts in a batch.
  - Verify the outputs of the `PixtralProcessor` match the expected outputs for all examples in the batch.

---

For more details, open the [Copilot Workspace session](https://copilot-workspace.githubnext.com/huggingface/transformers/issues/34204?shareId=fd463015-5d04-46dd-87f2-fa810d0d0456).",[],3,open
Whisper pipeline max_new_tokens generation parameter question,"### System Info

- `transformers` version: 4.45.2
- Platform: Linux-5.17.15-051715-generic-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.26.1
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.5.0+cpu (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: no

### Who can help?

@Rocketknight1 @yla

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

`reproducer.py`

```python
from transformers import pipeline
import datasets
import typing


def get_sample_from_dataset():
    ds = datasets.load_dataset(
        ""distil-whisper/meanwhile"",
        split=""test"",
        streaming=True,
        trust_remote_code=True,
    )

    ds = typing.cast(datasets.IterableDataset, ds)
    ds = ds.cast_column(""audio"", datasets.Audio(sampling_rate=16000))
    ds = ds.take(1)
    
    return next(iter(ds))[""audio""]


sample = get_sample_from_dataset()

whisper = pipeline(""automatic-speech-recognition"", ""openai/whisper-tiny"")

transcription = whisper(
    sample.copy(),
    return_timestamps=True,
)

print(transcription['text'])

#  Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the days, big stories, developing the central headline pawns, definitely maneuvering an OSO topical night to F6, faming of classic Sicilian, named or variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a Fisher shows in lip nitsky attack that culminates in the The elegant lethal slow played all-pass on checkmate that is my nightly monologue, but sometimes sometimes folks I sometimes I start a little wake upside down in the monkey bars of a condemned playground on a super fun site. Get all hepped up on goofballs, rummage that were discarded tag bag of defective toys. Yank out a fistball of disembodied doll limbs, toss them on a stained kid's place, mad from a defunct denies, set up a table inside a rusty cargo container down by the warf, and challenged toothless drifters to the godless, bug house blitz of tournament that is my segment. Me and Wild.

transcription = whisper(
    sample.copy(),
    max_new_tokens=10,
    return_timestamps=True,
)

print(transcription['text'])

#  Folks, if you watch the show, you that is my nightly monologue, but Let's have tournament that is my segment.

# Outputs from both runs
#  [1st chunk                           ][2dn chunk                           ][3rd chunk                                 ]
#  Folks, if you watch the show, you ... that is my nightly monologue, but ... tournament that is my segment. Me and Wild.
#  Folks, if you watch the show, you that is my nightly monologue, but Let's have tournament that is my segment.

```

Steps to reproduce:
1. `pip install datasets transformers=4.44.2`
2. `python reproducer.py`

### Expected behavior

There is a question about `max_new_tokens` for long-form audio sequential processing. It seems `max_new_tokens` currently applied for each processed chunk and not for the whole output sequence. From the reproducer it looks like pipeline captures first 10 tokens from the each chunk and then concatenates into output.
Is it expected behavior?","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",3,open
Vision (Auto)Processor multiple images finetuning example.,"### Feature request

Is it possible to upload an example of how to finetune PaLIGemma on multi-image inputs?
Something similar to [multi-image-inference](https://huggingface.co/docs/transformers/main/model_doc/paligemma#multi-image-inference), which shows how to perform multi-image inference over PaLIGemma.

### Motivation

enabling finetuning of multi-image PaLIGemma

### Your contribution

.","[{'id': 1936351150, 'node_id': 'MDU6TGFiZWwxOTM2MzUxMTUw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Examples', 'name': 'Examples', 'color': 'd4c5f9', 'default': False, 'description': 'Which is related to examples in general'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",4,open
Please don't kill BetterTransformer — 1.88x faster inference than SDPA,"### Feature request

I would like to request that BetterTransformer not be deprecated. See also [optimum#2083](https://github.com/huggingface/optimum/issues/2083).

This issue is intended to track the lack of feature-parity in Hugging Face `transformers` with BetterTransformer.

### Motivation

This is a simple example that demonstrates just how valuable BetterTransformer is to users of BERT-like models:
```python
```python
import torch

from transformers import RobertaModel, RobertaTokenizerFast

# BEGIN CONFIG #
MODEL_NAME = 'umarbutler/emubert'
EXAMPLE_INPUT = ""\
The Parliament shall, subject to this Constitution,\
have power to make laws for the peace, order, and good\
government of the Commonwealth with respect to:\
    (i) trade and commerce with other countries, and among\
        the States;\
    (ii) taxation; but so as not to discriminate between""""""
# END CONFIG #

sdpa_model = RobertaModel.from_pretrained(MODEL_NAME, attn_implementation = 'sdpa').to(torch.bfloat16).to('cuda').eval()
bettertransformer_model = RobertaModel.from_pretrained(MODEL_NAME).to(torch.bfloat16).to_bettertransformer().to('cuda').eval()
tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)
input_tokens = tokenizer(EXAMPLE_INPUT, return_tensors='pt').to('cuda')

with torch.inference_mode():
    # Do unbenched forward passes to control for potential caching effects.
    for _ in range(10):
        bettertransformer_model(**input_tokens)
        sdpa_model(**input_tokens)
    
    # Benchmark the models.
    %timeit bettertransformer_model(**input_tokens)
    %timeit sdpa_model(**input_tokens)
```

On my 4090, BetterTransformer achieves `1.93 ms ± 104 μs` and SDPA achieves `3.64 ms ± 259 μs`. BetterTransformer is almost 2x faster (1.88x)...

I have found both training and inference to be *significantly* faster with BetterTransformer enabled, even compared to SPDA and flash attention 2. I believe this is because of how it fuses layers into a single encoder block. Until/if ever that functionality and its associated performance gains can be incorporated into Hugging Face, I'd be fine with having BetterTransformer deprecated, but until then, BetterTransformer's removal would make my code and the code of other users of BetterTransformer significantly slower.

### Your contribution

This request.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Expand AcceleratorConfig to accommodate other features such as NCCL timeout etc,"### Feature request

Expand `AcceleratorConfig` and corresponding transformers trainer args to allow transformer users to use full feature set of accelerate through the config arguments supported by `Accelerator()`. The args are being materialized here for use - https://github.com/huggingface/transformers/blob/a769ed45e17c44fd17b85c025863c4e4f2f73634/src/transformers/trainer.py#L5000


### Motivation

When using `HF/transformers` or `HF/trl SFTTrainer` with accelerate under the hood, its sad that only a limited set of arguments are exposed in `AcceleratorConfig` thereby not having control over using other features of the `Accelerator` such modifying the NCCL timeout. 

### Your contribution

I will be glad to raise a PR to expand AcceleratorConfig to enable wide array of arguments supported by `Accelerator`. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Improve batching speed for Whisper models using VAD based chunking,"### Feature request

This feature request aims to improve the speed of Whisper's batched version by adding a VAD model (such as pyannote or from NeMO or Silero) and merging chunks up to 30 sec, instead of relying on the Sliding window technique that takes up more time. 

### Motivation

In addition to the benefits stated above, the semantic chunks take care of the transcription at the 30 sec boundaries. 

### Your contribution

A similar batching has already been implemented in [faster-whisper project](https://github.com/SYSTRAN/faster-whisper/blob/c2a1da1bd94e002c38487c91c2f6b50a048000cf/faster_whisper/transcribe.py#L361) with Silero VAD. The WER and Speed on internal test data and [youtube-commons-asr-eval](https://huggingface.co/datasets/mobiuslabsgmbh/youtube-commons-asr-eval) (for long-form transcription) are better on this implementation than the current HF implementation.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Update processing_pixtral.py,"instead of  aggregating all of the images into a length 1 list of lists like this  [[img1,img2,...]], we are  taking each image as list  into the list like this [[img1],[img2],[img3]...] so it doesn't mess with the zip iteration ",[],0,open
Beit image classification have different results compared from versions prior to 4.43.0,"### System Info

- `transformers` version: 4.43.0
- Platform: Windows-10-10.0.19045-SP0
- Python version: 3.10.9
- Huggingface_hub version: 0.26.1
- Safetensors version: 0.4.5
- Accelerate version: 0.23.0
- Accelerate config:    not found
- PyTorch version (GPU?): 1.13.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: Yes
- GPU type: NVIDIA GeForce RTX 3060 Ti

### Who can help?

@amyeroberts

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Given the following image:
![image](https://github.com/user-attachments/assets/4c4cf99d-e5fc-40ff-adaf-13d3c1b3d337)

Running the following pipeline for versions prior to `4.43.0` (4.42.4)
```py
from PIL import Image
from transformers import pipeline
import transformers

pipeline_aesthetic = pipeline(
    ""image-classification"", ""cafeai/cafe_aesthetic"", device=0
)
with Image.open(""F:\\Downloads\\Tower.jpg"") as img:
    predictions = pipeline_aesthetic(img, top_k=2)
    predict_keyed = {}
    for p in predictions:
        # print(type(p))
        if not isinstance(p, dict):
            raise Exception(""Prediction value is missing?"")
        predict_keyed[p[""label""]] = p[""score""]
    print(predict_keyed,transformers.__version__)
```

For 4.42.4, it returns:
```
{'aesthetic': 0.651885986328125, 'not_aesthetic': 0.3481140434741974} 4.42.4
```
For 4.43.0:
```
{'aesthetic': 0.43069663643836975, 'not_aesthetic': 0.2877475321292877} 4.43.0
```

### Expected behavior

Expected results from 4.42.4 instead of 4.43.0.


### Addn Notes.

I narrowed it down to this commit being the cause: https://github.com/huggingface/transformers/blob/06fd7972acbc6a5e9cd75b4d482583c060ac2ed0/src/transformers/models/beit/modeling_beit.py but unsure where exactly it is changed.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",6,open
cannot pass string values to the report_to variable after init constructor,"### System Info

- `transformers` version: 4.47.0.dev0
- Platform: Windows-11-10.0.26100-SP0
- Python version: 3.12.5
- Huggingface_hub version: 0.26.1
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.0+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce GTX 1660 Ti

### Who can help?

@muellerzr @SunMarc 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

1. Install wandb package
2. Run the below script with any model (model initialization is not added here for brevity)
```python
default_train_args = TrainingArguments(output_dir=""tmp"")

default_train_args.report_to = ""none""

trainer = Trainer(model=model, args=default_train_args)
```

### Expected behavior

The script will run without any integration when ```report_to``` value is assigned with ```none``` value.

## Problem
The check for the ```report_to```  variable is done in the ```post_init``` function of the ```TrainingArguments```. If you don't specify it in the constructor then the code set to ```all``` and fetch all the integrations. If you assing a value to ```report_to``` variable after initialization constructor, the below issue is happening.

In the ```Trainer``` constructor, with ```get_reporting_integration_callbacks``` function the class is fetching the integration callbacks but the function is not checking if string or array is the type of the ```report_to``` value. Because of that the functions raises ValueError if you assign string to the value because it is trying to check with char rather than string. This behaviour is same for the ```none``` and ```all``` string values.

- The responsible code parts for the issue can be found below:
  - [[code](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py#L1821)] TrainingArguments
  - [[code](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L627)] Trainer
  - [[code](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/integration_utils.py#L2149)] Integration Utils

## Solution
Adding below code to the beginning of the [get_reporting_integration_callbacks](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/integration_utils.py#L2149) function will fix the issue:

```python
if report_to is None:
     return []

if isinstance(report_to, str):
    if ""none"" == report_to:
        return []
    elif ""all"" == report_to:
        report_to = get_available_reporting_integrations()
    else:
        report_to = [report_to]
```

- [x] If the issue is approved, I will open PR.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",5,open
Fix ValueError by providing labels in the input,"Fixes #34392

Add `labels` argument to compute loss in model inputs.

* **Documentation**: Add a note in `docs/source/en/main_classes/trainer.md` to mention that the `labels` argument must be provided to compute the loss.
* **Train Command**: Ensure that the `labels` argument is included in the input when calling the model in the `run_torch` method in `src/transformers/commands/train.py`.
* **Seq2SeqTrainer**: Ensure that the `labels` argument is included in the input when calling the model in the `prediction_step` method in `src/transformers/trainer_seq2seq.py`.

---

For more details, open the [Copilot Workspace session](https://copilot-workspace.githubnext.com/huggingface/transformers/issues/34392?shareId=60c3084a-d630-4160-8c52-296df088a9a3).",[],2,open
[i18n-<languageCode>] Translating docs to <languageName>,"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the <languageName>-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [x] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) https://github.com/huggingface/transformers/pull/20180
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) (waiting for initial PR to go through)
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
🚀 Speed up Image Processing for CV models,"# What does this PR do?

This speeds up image processing by converting the data to numpy array before sending to BatchFeature.


Queries:
* Currently I am checking if all the images are of same size or not before converting the whole batch to  ndarray; for that I am utilizing the arguments like `do_resize`,`do_pad` etc.. but is it necessary to do ti this way or we can write a helper function which checks whether every sub array is uniform or not and if so then convert the whole array to ndarray. WDYT? 
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #31205  

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@yonigozlan Please review this first draft PR where I have applied the logic only on some model and once approach is finalised, same changes would be made to all other models",[],3,open
Fix: raise if block_size is too big,Fixes #34400 ,[],3,open
Trying to train a model using automatic1111. Error - Exception training model: 'module 'transformers.integrations' has no attribute 'deepspeed''.,"### System Info

Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.

- `transformers` version: 4.46.0
- Platform: Windows-10-10.0.22631-SP0
- Python version: 3.10.0
- Huggingface_hub version: 0.25.2
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 4090

### Who can help?

Im trying to fine tune a model. Keep getting this error - Exception training model: 'module 'transformers.integrations' has no attribute 'deepspeed''.


### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

downloaded model. installed dependencies. ran automatic1111 web userinterface. Installed dreambooth. trying to run model

### Expected behavior

a fine tuned model","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",21,open
Fix OffloadedStaticCache arguments,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)

https://github.com/huggingface/transformers/issues/34327

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
Memory Issues when Attempting to Load GGUF Tensors in transformers,"### System Info

Environment:
OS: Ubuntu 24.04
Python version: 3.11.8
Transformers version: transformers==4.45.2
Torch version: torch==2.3.0
Model: Meta-Llama-3.1-70B-Q2_K-GGUF  -  https://huggingface.co/Ffftdtd5dtft/Meta-Llama-3.1-70B-Q2_K-GGUF


### Who can help?

text models: @ArthurZucker
generate: @zucchini-nlp

### Information

- [X] The official example scripts


### Reproduction

Description:
I am attempting to load a quantized GGUF model (Meta-Llama-3.1-70B-Q2_K-GGUF) using the AutoTokenizer and AutoModel classes from Hugging Face transformers, but I am encountering a severe memory RAM usage during the de-quantization process (more than 90GB). 

**Steps to Reproduce:**
```
tokenizer = AutoTokenizer.from_pretrained(""Ffftdtd5dtft/Meta-Llama-3.1-70B-Q2_K-GGUF"", trust_remote_code=True, gguf_file=""Meta-Llama-3.1-70B-Q2_K-GGUF"")
model = AutoModel.from_pretrained(""Ffftdtd5dtft/Meta-Llama-3.1-70B-Q2_K-GGUF"",  device_map='auto', trust_remote_code=True, gguf_file=""Meta-Llama-3.1-70B-Q2_K-GGUF"")
```

**Observed Behavior:**
Memory Issues: When attempting to load the GGUF model (Meta-Llama-3.1-70B-Q2_K), the system quickly exhausts available memory during the de-quantization. It use more than 90GB. I also try other different models such as https://huggingface.co/mradermacher/Meta-Llama-3.1-70B-GGUF

### Expected behavior

- Efficient Loading: I expect the GGUF model (like Meta-Llama-3.1-70B-Q2_K-GGUF) to be loaded correctly using the transformers library, with a clear and efficient process for de-quantizing and loading the model without causing memory exhaustion, even in systems with less than 100GB of RAM.

- Support for Lower RAM Systems: Given that GGUF is a quantized format designed for efficiency, it would be ideal if transformers could either support a more memory-optimized loading process or allow partial model loading, enabling users with lower RAM systems (e.g., < 100GB) to load and run these models effectively.

- Alternative Loading Method: If loading such models directly with AutoModel and AutoTokenizer is not possible due to GGUF-specific constraints, it would be helpful to have documentation or tools to convert these models into a compatible format (such as PyTorch) that can be handled within transformers.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",15,open
Switch to sdpa_kernel api with newer torch version,"### System Info

- `transformers` version: 4.45.2
- Platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.35
- Python version: 3.10.14
- Huggingface_hub version: 0.26.1
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.0+cu124 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 4090


### Who can help?

I noticed that many files in transformers use the older sdp api `torch.backends.cuda.sdp_kernel`. We just discovered a bug in Pytorch 2.5.0 and the old sdp api that would make it run slower https://github.com/pytorch/pytorch/issues/138386

It would be a good idea to update to the new api (`from torch.nn.attention import sdpa_kernel, SDPBackend`) and set the appropriate compile flag to avoid losing as much as 20% of the performance !

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Gist here as a reference example: https://gist.github.com/mobicham/aa1e77689d9cf866cbea2cb75a53a9e4 
More details in the torch issue: https://github.com/pytorch/pytorch/issues/138386

### Expected behavior

Examples using sdp with torch 2.5.0 should run at least as fast as 2.4.1","[{'id': 1834067346, 'node_id': 'MDU6TGFiZWwxODM0MDY3MzQ2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Documentation', 'name': 'Documentation', 'color': '77cc3b', 'default': False, 'description': ''}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",3,open
Support dynamic batch size,"### Feature request

Hi thanks for the library! When training, I realize that, if a micro batch contains too few tokens, the throughput will be quite bad (i.e. average time per token is large). However, I cannot increase the batch size, because there are long (e.g. 2000 tokens) and short (e.g. 500 tokens) sequences in the training data. The batch size that make short sequences run fast will make long sequences OOM.

Therefore, I am proposing to have dynamic (micro) batch size. For example, suppose we have batch_size=16. Then, before this proposal, we have e.g. micro_batch_size=2 & grad_accum=8. After this proposal, for short sequences, use 4 samples in this micro batch; for long sequences, use 2 samples in this micro batch. After they sum up to 16 samples, we can compute the loss and consider this step is done.

### Motivation

(see above)

### Your contribution

I am happy to PR","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
"jinja2 is a necessary dependency, but it is not currently specified","### System Info

- `transformers` version: 4.45.2
- Platform: Linux-3.10.0-1160.114.2.el7.x86_64-x86_64-with-glibc2.17
- Python version: 3.10.15
- Huggingface_hub version: 0.26.1
- Safetensors version: 0.4.5
- Accelerate version: 1.0.1
- Accelerate config:    not found
- PyTorch version (GPU?): 1.12.1+cu102 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

I have addressed the issue: if Jinja2 is not installed, an ‘Extension’ undefined error will occur.

### Expected behavior

NA","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",5,open
Change HybridCache.get_seq_length return type to int,"# What does this PR do?
Changes `HybridCache.get_seq_length` return type to int to align with other cache classes.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@ArthurZucker",[],0,open
Fixing strided perplexity calculation for fixed-length models,"# What does this PR do?

Fixes an issue with perplexity of fixed length models with a strided window. The perplexity calculation assumes all batches are the same size. This calculates the average with different context sizes. The update is just to perplexity.md in the docs. 

Fixes  https://github.com/huggingface/transformers/issues/34138


## Before submitting

- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


@ArthurZucker



",[],3,open
[GGUF] Refactor and decouple gguf checkpoint loading logic,"# What does this PR do?

Since there are more and more GGUF architectures have supported, the gguf loading logic (especially the `load_gguf_checkpoint` function) become complicated and couple tensor rename and tensor ""reshape""/""permute"" reverse.

Therefore, I think there is a need to refactor the gguf checkpoint loading logic for better maintenance.

This PR is going to refactor and cleanup the gguf model loading logic with following methods:
- Introduce a [`get_gguf_hf_weights_map`](https://github.com/vllm-project/vllm/blob/f58454968fe1c5ddf84199b341a6ed5c99f0c0cc/vllm/model_executor/model_loader/loader.py#L1172-L1204) function that has been used in vLLM to infer `gguf_weights_map` (or `GGUF_TENSOR_MAPPING` we used in `transformers` currently) from model config with `gguf` package automatically.
- Since `GGUF_TENSOR_MAPPING` can be inferred with above function, we can try to remove most of existing hardcoded mapping. And we don't need to ask contributor to add new mapping in `GGUF_TENSOR_MAPPING` anymore at most of cases.
- ~~Separate the GGUF tensors rename and ""reshape""/""permute"" reverse logic.~~

Since vLLM and transformers have a different model weights loading logic, it wiil cost some time for me to figure out a better method to finish these works. So I marked this PR as a draft.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],8,open
Fix `past_key_values` as input when using `Cache`,"# What does this PR do?

This PR should fix #33966.",[],2,open
Added performance.mdx italian translation,"# What does this PR do?

Italian translation of performance doc

Fixes #17459 


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

",[],0,open
safetensor/mmap memory leak when per-layer weights are converted do other dtypes,"### System Info

While working on [GTPQModel](https://github.com/modelcloud/gptqmodel) which does gptq quantization of hf models and load each layer on to gpu, quantize, and then move layer back to cpu for vram reduction, we noticed a huge cpu memory leak == to layer weight of dtype when moving the layer from from dtypes. The layer stays in cpu memory, leaks, and we are unable to free it. The memory stays until the program ends. The leak happens happens when we do the dtype conversion on cpu or to gpu. 

Is this is a internal memory leak or are we doing something wrong or have the wrong expectation to how transformers/torch handles cpu tensor memory? 

Reproducing code on cpu only (to gpu has same bug). No gpu is nesssary, just load the model as `bfloat16` and do dtype transitions and observe memory leak.


### Who can help?

@ArthurZucker @SunMarc @MekkCyber

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Env: 
```
AMD Zen3
Ubuntu: 22.04

Name: torch
Version: 2.5.0

Name: accelerate
Version: 1.0.1

Name: transformers
Version: 4.45.2 
```

```python
import gc

import torch
from memory_profiler import memory_usage
from transformers import AutoModelForCausalLM

def mem(msg: str):
    gc.collect()
    m = memory_usage()[0]
    mm = f""{msg}. memory_usage: {m} MiB""
    print(mm)

MODEL_ID = ""meta-llama/Llama-3.1-8B-Instruct""

mem(""load model before"")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.bfloat16, 
    device_map=""cpu"",
)
mem(""load model after"")
print(""model"", model)

for i in range(0, 32):
    mem(""to float32 before"") # ref point: each layer is ~400MB in bfloat16
    model.model.layers[i].to(torch.float32)
    mem(""to float32 after"").  # <--- +1200MB ram == leak 400MB (1200MB - 800MB (float32)). 


```

Run the above code and watch the cpu memory usage grow linearly after each loop by 1200MB instead of expected 800MB (400MB leak per layer equal to the size of the layer in `bfloat16` before conversion). Gc() does not help. 

### Expected behavior

Constant memory equal to model weights/dtype combo. 

UPDATE: Looks the leak is isolated to model/layers loaded as `torch.bfloat16`. No memory leak observed if model/layer is first loaded as `torch.float16` or `torch.float32` and conversion to other dtypes.","[{'id': 1834056761, 'node_id': 'MDU6TGFiZWwxODM0MDU2NzYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Modeling', 'name': 'Core: Modeling', 'color': 'FF8446', 'default': False, 'description': 'Internals of the library; Models.'}, {'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6126880899, 'node_id': 'LA_kwDOCUB6oc8AAAABbTDIgw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/contributions-welcome', 'name': 'contributions-welcome', 'color': 'F99E09', 'default': False, 'description': ''}]",18,open
MsT: chunking the LM-head and MLP to extend sequence length and save memory,"### Feature request
I would like to contribute an LM-head and MLP implementation that optimizes intermediate memory(logits and MLP intermediate). Chunking the LM-head and MLP over sequence dimension and accumulating the gradient during training and fine-tuning. This would not reduce throughput and mathematical equivalence to standard training. Combining with gradient checkpoint, we can extend 12x-24x sequence length over vanilla huggingface/transformers, and 4-7x sequence length over gradient checkpoint. This is all implemented in [this repo](https://github.com/wdlctc/transformers/)

### Usage:
from transformers.integrations import replace_with_minis
replace_with_minis()

### Motivation
State-of-the-art transformer models introduced larger tokenizers with a vocabulary of 128K tokens (Llama3) or 256 tokens (Gemma2). Training/Finetune these models can easily run out of GPU memory, and we found a vast majority of the memory is consumed by the intermediate memory(logits and MLP intermediate). The intermediate memory is not necessary to be stored on GPU memory, therefore, using chunking technologies for these intermediate memory can significant reduce the peak memory usage, like gradient accumulation. The memory save can be significantly enlarged with gradient checkpoint, where intermediate memory takes most activation memory.

### Your contribution
I provided an initial implementation on this https://github.com/wdlctc/transformers/. Not sure what is the right way to integrate it. Could be a modification of modeling_llama.py (and other model implementation), or an new class, not sure. I'm also not very familiar with the PR process, since this is my first issue, so maybe it would be better if someone from the HF team can shepherd this through.

### Additional context
Blog: https://wdlctc.github.io/mst.html
Model Finetune Guidence with our tech: [LLAMA3](https://github.com/wdlctc/mini-s/blob/main/doc/llama3.md), [Qwen2](https://github.com/wdlctc/mini-s/blob/main/doc/qwen.md), [Memba](https://github.com/wdlctc/mini-s/blob/main/doc/falcon-mamba.md), [Mistral](https://github.com/wdlctc/mini-s/blob/main/doc/mistral.md), [Gemma2](https://github.com/wdlctc/mini-s/blob/main/doc/gemma.md)
Arxiv: https://arxiv.org/abs/2407.15892","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
#34146 - differential attention implementation for bert,"# What does this PR do?
Fixes https://github.com/huggingface/transformers/issues/34146
Implements differential attention for BERT

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],3,open
[MLU] remove deepspeed-mlu dependency,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Remove deepspeed-mlu dependency. # (https://github.com/microsoft/DeepSpeed/pull/6472)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@muellerzr @SunMarc

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
Modular phi,"# What does this PR do?
Adds Modular Phi https://github.com/huggingface/transformers/issues/33916
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker @LysandreJik 
Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],8,open
Remove coupled type imports from integration_utils.py,"In #30135, `TFPreTrainedModel` and `PreTrainedModel` were introduced as imports in `integration_utils.py` for the sole purpose of type checking (see diff [here](https://github.com/huggingface/transformers/pull/30135/files#diff-70576373928eee203a92800d5ea1f6270d45462a9bde494cde65338710a0a331)). This results in a fairly heavy set of imports that makes it difficult to separate out builds that are dependent on only TF or Torch (as any module importing `integration_utils` will now import both `modeling_utils.py` and `modeling_tf_utils.py`, which together require both TF and Torch). This logic removes the need for those coupled imports while still working as intended within this module.
",[],1,open
New dynamic cache for sliding window attention,"# What does this PR do?

This supersedes https://github.com/huggingface/transformers/pull/33619 to introduce a new `DynamicSlidingWindowCache`.  

This cache behaves the exact same way as `DynamicCache`, and contrarily to `SlidingWindowCache` (static variant), we can correctly continue generation from an existing cache instance (fully filled or not), with more than 1 new token added to the sequence (e.g. for prefix caching).

In order for this to work I had to do the following main modifications:

- I introduced `get_past_seen_tokens()`, which should replace `get_seq_length()` (almost) everywhere. This is because once the cache is filled, the 2 will not longer provide the same information, so we need to correctly differentiate to recreate the `cache_position`
- for every cache class except `DynamicSlidingWindowCache`, both `get_past_seen_tokens` and `get_seq_length` will return the same value, so no issue there
- For every generative model using sliding window, I modified the 4d causal mask creation for the new cache class for both sdpa and eager, and correctly sliced the 2d mask for FA2

Once `generate` no longer returns legacy cache by default (tuples), we can make this class the default for generative models with sliding window. We cannot do it before because we would lose the information of the past seen tokens if the cache is full.  

It also makes a nice precedent to support the same features with the static variant in a subsequent PR.
","[{'id': 7548528517, 'node_id': 'LA_kwDOCUB6oc8AAAABwe1nhQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-benchmark', 'name': 'run-benchmark', 'color': 'DD9AAA', 'default': False, 'description': ''}]",3,open
Add support for Allegro,"### Model description

Allegro is a powerful text-to-video model that generates high-quality videos up to 6 seconds at 15 FPS and 720p resolution from simple text input.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Model code: https://github.com/rhymes-ai/Allegro","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Added `segmentation maps` support for DPT image processor,"# Added `segmentation maps` support for DPT image processor

Most of image processors for vision models that support semantic segmentation task accept `images` and `segmentation_maps` as inputs, but for some reason DPT image processor does not process segmentation maps, only images. This PR can make code that one uses for training or evaluation of semantic segmentation models more reusable, as now DPT image processor can process segmentation maps as most of other image processors do.

I also added `do_reduce_labels` arg because other image processors that support segmentation masks use it.

I added two new tests: one that tests segmentation_masks support and one that tests if do_reduce_labels work as expected.

Most of the code is adapted from BEIT image processor.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@amyeroberts, @qubvel

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",7,open
Fast tokenizer for CANINE,"### Feature request

I have been using [CANINE](https://arxiv.org/pdf/2103.06874) for my experiments and I see that there does not exist a fast version of the tokenizer for the model. CANINE accepts unicode characters as its inputs, so the tokenizer splits the input into unicode characters. It would be great if a fast version of the tokenizer exists for this!

### Motivation

I have a large dataset (>1 million rows) that I want to tokenize. The CANINE tokenizer tokenizes at around ~1500 tokens/s for 2048 tokens (which is the max sequence length accepted by CANINE) on my machine (i9 13900HX), which would take about 1 hour and 40 minutes. Even while streaming, I feel like the tokenization speed greatly hampers training time (compared to a model such as xlm-roberta-base, another model I am using for my experiments).

If more information is required regarding tokenization time, please do ask, and I shall provide information!

### Your contribution

I am quite familiar with the transformers and tokenizers library. I do not have a huge amount of experience with Rust, but I am eager to learn and would love to contribute!","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
LLaMa 3 8B - offloaded_static cache - layer_device_map TypeError,"### System Info

Transformers Patch release v4.45.2
PyTorch 1.10.1
Python 3.8.0
cuda 11.1
NVIDIA V100

### Who can help?

@gante @zucchini-nlp @Rocketknight1 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Stack trace:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[19], line 1
----> 1 outputs = pipe(
      2     messages,
      3     max_new_tokens=3000,
      4     eos_token_id=terminators,
      5     do_sample=True,
      6     temperature=0.6,
      7     top_p=0.9,
      8     # cache_implementation=""static"",
      9     cache_implementation=""offloaded_static"",
     10 )
     11 assistant_response = outputs[0][""generated_text""][-1][""content""]
     12 print(assistant_response)

File python3.8/site-packages/transformers/pipelines/text_generation.py:267, in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)
    262 if isinstance(
    263     text_inputs, (list, tuple, KeyDataset) if is_torch_available() else (list, tuple)
    264 ) and isinstance(text_inputs[0], (list, tuple, dict)):
    265     # We have one or more prompts in list-of-dicts format, so this is chat mode
    266     if isinstance(text_inputs[0], dict):
--> 267         return super().__call__(Chat(text_inputs), **kwargs)
    268     else:
    269         chats = [Chat(chat) for chat in text_inputs]  # 🐈 🐈 🐈

File python3.8/site-packages/transformers/pipelines/base.py:1268, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1260     return next(
   1261         iter(
   1262             self.get_iterator(
   (...)
   1265         )
   1266     )
   1267 else:
-> 1268     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File python3.8/site-packages/transformers/pipelines/base.py:1275, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1273 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
   1274     model_inputs = self.preprocess(inputs, **preprocess_params)
-> 1275     model_outputs = self.forward(model_inputs, **forward_params)
   1276     outputs = self.postprocess(model_outputs, **postprocess_params)
   1277     return outputs

File python3.8/site-packages/transformers/pipelines/base.py:1175, in Pipeline.forward(self, model_inputs, **forward_params)
   1173     with inference_context():
   1174         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
-> 1175         model_outputs = self._forward(model_inputs, **forward_params)
   1176         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu""))
   1177 else:

File python3.8/site-packages/transformers/pipelines/text_generation.py:370, in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)
    367 if ""generation_config"" not in generate_kwargs:
    368     generate_kwargs[""generation_config""] = self.generation_config
--> 370 generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
    371 out_b = generated_sequence.shape[0]
    372 if self.framework == ""pt"":

File python3.8/site-packages/torch/autograd/grad_mode.py:28, in _DecoratorContextManager.__call__.<locals>.decorate_context(*args, **kwargs)
     25 @functools.wraps(func)
     26 def decorate_context(*args, **kwargs):
     27     with self.__class__():
---> 28         return func(*args, **kwargs)

File python3.8/site-packages/transformers/generation/utils.py:1921, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)
   1915 if (
   1916     inputs_tensor.shape[1] != input_ids_length
   1917     and model_input_name == ""inputs_embeds""
   1918     and not self.config.is_encoder_decoder
   1919 ):
   1920     max_cache_length += inputs_tensor.shape[1]
-> 1921 self._prepare_cache_for_generation(
   1922     generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device
   1923 )
   1925 # 8. determine generation mode
   1926 generation_mode = generation_config.get_generation_mode(assistant_model)

File python3.8/site-packages/transformers/generation/utils.py:1566, in GenerationMixin._prepare_cache_for_generation(self, generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device)
   1561     if generation_config.cache_implementation == ""static"" and not self._supports_static_cache:
   1562         raise ValueError(
   1563             ""This model does not support `cache_implementation='static'`. Please check the following ""
   1564             ""issue: https://github.com/huggingface/transformers/issues/28981""
   1565         )
-> 1566     model_kwargs[cache_name] = self._get_cache(
   1567         cache_implementation=generation_config.cache_implementation,
   1568         batch_size=max(generation_config.num_beams, generation_config.num_return_sequences) * batch_size,
   1569         max_cache_len=max_cache_length,
   1570         device=device,
   1571         model_kwargs=model_kwargs,
   1572     )
   1573 elif generation_config.cache_implementation == ""quantized"":
   1574     if not self._supports_quantized_cache:

File python3.8/site-packages/transformers/generation/utils.py:1476, in GenerationMixin._get_cache(self, cache_implementation, batch_size, max_cache_len, device, model_kwargs)
   1466 layer_device_map = get_layer_device_map(execution_device_map)
   1468 cache_kwargs = {
   1469     ""config"": self.config.get_text_config(),
   1470     ""max_batch_size"": batch_size,
   (...)
   1474     ""layer_device_map"": layer_device_map,
   1475 }
-> 1476 self._cache = cache_cls(**cache_kwargs)
   1477 if requires_cross_attention_cache:
   1478     encoder_kwargs = cache_kwargs.copy()

TypeError: __init__() got an unexpected keyword argument 'layer_device_map'
```

Code:
```
from transformers import pipeline
import torch

cuda_dev_id = 2

model_id = ""meta-llama/Meta-Llama-3-8B-Instruct""

pipe = pipeline(
    ""text-generation"",
    model=model_id,
    model_kwargs={""torch_dtype"": torch.float16},  # bfloat16 breaks on torch 1.10.1
    device=""cuda:"" + str(cuda_dev_id)
)

role = """"""
You are an AI assistant REDACTED.
""""""


prompt = """"""Here is the id: """""" + ""\n"" + str(example_id) + ""\n\n"" + """"""Here is the cid: """""" + ""\n"" + example_cid + ""\n\n"" + """"""Here is the s: """""" + ""\n""+ example_s + ""\n\n"" + """"""Here is the c: """""" + ""\n"" + example_c

messages = [
    {""role"": ""system"", ""content"": role},
    {""role"": ""user"", ""content"": prompt},
]

terminators = [
    pipe.tokenizer.eos_token_id,
    pipe.tokenizer.convert_tokens_to_ids(""<|eot_id|>"")
]

outputs = pipe(
    messages,
    max_new_tokens=3000,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
    cache_implementation=""offloaded_static"",
)
assistant_response = outputs[0][""generated_text""][-1][""content""]
print(assistant_response)
```

### Expected behavior

assistant_response should be a generated response from the LLaMa model.","[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNzcxMTg3OTI0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Pipeline', 'name': 'Core: Pipeline', 'color': 'FF7066', 'default': False, 'description': 'Internals of the library; Pipeline.'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",7,open
Improve EncoderDecoderModel docs,"Fixes #16135

Improve the `EncoderDecoderModel` documentation.

* Add example code to create an `EncoderDecoderModel` using `from_encoder_decoder_pretrained`.
* Add instructions on how to save the model.
* Add instructions on how to fine-tune the model.
* Add a warning about correctly setting configuration values.

---

For more details, open the [Copilot Workspace session](https://copilot-workspace.githubnext.com/huggingface/transformers/issues/16135?shareId=57362139-2e86-4020-90f9-f15402ecf6e3).",[],2,open
Fix PixtralProcessor to return outputs for all examples in a batch,"Fixes #34204

Update `PixtralProcessor` to handle batches of images and text prompts correctly.

* Modify the `__call__` method in `src/transformers/models/pixtral/processing_pixtral.py` to process each example in a batch individually.
* Update the handling of images to correctly iterate over the zip of images, image sizes, and text.
* Add a test case in `tests/models/pixtral/test_processor_pixtral.py` to verify the `PixtralProcessor` returns the outputs corresponding to all prompts and images in a batch.
* Ensure the test case includes multiple images and text prompts in a batch and verifies the outputs match the expected outputs for all examples in the batch.

---

For more details, open the [Copilot Workspace session](https://copilot-workspace.githubnext.com/huggingface/transformers/issues/34204?shareId=09fc5267-4419-410b-ad6f-87d460569bf7).",[],1,open
Mask Padded Tokens In Aspect Ratio Mask Preparation,"# What does this PR do?

Problem: The function `_prepare_aspect_ratio_attention_mask` is not correctly masking padded tokens. The output currently assigns 0s to both padded and non-padded tokens in the vision encoder.

Solution: Update `_prepare_aspect_ratio_attention_mask` to correctly mask padded tokens by setting their mask values to a large negative number.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
fix: unboundlocalerror when trying to retrieve attention feature maps using Qwen2FlashAttention2().forward(),"# What does this PR do?

Fix ""unboundlocalerror"" occurrs when trying to retrieve attention layer feature maps for models using `Qwen2FlashAttention2().forward()` to inference such as LLaVA-NeXT:

<img width=""906"" alt=""image"" src=""https://github.com/user-attachments/assets/da91f90d-60f4-4f23-9d65-f571c3169fdf"">


The main issue is in this method, the logic for computing `attn_weights` when flag `output_attentions` is set to `True` was not implemented, therefore leaving variable `atten_weights` unbounded.



Fixes # (issue)

Following code demonstrated in Qwen2Attention(nn.Module).forward(), several lines were added to compute the value of variable `atten_weights` when `output_attentions` is set to `True`:

<img width=""1387"" alt=""image"" src=""https://github.com/user-attachments/assets/d85dea7d-6188-4f46-a38d-0896b0abbf09"">



## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
speed up whisper compile time,"### Feature request


after torch compiling the whisper.text_decoder model, the inference time is crazy low !. Thank you for the work !
however the warm up time is very long since it needs to go through all logits (at a maximum of 448)

how can reduce this time ?
(i have looked into storing the compiled model with pytorch but it does not seem supported)
(i have tried compiling torch_tensorrt but i have the error EncoderDecoderCache encountered in the dynamo_compile input parsing)

### Motivation

the start up time of the model can take around 10m for a large model

### Your contribution

happy to do a pr but need guidance","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Initializing via AutoImageProcessor before AutoProcessor is imported causes `AttributeError`,"### System Info

- `transformers` version: 4.45.2
- Platform: Linux-4.18.0-513.18.1.el8_9.x86_64-x86_64-with-glibc2.28
- Python version: 3.10.4
- Huggingface_hub version: 0.24.5
- Safetensors version: 0.4.4
- Accelerate version: 0.33.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.0+cu121 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No

### Who can help?

Probably @zucchini-nlp @amyeroberts, and @qubvel

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

There seems to be an edge-case in the loading behavior that can sometimes be hit if something is initialized with `from_pretrained` through `AutoImageProcessor` before `AutoProcessor` is imported, and then `from_pretrained` is used on `AutoProcessor`. 

Repro case - this works as expected
```python
model_name = ""microsoft/Phi-3.5-vision-instruct""
from transformers import AutoImageProcessor
from transformers import AutoProcessor
AutoImageProcessor.from_pretrained(model_name, trust_remote_code=True)
AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
```

But this breaks:
```python
model_name = ""microsoft/Phi-3.5-vision-instruct""
from transformers import AutoImageProcessor
AutoImageProcessor.from_pretrained(model_name, trust_remote_code=True)

from transformers import AutoProcessor
AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
```
with `AttributeError: module transformers has no attribute Phi3VImageProcessor`.

I've tried to reproduce this with a few other models, but haven't been able to yet. It is also probably worth noting that the `AutoProcessor` doesn't return the same class as `AutoImageProcessor` for this model, which might matter

### Expected behavior

`AutoImageProcessor` / `AutoProcessor ` `from_pretrained` should have the same behavior if possible, regardless of import and invocation order","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",11,open
Sync VQA pipeline with huggingface_hub spec,,[],9,open
Set `open` encoding to `utf-8` for Windows compatability,"# What does this PR do?

Set `open` encoding to `utf-8` for Windows compatability

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.",[],2,open
[Trainer][Eval] Why the model output for the first element in eval batch is skipped in logits?,"I wonder why this is written this way. It causes a bug in my eval pipeline as I loose a one element from every batch in eval loop:

https://github.com/huggingface/transformers/blob/ca541bd4f4d932f486a4116deba833b4ffaebd15/src/transformers/trainer.py#L4370","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}]",9,open
Chat template: return vectorized output in processors,"# What does this PR do?

Part of https://github.com/huggingface/transformers/issues/33948. This PR adds support for `return_tensors=""pt""` when calling chat templates for processors. That way users can obtain inputs in tensor format and pass it directly to the model, instead of having to call processor with a formatted prompt + visuals.

For images we use the existing functionality `load_images` and for videos I added a few functions. We usually use `av` in all video related model docs since `decord` had problems with CUDA in the past. Apart from that we can use `opencv` or `torchvision` for video loading. I did a small benchmark run to load and sample uniformly 32 frames from around ~100 videos and `av` was the slowest of all them, while `decord` was the fastest. Therefore I decided to add helper with all possible backends and let users switch whenever they want to. By default we use `opencv` as it is a more common CV framework than any others provided here.

In the future we might start using `torchvision` when we add `VideoProcessor` class and support `VideoProcessorFast` (see https://github.com/huggingface/transformers/issues/33504).


These are the results of small benchmarking with ~100 videos:
```
# Time taken for decord: 475.2979 sec
# Time taken for opencv: 614.6062 sec
# Time taken for av: 1067.0860 sec
# Time taken for torchvision: 1924.0433 sec
```

Review from @Rocketknight1 for templates and @qubvel for general CV related modifications. 

",[],3,open
image_transforms preprocess quite slow when run large image with qwen2vl,"### System Info

- `transformers` version: 4.45.2
- Platform: Linux-5.4.0-132-generic-x86_64-with-glibc2.31
- Python version: 3.12.7
- Huggingface_hub version: 0.25.1
- Safetensors version: 0.4.5
- Accelerate version: 1.0.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA GeForce RTX 3090


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction


funcitons in image_transforms, `rescale`, `normalize` quite slow when preprocess large image.
https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py

here is benchmark

![image](https://github.com/user-attachments/assets/b402be6e-e916-45ad-ac17-78d3dc84d62b)

please refer to https://github.com/vllm-project/vllm/issues/9238

### Expected behavior

how to improve performance?","[{'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",6,open
added interpolation for owlvit & owlv2.,"# What does this PR do?

Fixes # (issue)

Towards: Community contribution: enable dynamic resolution input for more vision models. #30579
Added interpolation for dynamic resolution inputs.


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts do review this, the only task that needed a check in the box :)

",[],2,open
fix: DataCollatorWithFlattening incompatible with Tensor input ids,"# What does this PR do?
This PR provides a bug Fix for List Concatenation Error in __call__ Method in DataCollatorWithFlattening
Fixes # (https://github.com/huggingface/transformers/issues/33946)

## Problem Description
   The error traceback:
`TypeError: can only concatenate list (not ""Tensor"") to list `

## occurs at the following line of code:
  `line: 1649   ret[""labels""] += [separator_id] + features[idx][""input_ids""][1:]`

## How to reproduce the error ?

```
from transformers import GPT2Tokenizer, DataCollatorWithFlattening
tokenizer = GPT2Tokenizer.from_pretrained(""openai-community/gpt2"")
example = tokenizer(""A test sentence"", return_tensors=""pt"")
collator = DataCollatorWithFlattening()
batch = collator([example]*2, return_tensors=""pt"")
print({k: v.tolist() for k, v in batch.items()})
```


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker


<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Access to model outputs inside LogitProcessor,"### Feature request

![image](https://github.com/user-attachments/assets/cb57d5dd-6502-4b31-a948-bb46e535fea5)

The LogitProcessor __call__ method currently has access to the input_ids and the logits generated. It would be really helpful if it has access to the model output for that iteration.

### Motivation

I am trying to implement something similar to [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368) along with transformers, especially Llama models. I am required to add the attention output to the model's generated logit for pointing mechanism, but I do not have access to the attention value inside logit processor. As seen in the picture, if the standard model output object can be passes to the logit processor, then the user can extract required details from the output object and use it to further process the generated logits.

![image](https://github.com/user-attachments/assets/4103b2f8-a09c-4a2e-8916-9ebac8962dd1)


### Your contribution

I can implement the required changed and create a PR if allowed.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",1,open
Add support for Janus model from DeepSeek AI,"### Model description

Janus is an autoregressive framework that unifies multimodal understanding and generation. Unlike previous approaches that use a single visual encoder for both tasks, Janus decouples visual encoding into separate pathways while utilizing a unified transformer architecture for processing. This decoupling addresses the conflict between visual encoder roles in understanding and generation, enhancing flexibility and performance.

Key features:

- Unified framework for multimodal understanding and generation
- Decoupled visual encoding pathways
- Single, unified transformer architecture for processing
- Improved performance in multimodal understanding tasks
- Flexibility to select optimal encoding methods for each component

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The Janus model is developed by DeepSeek AI. Here are the relevant links for implementation:

Paper: [Janus: Bridging the Gap Between Multimodal Understanding and Generation](https://arxiv.org/pdf/2410.13848)
GitHub repository: [deepseek-ai/Janus](https://github.com/deepseek-ai/Janus)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Fix handling of Sequence post-processors in train_new_from_iterator,"# What does this PR do?
This PR fixes an issue where the post-processor special token IDs are not correctly updated when training a new tokenizer using `train_new_from_iterator` of a tokenizer with a `Sequence` post-processor. Instead, the special token IDs are copied directly from the original tokenizer.

For example, this affects training a new tokenizer from Llama-3 tokenizers, as reported in #33998 and #30752.

Running the following code:
```
from transformers import AutoTokenizer
from datasets import load_dataset
import json
from itertools import islice

tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3.1-8B"")
ds = load_dataset(""wikimedia/wikipedia"", ""20231101.et"", streaming=True, split=""train"")

new_tokenizer = tokenizer.train_new_from_iterator([x[""text""] for x in islice(ds, 100)], 1000)

print(f""bos_token_id={new_tokenizer.bos_token_id}"")
print(f""'Hello world!' tokenized as {new_tokenizer('Hello world!')['input_ids']}"")
print(json.dumps(json.loads(new_tokenizer._tokenizer.to_str())['post_processor'], indent=2))
```
the output is:
```
bos_token_id=0
'Hello world!' tokenized as [128000, 294, 569, 727, 399, 338, 541, 327, 319, 256]
{
  ""type"": ""Sequence"",
  ""processors"": [
    {
      ""type"": ""ByteLevel"",
      ""add_prefix_space"": true,
      ""trim_offsets"": false,
      ""use_regex"": true
    },
    {
      ""type"": ""TemplateProcessing"",
      ""single"": [
        {
          ""SpecialToken"": {
            ""id"": ""<|begin_of_text|>"",
            ""type_id"": 0
          }
        },
        {
          ""Sequence"": {
            ""id"": ""A"",
            ""type_id"": 0
          }
        }
      ],
      ""pair"": [
        {
          ""SpecialToken"": {
            ""id"": ""<|begin_of_text|>"",
            ""type_id"": 0
          }
        },
        {
          ""Sequence"": {
            ""id"": ""A"",
            ""type_id"": 0
          }
        },
        {
          ""SpecialToken"": {
            ""id"": ""<|begin_of_text|>"",
            ""type_id"": 1
          }
        },
        {
          ""Sequence"": {
            ""id"": ""B"",
            ""type_id"": 1
          }
        }
      ],
      ""special_tokens"": {
        ""<|begin_of_text|>"": {
          ""id"": ""<|begin_of_text|>"",
          ""ids"": [
            128000
          ],
          ""tokens"": [
            ""<|begin_of_text|>""
          ]
        }
      }
    }
  ]
}
```
As shown, the new tokenizer prepends an incorrect `bos_token_id` (`128000` instead of `0`)

Fixes #33998 #30752

I welcome feedback and suggestions on this fix.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

- tokenizers: @ArthurZucker

",[],1,open
GGUF support for BERT architecture,"### Feature request

I want to add the ability to use GGUF BERT models in transformers.
Currently the library does not support this architecture. When I try to load it, I get an error TypeError: Architecture 'bert' is not supported.
I have done most of the mapping, with some fields I am having difficulty.
Can anybody help me and provide comments on this feature?
### Motivation

I ran into a problem that I can't use gguf models in RASA(rasa uses standard from_pretrained). So I decided to make BERT support

### Your contribution

That's my extended ggml.py file
```python
GGUF_TENSOR_MAPPING = {
    ""bert"": {
        ""context_length"": ""max_position_embeddings"",
        ""block_count"": ""num_hidden_layers"",
        ""feed_forward_length"": ""intermediate_size"",
        ""embedding_length"": ""hidden_size"",
        ""attention.head_cgguf>=0.10.0ount"": ""num_attention_heads"",
        ""attention.layer_norm_rms_epsilon"": ""rms_norm_eps"",
        # ""attention.causal"": """",
        # ""pooling_type"": """",
        ""vocab_size"": ""vocab_size"",
    }
}
 
GGUF_CONFIG_MAPPING = {
    ""bert"": {
        ""context_length"": ""max_position_embeddings"",
        ""block_count"": ""num_hidden_layers"",
        ""feed_forward_length"": ""intermediate_size"",
        ""embedding_length"": ""hidden_size"",
        ""attention.head_cgguf>=0.10.0ount"": ""num_attention_heads"",
        ""attention.layer_norm_rms_epsilon"": ""rms_norm_eps"",
        # ""attention.causal"": """",
        # ""pooling_type"": """",
        ""vocab_size"": ""vocab_size"",
    }
}
 
GGUF_TOKENIZER_MAPPING = {
    ""tokenizer"": {
        # ""ggml.token_type_count"": """",
        # ""ggml.pre"": """",
        ""ggml.model"": ""tokenizer_type"",
        ""ggml.tokens"": ""all_special_tokens"",
        ""ggml.token_type"": ""all_special_ids"",
        ""ggml.unknown_token_id"": ""unk_token_id"",
        ""ggml.seperator_token_id"": ""sep_token_id"",
        ""ggml.padding_token_id"": ""pad_token_id"",
        ""ggml.cls_token_id"": ""cls_token_id"",
        ""ggml.mask_token_id"": ""mask_token_id"",
    },
    ""tokenizer_config"": {       
        ""ggml.unknown_token_id"": ""unk_token_id"",
        ""ggml.seperator_token_id"": ""sep_token_id"",
        ""ggml.padding_token_id"": ""pad_token_id"",
        ""ggml.cls_token_id"": ""cls_token_id"",
        ""ggml.mask_token_id"": ""mask_token_id"",
    },
}
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
fix loss scaling only when compute_loss_func is used,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

In [#34198](https://github.com/huggingface/transformers/commit/6ba31a8a94bf7cfeaf59ffc3bc9e0b0cd3e25788#diff-ed55888e6665791fe92cc8fc0c499da54f4ace6738551cd9a2591881cda076deR3629), the line `loss *= self.args.gradient_accumulation_steps` was introduced due to `Negate accelerate grad accum div`. This change was made to correct errors encountered during gradient accumulation. However, the scaling should only occur when compute_loss_func is used, so the code was modified accordingly.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@muellerzr 
@ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Fixed typos and formatting,"#hacktoberfest

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Allow for more tokenizer formats,"# What does this PR do?
Simplifies and extends the way we load tokenizer files",[],0,open
Missing timestamp offset using Whisper with pipeline and sequential decoding,"### System Info

- `transformers` version: 4.45.2
- Platform: macOS-15.0.1-arm64-arm-64bit
- Python version: 3.12.1
- Huggingface_hub version: 0.23.3
- Safetensors version: 0.4.3
- Accelerate version: 0.34.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: no

### Who can help?

@Rocketknight1  @gante @ylacombe

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

1. `pip install transformers==4.45.2`
2. Setup a Whisper pipeline using `chunk_length_s=0` (which is sequential long-form decoding according to the model card (at least for large-v3)) and `return_timestamps=True`
3. Transcribe an audio longer than 30s

    ```py
    from transformers import pipeline
    import torch

    audio_file = '<an-audio-file-longer-than-30-s>'
    chunked = False

    pipe = pipeline(
        'automatic-speech-recognition',
        model='openai/whisper-small',
        chunk_length_s=30 if chunked else 0,
        return_timestamps=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device='cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu',
    )

    result = pipe(audio_file)
    transcript = '\n'.join(
        f""({chunk['timestamp'][0]}, {chunk['timestamp'][1]})\t{chunk['text']}"" for chunk in result['chunks']
    )
    print(transcript)
    ```

4. See that the timestamps start at 0.0s after 30s

    ```
    (0.0, 4.44)      Er hatte schon mal eine Schnauze voll von allem und jedem.
    (4.44, 6.28)     Und er hat den Schluss getroffen.
    (6.28, 7.8)      Es hilft nichts mehr.
    (7.8, 9.28)      Ich wandere aus.
    (9.28, 11.4)     Das kann ein Grund sein,
    (11.4, 14.48)    wieso er eine Heimat für immer der Rückenträger will.
    (14.48, 16.72)   Oder es ist etwas ganz anderes.
    (16.72, 19.24)   Der wohl bekannt ist Grund...
    (19.24, 20.36)  ... die Liebe.
    (20.36, 22.44)   So ist es bei Hans Muster.
    (22.44, 24.72)   Die Liebe hat ihn nach Deutschland gezogen.
    (24.72, 27.0)    Und dort ist er seit vier Jahren.
    (27.0, 29.4)     Aber welter der für immer dort bleibt.
    (0.0, 1.0)       Gute Frage.
    (1.0, 4.0)       Ich stelle mir einen Gart am Viertel vor im PO bei den Leuten.
    (4.0, 7.0)       Und bis dort her, mein Name ist Peter Müller.
    (7.0, 11.0)      Und ich bin Wassermelone Heines vom Harry Styles.
    ```


### Expected behavior

The timestamps should be correct, also if the audio is longer than 30s (as if the chunked-algorithm is used):

```
(0.0, 4.44)      Er hatte schon mal eine Schnauze voll von allem und jedem.
(4.44, 6.28)     Und er hat den Schluss getroffen.
(6.28, 7.8)      Es hilft nichts mehr.
(7.8, 9.28)      Ich wandere aus.
(9.28, 11.4)     Das kann ein Grund sein,
(11.4, 14.48)    wieso er eine Heimat für immer der Rückenträger will.
(14.48, 16.72)   Oder es ist etwas ganz anderes.
(16.72, 19.24)   Der wohl bekannt ist Grund...
(19.24, 20.36)  ... die Liebe.
(20.36, 22.44)   So ist es bei Hans Muster.
(22.44, 24.72)   Die Liebe hat ihn nach Deutschland gezogen.
(24.72, 26.0)    Und dort ist er seit vier Jahren.
(26.0, 29.0)     Aber welter der für immer dort bleibt, gute Frage.
(29.0, 32.0)     Wir stellen es dir an, am Viertel vor, im PO bei den Leuten.
(32.0, 35.0)     Und bis dort her, mein Name ist Peter Müller.
(35.0, 39.0)     Und jetzt ein Wassermelon Heines vom Harry Styles.
```

The output is from above script using `chunked=True`","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",8,open
MLlama does not work with FSDP mix-precision,"### System Info

transformers==4.45.2

### Who can help?

@ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

MLlama is setting `self.dtype` from its own parameters' dytpe and casting some input/intermediate tensor into the `self.dtype`. Here are references
- https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/models/mllama/modeling_mllama.py#L1471
- https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/models/mllama/modeling_mllama.py#L1504
However running with FSDP's mix-precision FSDP expects model to hold fp32 master params and it will cast the params to low precision during training runtime. However MLlama will expect its `self.dtype` to be fp32 while it should actually be low precision. 

So will run into below error
```
F.conv2d(input, weight, bias, self.stride
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (CUDABFloat16Type) should be the same
[12:56](https://amzn-aws.slack.com/archives/D01D5TA374N/p1729108566881649)
```

### Expected behavior

MLlama should work with FSDP mix-precision","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 4101623725, 'node_id': 'LA_kwDOCUB6oc70ec-t', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/PyTorch%20FSDP', 'name': 'PyTorch FSDP', 'color': 'B60205', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",8,open
Support AcceleratorConfig.use_stateful_dataloader in Trainer ,"# What does this PR do?

This PR does the following:

1. Add a new field to `TrainerArguments.AcceleratorConfig`, `use_stateful_dataloader` which when set to true passes through `use_stateful_dataloader` to the DataLoaderConfiguration used to initialize the Trainer's backing Accelerator.
2. Add a new field `train_dataloader_state_dict` to TrainerState, which is used to persist the StatefulDataLoader's state_dict at the time of checkpointing.
3. When resuming from checkpoint for a StatefulDataLoader backed Trainer, instead of the training dataloader skipping batches to resume within an epoch, have the train dataloader load the state_dict derived from the loaded train_dataloader_state_dict.

This PR was tested through the following:

1. Unit test `TrainerIntegrationTest.test_train_and_eval_dataloaders_with_use_stateful_dataloader`, which is a sanity test on the dataloaders used.
2. Unit test `TrainerIntegrationTest.test_resume_training_with_stateful_dataloaders`, which mirrors the other `test_resume_training.*` tests, asserting that resuming from checkpoint behaves sensibly, and that the saved checkpoints do contain saved state_dicts.

Minor changes: this PR adds dependencies on `accelerate>=1.0.0` and `torchdata>=0.8.0` to `transformers` and a minor issue with Trainer test cases

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #31441


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?


## Who can review?
@muellerzr and @SunMarc

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],7,open
LLaVA-NeXT: add new model checkpoints,"# What does this PR do?

Enables support for new llava next video checkpoints trained in SigLIP and Qwen2
","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",3,open
feat: add support for tensor parallel using Pytorch,"# What does this PR do?

1. Add `apply_tensor_parallel` API to apply TP plan to Llama and Granite models
2. Introduce `tp_size` user facing argument to be further consumed by accelerate (see https://github.com/huggingface/accelerate/pull/3173)

Please review in conjunction with https://github.com/huggingface/accelerate/pull/3173

Fixes https://github.com/huggingface/transformers/issues/32470

# Results

See significant improvement in both memory and throughput compared against single gpu training, and FSDP across different settings (checkpointing on/off) and context lengths.

**Note:** Please be aware that the effective TPS for FSDP would be multiplicative of the parallel factor (number of GPUs/devices engaged in distributed training) whereas that is not the case with TP. Therefore, when effective throughput is considered we can find FSDP is better than TP in terms of throughput. However, that **may** be compensated by increasing the batch size utilizing the memory gains etc. 

Done on two models
1. ibm-granite/granite-8b-code-base-128k 
3. codellama/CodeLlama-7b-hf

Tables below show the max cuda memory and throughput for various configurations showing the potential of TP contributed in this PR. There is gains in both memory and throughput.

Model | Method | # of GPUs | Context Length | Batch Size | Grad Checkpointing | Cuda Max Mem (GiB) | Tokens/Sec/GPU
-- | -- | -- | -- | -- | -- | -- | --
ibm-granite/granite-8b-code-base-128k | Single GPU non-distributed | 1 | 8192 | 1 | FALSE | OOM | NA
ibm-granite/granite-8b-code-base-128k | FSDP | 4 | 8192 | 1 | FALSE | OOM | NA
ibm-granite/granite-8b-code-base-128k | TP (This PR) | 4 | 8192 | 1 | FALSE | 52.4 | 7675.4

Model | Method | # of GPUs | Context Length | Batch Size | Grad Checkpointing | Cuda Max Mem (GiB) | Tokens/Sec/GPU
-- | -- | -- | -- | -- | -- | -- | --
ibm-granite/granite-8b-code-base-128k | Single GPU non-distributed | 1 | 8192 | 1 | TRUE | OOM | NA
ibm-granite/granite-8b-code-base-128k | FSDP | 4 | 8192 | 1 | TRUE | 29.975586 | 2256.896
ibm-granite/granite-8b-code-base-128k | TP (This PR) | 4 | 8192 | 1 | TRUE | 26.5 | 5935.5

Model | Method | # of GPUs | Context Length | Batch Size | Grad Checkpointing | Cuda Max Mem (GiB) | Tokens/Sec/GPU
-- | -- | -- | -- | -- | -- | -- | --
ibm-granite/granite-8b-code-base-128k | Single GPU non-distributed | 1 | 16384 | 1 | FALSE | OOM | NA
ibm-granite/granite-8b-code-base-128k | FSDP | 4 | 16384 | 1 | FALSE | OOM | NA
ibm-granite/granite-8b-code-base-128k | TP (This PR) | 4 | 16384 | 1 | FALSE | OOM | NA

Model | Method | # of GPUs | Context Length | Batch Size | Grad Checkpointing | Cuda Max Mem (GiB) | Tokens/Sec/GPU
-- | -- | -- | -- | -- | -- | -- | --
ibm-granite/granite-8b-code-base-128k | Single GPU non-distributed | 1 | 16384 | 1 | TRUE | OOM | NA
ibm-granite/granite-8b-code-base-128k | FSDP | 4 | 16384 | 1 | TRUE | 36.8 | 2084.864
ibm-granite/granite-8b-code-base-128k | TP (This PR) | 4 | 16384 | 1 | TRUE | 33.5 | 5692.5


Model | Method | # of GPUs | Context Length | Batch Size | Grad Checkpointing | Cuda Max Mem (GiB) | Tokens/Sec/GPU
-- | -- | -- | -- | -- | -- | -- | --
codellama/CodeLlama-7b-hf | Single GPU non-distributed | 1 | 8192 | 1 | FALSE | OOM | NA
codellama/CodeLlama-7b-hf | FSDP | 4 | 8192 | 1 | FALSE | 70.7 | 3560
codellama/CodeLlama-7b-hf | TP (This PR) | 4 | 8192 | 1 | FALSE | 42.8 | 9216

Model | Method | # of GPUs | Context Length | Batch Size | Grad Checkpointing | Cuda Max Mem (GiB) | Tokens/Sec/GPU
-- | -- | -- | -- | -- | -- | -- | --
codellama/CodeLlama-7b-hf | Single GPU non-distributed | 1 | 8192 | 1 | TRUE | 75.3 | 2849
codellama/CodeLlama-7b-hf | FSDP | 4 | 8192 | 1 | TRUE | 26.4 | 5957
codellama/CodeLlama-7b-hf | TP (This PR) | 4 | 8192 | 1 | TRUE | 21.4 | 7125

Model | Method | # of GPUs | Context Length | Batch Size | Grad Checkpointing | Cuda Max Mem (GiB) | Tokens/Sec/GPU
-- | -- | -- | -- | -- | -- | -- | --
codellama/CodeLlama-7b-hf | Single GPU non-distributed | 1 | 16384 | 1 | FALSE | OOM | NA
codellama/CodeLlama-7b-hf | FSDP | 4 | 16384 | 1 | FALSE | OOM | NA
codellama/CodeLlama-7b-hf | TP (This PR) | 4 | 16384 | 1 | FALSE | OOM | NA

Model | Method | # of GPUs | Context Length | Batch Size | Grad Checkpointing | Cuda Max Mem (GiB) | Tokens/Sec/GPU
-- | -- | -- | -- | -- | -- | -- | --
codellama/CodeLlama-7b-hf | Single GPU non-distributed | 1 | 16384 | 1 | TRUE | 75.3 | 2599
codellama/CodeLlama-7b-hf | FSDP | 4 | 16384 | 1 | TRUE | 30.1 | 2433
codellama/CodeLlama-7b-hf | TP (This PR) | 4 | 16384 | 1 | TRUE | 26.6 | 6873


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

- trainer: @muellerzr and @SunMarc 
- @SeungyounShin (happy to include as co author - https://github.com/huggingface/transformers/pull/32597)

I have cycles to bring in more improvements over this PR to bring in Pytorch TP support to HF. Looking forward. Thank you

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
",[],2,open
Update stale.py,"Logging: Used the logging module for more structured logging.
Function Decomposition: Split functionality into smaller functions (close_stale_issues, should_close_issue, close_issue, should_mark_stale, mark_issue_stale).
Constants: Defined the stale comment message as a constant for easy modification.
Type Annotations: Added type hints for better clarity.

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Update language_modeling.py,"Initialization and Caching: Each dataset class can read data from files, cache processed data for efficiency, and ensure tokenization is performed correctly.
Line-by-Line Processing: The LineByLineTextDataset and LineByLineWithRefDataset classes support processing text files line by line, while also allowing for reference files when needed.
Sentence Order Prediction: The LineByLineWithSOPTextDataset is specifically designed for sentence order prediction tasks, making it versatile for various NLP applications.

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Adds the Molmo vision-language models,"# What does this PR do?

Adds the Molmo vision language models. This code is intended to replace the remote code used for the molmo models, such as [here](https://huggingface.co/allenai/Molmo-7B-D-0924).

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?",[],1,open
Add support for GOT-OCR2.0,"### Model description

As an OCR-2.0 model, GOT can handle all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Implementation: https://github.com/Ucas-HaoranWei/GOT-OCR2.0/
Paper: https://arxiv.org/abs/2409.01704","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",11,open
Image-Text-to-Text Support in Transformers Pipeline,"### Feature request

Implement the new feature to support a pipeline that can take both an image and text as inputs, and produce a text output. This would be particularly useful for multi-modal tasks such as visual question answering (VQA), image captioning, or image-based text generation.

```python
from transformers import pipeline

# Initialize the pipeline with multi-modal models
multi_modal_pipeline = pipeline(""image-text-to-text"", model=""meta-llama/Llama-3.2-11B-Vision-Instruct"")

# Example usage
messages = [
    {""role"": ""user"", ""content"": [
        {""type"": ""image""},
        {""type"": ""text"", ""text"": ""If I had to write a haiku for this one, it would be: ""}
    ]}
]
result = multi_modal_pipeline(messages )
print(result)  # Should return an answer or relevant text based on the image and question
```

### Motivation

- Simplifies workflows involving multi-modal data.
- Enables more complex and realistic tasks to be handled with existing Transformer models.
- Encourages more multi-modal model usage in research and production.

### Your contribution

 **Transformers Integration**
Ensure that the pipeline works well within the Hugging Face Transformers library:

- Implement the custom pipeline class (`ImageTextToTextPipeline`).
- Add support for handling different data types (image, text) and ensure smooth forward pass execution.

```python
class ImageTextToTextPipeline(Pipeline):
  ....
  
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Add support for Florence-2,"# What does this PR do?

This PR adds support for Florence-2.

Compared to the existing [remote code](https://huggingface.co/microsoft/Florence-2-base/blob/main/modeling_florence2.py) the main difference is removal of `MySequential`, removal of `einops.rearrange` and `trunc_normal_` and `DropPath` are copied from `timm`.

Fixes #34155

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
",[],7,open
Add support for Florence-2,"### Feature request

Add support for [Florence-2](https://huggingface.co/microsoft/Florence-2-large)

### Motivation

Currently requires `trust_remote_code=True`

### Your contribution

I can submit a PR.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Use safetensors from PR,,[],0,open
TimeSeriesTransformerModel: add no time lags option,"### Feature request

`TimeSeriesTransformerModel` has no option to insert no lags in the model. Setting `lags_sequence = []` in the `config` will return an error. An alternative to circumvent the problem, i.e. `lags_sequence = [0]` will result in having two identical copies of the temporal input (`lagged_sequence` and `time_feat`) to be concatenated in the `temporal_input`. 

### Motivation

Would make the model more user-friendly in cases where no additional lag is needed.

### Your contribution

A simple implementation would entail adding an `if len(lags_sequence) > 0` clause prior to adding the subsequences to the input tensor [here](https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1292).","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Differential Attention implementation for BERT.,"### Feature request

The paper ""Differential Transformers"" implements a differential attention mechanism which calculates the attention scores as the difference between two separate softmax attention maps leading to better long-context modeling and key information retrieval.

LINK : [Paper](https://arxiv.org/pdf/2410.05258)

### Motivation

Although the paper focuses on decoder only models, I think this differential attention mechanism might be helpful with encoder only models as well.

### Your contribution

I can work on this feature, if the HuggingFace team considers it as a valuable addition.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Request more specific info from bug reporters when opening deepspeed issues,"### Feature request

Hi!

I would like the bug reporters to be prompted (or have section to fill in the reports template) to provide `ds_report` info and `zero3` config when opening a bug report related to deepspeed integration (maybe it could be more general). Anything to make sure these bits of info are more likely to included upfront would make some of these issues much more actionable.

### Motivation

I've been looking at some deepspeed integration bugs lately (#28808,#29348,#31867), I noticed that often more deepspeed info has to be requested. I was wondering if some specific (and maybe **BOLDED**)  guidelines about what info to provide would go a long way when opening bug reports. I think a reminder to include `zero configs` and `ds_report` might be helpful. I believe this is particularily a pitfall for stuff that is often parsed in (configs, etc).  
Something like:

### Reproduction
Please provide a code sample that reproduces the problem you ran into. It can be a Colab link or just a code snippet.
If you have code snippets, error messages, stack traces please provide them here as well.
Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.
*If you are opening an issue related to one of the following please ensure the this info is included in your reproduction script:
Deepspeed - zero3 config, ds_report output,
Trainer - your trainer config file,
etc.*

@ArthurZucker @amyeroberts ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Incorrect average calculation in `Perplexity of fixed-length models`,"### System Info

- `transformers` version: 4.44.0
- Platform: macOS-14.6-arm64-arm-64bit
- Python version: 3.10.14
- Huggingface_hub version: 0.24.5
- Safetensors version: 0.4.3
- Accelerate version: 0.33.0
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.4.0 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No

### Who can help?

@ArthurZucker @stevhliu 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

(This takes longer than 30s, I apologize. But the example has a lot of data and I wanted to minimally change the provided code to demonstrate the point. Note this is the pytorch version.). 

1. [The example script for finding the perplexity of fixed length](https://huggingface.co/docs/transformers/perplexity) model using strided windows does not properly calculate the average negative log-likelihood for each token aggregated over all the strided context windows. The first context window has the maximum allowable size, which is 1024. That amounts to 1023 targets. The remaining windows have 511 targets, except the final window which has 411. The way the average is calculated assumes that the same number of targets are considered in each strided context. While this results in a minor difference (roughly 0.01 higher than it should be) in the case shown, it leads to much larger issues with shorter texts. 
2. Run the following [colab script](https://drive.google.com/file/d/13jCCj85-MOw4bhRTWJ8iyFvs4ApBg9R2/view?usp=sharing) (I don't have access to GPUs on colab, so I ran this on mac with `mps` with the exact same notebook). 
3. Compare the printed `ppl` value which is `16.44` to the reported one which is `16.45`

### Expected behavior

The value of `ppl` should reflect the average negative log-likelihood for each token across the entire corpus. ","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Refactor use gather object,"# What does this PR do?

This is the extension PR of https://github.com/huggingface/transformers/pull/31514

Since there is rollback function at the end of the evaluation process. https://github.com/huggingface/transformers/blob/617b21273a349bd3a94e2b3bfb83f8089f45749b/src/transformers/trainer.py#L4161

We need to change the `gather_for_metrics` original

There is another way to handle of this error. We can eliminate the line of 
```
        # After all calls to `.gather_function`, reset to `gather_for_metrics`:
        self.gather_function = self.accelerator.gather_for_metrics
```

@SunMarc @muellerzr Is there specific reason for the line?

cc. @qubvel 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
How to specific customized force_token_ids in whisper,"```
ValueError: A custom logits processor of type <class 'transformers.generation.logits_process.ForceTokensLogitsProcessor'> with values <transformers.generation.logits_process.ForceTokensLogitsProcessor object at 0x7f4230cfac50> has been passed to `.generate()`, but it has already been created with the values <transformers.generation.logits_process.ForceTokensLogitsProcessor object at 0x7f422829c510>. <transformers.generation.logits_process.ForceTokensLogitsProcessor object at 0x7f422829c510> has been created by passing the corresponding arguments to generate or by the model's config default values. If you just want to change the default values of logits processor consider passing them as arguments to `.generate()` instead of using a custom logits processor
```

this way don't work:

```
inputs = inputs.to(self.model.dtype)
        with torch.no_grad():
            if forced_decoder_ids is not None:
                generated_ids = self.model.generate(
                    inputs, forced_decoder_ids=forced_decoder_ids
                )
            else:
                generated_ids = self.model.generate(inputs)
```","[{'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",3,open
add qwen2vl for sequence classification,"# What does this PR do?
Adds sequence classification for qwen2-vl. This work was done because there are currently no way to do text-image classification in transformers. This is useful for rerankers, reward models etc. Mostly copied and stitched together from `Qwen2VLForConditionalGeneration` and `LLamaForSequenceClassification.`

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker I saw you review the original qwen2 vl PR, so would love for you to review this also. Let me know if you think I need to do some refactoring or other things.


<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
Add diffllama,"# What does this PR do?

This PR adds the codes for the DiffLlama, which is Llama model with Differential Transformer. Please refer to [Differential Transformer](https://arxiv.org/abs/2410.05258). @ArthurZucker","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",28,open
Add TimesFM Time Series Forecasting Model,"# What does this PR do?

This PR adds a new model, TimesFM, to HuggingFace. TimesFM is a time series forecasting model based on the transformer architecture. It was proposed in [A decoder-only foundation model for time-series forecasting](https://arxiv.org/html/2310.10688v2). 

## Code Example
```
import numpy as np
from transformers import TimesFMModel
from transformers import TimesFMConfig

config = TimesFMConfig()
model = TimesFMModel(config)

forecast_input = [
    np.sin(np.linspace(0, 20, 100)),
    np.sin(np.linspace(0, 20, 200)),
    np.sin(np.linspace(0, 20, 400)),
]
frequency_input = [0, 1, 2]

results = model(
    inputs=forecast_input,
    freq=frequency_input,
)
```


# TODOs
1. Unit Tests
2. Documentations and doc strings
3. Pretrained weights 
4. ONNX definition
5. Model parallelism 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Adding support for OpenLMForCausalLM from DataComp,"# What does this PR do?

Given the work from DataComp-LM: [https://arxiv.org/abs/2406.11794](url), this adds support for their architecture: OpenLMForCausulLM.

## Before submitting
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
Add support for Aria model,"### Feature request

Hi,
Would it be possible to add support for the [Aria model](https://huggingface.co/rhymes-ai/Aria)?
Thanks!

### Motivation

Aria is not supported without custom code

### Your contribution

N/A","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",2,open
Add MinGRU,"# What does this PR do?

Add `MinGRU` from the paper [Were RNNs all we needed?](https://arxiv.org/abs/2410.01201) by Leo Feng, Frederick Tung, Mohamed Osama Ahmed, Yoshua Bengio, and Hossein Hajimirsadegh.

MinGRU is a novel recurrent architecture such as S4, Mamba, and Aaren which (i) no longer need to BPTT and can be efficiently trained in parallel and (ii) use significantly fewer parameters than their traditional GRU counterpart. MinGRU is 175x faster than GRU for a sequence length of 512 while matching the empirical performance of recent sequence models.

The authors do not provide the original code nor the checkpoint of the model they trained to carry out the Language Modelling experiments in Section 4.3 of the paper. Still, the results are very promising. So, I also have trained a very small causal language model using the [Shakespeare dataset](https://huggingface.co/datasets/karpathy/tiny_shakespeare) they use in the paper and uploaded the weights to the [hub](https://huggingface.co/symanto/mingru-shakespeare).

Which I have done for `MinGRU` is listed below:

- [X] ""Fix"" an overflow issue in the parallel scan code from [Appendix B.1 Log-space Implementation](https://arxiv.org/pdf/2410.01201)
- [X] Added the Torch model skeleton to 🤗 Transformers for `Encoder`, `ForSequenceClassification`, `ForTokenClassification` and `ForCausalLM`
- [X] Ran forward() pass in 🤗 Transformers for all the model heads
- [X] Trained `MinGRU` for sequence classification, token classification, and causal language modeling using 🤗 Transformers' Trainer
- [X] Added tokenizer in 🤗 Transformers (~same as reported in the paper -BPE from GPT2-)
- [X] Uploaded model weights to the Hub

Some things that are pending and maybe I will need some help:

- [ ] Finish docstrings (`MinGRUConfig` and `MinGRUModel` docstrings do not match their signature, so the quality test fails)
- [ ] Attention masks are not implemented (especially useful for `SequenceClassification`, e.g., when pooling last then pick the last non-pad token)
- [ ] Cache is not implemented for decoding

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ArthurZucker

Anyone in the community is free to review the PR once the tests have passed.
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Sync token-classification pipeline with Hub spec,"One unusual change this time: The TokenClassificationPipeline put the docstrings in an odd place, with a decorator. As far as I can tell, this isn't necessary at all - I just put them in the normal place (which also un-breaks my compatibility checker).

cc @wauplin - there's an output discrepancy for the `zero-shot` pipeline, but I can't figure out what output format the Hub spec actually wants when `multi-label` is `True`. Where did that Hub spec come from?",[],2,open
Fix Llama QnA `from_pretrained`,"# What does this PR do?
Fixes issues with llama for qna when using `from_pretrained` to load any base model. Currently, when we load any llama model, we get a 100% mismatch (i.e. everything is randomly initialized). The workaround is to manually save the (base) model and then load it from disk (ref. #30381):
```python
from transformers import AutoModel, AutoModelForQuestionAnswering
model = AutoModel.from_pretrained(""meta-llama/llama-2-7b-hf"")
model.save_pretrained(""base_model"")
model = AutoModelForQuestionAnswering.from_pretrained(""base_model"")
```
This is very unintuitive and goes against the usage of auto classes and from_pretrained which should be quick and easy. 

I tried to use loading hooks in #34038 to keep it BC but to no avail (I could fool the error messages tho :D). So it might be breaking older versions which I think is warranted to ensure having an easy `from_pretrained` call instead. #29258 tried the same at first to and then opted for a version which doesn't work as expected.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@ArthurZucker ",[],5,open
Object detection training for Owl-vit/Owlv2,"Related to #33664

",[],1,open
"trainer resume from checkpoint，the learning rate is not the same as retraining,learning rate is discontinuous","### System Info

- Platform: Windows-10
- transformers version: 4.43.4
- Python version: 3.10.11
- PyTorch version (GPU?): 2.3.1+cu121

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

the Trainer does not set a warmup and the lr_scheduler is set to linear, and the training is continued from an interruption to complete all steps, the learning rates will be different from those when training all steps from the beginning. Here are the specific learning rates:

Learning rates for training from the beginning for each step:
- Step 1: ""learning_rate"": 1e-05,
- Step 2: ""learning_rate"": 1e-05,
- Step 3: ""learning_rate"": 9e-06,
- Step 4: ""learning_rate"": 8.000000000000001e-06,
- Step 5: ""learning_rate"": 7e-06,
- Step 6: ""learning_rate"": 6e-06,
- Step 7: ""learning_rate"": 5e-06,
- Step 8: ""learning_rate"": 4.000000000000001e-06,
- Step 9: ""learning_rate"": 3e-06,
- Step 10: ""learning_rate"": 2.0000.

If training is continued from a checkpoint at step 5, the learning rates for each step are:
- Step 6: ""learning_rate"": 7e-06,
- Step 7: ""learning_rate"": 7e-06,
- Step 8: ""learning_rate"": 6e-06,
- Step 9: ""learning_rate"": 5e-06,
- Step 10: ""learning_rate"": 4.000000000000001e-06.

Why are the learning rates for step 6 and step 7 different when training continues from a checkpoint compared to training from the start?


Reproduction steps:
1. Train from the beginning for 10 steps, save a checkpoint for each step, and record the learning rate in each step.
2. Delete the checkpoints for steps 6 through 7 in the folder.
3. Then use `trainer.train(resume_from_checkpoint=True)` to continue training from step 5, and after training is completed, record the learning rate in the new checkpoint.


### Expected behavior

Please explain why the learning rate is not continuous as it is when training from the beginning, for example:
Step 6: ""learning_rate"": 6e-06,
Step 7: ""learning_rate"": 5e-06.
........","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",19,open
lr inconsistency,"# What does this PR do?


Fixes  #28124 


","[{'id': 4608548278, 'node_id': 'LA_kwDOCUB6oc8AAAABErDdtg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/HACKTOBERFEST-ACCEPTED', 'name': 'HACKTOBERFEST-ACCEPTED', 'color': 'FF5733', 'default': False, 'description': ''}]",13,open
Support for torch._dynamo.export for Phi3,"### Feature request

Compared to `symbolic_trace`, the new (but I assume, experimental) entrypoint in `torch._dynamo.export` seems to provide a more robust way to extract modular FX graphs, that can't have any graph breaks.
I have been experimenting with some networks (Pythia, OPT, Llama, Mistral), and they all go through.

It seems that Phi3 breaks because of this line:
https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/src/transformers/models/phi3/modeling_phi3.py#L213


Where `self.inv_freq` is redefined at runtime in the forward pass.
This is a bit confusing, and I would recommend to drop `self` and use a `normal` runtime variable.
I'm not sure if this has potential side effects.

A similar patter seems to be repeated in other Embedding classes in Phi3.

To reproduce:
```python
model = AutoModelForCausalLM.from_pretrained(""microsoft/Phi-3.5-mini-instruct"")
model, guards = torch._dynamo.export(model)(**model.dummy_inputs)
```

@gante @ArthurZucker 

### Motivation

Dropping the reference to `self.inv_freq` would allow to obtain a fullgraph with dynamo.
Having full FX graph is also a requirement for torch.export, although I have not tested that API.

### Your contribution

I can't directly contribute with a PR at the moment.
I could test a PR from my side to check compatibility with dynamo and potential side effects, once the PR is open.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7439087267, 'node_id': 'LA_kwDOCUB6oc8AAAABu2d2ow', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Deployment', 'name': 'Deployment', 'color': '420E9C', 'default': False, 'description': ''}]",2,open
[Don't merge] check from external contributor,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Add support for Apple's Depth-Pro,"### Model description

**Depth Pro: Sharp Monocular Metric Depth in Less Than a Second.**

Depth Pro synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

- Research Paper: [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/pdf/2410.02073)
- Authors: [Aleksei Bochkovskii](https://arxiv.org/search/cs?searchtype=author&query=Bochkovskii,+A), [Amaël Delaunoy](https://arxiv.org/search/cs?searchtype=author&query=Delaunoy,+A), and others
- Implementation: [apple/ml-depth-pro](https://github.com/apple/ml-depth-pro)
- Models Weights: [apple/DepthPro](https://huggingface.co/apple/DepthPro)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",5,open
"Add Loss Functions for QFormer Training in BLIP-2 Model (ITC, ITM, and ITG)","### Feature request

I propose adding a loss calculation for QFormer training in the BLIP-2 model. Implementing this feature would allow fine-tuning the QFormer and language models for image-text retrieval and captioning tasks, which is crucial for practical applications.

### Motivation

I want to train the BLIP-2 model using the transformers library. In particular, loss functions for Image-Text Contrastive (ITC), Image-Text Matching (ITM), and Image-grounded Text Generation(ITG) are not included, which requires users to manually implement the loss functions.

### Your contribution

I would like to contribute to this open-source project by implementing the loss functions.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",1,open
PeftModel is not an instance of PreTrainedModel.  `No liger kernels will be applied.`,"### System Info

`transformers==4.45.1`
`peft==0.13.0`
`liger-kernel==0.3.1`


So `isinstance(model.base_model.model, PreTrainedModel)` returns true but `isinstance(model, PreTrainedModel)` returns false so no liger kernels are used.

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Load any model with a PeftModel wrapper via `get_peft_model` and try to run with the `use_liger_kernel` flag in the trainer.

### Expected behavior

Apply liger kernels. Be as simple as add a check for peft models that then checks `model.base_model.model`","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6712797012, 'node_id': 'LA_kwDOCUB6oc8AAAABkB0nVA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/optimization', 'name': 'optimization', 'color': '47E0D2', 'default': False, 'description': ''}, {'id': 6870118306, 'node_id': 'LA_kwDOCUB6oc8AAAABmX2vog', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/PEFT', 'name': 'PEFT', 'color': '93794B', 'default': False, 'description': ''}]",6,open
Refactor StableLM using modular,"# What does this PR do?

Refactor StableLM to remove all the copied-from and use modular instead.
",[],1,open
Enabled Flash Attention for PaliGemma models,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #33963 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@qubvel 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",9,open
[WIP] Add Gemma2 GGUF support,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],4,open
"from_pretrained's torch_dtype ""auto"" mode doesn't handle nested models","### Feature request

It would be nice if when setting `torch_dtype` to `auto` when calling `from_pretrained`, it properly respects _nested_ `torch_dtype`s specified in the model's config.  Right now it only pulls the dtype from the root config.

### Motivation

`from_pretrained` has a `torch_dtype` argument which can be set to `auto`.  The documentation says:  `""auto"" - A torch_dtype entry in the config.json file of the model will be attempted to be used.`  However it looks like the code doesn't handle the case where `config.json` contains nested models, for example in the case of `LlavaForConditionalGeneration` which contains two children models under the keys `text_config` and `vision_config` which each can have `torch_dtype` set separately and differently from the parent config.

In my use case, I'd like to have a `LlavaForConditionalGeneration` which is loaded in as `float32` except for `text_config` which I want loaded in as `bfloat16`.  Here is the code I'm debugging the issue with:

```
m = LlavaForConditionalGeneration.from_pretrained(""./saved_models/llama-joycaption-alpha-two-hf-llava"", torch_dtype=""auto"")
print(""LlavaConfig"")
print(""-"" * 80)
print(m.config)
print(""-"" * 80)
print()

params_by_dtype = defaultdict(list)
for k, v in m.state_dict().items():
	params_by_dtype[v.dtype].append(k)

print(""Params by dtype"")
print(""-"" * 80)
for dtype, params in params_by_dtype.items():
	print(f""{dtype}:"")
	for p in params:
		print(f""  {p}"")
	print()
	print(""-"" * 80)
	print()
```

The config that gets printed:

```
LlavaConfig {
  ""_name_or_path"": ""./saved_models/llama-joycaption-alpha-two-hf-llava"",
  ""architectures"": [
    ""LlavaForConditionalGeneration""
  ],
  ""ignore_index"": -100,
  ""image_seq_length"": 729,
  ""image_token_index"": 128077,
  ""model_type"": ""llava"",
  ""projector_hidden_act"": ""gelu"",
  ""text_config"": {
    ""_name_or_path"": ""unsloth/Meta-Llama-3.1-8B-Instruct"",
    ""architectures"": [
      ""LlamaForCausalLM""
    ],
    ""bos_token_id"": 128000,
    ""eos_token_id"": [
      128001,
      128008,
      128009
    ],
    ""intermediate_size"": 14336,
    ""max_position_embeddings"": 131072,
    ""model_type"": ""llama"",
    ""num_key_value_heads"": 8,
    ""pad_token_id"": 128004,
    ""rms_norm_eps"": 1e-05,
    ""rope_scaling"": {
      ""factor"": 8.0,
      ""high_freq_factor"": 4.0,
      ""low_freq_factor"": 1.0,
      ""original_max_position_embeddings"": 8192,
      ""rope_type"": ""llama3""
    },
    ""rope_theta"": 500000.0,
    ""torch_dtype"": ""bfloat16"",
    ""unsloth_version"": ""2024.9"",
    ""vocab_size"": 128256
  },
  ""torch_dtype"": ""float32"",
  ""transformers_version"": ""4.44.2"",
  ""vision_config"": {
    ""hidden_act"": ""gelu_pytorch_tanh"",
    ""hidden_size"": 1152,
    ""image_size"": 384,
    ""intermediate_size"": 4304,
    ""layer_norm_eps"": 1e-06,
    ""model_type"": ""siglip_vision_model"",
    ""num_attention_heads"": 16,
    ""num_hidden_layers"": 27,
    ""patch_size"": 14
  },
  ""vision_feature_layer"": -2,
  ""vision_feature_select_strategy"": ""full""
}
```

Based on the rest of what gets printed (which is too long to include here) all parameters get loaded as `float32`.

### Your contribution

As far as I can tell, calling `from_pretrained` loads the entire model at once; it doesn't call any logic in a nested way.  The auto detection logic seems to start here:

https://github.com/huggingface/transformers/blob/1bd604d11c405dfb8b78bda4062d88fc75c17de0/src/transformers/modeling_utils.py#L3988

Which just sets a single dtype; it doesn't analyze to check for any nested dtypes.  Then the model weights get loaded:

https://github.com/huggingface/transformers/blob/1bd604d11c405dfb8b78bda4062d88fc75c17de0/src/transformers/modeling_utils.py#L4181

`_load_pretrained_model` doesn't seem to have any logic to flexibly load different dtypes.

So I'm not sure how to fix/improve this logic short of major surgery to `_load_pretrained_model`?","[{'id': 1834056761, 'node_id': 'MDU6TGFiZWwxODM0MDU2NzYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Modeling', 'name': 'Core: Modeling', 'color': 'FF8446', 'default': False, 'description': 'Internals of the library; Models.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
force conversion for Llama when `legacy` is set to `False`,"# What does this PR do?
Fixes #31513",[],1,open
"../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [267,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.","### System Info

- `transformers` version: 4.44.0
- Platform: Linux-5.4.0-196-generic-x86_64-with-glibc2.31
- Python version: 3.12.0
- Huggingface_hub version: 0.23.4
- Safetensors version: 0.4.3
- Accelerate version: 0.31.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: yes, trainer helps me handle this.
- Using GPU in script?: yes, single gpu works ok, but multi-gpu cause problem
- GPU type: NVIDIA GeForce RTX 4090

### Who can help?

@muellerzr @SunMarc

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

The multi-card training on the 20 series and 30 series works fine. However, when using multiple 4090 cards for training, the following error occurs: ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [267,0,0], thread: [20,0,0] Assertion srcIndex < srcSelectDimSize failed. But there is no issue when using a single 4090 card.

1. I use chatgpt to generate a test training code on my environment setting, the code shows below:
```
from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments
from datasets import Dataset

model_name = ""bert-base-uncased""
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.resize_token_embeddings(len(tokenizer))

data = {
    ""text"": [""This is a positive example"", ""This is a negative example""] * 50,
    ""label"": [1, 0] * 50  
}

dataset = Dataset.from_dict(data)

def preprocess_function(examples):
    return tokenizer(examples['text'], padding=""max_length"", truncation=True, max_length=64)

encoded_dataset = dataset.map(preprocess_function, batched=True)


training_args = TrainingArguments(
    output_dir=""./results"",
    per_device_train_batch_size=8,  
    num_train_epochs=1,  
    logging_dir=""./logs"",
    logging_steps=10,
    evaluation_strategy=""no""
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset,
)

trainer.train()

print(""Training completed."")
```
2. When I use only one gpu, that's:
```
CUDA_VISIBLE_DEVICES=5 NCCL_P2P_DISABLE=""1"" NCCL_IB_DISABLE=""1"" python test.py
```
Everything is ok.

3. When I use multi-gpu, that's:
```
CUDA_VISIBLE_DEVICES=5,6 NCCL_P2P_DISABLE=""1"" NCCL_IB_DISABLE=""1"" python test.py
```
Error comes:
/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 5947.60 examples/s]
/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|                                                                                                                                                                                            | 0/7 [00:00<?, ?it/s]/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 14%|█████████████████████████▋                                                                                                                                                          | 1/7 [00:03<00:22,  3.74s/it]../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [126,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [126,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
......
../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [159,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
Traceback (most recent call last):
  File ""/home/jihuawei2/projects/AceRead/model/test.py"", line 47, in <module>
    trainer.train()
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/transformers/trainer.py"", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/transformers/trainer.py"", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/transformers/trainer.py"", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/transformers/trainer.py"", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py"", line 185, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py"", line 200, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py"", line 108, in parallel_apply
    output.reraise()
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/_utils.py"", line 705, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py"", line 83, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py"", line 1695, in forward
    outputs = self.bert(
              ^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py"", line 1107, in forward
    extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jihuawei2/miniconda3/envs/main/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py"", line 449, in _prepare_4d_attention_mask_for_sdpa
    if not is_tracing and torch.all(mask == 1):
                          ^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


 14%|█▍        | 1/7 [00:04<00:26,  4.38s/it]    
```
### Expected behavior

I believe this error is related to the package for parallel computation in Huggingface Trainer. I hope that eventually, I can achieve multi-card training on the 4090.","[{'id': 1834081910, 'node_id': 'MDU6TGFiZWwxODM0MDgxOTEw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Usage', 'name': 'Usage', 'color': 'e28436', 'default': False, 'description': 'General questions about the library'}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",8,open
Enhancing RoBERTa Robustness through Adversarial Training,"### Feature request

The goal of this feature is to implement Adversarial Training for the RoBERTa model to enhance its robustness against adversarial examples. Adversarial training involves generating perturbed inputs (adversarial examples) during the training phase, allowing the model to learn how to withstand such attacks. This improves the model's generalization and performance on unseen data.

# Implementation Overview:

### **Adversarial Example Generation:** 

1. Methods: Utilize techniques like FGSM (Fast Gradient Sign Method) or PGD (Projected Gradient Descent) 

2. Integration: Modify the training loop to incorporate a step that generates adversarial examples for each batch of data.

 Loss Function Adjustment:

Combine the traditional loss (e.g., cross-entropy) with a loss calculated on the adversarial examples. This can be done using a weighted sum to balance the two components.

Training Procedure:

Modify the training loop:

For each epoch:

1. Generate adversarial examples from the input batch.
2. Compute the loss on both clean and adversarial examples.
3. Update the model weights based on the combined loss.


***Hyperparameter Tuning:***

Introduce parameters such as the adversarial strength (epsilon) and the weighting factor (
𝜆
λ) to adjust the training dynamics and effectiveness.

### Evaluation Metrics:

Evaluate model performance using metrics like accuracy, precision, recall, and F1-score on both clean and adversarial datasets to measure the robustness improvements.

# Link to Paper:

[Adversarial Training for Natural Language Processing](https://arxiv.org/abs/1906.05955)
[Adversarial Examples for Evaluating Reading Comprehension Systems](https://arxiv.org/abs/1904.07236)
[Adversarial Training for Large Neural Language Models](https://arxiv.org/abs/1909.03247)
[Towards Robustness Against Adversarial Attacks in Natural Language Processing](https://arxiv.org/abs/2002.07677)
[Adversarial Training with Natural Language Processing](https://arxiv.org/abs/2103.09582)

### Motivation

The motivation for this feature arises from the increasing importance of model robustness in real-world applications. Many NLP models, including RoBERTa, are vulnerable to adversarial attacks that can lead to significant performance degradation.

Real-World Applications: In critical applications like sentiment analysis, spam detection, or other classification tasks, adversarial inputs can lead to serious consequences, such as misclassification of malicious content.

Frustration with Current Limitations: I often find that while RoBERTa performs excellently on clean datasets, its inability to generalize against adversarial examples hampers its deployment in production. This feature aims to address that gap.



### Your contribution

I would like to contribute to the implementation of adversarial training for the RoBERTa model to enhance its robustness against adversarial attacks. I have reviewed the CONTRIBUTING.md file and am familiar with the contribution guidelines.

# I plan to:

Implement adversarial training techniques based on insights from the relevant research papers.

[**1**] Create test cases to validate the effectiveness of the adversarial training implementation.

[**2**] Update the documentation to include usage examples and instructions for leveraging this feature.

I am excited to submit a Pull Request (PR) once the implementation is complete and ensure it aligns with the project standards.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
🌐 [i18n-KO] Translated `image_text_to_text.md` to Korean,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `image_text_to_text.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? --> 
@harheem, @jeongiin, @Seungahson, @win2dvp21 
<!-- @junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang --> 
<!-- @jungnerd, @cjfghk5697, @yijun-lee, @mreraser --> 
<!-- @heuristicwave, @nuatmochoi, @chhaewxn, @jun048098, @ahnjj, @fabxoe -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
<!-- @stevhliu May you please review this PR? -->",[],0,open
🌐 [i18n-KO] Translated `compressed_tensors.md` to Korean,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `compressed_tensors.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? --> 
@harheem, @jeongiin, @Seungahson, @win2dvp21
<!-- @junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang --> 
<!-- @jungnerd, @cjfghk5697, @yijun-lee, @mreraser --> 
<!-- @heuristicwave, @nuatmochoi, @chhaewxn, @jun048098, @ahnjj, @fabxoe -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
<!-- @stevhliu May you please review this PR? -->",[],0,open
🌐 [i18n-KO] Translated `quantization/overview.md` to Korean,"# What does this PR do?

Translated the `overview.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

@junejae, @Jwaminju, @010kim, @boyunJang 


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?

## Who can review? (Final)
<!-- @stevhliu May you please review this PR? -->",[],0,open
Jitter Noise added to input being passed to experts in Switch Transformers,"### System Info

System Info

- transformers version: 4.44.2
- Platform: Linux-6.5.0-35-generic-x86_64-with-glibc2.35
- Python version: 3.10.14
- Huggingface_hub version: 0.24.6
- Safetensors version: 0.4.5
- Accelerate version: 0.34.2
- Accelerate config: not found
- PyTorch version (GPU?): 2.3.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: No

### Who can help?

@ArthurZucker @younesbelkada 

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
import torch
import torch.nn as nn
from transformers import (
    SwitchTransformersConfig,
    SwitchTransformersTop1Router,
)
from transformers.models.switch_transformers.modeling_switch_transformers import SwitchTransformersDenseActDense


class MySwitchTransformersSparseMLP(nn.Module):
    r""""""
    Implementation of the Switch Transformers Sparse MLP module.
    """"""

    def __init__(self, config: SwitchTransformersConfig, expert_class: nn.Module = SwitchTransformersDenseActDense):
        super().__init__()
        # Step 1: Get the correct router according to its class
        self.router = SwitchTransformersTop1Router(config)

        # Step 2: Get the experts
        self.experts = nn.ModuleDict()
        for idx in range(config.num_experts):
            self.experts[f""expert_{idx}""] = expert_class(config)

    def forward(self, hidden_states):
        r""""""
        Hold on, this will be slightly tricky to understand In the correct order, a MoE layer does the following:

        1- Gets the `router_mask` from the router. The shape of the mask is `(batch_size, sequence_length, num_expert)`
        and corresponds to the argmax of the `router_probs`. The probabilities are needed in the computation of the
        hidden states : they are broadcasted to the hidden states values (can be interpreted as a scaling factor).

        2- Dispatch the tokens to its associated experts. We do a classic for loop over the experts and assign for each
        expert the corresponding hidden states.

        """"""

        prev_save = hidden_states.clone()

        # Step 1: Get the router_mask from the router as wel as the probabilities
        router_mask, router_probs, router_logits = self.router(hidden_states)
        expert_index = torch.argmax(router_mask, dim=-1)

        print(torch.allclose(prev_save, hidden_states))
        print(torch.mean(prev_save - hidden_states))

        # The routers introduced might not always map all the tokens, to a router, which means that some hidden states
        # can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the seleced ones.

        next_states = hidden_states.clone()

        router_mask = router_mask.bool()
        batch_size, seq_len, num_experts = router_mask.shape
        idx_mask = router_mask.transpose(1, 2).reshape(batch_size * seq_len, num_experts).sum(dim=0)
        idx_mask = torch.nonzero(idx_mask, as_tuple=True)[
            0
        ].tolist()  # length: number of ""activated"" expert / value: index
        for idx in idx_mask:
            next_states[router_mask[:, :, idx]] = getattr(self.experts, ""expert_{}"".format(idx))(
                hidden_states[router_mask[:, :, idx]]
            )

        hidden_states = router_probs * next_states
        return hidden_states, (router_logits, expert_index)

config = SwitchTransformersConfig()
model = MySwitchTransformersSparseMLP(config)

model.train()
in_data = torch.ones(1, 1, 768)
out = model(in_data)
```

The output is 
```bash
False
tensor(-0.0001)
```
which ideally should give True and the mean difference should be zero.

This is because in `SwitchTransformersTop1Router`, the `hidden_states` are multiplied with jitter noise which persists even when you pass it to the experts.

https://github.com/huggingface/transformers/blob/e71a01a104dd663c730e494eb0b6467bb51df357/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L159-L161

### Expected behavior

Ideally, no jitter noise should be present when passing the input to the experts, returning True and the mean difference as 0.","[{'id': 1834056761, 'node_id': 'MDU6TGFiZWwxODM0MDU2NzYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Modeling', 'name': 'Core: Modeling', 'color': 'FF8446', 'default': False, 'description': 'Internals of the library; Models.'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",8,open
llama3 not fx traced,"### System Info

- `transformers` version: 4.45.1
- Platform: Linux-5.4.247-162.350.amzn2.x86_64-x86_64-with-glibc2.26
- Python version: 3.10.12
- Huggingface_hub version: 0.24.0
- Safetensors version: 0.4.3
- Accelerate version: 0.32.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.4.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: Tesla V100-SXM2-16GB

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
from transformers.utils.fx import symbolic_trace
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(""meta-llama/Meta-Llama-3.1-8B"")
gm = symbolic_trace(model, input_names=[""input_ids"", ""attention_mask"", ""past_key_values""])
```

```bash
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/utils/fx.py"", line 1503, in symbolic_trace
    traced_graph = tracer.trace(model, concrete_args=concrete_args)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/utils/fx.py"", line 1326, in trace
    self.graph = super().trace(root, concrete_args=concrete_args)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 822, in trace
    (self.create_arg(fn(*args)),),
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"", line 1189, in forward
    outputs = self.model(
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 800, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/utils/fx.py"", line 1190, in call_module
    return super().call_module(m, forward, args, kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 518, in call_module
    ret_val = forward(*args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 793, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"", line 1000, in forward
    layer_outputs = decoder_layer(
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 800, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/utils/fx.py"", line 1190, in call_module
    return super().call_module(m, forward, args, kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 518, in call_module
    ret_val = forward(*args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 793, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"", line 729, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 800, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/utils/fx.py"", line 1190, in call_module
    return super().call_module(m, forward, args, kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 518, in call_module
    ret_val = forward(*args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 793, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"", line 646, in forward
    if query_states.device.type == ""cuda"" and causal_mask is not None:
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/utils/fx.py"", line 669, in __bool__
    return super().__bool__()
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/proxy.py"", line 447, in __bool__
    return self.tracer.to_bool(self)
  File ""~/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/torch/fx/proxy.py"", line 307, in to_bool
    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')
torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow
```


### Expected behavior

it should run without error. `symbolic_trace()` with `[""input_ids"", ""attention_mask""]` runs fine. However, when [""input_ids"", ""attention_mask"", ""past_key_values""] is fed as  input_names, the error occurs. If using past_key_values is incorrect, it should be warned and aborted before trying to trace the model.

While a fix on a related error (https://github.com/huggingface/transformers/issues/29923) is included in the released version, it seems there is still some bug.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 7644817455, 'node_id': 'LA_kwDOCUB6oc8AAAABx6qoLw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/torch%20fx', 'name': 'torch fx', 'color': '68FC9F', 'default': False, 'description': ''}]",6,open
Flash attention 2 support for PaliGemma model,"### Feature request

Hi,
 
Is it possible to enable flash attention for PaliGemma models?

### Motivation

This feature is required to speed up inference using PaliGemma VLMs

### Your contribution

If someone can point me to the steps required to do this I 'll be happy to help. Is it as simple as enabling [this flag?](https://github.com/huggingface/transformers/blob/main/src/transformers/models/paligemma/modeling_paligemma.py#L195)","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}]",3,open
"Add Molmo (7B-D, 7B-O, 70B)","# What does this PR do?

As mentioned in issue #33710 , this is a draft to add support for Molmo natively in `transformers`. 
It is also using the new `modular` framework introduced in #33248 . 

### Molmo has several existing variants: 
 - MolmoE, a mixture of experts multimodal model, which is not covered in this PR but will be in a follow-up one.
 - Molmo-7B-D, based on Qwen2 + CLIP.
 - Molmo-7B-O, based on a yet to be released Olmo model, and CLIP.
 - Molmo-70B, a scaled up version. 
 
The last three models share the same modeling, and thus will be covered by this PR. 

Relative to the modular framework:

### Choose a base model that's as close as possible from the one you're porting.

In my case, I'm using **Llava** as a reference. The differences I identify at a glance are the 2d pooling, 
### Figure out the differences. 
Some differences will be a complete modification of the original module, in that case, all have to be redefined.
```python
class MolmoMultiModalProjector(LlavaMultiModalProjector):
    def __init__(self, config: MolmoConfig):
        super().__init__()
        self.linear_1 = nn.Linear(
            config.vision_config.hidden_size,
            config.text_config.intermediate_size // 2,
            bias=False,
            )
        self.linear_2 = nn.Linear(
            config.text_config.intermediate_size // 2,
            config.text_config.hidden_size,
            bias=False,
            )
        self.linear_3 = nn.Linear(
            config.vision_config.hidden_size,
            config.text_config.intermediate_size // 2,
            bias=False,
            )
    
    def forward(self, image_features):
        hidden_states = self.linear_1(image_features)
        hidden_states = self.act(hidden_states)
        intermediate_states = self.linear_3(image_features)
        hidden_states = self.linear_2(hidden_states, intermediate_states)
        return hidden_states
```
 


Some differences will be very tiny. For instance, some layers might be the same, but initialized with a different configuration key. 
For instance, the position embeddings are slightly different.
```python
class MolmoVisionEmbeddings(CLIPVisionEmbeddings):
    def __init__(self, config):
        super().__init__()
        self.position_embedding = nn.Embedding(config.num_image_positions, config.hidden_size)
```

### Preserving inheritance across model components renames.

For instance, the code above will trigger 
```bash
python utils/modular_model_converter.py --files_to_parse src/transformers/models/molmo/modular_molmo.py  --old_model_name=""Llava"" --new_model_name=""Molmo""

> ValueError: Unable to find dependencies for CLIPVisionEmbeddings in transformers.models.clip.modeling_clip. Here are the dependencies found: {'molmo_loss': {'contrastive_loss'}, 'MOLMOVisionModelOutput': {'ModelOutput'}, 'MOLMOTextModelOutput': {'ModelOutput'}, 'MOLMOOutput': {'Mod
elOutput'}, 'MOLMOVisionEmbeddings': {'nn.Module'},
```
Because the supported pattern is currently searching for a caps-based model name. However, using `modular` is very promising and makes for a much smaller modeling file to review.

I'll write down hurdles encountered here for future reference so that adding multimodal models to `transformers` ends up being a breeze. 



","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",8,open
Add training support to Meta's EnCodec,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->
Part of [#24295]


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
"Request for Iterative Generation in Pipeline (e.g., LLaMA model)","### Feature request

I would like to ask if there is a way to perform iterative generation (n times) within the pipeline, specifically for models like LLMs. If this feature is not available, is there any plan to implement it in the future?

Example:
```python
pipeline = transformers.pipeline(
            ""text-generation"",
            model=""meta-llama/Llama-3.1-8B-Instruct"",
            model_kwargs={""torch_dtype"": torch.bfloat16},
            device_map=""auto"",
        ) 

# Generate once
outputs = llama_client(
              messages,
              max_new_tokens=max_tokens
          )
# Generate n times
outputs = llama_client(
              messages,
              max_new_tokens=max_tokens,
              n = n
          )
```

Similar GPT API
```python
response = client.chat.completions.create(
            model=model,
            messages=messages, 
            max_tokens=max_tokens,
            temperature=temperature, 
            n=n,  
        )
```

I am also aware that iterative generation can be done using a for loop, but I am wondering if there is a more efficient or optimized way to generate multiple iterations (n times) within the pipeline for models.

https://community.openai.com/t/how-does-n-parameter-work-in-chat-completions/288725


### Motivation

build connection between LLM api and transformer pipeline

### Your contribution

Request","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Major VLM tracker (standardize the API),"### Feature request

This will track general plans on VLM and composite models so that we can align with work in TGI and other libraries. I already have some trackers so in this one I'll lay out a more bigger picture with links to respective discussions/topics

### Motivation

We already have a pretty good working standards when it comes to language models, and when adding a new model usually a few ""copy from"" statements will do the work. We also cover most cases for LMs in out test suite. But for wave of multimodal models we still lack any form of standardization and uniform API. Each new model added to the library introduces something new, that forces us to accept it as is until we figure out how to handle it later

So we need to try to standardize those models, currently starting from VLMs. VLMs are the most commonly added models currently, but we may have more audio+text or pure multimodal ones in the future. For now we start off by working on VLM and see how things fit in the general API

### Your contribution

The major changes we are working on and planning to work are:

- Standardization for Processors:
  - We have ongoing  work on uniform processor kwargs which currently will help us enable pipelines for VLMs and thus we can have correct automodel tag on the hub. The work is under progress by @yonigozlan and @molbap  
  - Parallel to that I will work on separating out video models under a new class (VideoProcessor) and handling a whole lot of deprecation cycle for the processing config files. At the end we should have separate file/separate class for video processing and save its params in its own config file. That will be tracked in https://github.com/huggingface/transformers/issues/33504 and has discussions with Amy in the linked issue under that
 
- Standardization in terms of modeling code:
    - One major thing was to get rid of buggy `merge_embeds` method and cover VLMs with more generation related tests, as we were getting many issues after a small change. Slow tests unfortunately don't cover everything and are not run every time a PR is merged. That is being tracked in https://github.com/huggingface/transformers/issues/33374 
    - Another major topic is setting attention implementation for composite models (not only VLMs) which will fix red CI and add uniformity to how we work with composite models in general. After that PR we should enforce each composite model to have a separate PreTrainedConfig for each model backbone in its architecture. And each sub-config should be part of one major ModelConfig which may hold specific attr for the composte model only (not its sub-backbones). See https://github.com/huggingface/transformers/pull/32238
    - Separate out `get_image_features` method for all VLMs so we can have more modularity and prob make the code much cleaner. Was proposed by one of the community contributor and I'll handle propagating the change in all models. See https://github.com/huggingface/transformers/pull/33696 
 

- Standardization for chat templates:
    - We can support `(tokenize=True, return_tensors=""pt"")` kwargs in processor's apply_chat_template, so that the method returns already vectorized outputs. Similar to tokenizers, the main point is to feed in a chat history and get tensor inputs ready for generation/train. The only difference is that users will have to explicitly add image file/url or `ImageInput` so we can process it internally and turn into `pixel_values`. Below is the general design. No work started yet, I am planning to make a PR some time in October
```python
messages = [
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image"", ""image"": {""url"": ""https://....""}}},
            {""type"": ""text"", ""text"": ""What do you see here?""},
        ]
    },
    {
        ""role"": ""assistant"",
        ""content"": [
            {""type"": ""text"", ""text"": ""Stop sign [...]""},
        ]
    },
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image"", ""image"":  {""path"": ""my_image.png""}}},
            {""type"": ""text"", ""text"": ""What color is the cat?""},
        ]
    },       
]
 ```


- Standardization for tokenizers:
    - We can have new special tokens added to the tokinizers if they are loaded from a VLM model repo. Currently I have a plan to add at lest 3 new special tokens (image, boi and eoi), but given a wave of new models I might expand that list. I had a PR prev but that was a very basic design (https://github.com/huggingface/transformers/pull/31967). Currently working on making `SpecialTokenMixin` more flexible so that we can simply change the class attribute `SPECIAL_TOKENS_ATTRIBUTES` and everything else will work out-of-the-box. Seems to me the easiest way to expand special tokens for multimodal cases without flooding simple language model tokenizers.","[{'id': 1260952223, 'node_id': 'MDU6TGFiZWwxMjYwOTUyMjIz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Discussion', 'name': 'Discussion', 'color': '22870e', 'default': False, 'description': 'Discussion on a topic (keep it focused or open a new issue though)'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",1,open
Centralized Page for Domain Whitelisting (Hugging Face Models),"### Feature request

Is it possible to create a dedicated page that lists all the domains where Hugging Face models, such as those stored on cdn-lfs.hf.co, are hosted? This page would serve as a reference for users working in corporate environments, where whitelisting of individual domains is required.

### Motivation

Many organizations, particularly those behind corporate networks, require that domains be whitelisted individually for security and access reasons. For users working in such environments, identifying and manually whitelisting the necessary domains for model access can be a cumbersome process. A centralized list of the domains Hugging Face uses to host and serve models would significantly streamline this process, ensuring seamless access to models across corporate networks without delays or security issues.

","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
DataCollatorWithFlattening is incompatible with non - list input ids,"### System Info

latest transformers

### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

```python
from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained(""openai-community/gpt2"")
example = tokenizer(""A test sentence"", return_tensors=""pt"")
example = {k: v.flatten() for k, v in tensor_example.items()}
collator([example]*2)
```

### Expected behavior

Collator should work with all output types supported by tokenizer.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 4608548278, 'node_id': 'LA_kwDOCUB6oc8AAAABErDdtg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/HACKTOBERFEST-ACCEPTED', 'name': 'HACKTOBERFEST-ACCEPTED', 'color': 'FF5733', 'default': False, 'description': ''}]",3,open
Automatic dynamic batch size selection for DataCollatorWithFlattening,"### Feature request

Add a custom (batch index) sampler to automatically determine batch size to a fixed target number of tokens.

### Motivation

I'm keen to try out DataCollatorWithFlattening but unsure about how to set batch size, since no padding will be added so the total number of tokens is dynamic.

Im also uncertain whether fixing the total number of tokens is itself optimal...Does optimal memory allocation require accounting for the amount of attention masking that will be applied to the batch?

Is there any recommendation on how to handle this currently?

(Edit: seems like near-optimal solution for map-style datasets is provided by https://github.com/imoneoi/multipack_sampler/tree/master, which presumably just tries to ensure all batches are as full as possible given some max number of tokens. It would be nice to support similar functionality for Iterable Datasets - not optimal packing, but adjusting batch size to adapt to number of tokens in examples should be possible)

### Your contribution

May be able to try to implement something for iterable datasets if this is possible.","[{'id': 1834081910, 'node_id': 'MDU6TGFiZWwxODM0MDgxOTEw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Usage', 'name': 'Usage', 'color': 'e28436', 'default': False, 'description': 'General questions about the library'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
[Modular Transformers] Request for comments,"Hello :wave: 

Over the past few weeks, we've been merging PRs related to ""Modular Transformers"".

We explain why we chose to go that way in this [tweet](https://x.com/LysandreJik/status/1841505287879958730) and this [YouTube video](https://youtu.be/P-asaQVmA3o?si=aZIsaRCpGQYdS05L) at the PyTorch Conference by @ArthurZucker.

We detail how to use it, and have started putting in examples, in this [documentation page](https://huggingface.co/docs/transformers/modular_transformers). Finally, some models, like [GLM, are being contributed](https://github.com/huggingface/transformers/pull/33823) (by @Cyrilvallez) using the modular file.

We'd be eager to hear about your experience using the tool. This is very much experimental, it is brittle, but we'll work on having it be more flexible, and usable in a myriad of situations. Please do share your experience (positive or negative), thoughts, comments, we'd be eager to hear what could be improved in order to lower even more the bar to contributing new models.
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 7553455074, 'node_id': 'LA_kwDOCUB6oc8AAAABwjiT4g', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Modular', 'name': 'Modular', 'color': '3DEDD2', 'default': False, 'description': ''}]",1,open
🌐 [i18n-KO] Translated `processors.md` to Korean,"# What does this PR do?

Translated the `processors.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
 @stevhliu May you please review this PR? ",[],1,open
🌐 [i18n-KO] Translated `image_processor.md` to Korean,"# What does this PR do?

Translated the `image_processor.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR? ",[],2,open
Fix tied weigths isuue,"# What does this PR do?

Relate to #33689 and #33688

- This line
https://github.com/huggingface/transformers/blob/ab97a78130f96e72eec609c56cb5f719529ffb9e/src/transformers/modeling_utils.py#L4517

  may have `state_dict` containing weights with same `data_ptr` (tied). Those tensors are on `cpu` with `dtype=float32`.
  (Note this call doesn't take into account of `model.config`.)

- This line
https://github.com/huggingface/transformers/blob/ab97a78130f96e72eec609c56cb5f719529ffb9e/src/transformers/modeling_utils.py#L4537

  will arrive [line 1](https://github.com/huggingface/transformers/blob/ab97a78130f96e72eec609c56cb5f719529ffb9e/src/transformers/modeling_utils.py#L935) and **line 2**

https://github.com/huggingface/transformers/blob/ab97a78130f96e72eec609c56cb5f719529ffb9e/src/transformers/modeling_utils.py#L990

- If the specified `torch_dtype` is `torch.float32` and `device_map=""cpu""`, the `line 1` and `line 2` won't create new tensors. So the model will get the weights directly from `state_dict` and the tied weights remain tied.

- In other cases, either line 1 or line 2 will create new tensors and assign to `model`, so there is no tied weights inside the model with those new values.
  - This explains why #33689 and #33688 have issue only with `cpu+float32`

This PRs provides a possible fix to avoid tied weights being assigned within `_load_state_dict_into_meta_model`. 

The tied weights will be handled after `_load_pretrained_model` is called within `from_pretrained`, see

https://github.com/huggingface/transformers/blob/ab97a78130f96e72eec609c56cb5f719529ffb9e/src/transformers/modeling_utils.py#L4057-L4058



",[],7,open
How to implement weight decay towards the pre-trained model?,"Hello, let me one question.

If using HF Trainer for supervised fune-tuning, how do I implement penalizing the distance between starting and current weights? This was shown to be effective in https://arxiv.org/abs/1706.03610","[{'id': 1834081910, 'node_id': 'MDU6TGFiZWwxODM0MDgxOTEw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Usage', 'name': 'Usage', 'color': 'e28436', 'default': False, 'description': 'General questions about the library'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Add gguf for granite,"# What does this PR do?

This PR adds gguf support for granite models


Contributes to #33260 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@SunMarc 
",[],0,open
Implement LlamaGen for Image Generation,"### Feature request

Add support for LlamaGen, an autoregressive image generation model, to the Transformers library. LlamaGen applies the next-token prediction paradigm of large language models to visual generation.

Paper: https://arxiv.org/abs/2406.06525
Code: https://github.com/FoundationVision/LlamaGen

Key components to implement:

1. Image tokenizer
2. Autoregressive image generation model (based on Llama architecture)
3. Class-conditional and text-conditional image generation
4. Classifier-free guidance for sampling

### Motivation

LlamaGen demonstrates that vanilla autoregressive models without vision-specific inductive biases can achieve state-of-the-art image generation performance. Implementing it in Transformers would enable easier experimentation and integration with existing language models.

### Your contribution

I can help by contributing to this model, and provide examples and detailed explanations of the model architecture and training process if needed.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",12,open
Flex attention support with arbitrary 4d mask for LlamaModel,"### Feature request

It would be nice to combine the benefits of flex attention and 4d masking.

Perhaps the llama model could be a first case, allowing arbitrary 4d masks to be handled via an efficient flex attention path.

### Motivation

Custom attention masking/biasing patterns lead to considerable improvements in flexibility, and are central to state-of-the-art models like AlphaFold and recent multimodal models.

4d attention masking in Transformers already provides the user with the flexibility to define custom biases, however performance is limited by the fact that 4d masking is incompatible with flash attention.

A 4d mask supporting flex attention attention path would retain full flexibility while maintaining performance. As far as I understand, nothing comparable exists in Transformers currently.

### Your contribution

Not very familiar with what this would take.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}]",6,open
🌐 [i18n-KO] Translated ` deepspeed.md` to Korean,"# What does this PR do?

Translated the ` deepspeed.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
 @stevhliu May you please review this PR? ",[],0,open
[feat] add possibility to give kwargs handler to accelerator in trainer,"# What does this PR do?

At one point I wanted to change the `timeout` of `Accelerator`'s `init_process_group` and I couldn't find an easy way to access this property when using the `Trainer` since it handles the `Accelerator` instantiation internally but gives limited access to what arguments we can modify.

In this PR I added some fields to the `AcceleratorConfig` to allow passing arguments to construct the different `kwargs_handlers` available for `Accelerator` and also updated the `create_accelerator_and_postprocess` method to create and  inject the `kwargs_handlers` when instantiating the `Accelerator`.
I left out the `DistributedDataParallelKwargs` because it seems it's already handled internally in `Trainer`. 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@muellerzr and @SunMarc as this relates to `Trainer`.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Fix update stateful callbacks state to save checkpoint,"* fix: avoid appending to the same list in every callback state update, when there are multiple callbacks of the same class

* fix: make _tune_save_checkpoint method updates callbacks state too

* test_stateful_duplicate_callbacks: add modifiable field to test changes in state while training

# What does this PR do?

This PR fixes a problem when there are multiple stateful callbacks of the same class to be saved. Their states are updated, but they are appended to the same list (of all callbacks of that same class) every time a checkpoint is saved, causing it to grow without limit. In these cases, as a consequence, when training is resumed from a checkpoint, only the first callback states (of the same class) are loaded to the callback handler, possibly ignoring further changes in their states (the latest elements in the list). To solve this problem, a new list is created with the updated callback states, instead of appending them to the existing list.

Regarding the update of the callback states, this PR also makes method _tune_save_checkpoint(...) do the same thing as method _save_checkpoint(...).

## Who can review?

@muellerzr @amyeroberts @pedrobrs 
",[],1,open
Fix: Correct Documentation Errors in Transformers,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
🌐 [i18n-KO] Translated `video_text_to_text.md` to Korean,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `video_text_to_text.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)
@harheem, @jeongiin, @1kmmk1, @SeungAhSon, @win2dvp21
<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
<!-- @stevhliu May you please review this PR? -->",[],0,open
[Don't merge] Check circleci 2,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
"improved readability, error handling; installed pip and albumentations","# What does this PR do?
Imports & Type Annotations: Ensured that all necessary imports are included and updated the type annotations for format_image_annotations_as_coco to return a Dict[str, Any].

Error Handling: Added a check in the augment_and_transform_batch method to ensure that required keys ('images' and 'annotations') exist within the examples.

Augmentation Process: Incorporated a proper augmentation process within augment_and_transform_batch, including using the image_processor for transformation.

Logging Warnings: Implemented warning logs that inform the user about missing keys in the input mapping, promoting debugging.

Improved Readability: Enhanced code structure and comments for clarity, making it easier to follow the logic.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1936351150, 'node_id': 'MDU6TGFiZWwxOTM2MzUxMTUw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Examples', 'name': 'Examples', 'color': 'd4c5f9', 'default': False, 'description': 'Which is related to examples in general'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",4,open
[Don't merge] Debug CircleCI issue,"# What does this PR do?

Hello it's me @ydshieh",[],1,open
Whisper Scoring Model Saving Errors due to Config+GenerationConfig,"### System Info

- `transformers` version: 4.45.1
- Platform: Linux-5.10.225-213.878.amzn2.x86_64-x86_64-with-glibc2.31
- Python version: 3.11.9
- Huggingface_hub version: 0.25.1
- Safetensors version: 0.4.3
- Accelerate version: 0.34.2
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.3.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: Yes (Should error with or without GPU)
- GPU type: Tesla T4

### Who can help?

Tagging Speech team:
@ylacombe @eustlb
and
@gante 
since pull request #32863 seems very relevant, so they might have good insight

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I'm trying to use the latest Transformers version (4.45.1) but I am getting an error when trying to save a `WhisperForAudioClassification` model.

Here is an example of the code that fails.
```python
whisper_model_name = ""openai/whisper-base""

from transformers import WhisperForAudioClassification

import torch
print('Creating Model')

device='cuda'
model = WhisperForAudioClassification.from_pretrained(pretrained_model_name_or_path=whisper_model_name, num_labels=3)
model = model.to(device)

print('Model Created')

# Example inference
temp_batch_size = 4
with torch.no_grad():
    test_audio = torch.zeros([temp_batch_size, 80, 3000], device=device)
    labels = torch.tensor(temp_batch_size * [0], dtype=torch.int64, device=device)
    scores = model.forward(test_audio, labels=labels)
    print('Model Out: ' + str(scores))

model.save_pretrained(""models/whisper-scoring-base-v1"")
```

This is the error that I am getting.

```bash
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[50], line 46
     43     print(model.config.num_labels)
     45 # model.save_pretrained(output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors)
---> 46 model.save_pretrained(""models/whisper-scoring-base-v1"")
     47 # model.save

File /opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:2628, in PreTrainedModel.save_pretrained(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)
   2625             setattr(model_to_save.generation_config, param_name, param_value)
   2626             setattr(model_to_save.config, param_name, None)
-> 2628     model_to_save.config.save_pretrained(save_directory)
   2629 if self.can_generate():
   2630     model_to_save.generation_config.save_pretrained(save_directory)

File /opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py:383, in PretrainedConfig.save_pretrained(self, save_directory, push_to_hub, **kwargs)
    381 non_default_generation_parameters = self._get_non_default_generation_parameters()
    382 if len(non_default_generation_parameters) > 0:
--> 383     raise ValueError(
    384         ""Some non-default generation parameters are set in the model config. These should go into either a) ""
    385         ""`model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file ""
    386         ""(https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) ""
    387         f""\nNon-default generation parameters: {str(non_default_generation_parameters)}""
    388     )
    390 os.makedirs(save_directory, exist_ok=True)
    392 if push_to_hub:

ValueError: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) 
Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}
```


This is reproducible code that gives me this error. I am trying to actually fine-tune the model and when I use the Trainer to train, every time it tries to save the model, it fails.

The way I understand it, when I create/instantiate the model, it will get all get config and generationconfigs from the Whisper pretrained model.  The Whisper Scoring model only uses the Encoder, so all the generationconfigs are useless to it, but I can't figure out how to remove them or never load them.

The other option I was thinking of was to somehow move them to model.generation_config by modifying the WhisperScoringModel class or to move them to a GenerationConfig and save it to a file, but I'm not sure how I would avoid the error with that.

Thank you for the help!

### Expected behavior

Saving without an error.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",7,open
Whisper is ExecuTorch compatible,"### Feature request

Enable Whisper to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow

### Motivation

See details in #32253

### Your contribution

TBD","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",2,open
Mamba is ExecuTorch compatible,"### Feature request

Enable Mamba to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow

### Motivation

See details in #32253

### Your contribution

Mamba model enablement","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",2,open
Phi3 is ExecuTorch compatible,"### Feature request

Enable Phi3-mini to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow

### Motivation

See details in #32253

### Your contribution

model enablement","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",0,open
Stable Diffusion is ExecuTorch compatible,"### Feature request

Enable Stable Diffusion to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow

### Motivation

See details in #32253

### Your contribution

Stable Diffusion model enablement","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",0,open
T5 is ExecuTorch compatible,"### Feature request

Enable T5 to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow

### Motivation

See details in #32253

### Your contribution

model enablement","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",0,open
Mllama flash attention 2,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],3,open
Improve image processing time,"### Feature request

Optimize Transformers' image_processors to decrease image processing time, and reduce inference latency for vision models and vlms.

### Motivation

The Transformers library relies on PIL (Pillow) for image preprocessing, which can become a major bottleneck during inference, especially with compiled models where the preprocessing time can dominate the overall inference time.

![image](https://github.com/user-attachments/assets/10e19b7a-5e48-4bef-84a1-430458e17586)
![image-1](https://github.com/user-attachments/assets/be4f06f3-befd-44b7-b743-10f09471fa58)


In the examples above, the RT-DETR preprocessing necessitates only to resize the image, while the DETR one involves resize+normalize. 
In eager mode, image preprocessing takes a big part of the total inference time for RT-DETR, but is not the main bottleneck. However, with a compiled RT-DETR, image preprocessing takes up the majority of the inference time, underlining the necessity to optimize it. This is even clearer for DETR, where image preprocessing is already the main bottleneck in eager mode.

However, alternative libraries exist that leverage available hardware more efficiently for faster image preprocessing.
[OptimVision](https://github.com/yonigozlan/OptimVision) uses such libraries to get much better results compared to Transformers.

Much more details on OptimVision and image processing methods comparison are available on this [Notion page](https://www.notion.so/huggingface2/OptimVision-Optimize-preprocessing-time-10f1384ebcac8091a12debb87fe5f591?pvs=4).

### Your contribution

OptimVision is an experiment playground to optimize the different steps involved in inferring/training with vision models.
The current fast image preprocessing in OptimVision is a proof of concept and is not yet ready to be merged into Transformers, but that this the ultimate goal :).","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6712797012, 'node_id': 'LA_kwDOCUB6oc8AAAABkB0nVA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/optimization', 'name': 'optimization', 'color': '47E0D2', 'default': False, 'description': ''}, {'id': 7570656740, 'node_id': 'LA_kwDOCUB6oc8AAAABwz8N5A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Processing', 'name': 'Processing', 'color': '1E17DF', 'default': False, 'description': ''}]",1,open
Idefics3: Error/ Bugs Found when no images token input,"## Errors
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[11], line 117
    115     inputs = processor( images=images_raw, text=prompt, return_tensors=""pt"")
    116 else:
--> 117     inputs = processor(text=prompt, return_tensors=""pt"")
    119 inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    121 # Generate the model's response

File ~/data/code-envs/python/py310_mx_sample_rag/lib/python3.10/site-packages/transformers/models/idefics3/processing_idefics3.py:311, in Idefics3Processor.__call__(self, images, text, audio, videos, image_seq_len, **kwargs)
    309     sample = split_sample[0]
    310     for i, image_prompt_string in enumerate(image_prompt_strings):
--> 311         sample += image_prompt_string + split_sample[i + 1]
    312     prompt_strings.append(sample)
    314 text_inputs = self.tokenizer(text=prompt_strings, **output_kwargs[""text_kwargs""])

IndexError: list index out of range


Fixes # (issue)


## Before submitting
- I fixed the problem by updating the code for processing idefics3
- remove value errors if no images, because not every query will found similar images in my chunk


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],3,open
🌐 [i18n-KO] Translated `model_doc/bert.md` to Korean,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `model_doc/bert.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
May you please review this PR?
<!-- @harheem, @jeongiin, @1kmmk1, @Seungahson, @win2dvp21 --> 
@junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang
<!-- @jungnerd, @cjfghk5697, @yijun-lee, @mreraser --> 
<!-- @heuristicwave, @nuatmochoi, @chhaewxn, @jun048098, @ahnjj, @fabxoe -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",[],0,open
Add AudioQuestionAnswering pipeline,"### Feature request

A new AudioQuestionAnswering pipeline, just like DQA but instead of providing a document, applying OCR, and doing QA over it, provide audio file, apply STT, and do QA over the transcript.  Advanced version includes diarization+STT as speaker annotations provide important context and will improve QA/understanding.

### Motivation

This kind of pipeline is one that I have had to build on multiple occasions for processing audio, specifically phone call recordings.  Just like the other pipelines which provide accessibility to some applied ML based pipeline for those to use quickly and easily, this will provide the same thing just for a different modality than what is currently provided.

### Your contribution

I plan to contribute the entire pipeline. My inspiration and what I plan to base a lot of the PR for this pipeline comes from [#18414](https://github.com/huggingface/transformers/pull/18414).

I'm mostly just posting this issue to get feedback from HF team. Tagging @Narsil @NielsRogge as they also provided feedback on the DQA PR.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
[`AutoDocstring`] Based on inspect parsing of the signature,"# What does this PR do?
- Config stay explicit
- any place that had non explicit (dynamic docstring) gets replaced
- we can leverage inheritance for forward dynamic typing I think


",[],1,open
[WIP] Emu3: add model,"# What does this PR do?

As per title. The code can work for generating text in single-batch scenarios but the generated text doesn't match input image. For batched generation, seems like the orig impl neither supports it mostly because image features from processor are returned with different shapes (smart resize to converse as much orig image size as possible). We can try to do padding similar to llava-next but I am not sure if will just work, I'll contact the authors

TODO:
- [x] Batched generation
- [x] Upload chat template and change the image-placeholder token from `extra-0` to smth like `<image>`
- [x] Match the orig implementation on logit level
- [x] Tests, many more tests
- [x] Check out image generation and see how we can enable interleaved image+text generation as in Chameleon. Maybe not natively with transformers, but we can provide scripts with external libraries for structured generation -> not possible because text-generation and image-generation are two different checkpoints with different weights

```python
from PIL import Image
import torch
import requests

from transformers import (
    Emu3Config,
    Emu3ForConditionalGeneration,
    Emu3ImageProcessor,
    Emu3Processor,
)

output_dir = ""/raid/raushan/emu3""
processor = Emu3Processor.from_pretrained(output_dir)
model = Emu3ForConditionalGeneration.from_pretrained(output_dir, torch_dtype=""bfloat16"", device_map=""auto"")
processor.tokenizer.padding_side = ""left""

text = ""You are a helpful assistant. USER: <|extra_0|>Please describe the image. ASSISTANT:""
image = Image.open(""/raid/raushan/image.png"")
image2 = Image.open(requests.get(""https://www.ilankelman.org/stopsigns/australia.jpg"", stream=True).raw)

inputs = processor(
    text=[text, text],
    images=[image2, image],
    return_tensors=""pt"",
    padding=True,
)

inputs = inputs.to(device=""cuda:0"", dtype=torch.bfloat16)

out = model.generate(**inputs, max_new_tokens=100)
text_out = processor.batch_decode(out, skip_special_tokens=True)
print(text_out)

```

And for image generation:

```python
from PIL import Image
from transformers import AutoTokenizer, AutoModel, AutoImageProcessor, AutoModelForCausalLM
import torch
import requests

from transformers import (
    Emu3Config,
    Emu3ForConditionalGeneration,
    Emu3ImageProcessor,
    Emu3Processor,
)

output_dir = ""/raid/raushan/emu3-gen""
processor = Emu3Processor.from_pretrained(output_dir)
model = Emu3ForConditionalGeneration.from_pretrained(output_dir, torch_dtype=""bfloat16"", device_map=""auto"", ) # attn_implementation=""flash_attention_2"",


inputs = processor(
    text=[""a portrait of young girl. masterpiece, film grained, best quality."", ""a dog running under the rain""],
    padding=True,
    return_tensors=""pt"",
    return_for_image_generation=True,
)
inputs = inputs.to(device=""cuda:0"", dtype=torch.bfloat16)

image_sizes = inputs.pop(""image_sizes"")
HEIGHT, WIDTH = image_sizes[0]
VISUAL_TOKENS = model.model.vocabulary_mapping.image_tokens

def prefix_allowed_tokens_fn(batch_id, input_ids):
    height, width = HEIGHT, WIDTH
    visual_tokens = VISUAL_TOKENS
    image_token_id = processor.tokenizer.encode(""<|image token|>"", return_tensors=""pt"")[0].to(model.device) # torch.tensor([processor.tokenizer.image_token_id], device=model.device)
    eoi_token_id = processor.tokenizer.encode(""<|image end|>"", return_tensors=""pt"")[0] # torch.tensor([processor.tokenizer.eoi_token_id], device=model.device)
    eos_token_id = processor.tokenizer.encode(""<|extra_204|>"", return_tensors=""pt"")[0] # torch.tensor([processor.tokenizer.eos_token_id], device=model.device)
    pad_token_id = processor.tokenizer.encode(""<|endoftext|>"", return_tensors=""pt"")[0] # torch.tensor([processor.tokenizer.pad_token_id], device=model.device)
    eol_token_id = processor.tokenizer.encode(""<|extra_200|>"", return_tensors=""pt"")[0]
    eof_token_id = processor.tokenizer.encode(""<|extra_201|>"", return_tensors=""pt"")[0]

    position = torch.nonzero(input_ids == image_token_id, as_tuple=True)[0][0]
    offset = input_ids.shape[0] - position
    if offset % (width + 1) == 0:
        return (eol_token_id, )
    elif offset == (width + 1) * height + 1:
        return (eof_token_id, )
    elif offset == (width + 1) * height + 2:
        return (eoi_token_id, )
    elif offset == (width + 1) * height + 3:
        return (eos_token_id, )
    elif offset > (width + 1) * height + 3:
        return (pad_token_id, )
    else:
        return visual_tokens


out = model.generate(
    **inputs,
    max_new_tokens=50_000,
    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
    do_sample=True,
    top_k=2048,
    return_dict_in_generate=True,
)

print(out.sequences.shape, inputs.input_ids.shape)

image = model.model.decode_image_tokens(out.sequences[:, inputs.input_ids.shape[1]: ], height=HEIGHT, width=WIDTH)
images = processor.postprocess(list(image.float()), return_tensors=""PIL.Image.Image"") # internally we convert to np but it's not supported in bf16 precision
for i, image in enumerate(images['pixel_values']):
    image.save(f""result_{i}.png"")

```
",[],5,open
new add new model,"# What does this PR do?

Update the CLI to use `modular` and the new `__init__` (no dummies)",[],1,open
Request for a clear documentation for .generate(),"### Feature request

The `.generate()` function has a lot of parameters, for example `length_penalty` and `diversity_penalty`. However, the [documentation](https://huggingface.co/docs/transformers/v4.45.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) of this function does not document a full list of parameters, hiding them in **kwargs. It says that ""For an overview of generation strategies and code examples, check out the [following guide](https://huggingface.co/docs/transformers/v4.45.1/en/generation_strategies)"". However, this guide also do not document a full list of parameters. It only says that ""For the complete list of the available parameters, refer to the [API documentation](https://huggingface.co/docs/transformers/v4.45.1/en/main_classes/text_generation.md)."" However, the link is broken. So, could you please document all the available parameters?

UPD I found the [full documentation](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate) through Google in the earlier version of Transformers. Why it was removed?

### Motivation

The full documentation for `.generate()` is either missing or hidden somewhere

### Your contribution

No, sorry, I can't write a doumentation, on the contrary, I need it to understand how the library works.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Tuning generation_config in Trainer hyperparameter_search (Optuna backend),"### Feature request

Adding generation configurations to the parameters that can be tuned in a `Trainer`. 

### Motivation

When defining the Optuna hyper-parameter space, I would like to investigate whether or not different generation configurations can affect performance. For example, something as simple as: is beam search with groups better than standard beam search?

Example of implementation:

```python
def optuna_hp_space(trial):
    # Define default generation parameters
    generation_params = {
        ""max_length"": 512,
        ""max_new_tokens"": 512,
        'top_k': 20,
    }

    # Define the generation strategies and pick one with Optuna
    # REF: https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/generation/configuration_utils.py#L71
    generation_strategy_params = {
        ""greedy"": {""num_beams"": 1, ""do_sample"": False},
        ""contrastive_search"": {""penalty_alpha"": 0.1, ""top_k"": 10},
        ""multinomial_sampling"": {""num_beams"": 1, ""do_sample"": True},
        ""beam_search_decoding"": {""num_beams"": 5, ""do_sample"": False},
        ""beam_search_multinomial_sampling"": {""num_beams"": 5, ""do_sample"": True},
        ""diverse_beam_search_decoding"": {""num_beams"": 5, ""num_beam_groups"": 5, ""diversity_penalty"": 1.0},
    }
    gen_strategy = trial.suggest_categorical(""generation_strategy"", list(generation_strategy_params.keys()))
    generation_params.update(generation_strategy_params[gen_strategy])

    # Update the generation params with the temperature
    temperature = trial.suggest_float(""temperature"", 0.5, 1.1, log=False)
    generation_params[""temperature""] = temperature

    # Instantiate a GenerationConfig object to pass to the Trainer arguments
    generation_config = GenerationConfig(**generation_params)

    # Setup learning rate warmup ratio
    warmup_ratio = trial.suggest_float(""warmup_ratio"", 0.0, 0.1, step=0.01)
    
    # Setup learning rate scheduler type and its fixed kwargs
    lr_scheduler_type = trial.suggest_categorical(""lr_scheduler_type"", [""cosine"", ""cosine_with_restarts"", ""reduce_lr_on_plateau""]) # ""cosine_with_min_lr"", ""polynomial""
    if lr_scheduler_type == ""cosine"":
        lr_scheduler_kwargs = {}
    elif lr_scheduler_type == ""cosine_with_restarts"":
        lr_scheduler_kwargs = {""num_cycles"": 5}
    elif lr_scheduler_type == ""cosine_with_min_lr"":
        lr_scheduler_kwargs = {""min_lr"": 1e-6}
    elif lr_scheduler_type == ""polynomial"":
        lr_scheduler_kwargs = {""power"": 1.0}
    elif lr_scheduler_type == ""reduce_lr_on_plateau"":
        lr_scheduler_kwargs = {""min_lr"": 1e-6}

    return {
        ""learning_rate"": trial.suggest_float(""learning_rate"", 1e-6, 1e-3, log=True),
        ""lr_scheduler_type"": lr_scheduler_type,
        ""lr_scheduler_kwargs"": lr_scheduler_kwargs,
        ""warmup_ratio"": warmup_ratio,
        # ""generation_config"": generation_params, # <-- BREAKING: PASSING THE KWARGS
        # ""generation_config"": generation_config, # <-- BREAKING: PASSING THE INSTANTIATED OBJECT
        # **{f""generation_{k}"": v for k, v in generation_params.items()}, # <-- NOT BREAKING, BUT ORIGINAL VALUES ARE USED INSTEAD OF THESE
        **generation_params # <-- NOT BREAKING, BUT ORIGINAL VALUES ARE USED INSTEAD OF THESE
    }
```

### Your contribution

Currently I'm experiencing the following error:

```log
Traceback (most recent call last):
  File ""/cephyr/users/ribes/Alvis/PROTAC-Splitter/src/train_model.py"", line 18, in <module>
    CLI([train_model, train_ppo_model])
  File ""/opt/conda/lib/python3.10/site-packages/jsonargparse/_cli.py"", line 119, in CLI
    return _run_component(component, init.get(subcommand))
  File ""/opt/conda/lib/python3.10/site-packages/jsonargparse/_cli.py"", line 204, in _run_component
    return component(**cfg)
  File ""/cephyr/users/ribes/Alvis/PROTAC-Splitter/protac_splitter/llms/training.py"", line 277, in train_model
    best_trials = trainer.hyperparameter_search(
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 3217, in hyperparameter_search
    best_run = backend_obj.run(self, n_trials, direction, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/hyperparameter_search.py"", line 72, in run
    return run_hp_search_optuna(trainer, n_trials, direction, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py"", line 260, in run_hp_search_optuna
    study.optimize(_objective, n_trials=n_trials, timeout=timeout, n_jobs=n_jobs, gc_after_trial=gc_after_trial)
  File ""/opt/conda/lib/python3.10/site-packages/optuna/study/study.py"", line 475, in optimize
    _optimize(
  File ""/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py"", line 63, in _optimize
    _optimize_sequential(
  File ""/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py"", line 160, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File ""/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py"", line 248, in _run_trial
    raise func_err
  File ""/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py"", line 197, in _run_trial
    value_or_values = func(trial)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py"", line 247, in _objective
    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 1889, in train
    self._hp_search_setup(trial)
  File ""/opt/conda/lib/python3.10/site-packages/transformers/trainer.py"", line 1517, in _hp_search_setup
    value = type(old_attr)(value)
TypeError: GenerationConfig.__init__() takes 1 positional argument but 2 were given
```

Which makes me suspect that a _single_ `GenerationConfig` object is created _once for all trials_. This is ""in contrast"" to the model instantiation, which must be a `Callable`, as specified in the documentation for the `hyperparameter_search` method.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",1,open
Load a pretrainedfast tokenizer if fast=true and tokenizer.json exists,"Current status for AutoTokenizer with fast=True:
1. checks tokenizer_config.json if tokenizer_class name ends with Fast
2. if not, load a slow tokenizer
(This PR): (unchanged) 1. checks tokenizer_config.json if tokenizer_class name ends with Fast 2. if not, check if repo has a tokenizer.json file 2.1 if yes, load PreTrainedTokenizerFast 3. if not, load a slow tokenizer

prereq for https://github.com/huggingface/transformers/pull/29969
",[],9,open
Add support for TimesFM,"### Model description

**TimesFM** (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

- Research Paper: https://arxiv.org/abs/2310.10688

- Authors: [Abhimanyu Das](https://arxiv.org/search/cs?searchtype=author&query=Das,+A), [Weihao Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong,+W), [Rajat Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen,+R), [Yichen Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+Y)

- Implementation: [google-research/timesfm](https://github.com/google-research/timesfm)
  - The linked repository contains code for implementation in `jax` as well `pytorch`. To implement this in `huggingface` the `pytorch` specific code can be found at [src/timesfm/pytorch_patched_decoder.py](https://github.com/google-research/timesfm/blob/master/src/timesfm/pytorch_patched_decoder.py)

- Models Weights: [google/timesfm-1.0-200m-pytorch](https://huggingface.co/google/timesfm-1.0-200m-pytorch)
  - Although there are weights given in the repository, yet there are missing config files that are to be completed to ensure smooth loading of weights.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6462336551, 'node_id': 'LA_kwDOCUB6oc8AAAABgS9uJw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Time%20Series', 'name': 'Time Series', 'color': 'C1E56A', 'default': False, 'description': ''}]",3,open
Fix MoE tensor reshape,"Previous code transposes the tensor first, making it (B, L, E)->(B, E, L), then reshapes afterwards, leading to an incorrect tensor arrangement.

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
[whisper] added dropping of attention weights after DTW calculations related to word timestamps if these weights are not requested in the output,"# What does this PR do?
We observed significantly increased memory usage when using word-level timestamps  (`return_timestamps='word'`), which increased with each segment processed - especially when processing longer recordings.
It turned out that for DTW calculations, cross-attention weights are extracted for each segment, and then they are accumulated in intermediate segments. However, when the user doesn't need the model output - they are not used any more - just sequences are returned. This accumulation of weights unnecessarily increases memory consumption.

It's simple whisper patch for when attention weights are not explicitly requested (but need to be extracted to be used in DTW for timestamps), so weights are not accumulated across segment objects, minimizing memory usage.

Probably related to #27834


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
",[],8,open
 Will Trainer.predict() return data in the same order as the original dataset during multi-machine and multi-gpus inference?,"### Feature request

I want to use accelerate for multi-machine and multi-gpus inference. Since using trainer.predict does not return the original inference data, only the inference results, I am not sure if their order can still be maintained in the case of multi-machine and multi-gpus.

### Motivation

I want to use accelerate for multi-machine and multi-gpus inference. Since using trainer.predict does not return the original inference data, only the inference results, I am not sure if their order can still be maintained in the case of multi-machine and multi-gpus.

### Your contribution

this is my code：
def main(args):
    def tokenize_function(examples):
        return tokenizer(examples['text'], padding=False, truncation=False)

    # Step 1: Load the model
    model = LlamaForSequenceClassification.from_pretrained(args.model_name, num_labels=args.num_class, problem_type='multi_label_classification', trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)

    # Step 2: Load the dataset

    eval_dataset = load_dataset(args.eval_dataset_type, data_files=args.eval_dataset_name)

    tokenized_dataset = eval_dataset.map(tokenize_function, batched=True, batch_size=10000)


    # Step 3: Define the training arguments
    training_args = MyTrainingArguments(
        hdfs_path=args.hdfs_path,
        output_dir=args.output_dir,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        logging_steps=args.logging_steps,
        num_train_epochs=args.num_train_epochs,
        max_steps=args.max_steps,
        report_to=args.report_to,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        gradient_checkpointing=args.gradient_checkpointing,
        logging_dir=args.logging_dir,
        bf16=True,
        evaluation_strategy=""steps"",
        eval_steps=args.eval_steps 
    )
    
    # Step 5: Define the trainer 
    trainer = Trainer(model=model,
        args=training_args,
        eval_dataset=tokenized_dataset['train'],
        compute_metrics=compute_metrics,
        data_collator=DataCollatorWithPadding(tokenizer=tokenizer, max_length=args.max_length),
        tokenizer=tokenizer,)

    model, trainer = accelerator.prepare(model, trainer)
    thresholds = [0.177, 0.041, 0.299, 0.023, 0.049, 0.012, 0.057, 0.212, 0.187, 0.044, 0.107, 0.035, 0.257, 0.19, 0.258, 0.26, 0.166, 0.097, 0.263, 1.0, 0.549, 0.22, 0.03, 0.294, 0.232, 0.524, 0.113, 0.028, 0.064, 0.135, 0.289, 0.121, 0.016, 0.32, 0.095]
    thresholds = np.array(thresholds)

    with torch.no_grad():
        predictions = trainer.predict(tokenized_dataset['train'])
        score = predictions.predictions
        score = 1 / (1 + np.exp(-score))
        pred = np.where(score >= thresholds, 1, 0)
        pred = pred.tolist()","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6660834292, 'node_id': 'LA_kwDOCUB6oc8AAAABjQRD9A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Accelerate', 'name': 'Accelerate', 'color': '59003B', 'default': False, 'description': ''}]",1,open
Added Ukrainian translations,Added Ukrainian translations to README.md,[],0,open
[i18n-ZH] Sync and Localize Latest English Readme to Simplified Chinese Readme,"# What does this PR do?

Update README_zh-hans.md by adding latest contents from the English version of README, optimize the original Chinese expressions for localization to help Chinese developers quickly understand the latest progress in transformers.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->



## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
add sdpa and flash_attention2 support to speech2text,"I added support for sdpa and flash_attention2 to speech2text model (I started with sdpa but beacuse of ""#copied"" thought it would just be easier to add flash_attentnion2 as well). copy from bart. 
addresses #26350 and #28005 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@amyeroberts @fxmarty 

## Notes:
1. there was a bug in the class ```Speech2TextSinusoidalPositionalEmbedding``` which caused ```test_eager_matches_sdpa_generate``` to fail. I fixed it. The class was copied in ```SpeechT5```. so I fixed the bug there as well
2.  the test ```test_flash_attn_2_generate_reuse_cache``` was failing because he wasn't meant for speech2text model. I copied and tweaked the test from ```whisper```.
3. the only test currently failing on my machine is ```test_flash_attn_2_from_config``` with the error ``` ValueError: Unrecognized configuration class <class 'transformers.models.speech_to_text.configuration_speech_to_text.Speech2TextConfig'> for this kind of AutoModel: AutoModelForCausalLM.``` . speech2text doesn't have a model for causalLM but i couldn't find what other models that have the same situation and what they do with the test. bart has  ```BartForCausalLM```. so would love some guidance here ",[],1,open
Stable dropout show drop prob in model print,"What does this PR do?
Fixes # [issue](https://github.com/huggingface/transformers/issues/33646)

Before submitting
[-] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
[+ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
Pull Request section?
[+] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
to it if that's the case.
[+] Did you make sure to update the documentation with your changes? Here are the
[documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
[here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
[-] Did you write any new necessary tests?
Who can review?
[@LysandreJik](https://twitter.com/LysandreJik)",[],2,open
Add support for Molmo,"### Feature request

Hi,
Would it be possible to add support for [Molmo](https://huggingface.co/allenai/Molmo-7B-D-0924) (currently using custom code)?
Thanks!

### Motivation

Molmo is not supported

### Your contribution

N/A","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",6,open
Add index selection for `output_hidden_states`,"# What does this PR do?

This PR adds index selection for `output_hidden_states` by allowing a list of indices to be passed. In this initial version only `clip` is modified, note that models that copy from `clip` currently have some docstrings changed due to `check_copies`, after an initial review I'll apply the changes to those models, eventually these changes can be applied to all models.

Internally `output_hidden_states` normalises negative indices.

The order of indices supplied to `output_hidden_states` is maintained in `hidden_states` output and duplicate indices are supported, this is required for `vipllama` and the tests for `vipllama` respectively.

The usage of `output_hidden_states` in `llava` and other versions is updated with the exception of `llava_onevision` as this model uses `siglip`, so it will be updated when the changes from `clip` are applied to `siglip`. I looked at the checkpoints listed in the examples for other `llava` versions and they all appear to use `clip`.

~~Note the changes in `vipllava` also required `reversed` due to the order of selected indices `-2, -5, -8, -11 and 6`.~~ ~~`vipllava` needs some special handling, specifically because the test case uses `0, 0, 1, 1, 0` as the selected indices.~~

The result of these changes is increased memory efficiency.

Fixes #33698

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
cc @amyeroberts ",[],1,open
Fix paligemma `eager` vs `sdpa` + image transforms test fixup,"# What does this PR do?

Should fix a couple bugs identified in testing by @ydshieh, related to `eager` mode not being passed correctly + a torch dynamo breaking test due to `cache_position[0] == 0`. 

I also found out we rescale with -1 images to check the image processor in common tests, however we create an uint8 image first - so I changed up the `rescale` to detect this edge case. It should not be a common codepath as we don't rescale by negatives that often. ","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",3,open
Refactor `output_hidden_states` to allow index selection,"### Feature request


Models such as Llava use CLIPVision with `output_hidden_states=True` then select the index of `hidden_states` that it needs e.g. `image_outputs.hidden_states[vision_feature_layer]`.

`output_hidden_states` could be refactored to `output_hidden_states: Optional[Union[bool, int]] = None` and `hidden_states` output changed to `hidden_states: Optional[Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]] = None`.

The layer index would need to be normalized to account for negative index:
```python
if type(output_hidden_states) is int and output_hidden_states < 0:
    output_hidden_states = min(len(self.layers), len(self.layers) + output_hidden_states + 1) # or config.num_hidden_layers
```

`CLIPEncoder` could be changed from
```python
for idx, encoder_layer in enumerate(self.layers):
    if output_hidden_states:
        encoder_states = encoder_states + (hidden_states,)
```
to
```python
for idx, encoder_layer in enumerate(self.layers):
    if type(output_hidden_states) is int and output_hidden_states == idx:
        encoder_states = hidden_states
    elif output_hidden_states:
        encoder_states = encoder_states + (hidden_states,)
```
and after the loop changed from
```python
if output_hidden_states:
    encoder_states = encoder_states + (hidden_states,)
```
to
```python
if type(output_hidden_states) is int and output_hidden_states == len(self.layers):
    encoder_states = hidden_states
elif output_hidden_states:
    encoder_states = encoder_states + (hidden_states,)
```

Models such as Llava would then be able to do:
```python
image_outputs = self.vision_tower(pixel_values, output_hidden_states=vision_feature_layer)
selected_image_feature = image_outputs.hidden_states
```

### Motivation

Memory efficiency, as per the [comment](https://github.com/huggingface/transformers/blob/68049b17a6bb4c9b0d499e9e77121effa2f5a6c0/src/transformers/models/llava/modeling_llava.py#L454) in Llava `""this is not memory efficient at all (output_hidden_states=True) will save all the hidden stated.""`

This would also improve memory efficiency in CLIP Text, specifically in Diffusers pipelines such as Stable Diffusion XL where the penultimate layer is used or when `clip skip` is used.


### Your contribution

I can submit a PR.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",3,open
llama `tie_word_embeddings` ignored on cpu and with auto dtype only,"### System Info

platform: linux: `ubuntu 22.04`
python version: `3.10.12`
transformers version: `4.44.2`

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python3
import torch
import pytest
from transformers import AutoModelForCausalLM

@pytest.mark.parametrize(
    ""torch_dtype,tie_word_embeddings,device_map"",
    [
        (torch.float16, False, ""cpu""   ),  # passes
        (torch.float32, False, ""cpu""   ),  # fails
        (torch.float32, False, ""cuda:0""),  # passes

        (torch.float16, True, ""cpu""   ),  # passes
        (torch.float32, True, ""cpu""   ),  # passes
        (torch.float32, True, ""cuda:0""),  # passes
    ],
)
def test_model_shared(torch_dtype, tie_word_embeddings, device_map, tmp_path):
    # load model
    model = AutoModelForCausalLM.from_pretrained(
        ""Xenova/llama2.c-stories15M"",
        torch_dtype=torch_dtype,
        tie_word_embeddings=tie_word_embeddings,
        device_map=device_map
    )

    # modify lm head
    with torch.no_grad():
        model.lm_head.weight += 1

    # check that embed_tokens is not modified
    if tie_word_embeddings:
        assert torch.equal(model.lm_head.weight, model.model.embed_tokens.weight)
    else:
        assert not torch.equal(model.lm_head.weight, model.model.embed_tokens.weight)
```

### Expected behavior

I expect tied tensors should not be tied if `tie_word_embeddings=False`. Instead, the tensors are tied. Seems to be the root cause of #33688","[{'id': 1834056761, 'node_id': 'MDU6TGFiZWwxODM0MDU2NzYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Modeling', 'name': 'Core: Modeling', 'color': 'FF8446', 'default': False, 'description': 'Internals of the library; Models.'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",2,open
Saving model with shared tensors fails on cpu but succeeds on gpu,"### System Info

platform: linux: `ubuntu 22.04`
python version: `3.10.12`
transformers version: `4.44.2`

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python3
# example.py

import torch
import pytest
from transformers import AutoModelForCausalLM

@pytest.mark.parametrize(
    ""torch_dtype,tie_word_embeddings,device_map"",
    [
        (torch.float16, True,  ""cpu""   ),  # passes
        (torch.float16, False, ""cpu""   ),  # passes
        (torch.float32, True,  ""cpu""   ),  # passes
        (torch.float32, False, ""cpu""   ),  # fails
        (torch.float32, False, ""cuda:0""),  # passes
    ],
)
def test_model_save(torch_dtype, tie_word_embeddings, device_map, tmp_path):
    model = AutoModelForCausalLM.from_pretrained(
        ""Xenova/llama2.c-stories15M"",
        torch_dtype=torch_dtype,
        tie_word_embeddings=tie_word_embeddings,
        device_map=device_map,
    )
    model.save_pretrained(tmp_path, safe_serialization=True)

    # test that the model saved correctly
    reloaded = AutoModelForCausalLM.from_pretrained(
        tmp_path,
        torch_dtype=""auto"",
        device_map=device_map
    )

    model_dict = model.state_dict()
    reloaded_dict = reloaded.state_dict()
    assert model_dict.keys() == reloaded_dict.keys()
    for key in model_dict:
        assert torch.equal(model_dict[key], reloaded_dict[key])
        assert model_dict[key].device == reloaded_dict[key].device
```

```bash
python3 -m pytest example.py
```

```
RuntimeError: 
  Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'lm_head.weight', 'model.embed_tokens.weight'}].
  A potential way to correctly save your model is to use `save_model`.
  More information at https://huggingface.co/docs/safetensors/torch_shared_tensors
```

### Expected behavior

I expect `save_pretrained` to have the same behavior, regardless of model data type, and regardless of device","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",6,open
Updating Chameleon Image handling documentation,"# What does this PR do?

Improve Chameleon documentation. Especially focused on how the image processing works. 

Fixes #33647


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).",[],1,open
Add support for OmDet-Turbo multi-gpu inference with DataParallel.,"### Feature request

OmDet-Turbo will be added to Transformers soon, however it won't support using DataParallel for inference using multi-gpu, at least initially. 

### Motivation

If there is a large demand to support multi-gpu inference with DataParallel.

### Your contribution

A PR will be created if there is demand for it.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2107554019, 'node_id': 'MDU6TGFiZWwyMTA3NTU0MDE5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Distributed%20Training%20/%20Models', 'name': 'Distributed Training / Models', 'color': 'fef2c0', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Qwen2-VL: Multi-GPU training,"### System Info

- `transformers` version: 4.45.0.dev0
- Platform: Linux-4.18.0-477.10.1.el8_8.x86_64-x86_64-with-glibc2.28
- Python version: 3.11.5
- Huggingface_hub version: 0.24.0
- Safetensors version: 0.4.3
- Accelerate version: 0.34.2
- Accelerate config: 	- compute_environment: LOCAL_MACHINE
	- distributed_type: NO
	- mixed_precision: bf16
	- use_cpu: False
	- debug: False
	- num_processes: 1
	- machine_rank: 0
	- num_machines: 1
	- gpu_ids: all
	- rdzv_backend: static
	- same_network: True
	- main_training_function: main
	- enable_cpu_affinity: False
	- downcast_bf16: no
	- tpu_use_cluster: False
	- tpu_use_sudo: False
	- tpu_env: []
- PyTorch version (GPU?): 2.2.1+rocm5.7 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: AMD Instinct MI250X


### Who can help?

@muellerzr @ArthurZucker @gante 

Issue about both the Qwen-VL model and perhaps the trainer so not sure who is best suited to answer :) 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Replicating the setup is a bit tough, so this is more of a preliminary discussion issue to see if there is an obvious problem that surfaces.

1. Multi-GPU setup  + Huggingface trainer
2. Train Qwen2-VL model with dynamic image resolution
3. The processor creates BatchEncodings with pixel_values, input_ids, attention_mask and image_grid_thw.
4. Run a model forward pass with the model in data parallel mode of the trainer.

We observe that compared to mono-gpu setups, the rope values are disaligned with the hidden_states size.

Typically, in line 1109 (Qwen2VisionTransformerPretrainedModel forward pass):
```python
    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:
        hidden_states = self.patch_embed(hidden_states)
        rotary_pos_emb = self.rot_pos_emb(grid_thw)
```
we can see rotary_pos_emb is hidden_states have a sligtly  different dimension 0. 
ex: torch.Size([7820, 40]) torch.Size([7736, 1280])

Upon further inspection, we see rotary_pos_emb has the same dimension as what we would get in mono-gpu runs (normal since it only depends on the grid_thw argument). However, hidden_states (that correspond to pixel values) have a different  size.

This makes training crash:

```bash
  File ""/lus/home/CT10/cad15443/mfaysse/colpali/venv/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py"", line 395, in forward
    q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/lus/home/CT10/cad15443/mfaysse/colpali/venv/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py"", line 254, in apply_rotary_pos_emb_vision
    output = (tensor * cos) + (rotate_half(tensor) * sin)
              ~~~~~~~^~~~~
RuntimeError: The size of tensor a (7736) must match the size of tensor b (7808) at non-singleton dimension 1
```

### Expected behavior

[edited] see below for more details being investigated

Thanks !","[{'id': 2107554019, 'node_id': 'MDU6TGFiZWwyMTA3NTU0MDE5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Distributed%20Training%20/%20Models', 'name': 'Distributed Training / Models', 'color': 'fef2c0', 'default': False, 'description': ''}, {'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",8,open
Object detection training/fine-tuning for Owl-vit/Owlv2,"### Feature request

Currently the Owl-vit models support inference and CLIP-style contrastive pre-training, but don't provide a way to train (or fine-tune) the detection part of the model. According to [the paper](https://arxiv.org/pdf/2205.06230), detection training is similar to Detr.

### Motivation

It would be really awesome to be able to train or fine-tune one of these already-existing open-vocabulary object detection models.

### Your contribution

I may be able to help some with this, not sure at present","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",4,open
support schedule sampling,"### Feature request

To solve the exposure bias problem, can schedule sampling be supported?
https://arxiv.org/abs/1906.07651
https://github.com/deep-spin/scheduled-sampling-transformers

### Motivation

For the same data, the training loss is very low but the inference result is not good.

### Your contribution

no","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Remove attribute that is no longer used,These attributes were left after #33446 but they do not occur anywhere else.,[],0,open
ChameleonProcessor abstraction behaves unintuitively (ChameleonProcessor should also do VQ-GAE and BPE mapping and not ChameleonModel),"### Feature request

# Moving VQ-GAE and BPE mapping to ChameleonProcessor

I am relatively new to HuggingFace and the Transformers library, but have many years of ML and software engineering experience, so I expect there are probably legacy reasons for this unintuitive behavior, but I thought I would share my thoughts and offer to contribute the changes if you are interested.

## Background

Chameleon operates on a sequence of tokens, so as a user you need to supply the sequence of input tokens to make a prediction. But **getting the sequence of tokens is a bit involved**. For text we use a text tokenizer, and for images we use a sequence of transformations. Images undergo multiple transformations: center cropping, resizing, normalizing etc. Then this centered, cropped, normalized, image is fed to a Vector Quantized Variational AutoEncoder (VQ-GAE) to generate image tokens which are then mapped to Byte Pair Encoding (BPE) tokens.

Luckily, the ChameleonProcessor provides us with an easy way to make the tokens, or does it?

Intuitively the ChameleonProcessor should give me the sequence of tokens I need to feed to Chameleon....

## What does it give me?

The ChameleonProcessor when called on a string and list of images creates a dict with:
 

1. key ""input_ids"": mapped to the token sequence for the text and 1026 tokens for every instance of ""\<image\>"" in the original text.
2. key ""pixel_values"": mapped to the tensors of the pixel values for the centered, cropped, normalized images. 
3. key ""attention_mask"": not relevant to this discussion.


## What are these 1026 tokens for every ""\<image\>""? 

They are duplicated placeholder image tokens (1024 replications of the ""8711"" token) plus two sentinel tokens (beginning and end of image)

I.e. the **VQ-GAE and BPE mapping for the images and the substitution is left to later with the call to the Chameleon model**.

## What would I expect:

I would expect the ChameleonProcessor to finish its task, i.e. to also apply the VQ-GAE, and the BPE mapping and give me back the **actual input_ids that will be passed to the model**.

I would expect that the input_ids sequence **contains the actual image tokens from the and not a sequence of 1024 sentinel tokens to be swapped in later based on the pixel_values** during the call to the ChameleonModel.

Alternatively there could be some kind of middle step that combines the ""input_ids"" and ""pixel_values"" and gives the actual ""input_ids"", those that will be passed to the Chameleon model.


## Benefits

Mostly this would be more intuitive to users, they would get the actual input_ids and not have any mysterious changing of the tokens in their model call

It would also bring some latency reductions to some applications as the model call would not need to reprocess all the images every time, I.e. you don't need to do the VQ-GAE, BPE, mapping, token replacement on every call which may actually be worthwhile.

### Motivation

I am doing a project on crafting adversarial attacks for Chameleon and found this behaviour unintuitive and thought it would be useful to fix it upstream for the world.

### Your contribution

I can do the coding if you are interested. If you are open to it I can have a chat with some of your staff and send it in","[{'id': 1834067346, 'node_id': 'MDU6TGFiZWwxODM0MDY3MzQ2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Documentation', 'name': 'Documentation', 'color': '77cc3b', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Enable changing the loss function by making the hard-coded `loss_fct` an attribute of `BertForTokenClassification`.,"### Feature request

In the method `transformers.models.bert.modeling_bert.BertForTokenClassification.forward`, the `loss_fct = CrossEntropyLoss()` is currently hard-coded. To change the loss function (e.g., to set class weights in `CrossEntropyLoss`), one must currently monkey-patch the model. By making `loss_fct` an attribute (e.g., `self.loss_fct`), users can simply replace it and use custom loss functions during training.

### Motivation

The motivation behind this proposal stems from the need to change the loss function for fine-tuning a pre-trained BERT model for token classification, particularly when dealing with imbalanced classes. In my use case, I need to prioritize recall, as most tokens belong to the ""other"" class. To achieve this, I need to set custom weights in the `CrossEntropyLoss`, like this:

```python
loss_fct = CrossEntropyLoss(weight=torch.tensor([0.1, 1.0, 1.0, 2.0, 2.0], device=self.device) 
```

However, since the loss function is hard-coded inside the `forward` method, modifying it currently requires overriding the entire method just to change one line, as shown here:

```python
@patch
def forward(
        self: BertForTokenClassification,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple[torch.Tensor], 'TokenClassifierOutput']:
        r""""""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.
        """"""
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        sequence_output = outputs[0]

        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)

        loss = None
        if labels is not None:
            class_weights = torch.tensor([0.1, 1.0, 1.0, 2.0, 2.0], device=self.device)
            loss_fct = CrossEntropyLoss(weight=class_weights) # <------------------- only change
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
```

By turning `loss_fct` into an attribute, we could avoid the need monkey-patching. The change could be as simple as:

```python
class_weights = torch.tensor([0.1, 1.0, 1.0, 2.0, 2.0], device=model.device)
model.loss_fct = CrossEntropyLoss(weight=class_weights)
```

This would leave existing code unchanged but make it easier to swap in a custom loss function when needed.

### Your contribution

I am new to this repository and this would be my first pull request. I would like to ask if these types of changes are welcomed, and if it makes sense to proceed with submitting a pull request for this improvement.","[{'id': 1834056761, 'node_id': 'MDU6TGFiZWwxODM0MDU2NzYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Modeling', 'name': 'Core: Modeling', 'color': 'FF8446', 'default': False, 'description': 'Internals of the library; Models.'}, {'id': 1834081910, 'node_id': 'MDU6TGFiZWwxODM0MDgxOTEw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Usage', 'name': 'Usage', 'color': 'e28436', 'default': False, 'description': 'General questions about the library'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Fix the bug with loading DeepSpeed MoE models,"# What does this PR do?

Fix the bug with loading DeepSpeed MoE models with `from_pretrained`


# issue

When specifying the `torch_dtype` in `from_pretrained`, the weights of the MoE layers get overwritten, which causes an exception when using DeepSpeed MoE.

By adding a MoE check, we can determine whether the model contains DeepSpeed MoE layers, and if detected, set `param_buffer_assignment` to `False`.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@muellerzr



",[],1,open
Updated few changes to the code and updated the files which had few typos in it.,"I added few lines of code in parse_test_output.py as it had few issues, i made sure not to make changes to the original file that's why i made a new file for the updated code. I have also updated few other files which had typos in it.",[],3,open
#33512 handle last element out of range error,"https://github.com/huggingface/transformers/issues/33552
fix to handle out of range error",[],10,open
Kolmogorov–Arnold Transformer,"### Model description

The Kolmogorov–Arnold Transformer (KAT) replaces the standard MLP layers in transformers with Kolmogorov-Arnold Network (KAN) layers, improving the model's expressiveness and overall performance. 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The code is at https://github.com/Adamdad/kat and paper at https://arxiv.org/abs/2409.10594","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
🌐 [i18n-KO] Translated `quantization/contribute.md` to Korean,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `<your_file>.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? --> 
@harheem, @jeongiin, @1kmmk1, @Seungahson, @win2dvp21
<!-- @junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang --> 
<!-- @jungnerd, @cjfghk5697, @yijun-lee, @mreraser --> 
<!-- @heuristicwave, @nuatmochoi, @chhaewxn, @jun048098, @ahnjj, @fabxoe -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
<!-- @stevhliu May you please review this PR? -->",[],0,open
"Remove repeated test_model_from_pretrained, test_model, test_config","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",1,open
🌐 [i18n-KO] Translated `llava_next.md` to Korean,"<!-- PR의 제목은 ""🌐 [i18n-KO] Translated `<your_file>.md` to Korean"" 으로 부탁드립니다 -->
# What does this PR do?

Translated the `<your_file>.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조-->
<!-- May you please review this PR? --> 
@harheem, @jeongiin, @1kmmk1, @Seungahson, @win2dvp21
<!-- @junejae, @Jwaminju, @010kim, @4N3MONE, @boyunJang --> 
<!-- @jungnerd, @cjfghk5697, @yijun-lee, @mreraser --> 
<!-- @heuristicwave, @nuatmochoi, @chhaewxn, @jun048098, @ahnjj, @fabxoe -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR?",[],1,open
[Draft] Add counting tokens,"# What does this PR do?

Add the possibility to count tokens in an LLM engine to monitor costs of agent runs.","[{'id': 6838975861, 'node_id': 'LA_kwDOCUB6oc8AAAABl6J9dQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Agents', 'name': 'Agents', 'color': 'FBCA04', 'default': False, 'description': ''}]",0,open
Enable multi-GPU in object detection,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes https://github.com/huggingface/transformers/issues/33525

This is the concept of the PR that fundamentally fixes the following multi-GPU circumstances error. The reason why I wrote as `concept` is because torchmetrics does not accept multi-gpu problem + also I want to make some code more clean.

Main keypoint is that all the `Trainer` class accept the `prediction` and `labels` as nested tensor (e.g. batch size : 4, num_gpu : 2 -> length of 8). However in order to calculate proper evaluation metric it should be shape as `batch_size`. + We should always encourage to use `accelerate` as default.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],7,open
[Whisper] TypeError: '<=' not supported between instances of 'NoneType' and 'float',"### System Info

- `transformers` version: 4.44.2
- Platform: macOS-15.0-arm64-arm-64bit
- Python version: 3.12.6
- Huggingface_hub version: 0.24.7
- Safetensors version: 0.4.5
- Accelerate version: 0.34.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.6.0.dev20240916 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No

### Who can help?

@kamilakesbi @ArthurZucker @itazap

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Hi, I am attempting to transcribe several audio files; however, the process intermittently encounters an exception with some of the files. The transcription works successfully in approximately 90% of the cases, but certain files trigger this exception unexpectedly. I am attaching one of the audio files that generates this exception for your review. Thank you.

- I was able replicate it on a MacOS on CPU and Linux on CUDA.

1 Install Stable TS
`pip install stable-ts`


2 Run the code:
```python
import stable_whisper

model = stable_whisper.load_hf_whisper('medium')
result = model.transcribe(
    audio = 'radio_18596_1726554951_1726554981.mp3',
)
print(result.text)
```

Audio sample: https://filebin.net/hivqswoer298m65m

Than I receive the follow exception:
```
Traceback (most recent call last):
  File ""/tests/test.py"", line 4, in <module>
    result = model.transcribe(
             ^^^^^^^^^^^^^^^^^
  File ""/.venv/lib/python3.12/site-packages/stable_whisper/whisper_word_level/hf_whisper.py"", line 236, in transcribe
    return transcribe_any(
           ^^^^^^^^^^^^^^^
  File ""/.venv/lib/python3.12/site-packages/stable_whisper/non_whisper.py"", line 342, in transcribe_any
    result = inference_func(**inference_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/.venv/lib/python3.12/site-packages/stable_whisper/whisper_word_level/hf_whisper.py"", line 116, in _inner_transcribe
    output = self._pipe(audio, **pipe_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/.venv/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py"", line 284, in __call__
    return super().__call__(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py"", line 1255, in __call__
    return next(
           ^^^^^
  File ""/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py"", line 125, in __next__
    processed = self.infer(item, **self.params)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/.venv/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py"", line 587, in postprocess
    text, optional = self.tokenizer._decode_asr(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/.venv/lib/python3.12/site-packages/transformers/models/whisper/tokenization_whisper.py"", line 835, in _decode_asr
    return _decode_asr(
           ^^^^^^^^^^^^
  File ""/.venv/lib/python3.12/site-packages/transformers/models/whisper/tokenization_whisper.py"", line 1086, in _decode_asr
    resolved_tokens, resolved_token_timestamps = _find_longest_common_sequence(
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/.venv/lib/python3.12/site-packages/transformers/models/whisper/tokenization_whisper.py"", line 1193, in _find_longest_common_sequence
    matches = sum(
              ^^^^
  File ""/.venv/lib/python3.12/site-packages/transformers/models/whisper/tokenization_whisper.py"", line 1198, in <genexpr>
    and left_token_timestamp_sequence[left_start + idx]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: '<=' not supported between instances of 'NoneType' and 'float'
```

### Expected behavior

To be able to transcibe the audio files without this exception.","[{'id': 1834056635, 'node_id': 'MDU6TGFiZWwxODM0MDU2NjM1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Tokenization', 'name': 'Core: Tokenization', 'color': 'FF4446', 'default': False, 'description': 'Internals of the library; Tokenization.'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",16,open
Adding MSELoss for regression tasks in HubertForSequenceClassification model,"# What does this PR do?

Fixes [#33500 ](https://github.com/huggingface/transformers/issues/33500)
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@ylacombe
@eustlb

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],7,open
Pass input_ids to _update_model_kwargs_for_generation,"### Feature request

For flexibility in adding additional kwargs to models and handling them consistently with generation utils, it might be useful if input_ids was available in _update_model_kwargs_for_generation. 

### Motivation

My current use-case involves passing a user-defined position index, separate from position_ids, and incrementing it during generation depending on which tokens are generated (e.g. if a sep token is generated, the position index resets, whereas if a standard token is generated, it increments by 1).

The most natural way for me to handle generation in such a model would be to overwrite _update_model_kwargs_for_generation and handle the incrementation index there.

However, this would require access to input_ids. Although the logic can be handled by prepare_inputs_for_generation, because this cannot update model_kwargs, it cannot be computed incrementally, making the computation more complicated.

I expect other custom use cases would also benefit from this change.

### Your contribution

If this sounds reasonable it is a simple change which I'd happily help with","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",1,open
Correctly support resuming from checkpoint with a dataset without length,"# What does this PR do?

There is an inconsistency in `Trainer`'s behavior between training from scratch and resuming from checkpoint when the given dataset has no length like `datasets.IterableDataset`. For a reproducible example, see https://github.com/huggingface/transformers/issues/26413#issuecomment-1887357653 . This PR fixes the inconsistency by correctly supporting resuming from checkpoint with such a dataset.

Fixes #26413 

## Current behavior

When training starts with a dataset without length, Trainer assumes one epoch is equal to `max_steps` steps and tries to train for that many steps. There are two possible scenarios.
- A. If the dataset yields enough samples, the training finishes precisely after one epoch.
- B. If the dataset raises StopIteration before yielding samples enough for `max_steps` steps, Trainer increments the current epoch and re-iterate the dataset.

When resuming from a checkpoint, Trainer simply skips the first batches until `global_step` of the checkpoint. In scenario A, there is no problem. In scenario B, the dataset raises StopIteration during the skipping, but Trainer does not re-iterate the dataset. Instead, it just finishes training with a warning. This is inconsistent from what happens in training from scratch, and it contradicts with what the documents about `max_steps` says: https://github.com/huggingface/transformers/blob/ac5a0556f14dec503b064d5802da1092e0b558ea/src/transformers/training_args.py#L301-L304

## Solution

This PR modifies the skipping behavior so that Trainer now re-iterates the dataset until it catches up `global_step`. A caveat is that it does not support the `ignore_data_skip` option, as Trainer does not know what epoch to start from. I am also concerned that the logic is becoming too complicated.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker",[],4,open
🌐 [i18n-KO] Translated `decision transformer.md` to Korean,"# What does this PR do?

Translated the `decision_transformer.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [ ] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
<!-- @mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi -->
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->
@chhaewxn, @ahnjj, @jun048098, @fabxoe

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
@stevhliu May you please review this PR? ",[],1,open
tokenizers v0.20 not supported,"### Feature request

https://github.com/huggingface/tokenizers/releases/tag/v0.20.0 has been out for over a month, but there still isn't a compatible tag of transformers. Is there any plan to add this compatibility soon?

### Motivation

It would be great to be able to use the latest tags of the huggingface repos together.

### Your contribution

N/A","[{'id': 1834056635, 'node_id': 'MDU6TGFiZWwxODM0MDU2NjM1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Tokenization', 'name': 'Core: Tokenization', 'color': 'FF4446', 'default': False, 'description': 'Internals of the library; Tokenization.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
flash-attention-3,"# What does this PR do?

This PR adds preliminary support for Flash Attention 3.

- `is_flash_attn_3_available` required a workaround in `_is_package_available` as `package_version = importlib.metadata.version(pkg_name)` fails with `importlib.metadata.PackageNotFoundError: No package metadata was found for flash_attn_interface`.
- `_supports_flash_attn_3` and `_check_and_enable_flash_attn_3` added to `modeling_utils.py`, near duplicate of `_check_and_enable_flash_attn_2`.
- ~~`_flash_attention_3_forward` implemented in `modeling_flash_attention_3_utils.py`~~ `_flash_attention_forward` is now a unified interface for FAv2 and FAv3 controlled by `use_flash_attn_3` which is passed from `FlashAttention` classes based on `config._attn_implementation == ""flash_attention_3""`.
  - Currently FAv3 does not support dropout, ~~sliding window~~ (edit: sliding window is now supported) or softcap, and in FAv3 `flash_attn_func`/`flash_attn_varlen_func` return a tuple.
  - `attention_mask is not None` and `position_ids is not None` paths depend on `_upad_input` and `prepare_fa2_from_position_ids` respectively, ~~these are duplicated from `modeling_flash_attention_utils.py`~~ and are not included in FAv3 package therefore FAv3 depends on `flash_attn`, this is reflected in `is_flash_attn_3_available` which checks for `is_flash_attn_2_available`.
  - In the remaining path FAv3 supports FP8, this PR currently uses environment variable `FLASH_ATTENTION_3_FP8` for this purpose, we can probably add something like `attention_kwargs` to model forwards to control this, or maybe another `_attn_implementation` type `flash_attention_3_fp8`, best to get reviews first and consensus on the best way to do it[1]
- ~~`flash_attention_3` is added to Llama with `LlamaFlashAttention3`, similar to `LlamaFlashAttention2` with unsupported options like dropout and sliding window removed.~~ Edit: added to other models, see comment below.
- ~~`_update_causal_mask` is updated in various models due to `utils/check_copies.py`, and `_supports_flash_attn_3` is added in to some other models already for the same reason.~~ See comment below.

Fixes #33373

## Todo

- ~~Test `attention_mask is not None` and `position_ids is not None` paths~~
- ~~Implement FlashAttention3 classes for other models~~ Done.
- FP8 usage[1]
- ~~Documentation~~ Partly done.
- Benchmarks would be nice

## Notes

Llama tested on H100 SXM with:
```python
import torch
from transformers import AutoTokenizer, LlamaForCausalLM

tokenizer = AutoTokenizer.from_pretrained('NousResearch/Hermes-3-Llama-3.1-8B', trust_remote_code=True)
model = LlamaForCausalLM.from_pretrained(
    ""NousResearch/Hermes-3-Llama-3.1-8B"",
    torch_dtype=torch.float16,
    device_map=""auto"",
    attn_implementation=""flash_attention_3""
)

prompts = [
    """"""<|im_start|>system
You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>
<|im_start|>user
Write a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>
<|im_start|>assistant"""""",
    ]

for chat in prompts:
    print(chat)
    input_ids = tokenizer(chat, return_tensors=""pt"").input_ids.to(""cuda"")
    generated_ids = model.generate(input_ids, max_new_tokens=750, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)
    print(f""Response: {response}"")
```

(shortened) responses

FP16:
```
In the vast expanse of the universe, there existed a celestial realm where beings of extraordinary powers roamed freely. One such being was Goku, the legendary warrior known for his indomitable spirit and unconquerable will.

One fateful day, as Goku trained under the golden sun, he sensed an unusual disturbance in the cosmic energy. Puzzled by this anomaly, Goku rushed to the source of the disturbance and arrived at the edge of a hidden dimension.

To his horror, Goku witnessed Kirby and Majin Buu working together, devising devious plans to obliterate entire planets. Their combined strength was formidable, their intentions sinister, and their alliance unprecedented.
```

FP8:
```
In the vast expanse of the universe, there existed a planet called Planet Vegeta, where the powerful Saiyan warrior, Goku, lived among his friends in the city of Earth.

One fateful day, Goku was training on the top of a mountain when he sensed an unusual energy signature. His spidey senses immediately tingled with suspicion.

""Who could that be?"" he wondered aloud as he leapt into the sky, soaring towards the source of the disturbance.

As Goku arrived at the scene, he discovered something utterly shocking - Kirby, the infamous villain from another galaxy, had formed an alliance with Majin Buu, the mischievous yet formidable entity who had caused Goku so much trouble in the past.
```

~~All other models will be tested after I've finished adding FlashAttention3 classes.~~ Other models have been tested, see comment below.

## Who can review?

cc @ArthurZucker
",[],9,open
[WIP] [Whisper] Fix generate and tokenizer behavior with added tokens,"# What does this PR do?

Fixes #33082. 

### Description of the issue

Transformers' `PretTrainedTokenizer` [`_add_tokens`](https://github.com/huggingface/transformers/blob/ce62a41880b5b70a304d068eb58f55894a5a7af8/src/transformers/tokenization_utils.py#L510) add tokens at the end of the vocabulary. Likewise, `PreTrainedModel`'s [`_get_resized_embeddings`](https://github.com/huggingface/transformers/blob/ce62a41880b5b70a304d068eb58f55894a5a7af8/src/transformers/modeling_utils.py#L2080) will add newly initialized tokens at the end. This behavior is not compatible with Whisper's timestamp token identification. Indeed, official implementation as well as Transformer's one identifies timestamp tokens as the ones that have an id > `timestamp_begin`. For this reason, the newly added tokens are falsely considered timestamps and this poses decoding issues. 

###  Implementation possibilities

Two possibilities here: 
1. change Whisper's decoding logic using a timestamp_end
2. overwrite  [`_add_tokens`](https://github.com/huggingface/transformers/blob/ce62a41880b5b70a304d068eb58f55894a5a7af8/src/transformers/tokenization_utils.py#L510)  and [`_get_resized_embeddings`](https://github.com/huggingface/transformers/blob/ce62a41880b5b70a304d068eb58f55894a5a7af8/src/transformers/modeling_utils.py#L2080) in order to add new tokens before the first timestamp token (and not at the end as current implementation)

I chose option 1 to avoid overwriting the default methods of the Transformers library, focusing instead on modifying the Whisper-specific generation method and tokenizer logic.

## Implementation decision (see below discussion for details)

In the updated implementation, Whisper will now use both `timestamp_begin` and `timestamp_end`, instead of just `timestamp_begin` as in OpenAI’s original version.

**→  Justification:**

**what is the best for the users (least amount of work for them)**

We have two scenarios:

1.	Adding text tokens.
2.	Adding timestamp tokens.

These cases overlap, as both types of tokens must be added via the add_tokens method. For the system to work seamlessly:

- Text tokens should be inserted before the first timestamp token (Case 1).
- Timestamp tokens should be added at the end of the vocabulary (Case 2).

To handle Case 1, modifying the `add_tokens` method would also require changes to the `resize_token_embedding`. From a simplicity perspective, deviating from the standard implementation of these methods complicates things unnecessarily.

Since Case 1 is more common, we should prioritize simplifying it for the user. Hardcoding a default number of timestamp tokens achieves this, and the `number_timestamp_tokens` config option easily accommodates Case 2 for the few users who need it.

As for the tokenizer, the regex pattern currently used to detect timestamps prevents the use of custom timestamp tokens (meaning with a custom syntax not matching the pattern). However, since this is a rare use case and the current implementation already depends on regex, it’s best to keep things simple and do it this way.

## Who can review?

@ylacombe 

# TODO 

- [x] discuss different implementations and choose 
- [ ] add tests before merging",[],2,open
Add `private` to `trainer.push_to_hub`. Add `_update_repo_visibility` to trainer.,"# What does this PR do?

Currently setting `hub_private_repo` training argument only allows control over repo visibility at creation.

This PR adds `private` parameter to `trainer.push_to_hub`, this allows control over repo visiblity at creation in case `hub_private_repo` was not set in training arguments, and allows updating visibility of existing repos.

Fixes #33492
Fixes #32909


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

cc @muellerzr",[],2,open
Run nightly ci test new runner,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Video Processor as a separate class,"### Feature request

Since we currently have more and more VLMs that support image and video, and not always videos are processed same way as images are, I want to add a `VideoProcessor` class that inherits from `ImageProcessingMixin`. Thus we can have two separate classes for processing visuals, each with its own set of attributes and methods. We can also save different configs for both to avoid issues as #33484. The `VideoProcessor` will mainly use the same transform methods as slow image processors, by iterating over each frame and stacking it. Some additional helper fn can be added, like `load_video`  and `make_list_of_videos`. The main input name will be videos  and the output var name is  `pixel_values_videos`. 

For the `load_video`  we can prob rely on `av`, but I find it super slow compared to other video decoders. I'll try to get a small comparison benchmarks for that, and unfortunately `decord` can't be used as it had problems with models on cuda.

In the long term we might consider adding video transforms where each video is transformed in one call, instead of each video frame, similar to fast image processing with `torchvision`. 

To Do:
- [ ] Add the VideoProcessor class and integrate with llava-next-video which is one of the models with different processing for image and videos.
- [ ] After the changed are approved and merged, the following models will be easy to modify:
    - [ ] Video-LLaVa
    - [ ] Qwen2-VL
    - [ ] LLaVA-OneVision

- [ ] Instructblip-Video might need deprecation as it currently accepts images  as main arg and returns pixel_values . TBH, it is a video-only model so we can disregard changing it, same was as we won't touch VIVIT and other video-only models

### Motivation

Easier integration of multimodal LLMs

### Your contribution

@amyeroberts WDYT about this suggestion? Would love to hear your opinion 🤗 ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",3,open
fix llava next incorrect padding size,"# What does this PR do?
Fix incorrect llava next image padding, if i have an input image size 950x1256 and using siglip as image encoder (shortest_edge=384) for llava next, the original code will create padded image with size 768x767 not 768x768.
This PR will fix the issue by directly calculating the right pad size
",[],4,open
"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.","### System Info

- `transformers` version: 4.44.2
- Platform: Linux-6.8.0-44-generic-x86_64-with-glibc2.39
- Python version: 3.12.3
- Huggingface_hub version: 0.24.7
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.4.1+cu121 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No


### Who can help?

speech models: @ylacombe, @eustlb
pipelines: @Rocketknight1

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
import torch 
from transformers import pipeline

pipe = pipeline(
    ""automatic-speech-recognition"",
    model=""openai/whisper-base.en"",
    device=""cpu"",
    torch_dtype=torch.float32,
)

# https://github.com/openai/whisper/blob/main/tests/jfk.flac
pipe(""./jfk.flac"")
```

### Expected behavior

This does return the expected:

```python
{'text': ' And so my fellow Americans ask not what your country can do for you, ask what you can do for your country.'}
```

But it also prints the following, so would be nice to fix/suppress:

```
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
```

Thanks!
","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",8,open
Fix `inputs` deprecation warning in automatic_speech_recognition.py,"# What does this PR do?

Updates `inputs` to  `input_features` to fix the deprecation warning found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/generation_whisper.py#L493-L499) when running a `pipeline(""automatic-speech-recognition"",..)`.

## Who can review?
Models:
- speech models: @ylacombe, @eustlb

Library:
- pipelines: @Rocketknight1
",[],6,open
removed redundant creation of causal mask when attention mask is already 4D,"# What does this PR do?

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
add `private` parameter to trainer.push_to_hub,"### Feature request

Adding another parameter to the `trainer.push_to_hub` called `private`

### Motivation

a user at https://discuss.huggingface.co/t/how-to-push-trained-models-and-save-it-privately-on-hub/25538/10 has requested this, and although setting `hub_private_repo` training argument to True is a good alternative, but I agree with them on the basis that we need a unified `push_to_hub` method.

### Your contribution

I can submit a PR ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
"Clarify what kwargs can be accepted by ""AutoModelForCausalLM.from_pretrained()"" ","### Feature request

i saw in the doc saying:If a configuration is not provided, kwargs will be first passed to the configuration class initialization function ([from_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)). Each key of kwargs that corresponds to a configuration attribute will be used to override said attribute with the supplied kwargs value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model’s __init__ function.

but I can't seem to find the __init__(I found 3 lines with __init__, but no useful info). 

I saw some one added ""use_cache:Ture"" but don't know how this got handled and set,maybe it's discarded. 

### Motivation

i checked the doc and source code but still confused. 

### Your contribution

I'll keep looking","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Support context parallel training with ring-flash-attention,"### Feature request

Hi, I'm the author of [zhuzilin/ring-flash-attention](https://github.com/zhuzilin/ring-flash-attention).

I wonder if you are interested in integrating context parallel with [zhuzilin/ring-flash-attention](https://github.com/zhuzilin/ring-flash-attention), so that user can train llm with long data more efficiently.

### Motivation

As openai o1 released, it will probably be common for people to train model with really long cot data. And it will be nice if most model within the transformers library can support training with long context efficiently with certain type of context parallel, i.e. the context length scale linearly with the number of GPUs.

The 3 existing context parallel methods are the deepspeed ulysses, ring attention and the one proposed in the [llama3 tech report](https://arxiv.org/abs/2407.21783). The deepspeed ulysses will be limited by the number of kv heads (the maximum context length can be `num_head_kv * seq_length_per_gpu`), which makes it a little unfriendly to GQA models. So it will be great if the transformers library could support the one or both of the other 2 context parallel methods.

And both ring attention and the llama3 strategy are supported with flash attention in [zhuzilin/ring-flash-attention](https://github.com/zhuzilin/ring-flash-attention), whose correctness has been proved by [jzhang38/EasyContext](https://github.com/jzhang38/EasyContext). The library basically has the same api as flash attention, and hides the communication required from its user to make it a easy substitution from any origin flash attention api callsite.

Therefore, I believe it will be easy to support the context parallel with [zhuzilin/ring-flash-attention](https://github.com/zhuzilin/ring-flash-attention). For example, we could have different branch in `modeling_flash_attention_utils._flash_attention_forward`.

### Your contribution

I'd love to help if you have interests :)","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}]",5,open
Regression in tokenizer loading,"### System Info

There was a regression in commit b4727a1216bb21df2795e973063ed07202235d7e that prevents loading of some tokenizers.


### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

1. Install `transformers`
2. Run the following code:
```python
from transformers import AutoTokenizer
AutoTokenizer.from_pretrained(""adsabs/astroBERT"")
```

Error output:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[2], [line 1](vscode-notebook-cell:?execution_count=2&line=1)
----> [1](vscode-notebook-cell:?execution_count=2&line=1) AutoTokenizer.from_pretrained(""adsabs/astroBERT"")
...
AttributeError: add_special_tokens conflicts with the method add_special_tokens in BertTokenizerFast
```

### Expected behavior

It's my expectation that the above code should run and produce a working tokenizer. Or, if the tokenizer config is too old and needs to be updated there should be an error message & script/tool/API to guide the user to update the config.","[{'id': 1834056635, 'node_id': 'MDU6TGFiZWwxODM0MDU2NjM1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Tokenization', 'name': 'Core: Tokenization', 'color': 'FF4446', 'default': False, 'description': 'Internals of the library; Tokenization.'}, {'id': 1920687293, 'node_id': 'MDU6TGFiZWwxOTIwNjg3Mjkz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Fast%20Tokenizers', 'name': 'Fast Tokenizers', 'color': 'b60205', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",14,open
Support to use adam_mini from installing directly,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This repo implements the Adam-mini optimizer. 

Paper: https://arxiv.org/abs/2406.16793

Code: https://github.com/zyushun/Adam-mini


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

This pull request was discussed in https://github.com/huggingface/transformers/pull/31933. It is suggested to use the optimizer from the source library.   


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts @muellerzr @Ashwanth369 @chcoliang @zyushun

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts, @qubvel
- speech models: @ylacombe, @eustlb
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Rocketknight1
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc
- chat templates: @Rocketknight1

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
The examples in the examples directory are mostly for fine-tuning pre-trained models？how to trian from  scratch,"### Model description

no 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Exception raised with trainer + `accelerate launch` FSDP + large gradient accumulation steps + small dataset,"### System Info

- `transformers` version: 4.44.2
- Platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35
- Python version: 3.10.14
- Huggingface_hub version: 0.23.4
- Safetensors version: 0.4.3
- Accelerate version: 0.29.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.2.2+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: yes (accelerate FSDP)
- Using GPU in script?: yes
- GPU type: NVIDIA RTX A6000

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

This is a duplicate of #24098 and #25695, but I figured it'd still be useful to resubmit this issue since (1) I have a code example, and (2) I paste a different error message I get with mixed precision, which may increase visibility for other people who run into this problem and search for existing GitHub issues.

When I do multi-GPU training (launched with `accelerate launch --num_processes=2`) using `Trainer` with a small dataset size and `gradient_accumulation_steps > 2`, I often repeatedly get the following error:
```python-traceback
Traceback (most recent call last):
  File ""/workspace/program.py"", line 34, in <module>
    trainer.train()
  File ""/usr/local/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 1938, in train
    return inner_training_loop(
  File ""/usr/local/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 2341, in _inner_training_loop
    self.optimizer.step()
  File ""/usr/local/venv/lib/python3.10/site-packages/accelerate/optimizer.py"", line 150, in step
    self.optimizer.step(closure)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py"", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/adamw.py"", line 187, in step
    adamw(
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/adamw.py"", line 339, in adamw
    func(
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/adamw.py"", line 549, in _multi_tensor_adamw
    torch._foreach_lerp_(device_exp_avgs, device_grads, 1 - beta1)
RuntimeError: The size of tensor a (3219712) must match the size of tensor b (128) at non-singleton dimension 1
```

If FP16 mixed-precision is enabled then the error looks like this instead:
```python-traceback
Traceback (most recent call last):
  File ""/workspace/program.py"", line 34, in <module>
    trainer.train()
  File ""/usr/local/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 1938, in train
    return inner_training_loop(
  File ""/usr/local/venv/lib/python3.10/site-packages/transformers/trainer.py"", line 2341, in _inner_training_loop
    self.optimizer.step()
  File ""/usr/local/venv/lib/python3.10/site-packages/accelerate/optimizer.py"", line 137, in step
    self.scaler.step(self.optimizer, closure)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py"", line 457, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py"", line 352, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/accelerate/optimizer.py"", line 192, in patched_step
    return method(*args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py"", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/adamw.py"", line 187, in step
    adamw(
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/adamw.py"", line 339, in adamw
    func(
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/adamw.py"", line 516, in _multi_tensor_adamw
    grouped_tensors = Optimizer._group_tensors_by_device_and_dtype([
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 409, in _group_tensors_by_device_and_dtype
    return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/usr/local/venv/lib/python3.10/site-packages/torch/utils/_foreach_utils.py"", line 38, in _group_tensors_by_device_and_dtype
    torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices).items()
RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32 notwithstanding
```

Here's a minimal example — run the following with `accelerate launch --config_file=accelerate_config.yaml --num_processes=2 program.py`
```python
# program.py
from datasets import Dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
)

dataset = Dataset.from_dict(
    {""text"": [""positive"", ""negative""], ""label"": [1, 0]}
)  # tiny dataset of 2 examples

tokenizer = AutoTokenizer.from_pretrained(""EleutherAI/pythia-14m"")
tokenized_dataset = dataset.map(lambda x: tokenizer(x[""text""]), batched=True)

model = AutoModelForSequenceClassification.from_pretrained(
    ""EleutherAI/pythia-14m"", num_labels=2
)
model.config.pad_token_id = tokenizer.eos_token_id

training_args = TrainingArguments(
    output_dir=""/tmp/results"",
    num_train_epochs=10,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=16,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

trainer.train()
```
```yaml
# accelerate_config.yaml
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: ""no""
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: ""no""  # change this to ""fp16"" to get the other error
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```


My use case for this was that I had a codebase where we had added some end-to-end tests. We used a very small dataset size since we wanted the test to still be reasonably fast, but then we hit into these exceptions and were confused.

### Expected behavior

I think I expect this to just work without crashing.
But maybe it's not really a sensible setup to have such a small training set. In #24098 commenters suggested that the training set size
> has to be greater than gradient_accumulation_steps * num_GPUs * per_device_train_batch_size.

In that case it would be nice to have an error message saying that this is the problem.
","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 4101623725, 'node_id': 'LA_kwDOCUB6oc70ec-t', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/PyTorch%20FSDP', 'name': 'PyTorch FSDP', 'color': 'B60205', 'default': False, 'description': ''}, {'id': 6660834292, 'node_id': 'LA_kwDOCUB6oc8AAAABjQRD9A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Accelerate', 'name': 'Accelerate', 'color': '59003B', 'default': False, 'description': ''}]",10,open
Add French translation of task_summary and tasks_explained,"# What does this PR do?

I translated the `task_summary.md` and `tasks_explained.md` files into French. For technical terms, I opted to use the English version where it felt more appropriate. When a French Wikipedia page existed for a term, I used the French equivalent; otherwise, I either kept the English term or included it in parentheses alongside the translation. Please feel free to suggest any changes if something seems unclear or off. Thanks in advance for reviewing!

Following #18322

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@omarespejel @sgugger @stevhliu



",[],3,open
The same situation as #31377 occurred when using Qwen/Qwen2-VL-7B-Instruct,"### System Info


- `transformers` version: 4.45.0.dev0
- Platform: macOS-14.6.1-arm64-arm-64bit
- Python version: 3.12.4
- Huggingface_hub version: 0.24.6
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.4.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>


### Who can help?

@zucchini-nlp @amyer

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Run this code after git clone with the hash I specified above and pip install ./transformers

```
from transformers import Qwen2VLForConditionalGeneration,AutoModel,AutoProcessor

model_path="".models/Qwen/Qwen2-VL-7B-Instruct""
model = Qwen2VLForConditionalGeneration.from_pretrained(
             model_path,
             torch_dtype=torch.bfloat16,
             #attn_implementation=""default""
        ).to(self.device) #device=""mps""
min_pixels = 256*28*28
max_pixels = 1280*28*28
processor = AutoProcessor.from_pretrained(model_path,
                                                   min_pixels=min_pixels, 
                                                   max_pixels=max_pixels
                                                   )
messages = [
            {
                ""role"": ""user"",
                ""content"": [
                    {
                        ""type"": ""image""
                    },
                    {
                        ""type"": ""text"",
                        ""text"": ""Extract text from pdf""
                    }
                ]
            }
        ]
base64_data = image_data.split(',')[1]  # remove 'data:image/jpeg;base64,' 
image_bytes = base64.b64decode(base64_data)
image = Image.open(io.BytesIO(image_bytes))
text = processor.apply_chat_template(
      messages, tokenize=False, add_generation_prompt=True
)
inputs = processor(
      text=[text],
      images=[image],
).to(self.device)#device=""mps""

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
       generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
return output_text  # Dummy return
```

### Expected behavior

File ""/Users/dev/products/dev/workspaces/mixparse/llm/model/modelmanager.py"", line 429, in _run_safetensors_inference
    generated_ids = model.generate(**inputs, max_new_tokens=128)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dev/anaconda3/envs/all-parse/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dev/anaconda3/envs/all-parse/lib/python3.12/site-packages/transformers/generation/utils.py"", line 2015, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File ""/Users/dev/anaconda3/envs/all-parse/lib/python3.12/site-packages/transformers/generation/utils.py"", line 2965, in _sample
    outputs = self(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dev/anaconda3/envs/all-parse/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dev/anaconda3/envs/all-parse/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dev/anaconda3/envs/all-parse/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py"", line 1683, in forward
    inputs_embeds[image_mask] = image_embeds
RuntimeError: shape mismatch: value tensor of shape [630, 3584] cannot be broadcast to indexing result of shape [0, 3584]","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",21,open
A Trainer subclass for Decoder-Only LM with generation in evaluate(),"### Feature request

The main feature request involves a New Trainer Subclass, similar to Seq2SeqTrainer, but suitable for Decoder-Only LM.

### Motivation

`Seq2SeqTrainer` provides a great abstraction for Encoder-Decoder LM, when we need to conduct generation during `evaluate()`

But the current implementation of both `Trainer` and `Seq2SeqTrainer` seems to be not suitable for Decoder-Only LM due to the difference of `input_ids` and `labels` between teacher-forcing training and generation-involved evaluation.

For example in instruction tuning:
- During training (teacher-forcing)
```python
input_ids = 'Translation the following texts: {Text in Chinese...} {Text in English...}'
labels = 'Translation the following texts: {Text in Chinese...} {Text in English...}'
```
- During evaluation
```python
input_ids = 'Translation the following texts: {Text in Chinese...}'
labels = '{Text in English...}'
```

So we need to prepare two kinds of inputs_ids during evaluation for calculation of both `loss` and `bleu_metrics`. It leads to different columns in eval_dataset. However, `Trainer._remove_unused_columns()` will remove columns for both `eval_dataset` and `train_dataset` not accepted by `model.forward()`. During training, this behaviour is expected (we only need the teacher-forcing inputs). But it will make evaluation difficult.

This feature is nearly identical across all CausalLM models when performing generation during evaluation, making it highly reusable. Given the increasing number of Decoder-only LMs (CausalLMs) in the community, I strongly recommend implementing a dedicated CausalTrainer to simplify deployments.

I may have missed something. If there is already a simpler way to customize such a Trainer, please let me know.

### Your contribution

I'm willing to help submit a PR. But I'm not familiar with some integrations such as fsdp and deepspeed. I may need someone to help me finish this feature.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
fix flash attention comment,"@ArthurZucker, @gante 

I think this fix makes the comment correct, given the `view` calls in the following lines. If this is the case, I can copy these changes to all flash attention functions.
",[],1,open
how can calculate the predict score of every pixel use mask2former swin-l model?,"### Feature request

I has download the mask2former swin-l model from huggingface website,
and use example code get segmentation map of image,
the example code is:
```python
import requests
import torch
from PIL import Image
from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation


# load Mask2Former fine-tuned on COCO panoptic segmentation
processor = AutoImageProcessor.from_pretrained(""facebook/mask2former-swin-large-coco-panoptic"")
model = Mask2FormerForUniversalSegmentation.from_pretrained(""facebook/mask2former-swin-large-coco-panoptic"")

url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors=""pt"")

with torch.no_grad():
    outputs = model(**inputs)

# model predicts class_queries_logits of shape `(batch_size, num_queries)`
# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
class_queries_logits = outputs.class_queries_logits
masks_queries_logits = outputs.masks_queries_logits

# you can pass them to processor for postprocessing
result = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
# we refer to the demo notebooks for visualization (see ""Resources"" section in the Mask2Former docs)
predicted_panoptic_map = result[""segmentation""]
```

the code can get the seg map, but not pred score of every pixel,
so how to add calculate code to get perd score of every pixel?



### Motivation

not motivation

### Your contribution

a littlecontribution","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",3,open
Update from pretrained error when loading,"# What does this PR do?
Simplifies the error messaged, by grouping layer numbers: 

```python
'model.language_model.0.self_attn.0.mlp0', 
'model.language_model.0.self_attn.1.mlp.conv1.weight', 
'model.language_model.1.self_attn.0.mlp.conv1.weight', 
'model.language_model.1.self_attn.1.mlp.conv1.weight',
'model.language_model.2.self_attn.0.mlp.conv1.weight', 
'model.language_model.2.self_attn.1.conv1.weight',
'model.language_model.3.self_attn.0.mlp.conv1.weight',
'model.language_model.3.self_attn.1.mlp.conv1.weight', 
'model.language_model.0.self_attn.0.mlp.layer.conv1.weight', 
'model.language_model.1.self_attn.0.mlp.layer.conv1.weight', 
'model.language_model.2.self_attn.0.mlp.layer.conv1.weight', 
'model.language_model.3.self_attn.0.mlp.layer.conv1.weight',
'model.language_model.0.self_attn.0.mlp.layer.1.weight',
'model.language_model.1.self_attn.0.mlp.layer.1.weight', 
'model.language_model.2.self_attn.0.mlp.layer.1.weight', 
'model.language_model.3.self_attn.0.mlp.layer.1.weight'
'model.language_model.3.self_attn.0.mlp.layer.conv2.weight'
```
to 
```python
'model.language_model.{0, 1, 2, 3}.self_attn.{0, 1}.mlp.layer.1.weight', 
'model.language_model.{0, 1, 2, 3}.self_attn.{0, 1}.mlp.conv1.weight', 
'model.language_model.{0, 1, 2, 3}.self_attn.{0, 1}.mlp0', 
'model.language_model.{0, 1, 2, 3}.self_attn.{0, 1}.conv1.weight', 
'model.language_model.{0, 1, 2, 3}.self_attn.{0, 1}.mlp.layer.conv1.weight'
'model.language_model.{0, 1, 2, 3}.self_attn.{0, 1}.mlp.layer.conv2weight'
```

On a recent example, went from 3000 no match to 56.
This does not apply to `layer.my_name_with_digit_0`   and single element are left alone
",[],5,open
The _crop_past_key_values function should be a member function of Cache.,"### Feature request

The _crop_past_key_values function should be a member function of Cache.

### Motivation

I suppose all models will use Cache class instead of tuple to save past_key_values. It will make more sense to make [_crop_past_key_values](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/candidate_generator.py#L375) as a member function of Cache.

### Your contribution

cc @gante @echarlaix ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",0,open
Adding pruning integration to transformer,"# What does this PR do?

This PR adds width pruning (attention, embedding, MLP) to transformer models. It adds the following features:

- A PrunerMixin class for handling multiple pruning types (embeddings, neurons, GQA, heads).
- Pruning utilities for nn.Embedding and nn.Parameter to pytorch_utils.

## Fixes # (issue)

This PR generalizes the work done in #33180. 

## Still needs to do

- [ ] Add documentation.
- [ ] Add necessary tests.
- [ ] Add depth pruning.


## Who can review?

cc @ArthurZucker @younesbelkada ",[],0,open
Track progress for VLMs refactoring,"This issue tracks the progress on improving the handling and testing of Vision-Language Models. The main goals are to enhance/enable generation tests, handle other generation techniques like assisted decoding and ensure all models pass CI checks. 

I already started working on it and merged/opened some PRs. This issue should help us track how much is left until VLMs are standardized from modeling code perspective.  

- [x] **Enable Generation Tests for VLMs**
  - [x] Merged a PR to calculate and expand text with ""image"" tokens in processing. VLMs currently add only one placeholder per visual. During the modeling phase, we expand the inputs to match the actual length of image embeddings. This approach limits the functionality of `generate()` , especially in enabling other cache formats and torch.compile and introduces hidden bugs. (https://github.com/huggingface/transformers/pull/30962)
  - [ ] Verify that the addition of `processor_config.json` on the hub does not break existing functionality. Related discussion on slack: https://huggingface.slack.com/archives/C01N44FJDHT/p171957701917237). TL;DR: we can't avoid breaking BC but we still want the feature as it has so many benefits. So we'll just try again and hope that users don't use the old version anymore

- [x] **Fix Failing Edge Cases in Current VLMs**
  - [x] Identified edge cases involving multi-image inputs and cache position preparation after merging the above PR (https://github.com/huggingface/transformers/pull/32907)
  - [x] Introduce `num_image_tokens` attribute for specifying image sequence length. It ensures text expansion to the correct length based on the image backbone, otherwise we can't currently use the same processing class for different image backbones. https://github.com/huggingface/transformers/pull/33424

- [x] **Add Generation Tests to VLM Classes**
  - [x] Already added in LLaVA-Onevision and Qwen2-VL (https://github.com/huggingface/transformers/pull/32673, https://github.com/huggingface/transformers/pull/33354)
  - [x] Implement `GenerationTesterMixin` to include tests with both image and text inputs. Current tests accept only text as input. Enable for all models except BLIP ([draft available locally](https://github.com/huggingface/transformers/pull/33533))
  - [x] Add tests for Idefics models and fix Mllama tests which are a bit different from llava style https://github.com/huggingface/transformers/pull/34062
  
  - [x] **Special Case for BLIP**
    - [x] Create a PR to adapt testing suite for BLIP's `main_input_name` which is not `input_ids` like in other model, but is `pixel_values`. Check that we don't cause red CI if we rely on model's `main_input_name` for tests (related or fixed by https://github.com/huggingface/transformers/pull/33685)
    - [x] Remove (optionally) BLIP's custom generation logic and enable generation tests, that should also help us get rid of extra hacks for handling maximum length or `BOS` token in modeling code (https://github.com/huggingface/transformers/pull/34174)
  
  - [ ] **Finalizing CI for VLMs**
    - [x] Resolve `attention_Implementation` related failures to make CI fully happy for VLMs (https://github.com/huggingface/transformers/pull/32238)
    - [ ] Ensure all VLMs pass all CI checks, including slow tests. Identify the reason and fix if there are failures (most probably failure is related to torch version, but need double check)


### Motivation

,

### Your contribution

.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",1,open
Any plans on adding Flash Attention 3?,As title,"[{'id': 1834056761, 'node_id': 'MDU6TGFiZWwxODM0MDU2NzYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Modeling', 'name': 'Core: Modeling', 'color': 'FF8446', 'default': False, 'description': 'Internals of the library; Models.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}]",2,open
Fix exception AttributeError for missing attribute is_zero3 from HfDeepSpeedConfig object,"Fix exception for missing attribute is_zero3 for stage 2 deepspeed.
This issue was observed on Google Colab with runtime GPU A100, when trying to train a Mistral model using deepspeed acceleration.
In the corresponding config json file, stage 2 optimization was specified.
",[],1,open
Support Unified Multimodal Model,"### Feature request

Hi, I am wondering that can this repository supports the unified multimodal model like Show-o? [https://github.com/showlab/Show-o](https://github.com/showlab/Show-o)

### Motivation

The unified multimodal model may be a trend with multimodality

### Your contribution

trying for integration","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",1,open
"Enhanced Model Configuration Testing: An Inclusive Reference Manual for Enhanced Error Management, Reporting and Enhanced Program Code ","In this update, we have significantly improved the script responsible for testing and validating model configuration files, focusing on several key areas: relative to other programming paradigms, the advantages of functional programming are generic variable that allow to work with collections of data, error handling, logging, type checking, and the readability of code. 
 
 Error Handling: This gives a glimpse of some of the enhancements done in this version, one of which is that there are enhanced error handling capabilities. Earlier on, the issues could have not well been captured in the script, and hence, reporting and debugging were complicated. With these changes in place, all the problems that may be experienced when testing are now managed more gracefully. Certain exceptions are captured, and proper messages are passed to facilitate ability to handle problems and solve them easily. 
 
 Logging: Logging system is another significant improvement made in the program. Its logging enables the users to get the track of operations of the script easily. This comes in handy especially when a number of configuration properties can never be configured because of their implementation. Seeing the flow of the script from the logs and knowing what the script did at each point makes the flow much easier to trace and bugs easier to pin point. 
 
 Type Checking: In a bid to making the script more reliable type checking has been included. This enhancement ensures that all the inputs that the script receives have correct data types thus minimizing on mishaps during the runtime stage. When such type mismatches are caught early enough, then the script is much more forgiving and less likely to fail, especially when it comes to different settings. 
 
 Code Readability and Maintainability: It has also been refactored to make it easier to read and easier to maintain or support in What’s New. The unnecessary code is deleted; the logic is made clearer and got more modular so the script is cleaner. Not only does this enhance today’s operation but it is also necessary for future upgrades as well as for handling problems and bugs. 
 
 To this end, all these enhancements have made the configuration testing script more accurate, user-friendly and effective in the validation of model configurations.




## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
bus error on version 4.43.0 with pretrained community CLIP model - MacOS,"### System Info

- `transformers` version: 4.43.0
- Platform: macOS-13.0-arm64-arm-64bit
- Python version: 3.10.9
- Huggingface_hub version: 0.24.6
- Safetensors version: 0.4.5
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.4.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```
from transformers import CLIPModel, CLIPTokenizerFast

tokenizer = CLIPTokenizerFast.from_pretrained(""patrickjohncyh/fashion-clip"")
model = CLIPModel.from_pretrained(""patrickjohncyh/fashion-clip"")

tokenized = tokenizer([""hello""], return_tensors=""pt"", padding=True)
print(""tokenized"", tokenized)

# bus error occurs here
embed = model.get_text_features(**tokenized).detach().cpu().numpy()
print(""embedded"", tokenized)


```



gives :

```
tokenized {'input_ids': tensor([[49406,  3497, 49407]]), 'attention_mask': tensor([[1, 1, 1]])}
zsh: bus error  python test_hf.py
```

I don't think the issue has been posted already.
After bisecting versions, it looks like `4.42.4` does not have the issue and `4.43.0` has the issue


I have little insight to provide except the `bus error`, and that this does not occur with the `clip-vit-base-patch32` model.
I saw some breaking changes in this version release, but only about the tokenizer.
I did not have time to test on a linux distribution yet

Thanks !


### Expected behavior

By using the exact same script with the hugging face CLIP pretrained model, the embedding get computed as they should
```
processor = CLIPProcessor.from_pretrained(""openai/clip-vit-base-patch32"")
tokenizer = CLIPTokenizerFast.from_pretrained(""openai/clip-vit-base-patch32"")
```","[{'id': 1834053813, 'node_id': 'MDU6TGFiZWwxODM0MDUzODEz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/PyTorch', 'name': 'PyTorch', 'color': 'a12bef', 'default': False, 'description': 'Anything PyTorch'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",17,open
AI–Enhanced Benchmarking: New and enhanced features of Open Source and Integration," #### 1. **Summary**: 
 This PR has brought in several improvements that are based on AI in an attempt to boost the functionality of the benchmarking script. The changes are primarily concerning the ways to refine the artificial intelligence for the purposes of metrics processing, commit analysis, and summary generation. Some of the enhancements include: AI based approach for metric selection in the benchmarking report, automatic commit analysis to determine which changes can be expected to affect the performance, the summary of the key results of the benchmarking, the improved management of the models and configurations. Also, the AI is used for Hugging Face Hub interactions and to validate the data format during the upload process. These changes are useful in enhancing the effectiveness of the benchmarking script, gives out more information and can work with large and complex dataset. 
 
 #### 2. **Related Issues**: 
 - This update addresses the challenge of analyzing the large datasets that was a time consuming and a complex process. 
 - The application of AI in commit analysis helps to solve the problem of figuring out the performance changes of large projects distributed in many commits. 
 - The issue of how to prepare brief and concise reports based on numerous benchmarking outcomes is resolved with the use of Artificial Intelligence for summary generation. 
 - To overcome the compatibility issues in terms of model and configurations, automated artificial intelligence based management was employed. 
 
 #### 3. **Discussions**: 
 The arguments presented were on how the use of artificial intelligence would be of advantage in the benchmarking process. The team also pointed out that the selection of metrics, commits analysis and summarization should also be made automatic in order to enhance the efficiency of the process. Further, there was focus on increasing the robustness of the model and configuration management and to allow for proper interaction with the Hugging Face Hub for data upload. This was to enhance the efficiency of the benchmarking script and to reduce on the possibility of errors. 
 
 #### 4. **QA Instructions**: 
 - **AI-Enhanced Metrics Processing**: Make sure that the AI can choose the right metrics to display in the benchmarking reports and that such metrics are labeled. 
 - **Commit Analysis**: Ensure that the commit analysis based on AI is right in identifying the commits that may have effected changes in the performance either positively or negatively. 
 - **Smart Summary Generation**: It has been observed that the summaries generated from the AI should be brief and should only contain information of comparison of performance and recommendations for improvement. 
 - **Model and Configuration Management**: Make sure that the AI takes into consideration a number of model and configuration parameters and that it gives out an alarm should they interfere in one way or another. 
 - **Hugging Face Hub Integration**: Ensure that the auto upload of results is in order and also ensure that the format of the data is correct if not correct it. 
 
 #### 5. **Merge Plan**: 
 After the QA testing part is over and the developer is satisfied with the performance of the AI components, this branch will be merged with the master branch. Extra focus will be placed on the fact that new features incorporated through the use of AI will not harm the benchmarking activity, but rather improve it. It will be ensured that all the new features which will be included will be fine tuned for the production. 
 
 #### 6. **Motivation and Context**: 
 This is because the benchmark script is to be optimized using artificial intelligence approaches and thus there is need to modify it. Some of the areas that can be automated include the selection of metric, commit analysis, and summary generation; this will ensure that the developers are able to spend their time more constructively and also gain more insight on performance. The inclusion of the AI-based model and configuration management also improve compatibility while the integration with the Hugging Face Hub aids in sharing of data. These changes make the benchmarking process much more accurate, accurate and time efficient. 
 
 #### 7. **Types of Changes**: 
 - **New Feature**: Automation of the commit analysis process, identification of metrics and summarization has been enhanced. 
 - **Enhancement**: Improvement in the model and configuration management for the compatibility and increasing the performance of the system. 
 - **Automation**: Quick and seamless file submission together with the possibility of integrating the model to the Hugging Face Hub and automatic formatting check. 
 - **Performance**: A refined benchmarking process that would give a better analysis of data and reducing the aspect of manual analysis.



## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Accelerate x Trainer issue tracker:,"A bunch of issues are a bit stale, and @SunMarc + @muellerzr are a bit short on bandwidth! 
Thus we would love to have community support to solve the following: 

### Help needed
- [x] #27830
- [ ]  #30911
- [x] #30702
- [x] #30239
- [ ] #29348
- [ ]  #33157
- [ ] #28469 
- [x] #30663 
- [x] #30811
- [x] #30819
- [x] #31313 
- [x] #31457
- [x] #30859
- [x] #31897
- [x] #30340 
- [x] #31892
- [x] #28914 
- [x] #29518 
- [x] #30277
- [x] #33376

### Feature request

- [ ] #30725


### Replied with potential fix and following 
- [ ] #31734 followed by @irislin1006 
- [ ] #32312 followed by @irislin1006 
- [x] #31818 followed by @mekkcyber
- [x] #31439 followed by @nnilayy
- [ ] #28124 followed by @muellerz and @WizKnight
- [ ] #30767 followed by @SunMarc 
- [ ] #33147 followed by @SunMarc 
- [x] #33400 followed by @SunMarc 
- [ ] #26413 followed by @muupan, @muellerzr and @SunMarc 
- [x] #28808 followed by @Ben-Schneider-code
- [x] #31357 followed by @mekkcyber
- [x] #33733 resolved by the author
- [ ] #30330 followed by @SunMarc 
- [ ] #30913 followed by @mekkcyber
- [ ] #27487 followed by @SunMarc 
- [ ] #33336 followed by @MekkCyber 
- [ ] #25695 followed by @MekkCyber 
- [ ] #31278 followed by @muellerzr
- [ ] #32427 followed by @muellerzr and @SunMarc 
- [ ] #31034 followed by @muellerzr
- [ ] #30822 followed by @muellerzr
- [ ] #31867 followed by @Ben-Schneider-code and @SunMarc 
","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 4101623725, 'node_id': 'LA_kwDOCUB6oc70ec-t', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/PyTorch%20FSDP', 'name': 'PyTorch FSDP', 'color': 'B60205', 'default': False, 'description': ''}, {'id': 4608548278, 'node_id': 'LA_kwDOCUB6oc8AAAABErDdtg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/HACKTOBERFEST-ACCEPTED', 'name': 'HACKTOBERFEST-ACCEPTED', 'color': 'FF5733', 'default': False, 'description': ''}, {'id': 6660834292, 'node_id': 'LA_kwDOCUB6oc8AAAABjQRD9A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Accelerate', 'name': 'Accelerate', 'color': '59003B', 'default': False, 'description': ''}]",19,open
"Add ""EAT: Self-Supervised Pre-Training with Efficient Audio Transformer""","### Model description

The original authors of the model write: 
> EAT is an audio self-supervised learning model with high effectiveness and efficiency during self-supervised pre-training. You can find details in the paper [EAT: Self-Supervised Pre-Training with Efficient Audio Transformer](https://arxiv.org/abs/2401.03497).

A self-supervised learning model can benefit the community greatly, since it requires no labelled data, and can be trained on any dataset. Especially since, the strength of this approach is that it can be applied to variable-length audio. With enough resources (for example, compute, and, data), it could have a similar reach as BERT.  

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

- GitHub Repo: https://github.com/cwx-worst-one/EAT
- Links for model checkpoints:
    - [EAT-base_epoch30](https://drive.google.com/file/d/19hfzLgHCkyqTOYmHt8dqVa9nm-weBq4f/view?usp=sharing) (pre-training)
    - [EAT-base_epoch30](https://drive.google.com/file/d/1aCYiQmoZv_Gh1FxnR-CCWpNAp6DIJzn6/view?usp=sharing) (fine-tuning on AS-2M)
    - [EAT-large_epoch20](https://drive.google.com/file/d/1PEgriRvHsqrtLzlA478VemX7Q0ZGl889/view?usp=sharing) (pre-training)
    - [EAT-large_epoch20](https://drive.google.com/file/d/1b_f_nQAdjM1B6u72OFUtFiUu-4yM2shd/view?usp=sharing) (fine-tuning on AS-2M)
- Paper: https://www.ijcai.org/proceedings/2024/421","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Add inverse chat templating,"Very experimental PR for now! This PR adds **inverse chat templating**, where we convert a formatted chat back into message dicts in a universal format. This has been requested several times, and it's critical to allow seamless tool use in pipelines, without requiring users to manually write parsers for each model.

Inverse templating requires the ability to extract text from a large input string. After testing several templates, I found that we can generally handle this by exposing two functions in the inverse templating environment: a slightly modified `re.finditer()` and a totally unmodified `json.loads()`

TODO:
- [x] Make some model PRs to test with this, but don't merge yet! (PRs not open, but templates written)
- [x] Add tests for loading/saving of inverse templates
- [x] Add tests for inverse template function
- [x] Test recovery of tools as well as messages
- [x] Make sure I don't need any extra functions
- [x] Refactor chat template tests out of `tokenization_common` so they're not run for every model
- [x] Add chat template tests to CircleCI
- [x] ~Make sure extraction works correctly with generation, and add some tests that use (static!) generation outputs~
- [x] ~Add tool use to pipelines~ (will put this in a separate PR)",[],16,open
Enhanced Logging for Seq2SeqTrainingArguments Initialization,"Enhanced logging in Seq2SeqTrainingArguments to provide detailed information during instance initialization. This change will help in better debugging and tracking the argument values used in Seq2Seq training, without impacting the core functionality.",[],0,open
Make Ignored Columns ValueError More Informative,"# What does this PR do?
Included forward method signature columns in the ValueError so end users will know what columns are expected to be passed to the model in addition to those which are ignored.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? [issue 33119](https://github.com/huggingface/transformers/issues/33119#issuecomment-2321681582)
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@SunMarc I made the modification discussed previously in the thread linked above.
",[],0,open
Generate: all caches can be reused,"# What does this PR do?

WIP",[],1,open
oom when using adafactor optimizer in deepspeed,"### System Info

```python
- `transformers` version: 4.44.2
- Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31
- Python version: 3.10.0
- Huggingface_hub version: 0.23.4
- Safetensors version: 0.4.2
- Accelerate version: 0.33.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.0+cu118 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A800 80GB PCIe
```

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

i'm running train_xl.sh in [this repo](https://github.com/yisol/IDM-VTON). and i change the 8bit adam optimizer to adafactor optimizer using transformers.optimization.Adafactor. i'm using two 40GB a100, deepspeed stage 2, batchsize=1,VTON-HD dataset. 

the adafactor optimizer should use less gpu memory, because of less optimizer states than 8bit adam, but it get oom in [this line](https://github.com/huggingface/transformers/blob/ecd61c62862f925a18b4f063dc17fcaf01826e25/src/transformers/optimization.py#L877)

and oom happens after 10 steps, i don't know what happen in 10th step, i call the ```accelerate.backward()``` and``` optimizer.step()``` every step.

and in 10th step, the memory usage increased from 29GB to 39GB when using 8bit adam optimizer, and get oom when using adafactor optimizer



### Expected behavior

could anybody explain this phenomenon","[{'id': 1834081910, 'node_id': 'MDU6TGFiZWwxODM0MDgxOTEw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Usage', 'name': 'Usage', 'color': 'e28436', 'default': False, 'description': 'General questions about the library'}, {'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Qwen2-VL Doesn't Execute on TPUs,"### System Info

- `transformers` version: 4.45.0.dev0
- Platform: Linux-5.4.0-1043-gcp-x86_64-with-glibc2.31
- Python version: 3.10.14
- Huggingface_hub version: 0.24.6
- Safetensors version: 0.4.4
- Accelerate version: 0.33.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.5.0.dev20240830+cpu (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

_No response_

### Information

- [x] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

#Following this Qwen2-VL guide => https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct#quickstart

1. Script
```
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
import numpy as np
import torch
import torch_xla as xla
import torch_xla.core.xla_model as xm
import torch_xla.distributed.spmd as xs

from torch.distributed._tensor import DeviceMesh, distribute_module
from torch_xla.distributed.spmd import auto_policy

from torch_xla import runtime as xr
from torch_xla.experimental.spmd_fully_sharded_data_parallel import (
    _prepare_spmd_partition_spec,
    SpmdFullyShardedDataParallel as FSDPv2,
)

import time

start = time.time()

device = xm.xla_device()

# default: Load the model on the available device(s)
model = Qwen2VLForConditionalGeneration.from_pretrained(
    ""Qwen/Qwen2-VL-2B-Instruct"",
    torch_dtype=torch.bfloat16,
    attn_implementation=""eager"",
).to(device)


print(model.device)

# default processer
processor = AutoProcessor.from_pretrained(""Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4"")


message = [
    {
        ""role"": ""user"",
        ""content"": [
            {
                ""type"": ""image"",
                ""image"": ""image1.jpg"",
            },
            {""type"": ""text"", ""text"": ""Describe this image in detail.""},
        ],
    }
]

all_messages = [[message] for _ in range(1)]
for messages in all_messages:

    # Preparation for inference
    texts = [
        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)
        for msg in messages
    ]

    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=texts,
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors=""pt"",
    )
    inputs = inputs.to(device)

    # Inference: Generation of the output
    generated_ids = model.generate(**inputs, max_new_tokens=512)
    generated_ids_trimmed = [
        out_ids[len(in_ids) :]
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )
    for i, text in enumerate(output_text):
        print(f""Output {i}: {text}"")

print(f""Time taken: {time.time() - start}"")
```

2. Output Logs
```
kojoe@t1v-n-cb70f560-w-0:~/EasyAnimate/easyanimate/image_caption$ python caption.py
WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.39it/s]
xla:0
```


### Expected behavior

The model works fine when chaging ```device``` to ```""cpu""```, but stuck executing on TPUs. The model should run on TPUs","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5160774128, 'node_id': 'LA_kwDOCUB6oc8AAAABM5sp8A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/TPU', 'name': 'TPU', 'color': 'EF97D1', 'default': False, 'description': ''}]",3,open
Add support for JAIS Family of Arabic Language Models,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Add a new model


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

cc : @ArthurZucker

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
scale gradient accumulation steps with train batch size to keep effective batch size about the same,"# What does this PR do?
goes with https://github.com/huggingface/accelerate/pull/3071
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
feat: support FSDP config through a dataclass and allow for state_dict_type,"# What does this PR do?

The PR introduces two changes broadly

1. Introduces a new data class to parse FSDP config for transformer + torchrun users
2. allows for `state_dict_type` FSDP option by passing it to FSDP accelerator plugin through env variable

For (1) changes in the trainer code are kept to 0 and impact is mostly concentrated on the trainer args by equipping the dataclass object with `get` and `subscriptable` dict APIs.


Fixes #29476

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. #29476
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@muellerzr and @SunMarc

",[],0,open
Community contribution: Adding GGUF support for more architectures,"### Feature request

Recently, we have added the ability to load `gguf` files within [transformers](https://huggingface.co/docs/hub/en/gguf).

<img src=""https://github.com/user-attachments/assets/61df6455-6016-449e-a37f-9dfc7f918902"" width=""600"">


The goal was to offer the possibility to users to further train/fine-tune their gguf models. 
<details>
<summary>See Workflow</summary>
1) Load gguf file in transformers: we dequantize the weights to fp32, then we load the weights to be used with PyTorch.

```py 
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = ""TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF""
filename = ""tinyllama-1.1b-chat-v1.0.Q6_K.gguf""

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
```
2) train/finetune

3) Convert the model back to gguf to use in the ggml ecosystem using [convert_hf_to_gguf](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) script or using [gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space if you pushed your model on the hub :
```py
tokenizer.save_pretrained('directory')
model.save_pretrained('directory')

!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}
```
</details>

Let's try to add GGUF support for more architectures! Currently supported architectures are

- [x] Llama
- [x] Mistral
- [x] Qwen2

It would be great to add the support for more architectures such as

- [x] Phi3 https://github.com/huggingface/transformers/pull/31844
- [x] Qwen2Moe https://github.com/huggingface/transformers/pull/33264
- [ ] Gemma2 
- [ ] T5 https://github.com/huggingface/transformers/pull/33389
- [x] Falcon https://github.com/huggingface/transformers/pull/33437
- [x] Bloom https://github.com/huggingface/transformers/pull/33473
- [ ] Codestrall
- [ ] dbrx
- [ ] Deepseek (when it will be added to transformers)
- [x] StableLM https://github.com/huggingface/transformers/pull/33793
- [x] gpt2 https://github.com/huggingface/transformers/pull/34044
- [x] starcoder2 https://github.com/huggingface/transformers/pull/34094

... and many more (Feel free to suggest more architectures ! The model needs to integrated in transformers)

Adding this feature would require to follow the same protocol as in this [PR](https://github.com/huggingface/transformers/pull/31175/files) : 
1) Update `GGUF_TENSOR_MAPPING` and `GGUF_CONFIG_MAPPING` in order to map the tensor/config of the gguf file to the one on transformers. 
2) Create a `GGUFXXXConverter(XXXConverter)` class to convert the gguf tokenizer to a transformers one. 
3) Write tests


If you are interested to take up the challenge, comment below with the architecture name you want to integrate and open a PR! 

Once you open a PR, feel free to ping @SunMarc @LysandreJik @ArthurZucker for a review ! 

### Motivation

Support for more gguf models

### Your contribution

Reviewing PRs and possibly adding the support for more models","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",28,open
add Cohere using modular transformers,"# What does this PR do?
Superseeds #31518",[],1,open
How to use hugginface for training:  google-t5/t5-base,"### Feature request

How to use hugginface for training / 如何使用huggingface来训练：
 https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation

#What is the format and how do I write it? / 这个格式是怎么样的，怎么写呢？
def batch_collator(data):
    print(data)  #?????????????????????????????????????????????   
    return {
        'pixel_values': torch.stack([x for x in pixel_values]), 
        'labels': torch.tensor([x for x in labels]) 
    }

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=batch_collator,//这个需要怎么写?
    train_dataset=dataset['train'],    
)

### Motivation

无

### Your contribution

无


我已经试了可以用： https://www.kaggle.com/code/weililong/google-t5-t5-base 
不知道有没有什么坑","[{'id': 1834081910, 'node_id': 'MDU6TGFiZWwxODM0MDgxOTEw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Usage', 'name': 'Usage', 'color': 'e28436', 'default': False, 'description': 'General questions about the library'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
prune_heads() method for AutoModelForCausalLM,"### Feature request

We have `prune_heads()` method for `AutoModel` class, but not for `AutoModelForCausalLM`. Please provide `prune_heads()` method to `AutoModelForCausalLM` class.

### Motivation

Mechanistic interpretability study from the paper [Are Sixteen Heads Really Better than One?](https://arxiv.org/abs/1905.10650) explore how pruning attention heads effect the model's (BERT) performance. I want to do something similar for LLMs. Since LLMs are supported via `AutoModelForCausalLM` class, I am unable to do this experiment.


### Your contribution

Just getting started with HuggingFace. Not confident about implementing this feature on my own as of now. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Agents - Delineate sequences with easier tags,"# What does this PR do?

Based on an idea by @plaggy.
This PR changes the tags that we use for now (successive headings like `Action:`, then `Observation:`) to more LLM-ready sequences (`<action>` and `<end_action>` tags) to make the different parts easier to read for the LLM.
The impact of this change on perf needs to be assessed though, I'm currently running tests.","[{'id': 6838975861, 'node_id': 'LA_kwDOCUB6oc8AAAABl6J9dQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Agents', 'name': 'Agents', 'color': 'FBCA04', 'default': False, 'description': ''}]",1,open
Add propainter,"# What does this PR do?
This PR adds ProPainter, a Video Inpainting model with 5.4k stars and 635 forks [repo](https://github.com/sczhou/ProPainter). It fixes #26360 and resolve stale PR #26391 for the above issue from complete scratch to build on with transformers standard.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@amyeroberts @ArthurZucker @NielsRogge (?)
@rafaelpadilla(as he was the initial reviewer on the stale PR)

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

#### The PR is more than ready for first pass of review!!!

#### TODO(will be done in a fly :)):
- [x] Fix all common test failures
- [x] Update weights conversion scripts with the working one on local machine
- [x] Review batching nits one more time in the applicable files
- [x] Update docs in corresponding files
- [x] Check for video 'outpainting' error

#### Results:
Here, I am attaching the GIFs for original video, original model's output for object removal through video inpainting and the current PR' HF model's output for object removal through video inpainting:

Original video:
![original](https://github.com/user-attachments/assets/7b768c66-4944-407b-8f6a-e10e7a53bdcb)


Original model output:
![original_removal](https://github.com/user-attachments/assets/eaa40751-9a04-4e0b-b87b-9302da3f259b)

HF ported model output:

![hf_removal](https://github.com/user-attachments/assets/126b6d45-78e9-400a-95a5-ac5575db5896)


##### Example usage is provided in the doc file [here](https://github.com/RUFFY-369/transformers/blob/add_propainter/docs/source/en/model_doc/propainter.md)","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",36,open
"CI: avoid human error, automatically infer generative models","# What does this PR do?

This PR:
1. Replaces manual definition of generative models to test with `generate` with an automatic definition -- if `model_class.can_generate()`, then it runs tests from `GenerationTesterMixin`. No more human mistakes, which happened frequently in the past months 🐛 🔫 
2. Now that we run generation tests for all models that can generate, there are a few (old) bad apples. Explicitly skip them i.e. overwrite the automated `all_generative_model_classes` and explain why certain classes are being skipped. (bad apples detected with `py.test tests/models/ -k test_greedy_generate -vv`) 💔 
3. Moves tests that call `generate` from the generic test mixin to `GenerationTesterMixin`. This means a) we can have a better overview of what's being tested with `generate`; and b) model architectures without generative capabilities will have fewer skips 🎯 

_____________________________________________
In a follow-up PR:
1. ~We need to manually define the model's main input name (e.g., `pixel_values`) in the model tester. Make it use `model.main_input_name` instead, to avoid human error~ Done ✅ 
2. Despite the changes in this PR, `generate` tests will only run if `GenerationTesterMixin` is inherited. We can easily forget to add the mixin, resulting in a false positive. Add an automated check: if any of the model classes can generate, then `GenerationTesterMixin` must be inherited in the tester",[],2,open
"The model's address is https://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx。I don't know how to load encode.onnx and decoder.onnx, and successfully translate a sentence into another language. Can you help me write an inference code to achieve the translation effect through the encoder and decoder? thank you","### Feature request

hello，The model's address is [https://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx。I](https://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx%E3%80%82I) don't know how to load encode.onnx and decoder.onnx, and successfully translate a sentence into another language. Can you help me write an inference code to achieve the translation effect through the encoder and decoder? thank you

### Motivation

hello，The model's address is [https://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx。I](https://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx%E3%80%82I) don't know how to load encode.onnx and decoder.onnx, and successfully translate a sentence into another language. Can you help me write an inference code to achieve the translation effect through the encoder and decoder? thank you

### Your contribution

hello，The model's address is [https://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx。I](https://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx%E3%80%82I) don't know how to load encode.onnx and decoder.onnx, and successfully translate a sentence into another language. Can you help me write an inference code to achieve the translation effect through the encoder and decoder? thank you","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
MultiTask Classification and label_names on Trainer,"### System Info

Transformers: 4.40.2 

### Who can help?

@muellerzr @SunMarc @ArthurZucker

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

I’m working on a multi task classification with DistilBert with 4 labels.

I started training the model and it finished the first epoch, then it starts evaluation and throws the error below at the end of the evaluation. If I take the` load_best_model_at_end ` out of the trainer args it runs the eval, but i get no eval loss. I also ran predict and I found out that I got `label_ids=None`. It seems that labels_names is not correctly working and is not passed to the predict.

I ran: 
`for batch in trainer.get_eval_dataloader(data['test']):
    print(batch)
    break
`
And got the follwoing:
```
`{'input_ids': tensor([[  101, 67618, 10671,  ...,     0,     0,     0],
        [  101, 67618, 10671,  ...,   169, 12211,   102],
        [  101, 27746, 13386,  ...,     0,     0,     0],
        [  101, 73219, 14002,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'line_labels': tensor([3, 1, 1, 1], device='cuda:0'), 'cat_labels': tensor([ 9, 16, 16, 16], device='cuda:0'), 'sub_cat_labels': tensor([77, 48, 48, 48], device='cuda:0'), 'motive_labels': tensor([ 2, 34, 34, 34], device='cuda:0')}`
```

I really need help figuring out what is going on here I out of options I can’t understand what is going on. If you could shed a light I would appreciate it. I'm really at the point where I feel I need to use a custom trainer, because I have no more solution to try.


Code: 

```
# Defining the metrics 

LINE_METRIC = evaluate.load(""f1"")
CAT_METRIC = evaluate.load(""f1"")
SUB_CAT_METRIC = evaluate.load(""f1"")
MOTIVE_METRIC = evaluate.load(""f1"")

def compute_metrics(eval_pred):
    print(eval_pred)
    all_logits, all_labels = eval_pred
    logits_line, logits_cat, logits_sub_cat, logits_motive = all_logits 
    line_labels, cat_labels, sub_cat_labels, motive_labels = all_labels

    line_predictions = np.argmax(logits_line, axis=-1)
    cat_predictions = np.argmax(logits_cat, axis=-1)
    sub_cat_predictions = np.argmax(logits_sub_cat, axis=-1)
    motive_predictions = np.argmax(logits_motive, axis=-1)
    
    print(""PRED"")
    print(line_predictions, cat_predictions, sub_cat_predictions, motive_predictions)
    
    line_computed_metrics = LINE_METRIC.compute(predictions=line_predictions, references=line_labels, average='weighted')
    cat_computed_metrics = CAT_METRIC.compute(predictions=cat_predictions, references=cat_labels, average='weighted')
    sub_cat_computed_metrics = SUB_CAT_METRIC.compute(predictions=sub_cat_predictions, references=sub_cat_labels, average='weighted')
    motive_computed_metrics = MOTIVE_METRIC.compute(predictions=motive_predictions, references=motive_labels, average='weighted')

    print(""SCORE"")
    print(line_computed_metrics, cat_computed_metrics, sub_cat_computed_metrics, motive_computed_metrics)

    return {
        'f1_line': line_computed_metrics['f1'],
        'f1_cat': cat_computed_metrics['f1'],
        'f1_sub_cat': sub_cat_computed_metrics['f1'],
        'f1_motive': motive_computed_metrics['f1'],
    }
`

`
output_directory = RESULTS_DIRECTORY
evaluation_strategy = 'epoch'
per_device_train_batch_size = 4
per_device_eval_batch_size = 4
gradint_accumulation_steps = 2
learning_rate = 2e-5
weight_decay = 0.01
max_grad_norm = 1
num_train_epochs = NUM_TRAIN_EPOCHS
lr_scheduler_type = 'linear'
warmup_ratio = 0.05
logging_dir = LOGGING_DIRECTORY
logging_strategy = 'epoch'
save_strategy = 'epoch'
save_total_limit = 1
**label_names = ['line_labels', 'cat_labels', 'sub_cal_label','motive_labels']**
load_best_model_at_end = True
metric_for_best_model = 'eval_f1_cat'
greater_is_better = True
label_smoothing_factor = 0
#report_to = 'tensorboard'
gradient_checkpointing = False
`


`
# Setup training arguments
training_args = TrainingArguments(
    output_dir=output_directory,
    evaluation_strategy=evaluation_strategy,
    learning_rate=learning_rate,
    per_device_train_batch_size=per_device_train_batch_size,
    per_device_eval_batch_size=per_device_eval_batch_size,
    num_train_epochs=num_train_epochs,
    weight_decay=weight_decay,
    logging_dir=logging_dir,
    label_names=label_names,
    max_grad_norm=max_grad_norm,
    lr_scheduler_type=lr_scheduler_type,
    warmup_ratio=warmup_ratio,
    logging_strategy=logging_strategy,
    save_strategy=save_strategy,
    save_total_limit=save_total_limit,
    load_best_model_at_end=load_best_model_at_end,
    #metric_for_best_model=metric_for_best_model,
    #greater_is_better=greater_is_better,
    label_smoothing_factor=label_smoothing_factor,
    #report_to=report_to,
    gradient_checkpointing=gradient_checkpointing
)

#early_stop_callback = EarlyStoppingCallback(3)
# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data['train'],
    eval_dataset=data['test'],
    #tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    #callbacks=[early_stop_callback])
`

### Expected behavior

Error:

`
KeyError                                  Traceback (most recent call last)
Cell In[36], line 1
----> 1 trainer.train()

File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1859, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1857         hf_hub_utils.enable_progress_bars()
   1858 else:
-> 1859     return inner_training_loop(
   1860         args=args,
   1861         resume_from_checkpoint=resume_from_checkpoint,
   1862         trial=trial,
   1863         ignore_keys_for_eval=ignore_keys_for_eval,
   1864     )

File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2298, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   2295     self.control.should_training_stop = True
   2297 self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)
-> 2298 self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
   2300 if DebugOption.TPU_METRICS_DEBUG in self.args.debug:
   2301     if is_torch_xla_available():
   2302         # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)

File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2673, in Trainer._maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
   2670         self.lr_scheduler.step(metrics[metric_to_check])
   2672 if self.control.should_save:
-> 2673     self._save_checkpoint(model, trial, metrics=metrics)
   2674     self.control = self.callback_handler.on_save(self.args, self.state, self.control)

File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2765, in Trainer._save_checkpoint(self, model, trial, metrics)
   2763 if not metric_to_check.startswith(""eval_""):
   2764     metric_to_check = f""eval_{metric_to_check}""
-> 2765 metric_value = metrics[metric_to_check]
   2767 operator = np.greater if self.args.greater_is_better else np.less
   2768 if (
   2769     self.state.best_metric is None
   2770     or self.state.best_model_checkpoint is None
   2771     or operator(metric_value, self.state.best_metric)
   2772 ):

KeyError: 'eval_loss'
```","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",9,open
Add warning and info  message for beta and gamma parameters,"# What does this PR do?
This adds a warning message to notify about the renaming of gamma and beta parameters during initialisation and also during loading.

before:
```
(vqa-audio) (base) jeeves@notebook-5064-cadence:~/ChatTTS/rhapsodyaudio$ python tmp_save_pretrain.py 
bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████| 8/8 [00:16<00:00,  2.02s/it]
Some weights of Qwen2AudioForConditionalChatTTS were not initialized from the model checkpoint at /mnt/data/user/tc_agi/luoyuanZ/ChatTTS_default and are newly initialized: 

['tts.dvae.decoder.decoder_block.0.gamma', 'tts.dvae.decoder.decoder_block.1.gamma', 'tts.dvae.decoder.decoder_block.10.gamma', 'tts.dvae.decoder.decoder_block.11.gamma', 'tts.dvae.decoder.decoder_block.2.gamma', 'tts.dvae.decoder.decoder_block.3.gamma', 'tts.dvae.decoder.decoder_block.4.gamma', 'tts.dvae.decoder.decoder_block.5.gamma', 'tts.dvae.decoder.decoder_block.6.gamma', 'tts.dvae.decoder.decoder_block.7.gamma', 'tts.dvae.decoder.decoder_block.8.gamma', 'tts.dvae.decoder.decoder_block.9.gamma', 'tts.dvae.encoder.decoder_block.0.gamma', 'tts.dvae.encoder.decoder_block.1.gamma', 'tts.dvae.encoder.decoder_block.10.gamma', 'tts.dvae.encoder.decoder_block.11.gamma', 'tts.dvae.encoder.decoder_block.2.gamma', 'tts.dvae.encoder.decoder_block.3.gamma', 'tts.dvae.encoder.decoder_block.4.gamma', 'tts.dvae.encoder.decoder_block.5.gamma', 'tts.dvae.encoder.decoder_block.6.gamma', 'tts.dvae.encoder.decoder_block.7.gamma', 'tts.dvae.encoder.decoder_block.8.gamma', 'tts.dvae.encoder.decoder_block.9.gamma']
```
after:
```
(vqa-audio) (base) jeeves@notebook-5064-cadence:~/ChatTTS/rhapsodyaudio$ python tmp_save_pretrain.py 
bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
This model <class 'muffin.model.infer_qwen2tts.Qwen2AudioForConditionalChatTTS'>contains parameters that have been renamed internally (a few are listed below but more are present in the model):

Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████| 8/8 [00:14<00:00,  1.81s/it]
```

<!-- Remove if not applicable -->

Fixes #29554 and #33190 (issue)


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
<img width=""999"" alt=""image"" src=""https://github.com/user-attachments/assets/66ba9274-66c3-494f-afaa-02953c3f08d7"">


<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
Add LlavaImageProcessor,"# What does this PR do?

Fixes #33175. It adds the option to pad an image before applying the same preprocessing as `CLIPImageProcessor` based on the [original implementation](https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/mm_utils.py#L152). This allows people to match the logits with the original implementation.

For now I decided to set `do_pad` to `False` by default as otherwise it would be a breaking change.

To do:

- [x] update padding method to take numpy as input and produce numpy as output
- [x] add equivalence test
- [x] add tests",[],1,open
Support TF32 flag for MUSA backend,"# What does this PR do?
Since both accelerate and transformers have merged the supporting of MUSA backend (see #31913 ), we have trained several models on our MTGPU.

 Recently we met an issue that if we enable TF32 by adding flag of `--tf32` from `accelerate` or `deepspeed`, it would fail while we already support TF32 computing on MUSA (ref: [here](https://github.com/MooreThreads/torch_musa/blob/7dcb8b28a19d2cd7b533d2386f255b37c113efa5/torch_musa/csrc/aten/utils/Context.cpp#L24-L30) ), so we'd like to enable this feature from transformers.

",[],0,open
Is it possible to infer the model separately through encoder.onnx and decoder.onnx,"### Feature request

Is it possible to infer the model separately through encoder.onnx and decoder.onnx

### Motivation

Is it possible to infer the model separately through encoder.onnx and decoder.onnx

### Your contribution

Is it possible to infer the model separately through encoder.onnx and decoder.onnx","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Supporting Padding in llava processor,"### Feature request

https://github.com/haotian-liu/LLaVA uses padding for pre-processing the images by default. Current transformers implementation does not support that.

### Motivation

Request per @NielsRogge at (https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/26#66cf46a5a523b74b5f90fa72).

### Your contribution

I successfully reproduced logits after conversion if we add padding in the Transformers library.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
[Zero-shot image classification pipeline] Remove tokenizer_kwargs,"# What does this PR do?

This PR is a follow-up of #29261, namely the `tokenizer_kwargs` argument is unnecessary, one can just update the `model_input_names` attribute of the tokenizer.",[],6,open
Add Sapiens model,"# What does this PR do?

Add [Sapeins](https://about.meta.com/realitylabs/codecavatars/sapiens/) model released by Meta for 
 - semantic segmentation
 - pose keypoint detection
 - normal estimation
 - depth estimation

Original code: https://github.com/facebookresearch/sapiens
Paper: https://arxiv.org/abs/2408.12569

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",4,open
Failed to load universal_checkpoint with deepspeed integreation,"### System Info

- `transformers` version: 4.44.2
- Platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.17
- Python version: 3.8.18
- Huggingface_hub version: 0.24.6
- Safetensors version: 0.4.4
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.4.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A800 80GB PCIe

### Who can help?

@muellerzr

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

The [Universal Checkpointing](https://github.com/microsoft/Megatron-DeepSpeed/tree/main/examples_deepspeed/universal_checkpointing) feature allows loading with different world sizes. However, when using the Hugging Face `Trainer`, the loading of the converted universal checkpoint fails.

The failure seems to be due to `HfTrainerDeepSpeedConfig` not correctly handling the `""load_universal_checkpoint"": true` or `""universal_checkpoint"": true` arguments in the DeepSpeed configuration. Consequently, the [`load_universal_checkpoint`](https://github.com/microsoft/DeepSpeed/blob/eb37cacf229ece71580a30bec723bf361f4ba82f/deepspeed/runtime/engine.py#L862) function returns `False`.

**Related Issues:**
- https://github.com/microsoft/DeepSpeed/issues/5430
- https://github.com/microsoft/DeepSpeed/issues/2921


### Expected behavior

Universal checkpoint should be loaded correctly.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",5,open
Multi-GPU setup: indices should be either on cpu or on the same device as the indexed tensor (cuda:1),"### System Info

python version: 3.11.9
transformers version: 4.44.2
accelerate version: 0.33.0
torch version: 2.4.0+cu121

### Who can help?

@gante

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Hello!

I have a setup with 8xH100 and I need to run really large models. To get started I went through your official example, it is https://huggingface.co/docs/accelerate/en/concept_guides/big_model_inference

First of all, there is a typo in there:

```python
from mingpt.bpe import BPETokenizer
tokenizer = BPETokenizer()
inputs = tokenizer(""Hello, my name is"").to(0)

outputs = model.generate(x1, max_new_tokens=10, do_sample=False)[0]
tokenizer.decode(outputs.cpu().squeeze())
```

There is no x1 variable. Example from this guide works well.

However, I tried to do the same using Mistral models, so I adapted code form an example to run it with Mistral model:

```python
from huggingface_hub import snapshot_download
from accelerate import init_empty_weights
from transformers import AutoModelForCausalLM, AutoTokenizer
from accelerate import load_checkpoint_and_dispatch

checkpoint = 'mistralai/Mixtral-8x22B-Instruct-v0.1'

weights_location = snapshot_download(repo_id=checkpoint, cache_dir='./cache')
model = AutoModelForCausalLM.from_pretrained(checkpoint)

from accelerate import load_checkpoint_and_dispatch

model = load_checkpoint_and_dispatch(
    model, checkpoint=weights_location, device_map=""auto"", no_split_module_classes=['Block']
)

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(""Hello, my name is"", return_tensors=""pt"").to(0)
outputs = model.generate(inputs['input_ids'], max_new_tokens=10, do_sample=False)[0]

```
This code fails on the last line giving the following exception:

```
RuntimeError                              Traceback (most recent call last)
Cell In[37], line 1
----> 1 outputs = model.generate(inputs['input_ids'], max_new_tokens=10, do_sample=False)[0]
      2 tokenizer.decode(outputs.cpu().squeeze())

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)
    113 @functools.wraps(func)
    114 def decorate_context(*args, **kwargs):
    115     with ctx_factory():
--> 116         return func(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/generation/utils.py:2024, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)
   2016     input_ids, model_kwargs = self._expand_inputs_for_generation(
   2017         input_ids=input_ids,
   2018         expand_size=generation_config.num_return_sequences,
   2019         is_encoder_decoder=self.config.is_encoder_decoder,
   2020         **model_kwargs,
   2021     )
   2023     # 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)
-> 2024     result = self._sample(
   2025         input_ids,
   2026         logits_processor=prepared_logits_processor,
   2027         logits_warper=prepared_logits_warper,
   2028         stopping_criteria=prepared_stopping_criteria,
   2029         generation_config=generation_config,
   2030         synced_gpus=synced_gpus,
   2031         streamer=streamer,
   2032         **model_kwargs,
   2033     )
   2035 elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):
   2036     # 11. prepare logits warper
   2037     prepared_logits_warper = (
   2038         self._get_logits_warper(generation_config, device=input_ids.device)
   2039         if generation_config.do_sample
   2040         else None
   2041     )

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/generation/utils.py:2982, in GenerationMixin._sample(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)
   2979 model_inputs.update({""output_hidden_states"": output_hidden_states} if output_hidden_states else {})
   2981 # forward pass to get next token
-> 2982 outputs = self(**model_inputs, return_dict=True)
   2984 if synced_gpus and this_peer_finished:
   2985     continue  # don't waste resources running the code we don't need

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/accelerate/hooks.py:169, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    167         output = module._old_forward(*args, **kwargs)
    168 else:
--> 169     output = module._old_forward(*args, **kwargs)
    170 return module._hf_hook.post_forward(module, output)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:1274, in MixtralForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position)
   1271 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1273 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-> 1274 outputs = self.model(
   1275     input_ids=input_ids,
   1276     attention_mask=attention_mask,
   1277     position_ids=position_ids,
   1278     past_key_values=past_key_values,
   1279     inputs_embeds=inputs_embeds,
   1280     use_cache=use_cache,
   1281     output_attentions=output_attentions,
   1282     output_hidden_states=output_hidden_states,
   1283     output_router_logits=output_router_logits,
   1284     return_dict=return_dict,
   1285     cache_position=cache_position,
   1286 )
   1288 hidden_states = outputs[0]
   1289 logits = self.lm_head(hidden_states)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:1068, in MixtralModel.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position)
   1056     layer_outputs = self._gradient_checkpointing_func(
   1057         decoder_layer.__call__,
   1058         hidden_states,
   (...)
   1065         cache_position,
   1066     )
   1067 else:
-> 1068     layer_outputs = decoder_layer(
   1069         hidden_states,
   1070         attention_mask=causal_mask,
   1071         position_ids=position_ids,
   1072         past_key_value=past_key_values,
   1073         output_attentions=output_attentions,
   1074         output_router_logits=output_router_logits,
   1075         use_cache=use_cache,
   1076         cache_position=cache_position,
   1077     )
   1079 hidden_states = layer_outputs[0]
   1081 if use_cache:

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:812, in MixtralDecoderLayer.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache, cache_position, **kwargs)
    810 residual = hidden_states
    811 hidden_states = self.post_attention_layernorm(hidden_states)
--> 812 hidden_states, router_logits = self.block_sparse_moe(hidden_states)
    813 hidden_states = residual + hidden_states
    815 outputs = (hidden_states,)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:738, in MixtralSparseMoeBlock.forward(self, hidden_states)
    733 idx, top_x = torch.where(expert_mask[expert_idx])
    735 # Index the correct hidden states and compute the expert hidden state for
    736 # the current expert. We need to make sure to multiply the output hidden
    737 # states by `routing_weights` on the corresponding tokens (top-1 and top-2)
--> 738 current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)
    739 current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]
    741 # However `index_add_` only support torch tensors for indexing so we'll use
    742 # the `top_x` tensor here.

RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cuda:1)
```

I think the most important is:

```
RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cuda:1)
```

There are few additions to that:
1. I am running this code using Jupyter notebook.
2. Model seems like properly distributed across all GPUs:

```
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          Off |   00000000:0F:00.0 Off |                    0 |
| N/A   32C    P0            111W /  700W |   67576MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          Off |   00000000:2D:00.0 Off |                    0 |
| N/A   37C    P0            115W /  700W |   68344MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          Off |   00000000:44:00.0 Off |                    0 |
| N/A   31C    P0            110W /  700W |   68344MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          Off |   00000000:5B:00.0 Off |                    0 |
| N/A   36C    P0            116W /  700W |   68552MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          Off |   00000000:89:00.0 Off |                    0 |
| N/A   32C    P0            111W /  700W |   68192MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          Off |   00000000:A8:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |   68344MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          Off |   00000000:C0:00.0 Off |                    0 |
| N/A   40C    P0            116W /  700W |   68360MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          Off |   00000000:D8:00.0 Off |                    0 |
| N/A   32C    P0            110W /  700W |   67568MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      67566MiB |
|    1   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68334MiB |
|    2   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68334MiB |
|    3   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68542MiB |
|    4   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68182MiB |
|    5   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68334MiB |
|    6   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68350MiB |
|    7   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      67558MiB |
+-----------------------------------------------------------------------------------------+
```

However, I found easier step of reproduction with smaller model, it uses a bit different approach, but the result is the same:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from accelerate import infer_auto_device_map, dispatch_model

# tried with smaller mistral, result is the same
model_name = 'meta-llama/Meta-Llama-Guard-2-8B'

model_name = 'meta-llama/Meta-Llama-Guard-2-8B'

token = 'my-hf-token'

tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
model = AutoModelForCausalLM.from_pretrained(model_name, token=token)

device_map = infer_auto_device_map(
    model,
    max_memory={i: ""8GiB"" for i in range(torch.cuda.device_count())}
)

model2 = dispatch_model(model, device_map=device_map)

# Prepare input messages
messages = [
    {""role"": ""user"", ""content"": ""Hello, how are you?""},
    {""role"": ""assistant"", ""content"": ""I'm doing well, thank you! How can I assist you today?""},
    {""role"": ""user"", ""content"": ""Can you tell me about the weather today?""}
]

# Concatenate the messages into a single string
conversation = ""\n"".join([f""{msg['role']}: {msg['content']}"" for msg in messages])

inputs = tokenizer(conversation, return_tensors=""pt"")
# I tried also
# inputs = tokenizer(conversation, return_tensors=""pt"").to('cuda:0')
# and
# inputs = tokenizer(conversation, return_tensors=""pt"").to('cuda:1')
# It is the same
# and even like this:
# first_device = list(device_map.values())[0]
# inputs = inputs.to(f'cuda:{first_device}')
# still the same

output = model.generate(
    inputs[""input_ids""],
    max_length=150,  # Adjust according to your needs
    num_return_sequences=1,
    no_repeat_ngram_size=2,
    do_sample=True,
    top_k=50,
    top_p=0.95,
)
```

The result is the same:

```
RuntimeError                              Traceback (most recent call last)
Cell In[21], line 2
      1 # Generate text with the model
----> 2 output = model.generate(
      3     inputs[""input_ids""],
      4     max_length=150,  # Adjust according to your needs
      5     num_return_sequences=1,
      6     no_repeat_ngram_size=2,
      7     do_sample=True,
      8     top_k=50,
      9     top_p=0.95,
     10 )
     12 # Decode the output to readable text
     13 generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)
    113 @functools.wraps(func)
    114 def decorate_context(*args, **kwargs):
    115     with ctx_factory():
--> 116         return func(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/generation/utils.py:2024, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)
   2016     input_ids, model_kwargs = self._expand_inputs_for_generation(
   2017         input_ids=input_ids,
   2018         expand_size=generation_config.num_return_sequences,
   2019         is_encoder_decoder=self.config.is_encoder_decoder,
   2020         **model_kwargs,
   2021     )
   2023     # 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)
-> 2024     result = self._sample(
   2025         input_ids,
   2026         logits_processor=prepared_logits_processor,
   2027         logits_warper=prepared_logits_warper,
   2028         stopping_criteria=prepared_stopping_criteria,
   2029         generation_config=generation_config,
   2030         synced_gpus=synced_gpus,
   2031         streamer=streamer,
   2032         **model_kwargs,
   2033     )
   2035 elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):
   2036     # 11. prepare logits warper
   2037     prepared_logits_warper = (
   2038         self._get_logits_warper(generation_config, device=input_ids.device)
   2039         if generation_config.do_sample
   2040         else None
   2041     )

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/generation/utils.py:2982, in GenerationMixin._sample(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)
   2979 model_inputs.update({""output_hidden_states"": output_hidden_states} if output_hidden_states else {})
   2981 # forward pass to get next token
-> 2982 outputs = self(**model_inputs, return_dict=True)
   2984 if synced_gpus and this_peer_finished:
   2985     continue  # don't waste resources running the code we don't need

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/accelerate/hooks.py:169, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    167         output = module._old_forward(*args, **kwargs)
    168 else:
--> 169     output = module._old_forward(*args, **kwargs)
    170 return module._hf_hook.post_forward(module, output)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)
   1186 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1188 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-> 1189 outputs = self.model(
   1190     input_ids=input_ids,
   1191     attention_mask=attention_mask,
   1192     position_ids=position_ids,
   1193     past_key_values=past_key_values,
   1194     inputs_embeds=inputs_embeds,
   1195     use_cache=use_cache,
   1196     output_attentions=output_attentions,
   1197     output_hidden_states=output_hidden_states,
   1198     return_dict=return_dict,
   1199     cache_position=cache_position,
   1200 )
   1202 hidden_states = outputs[0]
   1203 if self.config.pretraining_tp > 1:

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1001, in LlamaModel.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)
    989     layer_outputs = self._gradient_checkpointing_func(
    990         decoder_layer.__call__,
    991         hidden_states,
   (...)
    998         position_embeddings,
    999     )
   1000 else:
-> 1001     layer_outputs = decoder_layer(
   1002         hidden_states,
   1003         attention_mask=causal_mask,
   1004         position_ids=position_ids,
   1005         past_key_value=past_key_values,
   1006         output_attentions=output_attentions,
   1007         use_cache=use_cache,
   1008         cache_position=cache_position,
   1009         position_embeddings=position_embeddings,
   1010     )
   1012 hidden_states = layer_outputs[0]
   1014 if use_cache:

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:750, in LlamaDecoderLayer.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)
    748 residual = hidden_states
    749 hidden_states = self.post_attention_layernorm(hidden_states)
--> 750 hidden_states = self.mlp(hidden_states)
    751 hidden_states = residual + hidden_states
    753 outputs = (hidden_states,)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:309, in LlamaMLP.forward(self, x)
    307     down_proj = sum(down_proj)
    308 else:
--> 309     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
    311 return down_proj

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cuda:1!
```

Since it happens not only on Mistral models, I believe it is either something wrong in my code or there is a bug in the library, please help me to find out. Thanks.

### Expected behavior

Inference should work on multiple GPU devices.

I can provide any additional information and try some adjustments.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",24,open
[i18n-ur] Translating docs to Urdu (اردو),"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the Urdu-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

@akkefa 

## Get Started section

- [ ] index.md ( Work in progress )
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) (waiting for initial PR to go through)
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
"When the model outputs Chinese, the dimension of the attention map output by output_attentions is wrong","### Feature request

When outputting English, the last layer of attention map is (960, 960), and when outputting Chinese, the last layer of attention map is (1910,). How can this problem be solved?

### Motivation

When I use llama to output Chinese

### Your contribution

no","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Extend Fx supported models with KV cache,"### Feature request

I noticed only llama and opt models are supported for FX tracing with KV Cache right now, can I check what is the plan to extend it to more models? Thanks!

### Motivation

I would like run fx traced graphmodules for generate(), which uses KV Cache. Right now it works for OPT and LLama, but I would like try on more models.

### Your contribution

If someone could point me to the general design pattern to make a model FX supported with KV cache or the lines of changes in modeling_opt.py or modeling_llama.py that made them work, I would be happy to submit PRs to make more models work.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Is it possible to add L1/L2 regularization using the trainer class ?,"### Feature request

I want to add L1/L2 regularization to the transformer training.

### Motivation

Adding L1/L2 reg can promote sparser models that can accelerate inference and reduce storage.

### Your contribution

Not sure.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
[Backbone] Remove out_features everywhere,"# What does this PR do?

As `out_features` is deprecated in favor of `out_indices` for the `xxxBackbone` classes, this PR removes a first batch of `out_features` mentions in favor of `out_indices`.",[],1,open
Enable multi-device for Segformer,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #29786  (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts",[],1,open
how to fine tune TrOCR on specifique langage guide.,"### Model description

hello , just passed through issues and other , but none of them talked on how to fine-tune TrOCR on specifique langage , like how to pick encoder and decoder , model .. etc , 
can you @NielsRogge , write a simple instructions/guide on this topic ?


### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
updates_readmedocs,"# This PR is done for the update of Quick_tour under readme and added some piece of code for Enhanced Output Formatting
ie 
```python
from transformers import pipeline
# Allocate a pipeline for sentiment-analysis
classifier = pipeline('sentiment-analysis')
result = classifier('We are very happy to introduce pipeline to the transformers repository.')
# Enhanced output formatting
label = result[0]['label']
score = result[0]['score']
print(f""Sentiment: {label}\nScore: {score:.4f}"")
```
 
",[],0,open
llama3 position_ids error with left padding,"### Feature request

The LLaMA 3 implementation should generate default `position_ids` that take the `attention_mask` into account.

@ArthurZucker @younesbelkada 

### Motivation

Is there a specific reason why the default `position_ids` generation doesn’t consider the `attention_mask`? A friend mentioned that this issue has persisted for almost half a year now.

https://github.com/huggingface/transformers/blob/adb91179b9e867b7278e0130c87558974056c7b4/src/transformers/models/llama/modeling_llama.py#L962

The problem arises when using left padding, as the default position_ids start from the first index in the sequence length  rather than from the first non-zero index in the `attention_mask`.

As far as I know, this is handled correctly in the `generate` function, but I would expect consistency during training as well.
https://discuss.huggingface.co/t/llama-position-ids/75870

### Your contribution

I can submit a PR changing 

https://github.com/huggingface/transformers/blob/adb91179b9e867b7278e0130c87558974056c7b4/src/transformers/models/llama/modeling_llama.py#L963

to

```
position_ids = (attn_mask.cumsum(-1) - 1).clamp(min=0)
position_ids.masked_fill_(attn_mask.to(torch.bool) == 0, 0)
```

if noone has any objections on correctness of that. Otherwise, let's discuss.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Create the FSDP Plugin in the trainer explicitly rather wait for it to be setup by accelerate,"# What does this PR do?

See the corresponding PR here https://github.com/huggingface/accelerate/pull/3033. Ultimately I want to see device_mesh support, but this brings the fsdp_plugin to parity with the deepspeed plugin in training args.

@ArthurZucker @muellerzr 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
This is the corresponding PR to https://github.com/huggingface/accelerate/pull/3033 that will enable passing a device_mesh
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
[i18n-ar] Translated file : `docs/source/ar/fsdp.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/fsdp.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",[],0,open
[i18n-ar] Translated file : `docs/source/ar/deepspeed.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/deepspeed.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",[],0,open
[i18n-ar] Translated file : `docs/source/ar/debugging.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/debugging.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",[],0,open
[i18n-ar] Translated file : `docs/source/ar/contributing.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/contributing.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",[],0,open
[i18n-ar] Translated file : `docs/source/ar/big_models.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/big_models.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",[],0,open
[i18n-ar] Translated file : `docs/source/ar/add_new_pipeline.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/add_new_pipeline.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",[],0,open
[i18n-ar] Translated file : `docs/source/ar/add_new_model.md` into Arabic,"
## What does this PR do?
Translated the `docs/source/ar/add_new_model.md` file of the documentation to `Arabic العربية`. I use transformers for my side-project and wanted to contribute. As a beginner, this seemed appropriate.

### Some questions:

- I can't access the pr documentation endpoint for `Arabic العربية`. Is there a missing step?
- I left all the other files alone after copying the English docs. Is this acceptable for future updates or should I add contents to the `_toctree.yml` only when translation is complete? Should I delete all other files for a cleaner review?
- In `Arabic`, people use both `Transformer` and `المحولات` to describe the model. However, as this is more of a product of HuggingFace I opted not to translate it. May you please let me know which you would prefer?

Thank you in advance for your review.

Part of #32435

### Before submitting

- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/) ?Please add a link to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

 ### Who can review?
 @stevhliu , @MKhalusova , @abodacs, may you please review this PR?
 ",[],0,open
Support `StaticCache` in assisted generation,"Looking for contributions!

Assisted generation (or speculative decoding) is a strategy to speed up generation. Using `StaticCache` and `torch.compile` is another strategy to speed up generation. Currently, the two are not compatible. It would be nice to be able to use both at the same time, for maximum speed 😎 

In a nutshell, assisted generation has to clear the cache of the models for the tokens that were rejected. `StaticCache` doesn't have the functions to do it implemented.","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",1,open
clarify the label shifting behavior of llama models when `labels` is given.,"### Feature request

i believe `labels` in the training of causal LMs means the value to predict at time `n`, i.e., the next token. in other words, i'd assume, if `labels` is given, it should be already shifted by one in the data loader w.r.t. the `input_ids`.


however, in `LlamaForCausalLM.forward()`, i found the labels are always shifted, silently.

https://github.com/huggingface/transformers/blob/f1d822ba337499d429f832855622b97d90ac1406/src/transformers/models/llama/modeling_llama.py#L1205-L1210

```python

        Args:
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

```

...

```python


        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)

```

i found it quite unexpected hence calling it ""silently"". as this is for a causal LM, shouldn't it be not shifting the labels by default? in modeling GPT2, this is at least documented explicitly.

https://github.com/huggingface/transformers/blob/f1d822ba337499d429f832855622b97d90ac1406/src/transformers/models/gpt2/modeling_gpt2.py#L1309-1314

in gemma2, it has the same behavior and no explicit mentioning in the docstring.

https://github.com/huggingface/transformers/blob/f1d822ba337499d429f832855622b97d90ac1406/src/transformers/models/gemma2/modeling_gemma2.py#L978-L982

i think at least we should force the docstring to mention this, if making a change is too dangerous at this point. 

### Motivation

i didn't expect this behavior and used my data loader, which does the shifting already, as i believe that is what `labels` should mean. as a result, i ended up finetuning a model to predict the next next token, which outputted gibberish.



### Your contribution

- hopefully leaving this issue helps communication across users
- i can make a one line change in the docstring.
- not sure how exactly, but if this potential misunderstanding could be checked, it'd be great. technically, we can check if the labels are already shifted. though i don't know where is the best place for this.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Convert new SigLIP checkpoints,"@NielsRogge as requested, I have modified the conversion script to convert new SigLIP ckpts.

I have modified model max length and pos embed shape because normally it was 64 and for the so-400m-patch14-224 it is now 16. 
My problem is that the expected output and the converted model outputs aren't close enough, see below. Did you come across the same issue?

```
outputs.logits_per_image[:3, :3]: tensor([[-1.0865,  1.1704],
        [-0.7178,  1.4354]])
expected_slice: tensor([[-1.0837,  1.1743],
        [-0.7196,  1.4371]])
```

Skipping the other model for now, the vocab dim of text embeddings is a bit of an issue, it is said to be 250k but is 256k, waiting for response from google folks (I think they used a different tokenizer, the demo notebook of big-vision also doesn't work for this reason)

Update: I confirmed with folks that there seems to be a mistake with tokenizer.",[],10,open
Some casualLM models don't get position_ids in their forward pass. ,"### Feature request

There are some models such that their forward pass doesn't get position_ids. e.g. we can see that OPTModel doesn't get position_ids, while GPTJModel does get position_ids. most newer models do have position_ids.



### Motivation

There are two main reasons we would like for all LM models to get positions ids.
1. to have the API be consistent with all models.
2. position_ids are very important if you want to use flash-attention without padding, during training. if i want to be able to pack two or more sentences in the same sequence. I would like to know that the model handles the sentences accordingly and treats each sentence as it's own different sentence. flash-attention code uses position_ids to check if some sequences are packed and runs an appropriate function to make sure there is no cross example contamination. but without this the model can't use this feature. the code always checks if position_ids is not None:

https://github.com/huggingface/transformers/blob/v4.44.1/src/transformers/modeling_flash_attention_utils.py#L270

### Your contribution

I may be able to fix this and help with a PR. but would love a more experienced person to guide me.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Add HHEMv2 Model,"This PR adds the support of codes for the HHEMv2 model. For information about HHEMv2 , please visit [model card](https://huggingface.co/vectara/hallucination_evaluation_model)
cc @forrestbao",[],3,open
Auto model & pipeline for image-text-to-image-text models,"### Feature request

This is a tracker issue for work on _interleaved_ in-and-out image-text generation.

There are now >= 5 open-source models that can do _interleaved_ image-text generation--and many more are expected to be released. Thus, it would now be practical & useful for us to (1) add native support for such models and (2) standardize the logic flow of data through processors and pipelines as done in https://github.com/huggingface/transformers/issues/31911 and https://github.com/huggingface/transformers/pull/32472

Model | Github | Notes | PR
--- | --- | --- | ---
[Anole](https://arxiv.org/abs/2407.06135) | https://github.com/GAIR-NLP/anole | - | https://github.com/huggingface/transformers/pull/32013
[Chameleon](https://arxiv.org/abs/2405.09818) | https://github.com/facebookresearch/chameleon | - | https://github.com/huggingface/transformers/pull/32013
[Llava-NeXT-Interleaved](https://arxiv.org/abs/2407.07895) | https://github.com/LLaVA-VL/LLaVA-NeXT | - | -
[Lumina-mGPT](https://arxiv.org/abs/2408.02657) | https://github.com/Alpha-VLLM/Lumina-mGPT | - | -
[Show-o](https://arxiv.org/abs/2408.12528) | https://github.com/showlab/Show-o | - | -
[Transfusion](https://arxiv.org/abs/2408.11039) | - | Not open-source (yet, perhaps) | -
[XGen-MM](https://arxiv.org/abs/2408.08872) | https://github.com/salesforce/LAVIS/tree/xgen-mm | The paper & the github repo don't actually demonstrate interleaved image-text generation yet, but they did train the model on such datasets & the model architecture(s) is perfectly suited for it | -
[Emu3](https://baai-solution.ks3-cn-beijing.ksyuncs.com/emu3/Emu3-tech-report.pdf?KSSAccessKeyId=AKLTgew6Kdg6RsK92QSfB2KLA&Expires=2591406552&Signature=6BvwfLVqvfww26Bhwvk3mG0FrL8%3D) | https://github.com/baaivision/Emu3 | The official repo only has demos for text-only & image-only generation, but the model seems to have been trained on text-image datasets | -

Initial work for Chameleon & Anole can be found here: https://github.com/huggingface/transformers/pull/32013 for reference.

Notes:
- We explicitly exclude models that can _only_ do text-only generation or image-only generation. We also exclude models that can do image-text generation but not in an interleaved manner.
- As I've demonstrated in my repo, _explicitly_ implementing the Finite State Machine (FSM) for switching between text-generation and image-generation modes as done in Chameleon's repo is not necessary. Implicitly implementing the FSM with Logits Processors suffices. Although more work is needed on finding the most efficient implementation.

TODOs:
- [ ] Add support for interleaved image-text generation with:
  - [x] Chameleon -> https://github.com/huggingface/transformers/pull/32013
  - [x] Anole -> https://github.com/huggingface/transformers/pull/32013
  - [ ] Lumina-mGPT
  - [ ] Show-o
  - [ ] Transfusion
  - [ ] XGen-MM
- [ ] Add auto model for image-text-to-image-text
  - [ ] [Optional] Add auto model for image-to-image-text
  - [ ] [Optional] Add auto model for text-to-image-text
- [ ] Add pipeline for image-text-to-image-text
  - [ ] [Optional] Add pipeline for image-to-image-text
  - [ ] [Optional] Add pipeline for text-to-image-text
- [ ] Benchmark different implementations of Logits Processors & FSMs for switching between text-generation and image-generation modes


### Motivation

1. To make benchmarking and evaluating models for interleaved image-to-text tasks saner
2. To continue work on Multimodal In-and-Out, Interleaved Structured Generation: https://github.com/leloykun/mmsg

### Your contribution

I've already started work on Chameleon & Anole here: https://github.com/huggingface/transformers/pull/32013

But I'm currently blocked by (1) not having enough time due to other responsibilities and (2) not having enough compute resources.

Any help would be appreciated!","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Update convert.py,"Key Changes
Added import os: To perform file and directory operations. Path Checks and Creation: Added checks to ensure the TensorFlow checkpoint path exists and created the output directory if it does not exist. Error Handling and Logging: Included detailed error messages and logging for better user feedback. These changes will help ensure that the necessary files are present and paths are correctly managed, reducing the chance of runtime errors due to missing files or directories.

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
"fix: Issue #32917, fixed _prune_heads in T5Model ","# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes issue #32917
Changed the `_prune_heads()` method in T5Model. This can now be used with the generic `model.prune_heads(heads_to_prune)` found in modeling_utils.py
",[],1,open
Trainer.model.push_to_hub() does not allow a private repository flag,"### System Info

As the title described.

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Use the function Trainer.model.push_to_hub().

### Expected behavior

Trainer.model.push_to_hub() allows a private repository flag.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",13,open
Uniform kwargs for processors of audio-text models,"# What does this PR do?

- Uniformizes kwargs for processors of audio-text models.
- An extension of https://github.com/huggingface/transformers/issues/31911
- NOTE: don't review nor merge until this PR is complete: https://github.com/huggingface/transformers/pull/32841

TODO Models:
- [x] Clap
- [x] CLVP
- [x] MusicGen Melody
- [ ] Qwen2 Audio
- [x] Seamless M4T
- [x] SpeechT5
- [x] Wav2Vec2 Bert

TODO tests
- [ ] Add audio-text-specific processor tests
- [ ] Remove unnecessary/duplicated tests

Models with special args (will not be done in this PR):
- PopPiano

Models with weird `_in_target_context_manager` logic (will not be done in this PR):
- MusicGen
- SpeechToText
- Wav2Vec2
- Wav2Vec2 w/ LM
- Whisper

Fixes # (issue)

- https://github.com/huggingface/transformers/issues/31911

## Who can review?

@zucchini-nlp @molbap @yonigozlan
",[],1,open
StoppingCriteria for Repetition,"### Feature request

similar to repetition_penalty for generation config, but as a stopping criteria.


### Motivation

(small?) models tend to generated endless loops of the same few tokens, or a combination where they only increase like a single digit.  (could not find any similar FRs)

I run into this quite a lot when doing evaluation runs (with greedy decoding) for code completion tasks. here is a screenshot of multiple generations saved to a file. the blocks of repetition can easily be spotted. 
![image](https://github.com/user-attachments/assets/305123b0-5623-4c83-af2c-ab0ff16dd494)

Having a stopping criterion that detects such behaviour would massively speed up evaluation runs, since generation could stop early and not reach the `max_new_token` set. Some parameters might be helpful to expose like number of repetitions, and n-gram overlap for example.

### Your contribution

I am happy to contribute with a PR myself, but will not find the time to do so in the next ~6-8 weeks. It doesn't look straight forward, but I am also not too familiar with the deeper parts of the generation code - so it might take me a while.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Make qwen2 `attention_qkv_bias` optional,"# What does this PR do?
Qwen2 and Qwen2-MoE model is forced to add bias to the query, key and value linear projections.
However, following the trend with other recent models (e.g. llama), I refactored these `attention_qkv_bias` to be optional so that we can configure it in config file.

Fixes #32892 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker @stevhliu ",[],5,open
Optional `bias` for qwen2 model,"### Feature request

`bias` of linear layers in `qwen2` model is hard coded as following:
- https://github.com/huggingface/transformers/blob/85345bb439652d3f03bb4e123cef7a440f2ba95b/src/transformers/models/qwen2/modeling_qwen2.py#L217-L219
- https://github.com/huggingface/transformers/blob/85345bb439652d3f03bb4e123cef7a440f2ba95b/src/transformers/models/qwen2/modeling_qwen2.py#L271-L274

It would be good to make bias optionally configurable through a config file to ensure compatibility with the latest models. (e.g. llama)

### Motivation

`bias` is optional in llama model as following:
- https://github.com/huggingface/transformers/blob/85345bb439652d3f03bb4e123cef7a440f2ba95b/src/transformers/models/llama/modeling_llama.py#L286-L288

### Your contribution

I'll submit PR for this feature","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
`truncate_dim` on `BertModel`,"### Feature request

I have a pipeline to finetune an instance of `BertModel`, on a `text-classification` task. 
I would like to use [this new embedding model](https://huggingface.co/aari1995/German_Semantic_V3#usage) as my base embedding now.
As can be seen in the example they provide, they are able to pass different values for `matryoshka_dim` into the `SentenceTransformer` instance through the `truncate_dim` argument.
However, I was not able to do this on the `BertModel` in the following code snippet that I have in my code:

```python
 self.bert_backbone = BertModel.from_pretrained(
            pretrained_model_name_or_path=self.config.embedding_model_file.model_name,
            cache_dir=Path(self.config.embedding_model_file.cache_dir),
        ).to(self.device)
```

And I do not want to use a `SentenceTransformer` instance either as in my training loop I would like to be able to get:

```python
bert_outputs: BaseModelOutputWithPoolingAndCrossAttentions = (
                    self.bert_backbone(
                        input_ids=input_ids, attention_mask=attention_mask
                    )
                )
                bert_logits: Tensor = bert_outputs.last_hidden_state[
                    :, 0, :
                ]  # Take the [CLS] token output
```

and I am not sure if this code would work also with a simple swap to `SentenceTransformer`. In any case, I think that this is a potential parameter that `BertModel` should support, and maybe it does but I am just missing it.

Thanks in advance!

### Motivation

To be able to extend the `BertModel` further

### Your contribution

.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
How to use 【examples/pytorch/contrastive-image-text】 to inter  inference,"### Feature request

I have reviewed the training code for CLIP and successfully executed it. Now, I want to use the obtained model for inference testing.


### Motivation

I would like to test the performance of the model I have trained.


### Your contribution

I hope I can get a example script to inference testing like below script :

python examples/pytorch/contrastive-image-text/run_clip.py \
    --output_dir ./clip-roberta-finetuned \
    --model_name_or_path ./clip-roberta \
    --data_dir $PWD/data \
    --dataset_name ydshieh/coco_dataset_script \
    --dataset_config_name=2017 \
    --image_column image_path \
    --caption_column caption \
    --remove_unused_columns=False \
    --do_train  --do_eval \
    --per_device_train_batch_size=""64"" \
    --per_device_eval_batch_size=""64"" \
    --learning_rate=""5e-5"" --warmup_steps=""0"" --weight_decay 0.1 \
    --overwrite_output_dir \
    --push_to_hub","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Update modeling_seamless_m4t.py SeamlessM4TForSpeechToText needs text_decoder it should not be in _keys_to_ignore_on_load_missing.,"SpeechToText needs text decoder but not text encoder.

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Added resource class configuration option for `check_circleci_user` job,"# What does this PR do?
I was reading `CircleCI` docs and [here](https://circleci.com/docs/concepts/#resource-class) i found that not mentioning a resource class for a job will provide a default one which is the `medium` Class and if we see the resources used by this [check_circleci_user job](https://app.circleci.com/pipelines/github/huggingface/transformers/101143/workflows/c8c67fcb-87dd-4f82-a5bc-81b7da1cd712/jobs/1347251/resources). It's CPU and RAM usage is **zero**, so it causes un-necessary extra (double) costs as, given [here](https://circleci.com/pricing/price-list/). 
Also, from the [insights](https://app.circleci.com/insights/github/huggingface/transformers/workflows/setup_and_quality/jobs?branch=b1d8acd4-8701-4c76-8a87-7ff2320aa95f&job-name=check_circleci_user&reporting-window=last-30-days) we can see that the check is running so many times > 3000/month. So, i think specifying a `small` resource_class for this job will save some credits.

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ydshieh @amyeroberts ",[],0,open
feat: DeepSeekMoE,"# What does this PR do?

Upstream custom code from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base/blob/main/modeling_deepseek.py to huggingface/transformers. This is not DeepSeek V2. The newly released DeepSeek-Prover-V1.5 runs on this architecture for example (though without MoE layers, so it is actually just Llama).

https://huggingface.co/models?other=deepseek


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker",[],5,open
Integrate Liger (Linkedin GPU Efficient Runtime) Kernel to HuggingFace,"### Feature request

Integrate Liger (Linkedin GPU Efficient Runtime) Kernel to HuggingFace Trainer, user could decide whether to enable kernel with a simple flag

### Motivation

Liger (Linkedin GPU Efficient Runtime) Kernel is a collection of Triton kernels designed specifically for LLM training. We have implemented Hugging Face Compatible RMSNorm, RoPE, SwiGLU, CrossEntropy, FusedLinearCrossEntropy, and more to come. It can effectively increase multi-GPU training throughput by 20% and reduces memory usage by 60%. The kernel works out of the box with [flash attention](https://github.com/Dao-AILab/flash-attention), PyTorch FSDP, and Microsoft DeepSpeed. We welcome contributions from the community to gather the best kernels for LLM training.

### Your contribution

We (LinkedIn) will take care of work for a smooth integration and would need HF review and feedback for changes.


### Benchmark

Benchmark conditions: LLaMA 3-8B, Alpaca Dataset, Max seq len = 512, Data Type = bf16, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 4 A100s.

The throughput increases by approximately 20% with more data, but the GPU memory is reduced by 40%. This means you can train the model on smaller GPUs, with larger batch sizes, or with longer sequence lengths at no additional cost.


![image (3)](https://github.com/user-attachments/assets/779240ce-531a-48ad-84a2-570421d2a43c)
![image (4)](https://github.com/user-attachments/assets/3ac951d9-ffc0-4b61-b98a-b27ec3bfc1dc)


For more detailed benchmark setup and more exciting efficiency for multi-head training (Medusa), please refer to original repo: https://github.com/linkedin/Liger-Kernel (Repo will be public soon)","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Fix assertion and value errors in sam-vit-h convertion script,"# What does this PR do?

When converting a `sam-vit-h` model, you encounter the following two errors:

1. An assertion error.
2. A `ValueError`: ""Input boxes must be a list of list of list of floating points.""

```python
raise ValueError(""Input boxes must be a list of list of list of floating points."")
inputs = processor(
    images=np.array(raw_image), 
    input_points=[input_points], 
    input_labels=input_labels, 
    return_tensors=""pt""
).to(device)
```

This PR fixes these two errors by:
- Updating the assertion to the correct values.
- Passing the correctly formatted `input_boxes` to the processor (by updating the format of `input_boxes`).

## How to reproduce the error (before the fix)?

Download the checkpoint file from [here](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth) or any checkpoint using `sam-vit-h`:

   ```bash
   !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O sam_vit_h_4b8939.pth
   ```

Copy and paste the SAM conversion file and try running it:

   ```bash
   !python convert.py --model_name sam_vit_h_4b8939 --checkpoint_path sam_vit_h_4b8939.pth --pytorch_dump_folder_path OUTPUT_PATH
   ```

will ping @amyeroberts & @NielsRogge  for a quick check :)",[],7,open
split head_dim from hidden_size for llama like gemma or mistral,"### Feature request

split head_dim from hidden_size like gemma or mistral

### Motivation

make not to align head_dim with hidden_size

### Your contribution

slightly revise modeling code and submit PR.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Uniformize model processors (models w/o special arg names),"# What does this PR do?

- Uniformizes kwargs for processors of AltCLIP, Flava, Git, InstructBlipVideo, LLaVa-NeXT-Video, MGP, Siglip, TVP, VideoLLaVa, VILT, X-CLIP as discussed in https://github.com/huggingface/transformers/issues/31911

TODO:
- [x] add tests
  - [x] add tests for AltCLIP
  - [x] add tests for Flava
  - [x] add tests for Git
  - [x] add tests for InstructBlipVideo
  - [x] add tests for Llava-NeXT-Video
  - [x] add tests for MGP
  - [x] add tests for Siglip
  - [x] add tests for TVP
  - [x] add tests for VideoLLava
  - [x] add tests for VILT
  - [x] add tests for X-CLIP

Fixes # (issue)

- https://github.com/huggingface/transformers/issues/31911

## Who can review?

@zucchini-nlp @molbap @NielsRogge ",[],1,open
Uniformize model processors (models *with* special arg names) ,"# What does this PR do?

- Uniformizes kwargs for processors of ClipSeq, Nougat, Owlv2, OwlVIT as discussed in https://github.com/huggingface/transformers/issues/31911
- Adds backward compatibility for special call arguments passed as positional arguments. Special call args are arguments that carry data (e.g. negative prompt, segmentation images, etc.), but aren't `[text, images, audio, videos]` and not config values for the tokenizer, image processor, etc.

 (arguments that carry data 

TODO:

- [x] add tests
  - [x] ClipSeg
  - [x] Nougat
  - [x] OwlV2
  - [x] OwlVIT

Fixes # (issue)

- https://github.com/huggingface/transformers/issues/31911

## Who can review?

@zucchini-nlp @molbap @NielsRogge ",[],3,open
Q-GaLore Support,"### Feature request

Add support for https://github.com/VITA-Group/Q-GaLore (https://arxiv.org/abs/2407.08296)

### Motivation

Q-GaLore allows more memory-efficient training

### Your contribution

M/A","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6712797012, 'node_id': 'LA_kwDOCUB6oc8AAAABkB0nVA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/optimization', 'name': 'optimization', 'color': '47E0D2', 'default': False, 'description': ''}]",2,open
"align dtype of queries, keys in Grounding DINO","To support #32672

Support fp16 and bfloat16 by aligning dtype of queries and keys for Grounding DINO.

Models:
- vision models: @amyeroberts
",[],8,open
[Docs] Update resources,"# What does this PR do?

This PR updates the resources section of various models.",[],1,open
Fix all_special_tokens to have special tokens from added_tokens,"# What does this PR do?

for phi-3 models,
![image](https://github.com/user-attachments/assets/0d89f2e5-d8b3-42a9-b33c-5413b72de1d0)

some important special tokens (such as template tokens `<|assistant|>`) are not in Tokenizer.all_special_tokens

but they re in added_vocab
<img width=""1222"" alt=""image"" src=""https://github.com/user-attachments/assets/9af81b21-8af1-4d23-a8ba-706841b6d959"">

 This PR adds those special tokens in added vocab into special tokens.

if I ran correctly, rust tokenizer implementation find special tokens well.

```rust
eprintln!(""{:?}"", self.special_tokens_set.iter().collect::<Vec<_>>());

> [""<|placeholder15|>"", ""<|endoftext|>"", ""<|placeholder13|>"", ""<|placeholder16|>"", ""<|placeholder11|>"", ""<|placeholder10|>"", ""<|system|>"", ""<|placeholder18|>"", ""<|placeholder23|>"", ""<|placeholder36|>"", ""<|image|>"", ""<|placeholder28|>"", ""<|placeholder29|>"", ""<|user|>"", ""<|placeholder3|>"", ""<|placeholder7|>"", ""<|placeholder20|>"", ""<unk>"", ""<|placeholder24|>"", ""<|placeholder25|>"", ""<|placeholder26|>"", ""<|placeholder1|>"", ""<|placeholder33|>"", ""<|placeholder37|>"", ""<|placeholder12|>"", ""<|placeholder32|>"", ""<|placeholder39|>"", ""<|placeholder27|>"", ""<|placeholder5|>"", ""<s>"", ""<|assistant|>"", ""<|placeholder38|>"", ""<|end|>"", ""<|placeholder14|>"", ""<|placeholder4|>"", ""<|placeholder9|>"", ""<|placeholder2|>"", ""<|placeholder22|>"", ""<|placeholder31|>"", ""<|placeholder34|>"", ""<|placeholder21|>"", ""<|placeholder8|>"", ""<|placeholder6|>"", ""<|placeholder17|>"", ""<|placeholder19|>"", ""<|placeholder30|>"", ""<|placeholder35|>""]
```

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?
",[],9,open
class Cache must not be a subclass of `torch.nn.Module`,"### System Info

I'm using `transformers==4.44.0`.

* The script that I used for collecting my system info is as follows:
```bash
$ curl -OL https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py
$ python3 collect_env.py
```
* The collected system info is as follows:
```
Collecting environment information...
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.26.4
Libc version: glibc-2.31

Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-102-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 10.1.243
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA RTX A6000
GPU 1: NVIDIA RTX A6000

Nvidia driver version: 535.171.04
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Byte Order:                         Little Endian
Address sizes:                      43 bits physical, 48 bits virtual
CPU(s):                             32
On-line CPU(s) list:                0-31
Thread(s) per core:                 2
Core(s) per socket:                 16
Socket(s):                          1
NUMA node(s):                       1
Vendor ID:                          AuthenticAMD
CPU family:                         23
Model:                              49
Model name:                         AMD Ryzen Threadripper PRO 3955WX 16-Cores
Stepping:                           0
Frequency boost:                    enabled
CPU MHz:                            2082.992
CPU max MHz:                        4402.7339
CPU min MHz:                        2200.0000
BogoMIPS:                           7785.71
Virtualization:                     AMD-V
L1d cache:                          512 KiB
L1i cache:                          512 KiB
L2 cache:                           8 MiB
L3 cache:                           64 MiB
NUMA node0 CPU(s):                  0-31
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow: Mitigation; safe RET
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es

Versions of relevant libraries:
[pip3] mypy==1.11.1
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.4
[pip3] pytorchvideo==0.1.5
[pip3] torch==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] Could not collect
```

### Who can help?

People who have been involved in https://github.com/huggingface/transformers/pull/32168
@gante @guangy10 @amyeroberts @ArthurZucker 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

#### Steps to reproduce
1. Create an environment with python 3.10 with pytorch-2.4.0 and transformers-4.44.0 installed.
For example, if you don't mind using conda:
```bash
conda create -n repro python=3.10
conda activate repro
conda install pytorch pytorch-cuda=12.4 -c pytorch -c nvidia
pip install transformers==4.44.0
```
2. Copy and paste the example code under the `Torch export for static cache` section in [the transformers v4.44.0 release page](https://github.com/huggingface/transformers/releases/tag/v4.44.0). (Say this code is saved as `example.py` in your working directory.) The code is as follows:
```python
import os, torch, copy
from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache
device = ""cuda""
ckpt = ""meta-llama/Meta-Llama-3.1-8B-Instruct""

INITIAL_PROMPT = ""From now on, you are going to answer all my questions with historical details. Make sure to always add a bit of french here and there, for style.""

model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16)
model.to(device)
tokenizer = AutoTokenizer.from_pretrained(ckpt)

prompt_cache = DynamicCache()
inputs = tokenizer(INITIAL_PROMPT, return_tensors=""pt"").to(""cuda"")
prompt_cache = model(**inputs, past_key_values = prompt_cache).past_key_values

prompt = ""Why are french people obsessed with french?""
new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=""pt"").to(""cuda"")
past_key_values = copy.deepcopy(prompt_cache)
outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20) 
response = tokenizer.batch_decode(outputs)[0]
print(response)

prompt = ""What is the best city to swim in?""
new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=""pt"").to(""cuda"")
outputs = model.generate(**new_inputs, past_key_values=copy.deepcopy(prompt_cache),max_new_tokens=20) 
response = tokenizer.batch_decode(outputs)[0]
```
3. Run the code: `python example.py`

#### The error message
You'll be able to see the error message, basically complaining about that the example code is calling `copy.deepcopy` on the `torch.nn.Module` instance `prompt_cache`.
```
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 12.45it/s]
Traceback (most recent call last):
  File ""/path/to/your/working/directory/example.py"", line 18, in <module>
    past_key_values = copy.deepcopy(prompt_cache)
  File ""/path/to/your/repro/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/path/to/your/repro/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/path/to/your/repro/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/path/to/your/repro/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/path/to/your/repro/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/path/to/your/repro/lib/python3.10/copy.py"", line 206, in _deepcopy_list
    append(deepcopy(a, memo))
  File ""/path/to/your/repro/lib/python3.10/copy.py"", line 153, in deepcopy
    y = copier(memo)
  File ""/path/to/your/repro/lib/python3.10/site-packages/torch/_tensor.py"", line 87, in __deepcopy__
    raise RuntimeError(
RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https:/github.com/pytorch/pytorch/pull/103001
```

#### Comments
This is due to the change made in https://github.com/huggingface/transformers/pull/32168, where the class `Cache` has become a subclass of `torch.nn.Module`. - See the [comment](https://github.com/huggingface/transformers/pull/32168/files#r1716754408) that I wrote in this PR.

Considering that the class `Cache` (and its subclasses, such as `DynamicCache`) represents KV cache generated by a model (which is a `torch.nn.Module` object itself), it is not natural to define `Cache` as a subclass of `torch.nn.Module`.
It looks like the purpose was to enable `copy.deepcopy` for `Cache` objects, but apparently, PyTorch 2.4 won't allow it.

### Expected behavior

The example code from the release page runs without the error, demonstrating support for prompt reuse introduced in transformers-4.44.0.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",11,open
fp16 support for grounding dino,"### Feature request

Currently, if fp16 is used with grounding dino via https://huggingface.co/docs/transformers/main/en/model_doc/grounding-dino, there is an error of the following:

```
...
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py"", line 3023, in forward
    outputs = self.model(
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py"", line 2360, in forward
    encoder_outputs = self.encoder(
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py"", line 1753, in forward
    (vision_features, text_features), attentions = encoder_layer(
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py"", line 1274, in forward
    (text_features, text_enhanced_attn) = self.text_enhancer_layer(
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py"", line 828, in forward
    attention_output, attention_weights = self.self_attn(
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py"", line 1331, in forward
    query_layer = self.transpose_for_scores(self.query(queries))
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 117, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half
```

### Motivation

It would be good to add support for fp16 to speed up the inference time.

### Your contribution

Happy to contribute if this feature is deemed to be useful.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",1,open
Use Self type for PreTrainedModel.from_pretrained,"# What does this PR do?

This type is more precise and more accurately captures the type-signature. Without it, in our codebase we get a bunch of type-checking errors like:

```
from transformers import BertForTokenClassification

# Incompatible attribute type [8]: Variable `model` has type `BertForSequenceClassification` but is used as type `PreTrainedModel`
model: BertForTokenClassification = BertForTokenClassification.from_pretrained(...)
```


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts 
",[],2,open
Fix RT-DETR optional losses,"# What does this PR do?

In the modeling file of RT-DETR there are the following losses defined 
 - labels_varifocal (used by default)
 - boxes (used by default)
 - labels_focal (not used, broken)
 - labels_cross_entorpy (not used, broken)
 - labels_binary_cross_entropy (not used, broken)
 
This PR fixes additional losses that might be defined for fine-tuning and enable their usage for fine-tuning with config option

```python
model = RTDetrForObjectDetection.from_pretrained(
    ""PekingU/rtdetr_r18vd"",
    losses=[""labels_varifocal"", ""boxes""],  # default value
    
    # Changing RT-DETR labels loss:
    # losses=[""labels_focal"", ""boxes""],
    # losses=[""labels_binary_cross_entropy"", ""boxes""],
    # losses=[""labels_cross_entropy"", ""boxes""],
    
    # Combining multiple losses
    # losses=[""labels_focal"", ""labels_varifocal"", ""boxes""],
)
```

I made several finetuning runs to make sure the model converges fine for every loss function
<img width=""1400"" alt=""Screenshot 2024-08-13 at 12 19 22"" src=""https://github.com/user-attachments/assets/fe0e31a6-0582-421a-950b-510f53f24cab"">


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

",[],1,open
Device selection for Qwen2Audio,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution. And I could not fix this problem without modifying the relevant code in `transformers`.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes runtime errors like ""inputs are on different devices"" when Qwen2 Audio runs on devices like ""mps"". This problem occurs when I tried to run the model on my Mac using `mps` device.

Tests in `transformers/tests/models/qwen2_audio` have passed and I have also test it with the official demo from Qwen 2 Audio with a few modification to run it on MPS device (see below).

## Problem and Code References

https://github.com/huggingface/transformers/blob/342e3f9f2067d45c27c30fbd4d748d233bca3adc/src/transformers/models/qwen2_audio/processing_qwen2_audio.py#L94

Here it calls
https://github.com/huggingface/transformers/blob/342e3f9f2067d45c27c30fbd4d748d233bca3adc/src/transformers/models/whisper/feature_extraction_whisper.py#L180

which has an optional argument `device` that defaults to ""cpu"". So, the output of the whisper feature extractor will by default 

But we can't just pass `device=""mps""` when calling `Qwen2AudioProcessor.__call__`, which will cause another runtime error that says `self.tokenizer()` does not have a `device` argument.

## Who can review?

Probably @faychu @ylacombe  can take a look because of #32137?

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

## Modified Demo Code

Modified from https://github.com/QwenLM/Qwen2-Audio?tab=readme-ov-file#audio-analysis-inference

```python
from io import BytesIO
from urllib.request import urlopen
import librosa
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor

DEFAULT_DEVICE = ""mps""  # or ""cuda"", ""cpu"", NEW

processor = AutoProcessor.from_pretrained(""Qwen/Qwen2-Audio-7B-Instruct"")
model = Qwen2AudioForConditionalGeneration.from_pretrained(""Qwen/Qwen2-Audio-7B-Instruct"", device_map=""auto"")

conversation = [
    {'role': 'system', 'content': 'You are a helpful assistant.'}, 
    {""role"": ""user"", ""content"": [
        {""type"": ""audio"", ""audio_url"": ""https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3""},
        {""type"": ""text"", ""text"": ""What's that sound?""},
    ]},
    {""role"": ""assistant"", ""content"": ""It is the sound of glass shattering.""},
    {""role"": ""user"", ""content"": [
        {""type"": ""text"", ""text"": ""What can you do when you hear that?""},
    ]},
    {""role"": ""assistant"", ""content"": ""Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.""},
    {""role"": ""user"", ""content"": [
        {""type"": ""audio"", ""audio_url"": ""https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac""},
        {""type"": ""text"", ""text"": ""What does the person say?""},
    ]},
]
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios = []
for message in conversation:
    if isinstance(message[""content""], list):
        for ele in message[""content""]:
            if ele[""type""] == ""audio"":
                audios.append(
                    librosa.load(
                        BytesIO(urlopen(ele['audio_url']).read()), 
                        sr=processor.feature_extractor.sampling_rate)[0]
                )

inputs = processor(text=text, audios=audios, return_tensors=""pt"", padding=True, device=DEFAULT_DEVICE)  # NEW
# inputs.input_ids = inputs.input_ids.to(""cuda"") # COMMENTED OUT, NOT NEEDED ANYMORE

generate_ids = model.generate(**inputs, max_length=256)
generate_ids = generate_ids[:, inputs.input_ids.size(1):]

response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
```
",[],2,open
Fix of type mis-match error in make_log_bucket_position,"Fix of type mis-match error in make_log_bucket_position

# What does this PR do?

This PR fixes the type mismatch error in the function of make_log_bucket_position in modeling_tf_deberta_v2.py

In the function, `log_pos` is type of float32 and computed with the following code:
```
    log_pos = (
        tf.math.ceil(
            tf.cast(tf.math.log(abs_pos / mid), tf.float32) / tf.math.log((max_position - 1) / mid) * (mid - 1)
        )
        + mid
```
In the tf.math.ceil(), the second `tf.math.log()` gives type of float64 and the third term `(mid-1)` gives type of int32. These terms cause type mismatch error when dividing or multiplying with the first term with type of float32. And the final 'add' term `mid` is also type of int32 and causes type mismatch error. So, all these terms should be casted to float32.

This PR fixes #31988 


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [V] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [V] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker
",[],0,open
Add warm up in evaluation_loop to get more accurate performance data,"Add warm up in evaluation_loop, this will make performance metrics more accurate when using torch_compile, take `text-classification+albert-base-v1` as example:
`python -u ./transformers/examples/pytorch/text-classification/run_glue.py --model_name_or_path albert-base-v1 --task_name MRPC --do_eval --max_seq_length 16 --learning_rate 2e-5 --overwrite_output_dir --output_dir /tmp/tmp_huggingface/ --torch_compile --report_to=none`
before this PR:
```
***** Running Evaluation *****
[INFO|trainer.py:3831] 2024-08-12 07:22:17,390 >>   Num examples = 408
[INFO|trainer.py:3834] 2024-08-12 07:22:17,390 >>   Batch size = 8
100%|████████████████████████████████████████████████████████████████████████| 51/51 [00:00<00:00, 69.95it/s]
***** eval metrics *****
  eval_accuracy               =     0.5637
  eval_combined_score         =     0.6024
  eval_f1                     =     0.6411
  eval_loss                   =     0.6855
  eval_model_preparation_time =     0.6604
  eval_runtime                = 0:00:06.56
  eval_samples                =        408
  eval_samples_per_second     =     62.169
  eval_steps_per_second       =      7.771
```
after this PR:
```
***** Running Evaluation *****
[INFO|trainer.py:3831] 2024-08-12 07:11:18,818 >>   Num examples = 408
[INFO|trainer.py:3834] 2024-08-12 07:11:18,818 >>   Batch size = 8
100%|██████████| 51/51 [00:00<00:00, 68.18it/s]
***** eval metrics *****
  eval_accuracy               =     0.5637
  eval_combined_score         =     0.6024
  eval_f1                     =     0.6411
  eval_loss                   =     0.6855
  eval_model_preparation_time =     6.4489
  eval_runtime                = 0:00:00.76
  eval_samples                =        408
  eval_samples_per_second     =    533.526
  eval_steps_per_second       =     66.691
```",[],1,open
bump tokenizers to support latest release,"# What does this PR do?

tokenizers is currently @ 0.20 https://github.com/huggingface/tokenizers/releases

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
RT-DETR postprocessing bug,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR addresses two important issues:

1. Fixes a critical bug in the [`post_process_object_detection`](https://github.com/huggingface/transformers/blob/984bc11b0882ff1e5b34ba717ea357e069ceced9/src/transformers/models/rt_detr/image_processing_rt_detr.py#L1084) method of `RTDetrImageProcessor` for the RT-DETR model.
2. Improves the documentation for the [`labels`](https://github.com/huggingface/transformers/blob/984bc11b0882ff1e5b34ba717ea357e069ceced9/src/transformers/models/rt_detr/modeling_rt_detr.py#L2555) parameter in `RTDetrForObjectDetection`.

Fixes #  [#32578](https://github.com/huggingface/transformers/issues/32578)(issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@amyeroberts


<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

## Improve RT-DETR documentation: Clarify bounding box format for labels

### Current Documentation
- The output bounding box format is clearly specified as (top_left_x, top_left_y, bottom_right_x, bottom_right_y) in the [RTDetrImageProcessor documentation](https://huggingface.co/docs/transformers/model_doc/rt_detr#transformers.RTDetrImageProcessor.post_process_object_detection).
- However, the format for input bounding boxes in the `labels` parameter is not explicitly stated in the `RTDetrForObjectDetection` documentation.

### Missing Information
The `labels` parameter requires bounding boxes in the following format:
- (center_x, center_y, width, height)
- Values should be normalized to the range [0, 1]
This information is crucial for correctly calculating the loss but is currently missing from the documentation.

### Proposed Solution

Add the following clarification to the documentation for the `labels` parameter in `RTDetrForObjectDetection`:

""The bounding box coordinates in the 'boxes' key should be in the format (center_x, center_y, width, height) and have normalized values in the range [0, 1].""

### Impact

Adding this information will significantly improve the user experience by:
1. Reducing confusion about the required input format
2. Ensuring correct loss calculation
3. Saving users time in debugging and troubleshooting

### Additional Notes

- This issue was discovered while attempting to use the RT-DETR model for custom training.
- The lack of this information in the documentation led to difficulties in properly preparing the input data and calculating the loss.

### Related Links

- [RTDetrImageProcessor documentation](https://huggingface.co/docs/transformers/model_doc/rt_detr#transformers.RTDetrImageProcessor.post_process_object_detection)
- [RTDetrForObjectDetection documentation](https://huggingface.co/docs/transformers/model_doc/rt_detr#transformers.RTDetrForObjectDetection)","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",18,open
cleaned get_test_pipeline() unused variables from tests/pipelines files,"# What does this PR do?

Fixes #32397 

Removes unused variables from the pipeline test's get_test_pipeline() in accordance with the style guide 


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. 

",[],1,open
use new home for AnyPrecisionAdamW,"# What does this PR do?

the original home of AnyPrecisionAdamW in torchdistx is no longer maintained and is now maintained in [llama-recipes](https://github.com/meta-llama/llama-recipes/blob/9b3dabcaac78980eae40005bbc8b1a8276c82af3/src/llama_recipes/policies/anyprecision_optimizer.py#L16)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

@muellerz 

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
Add reorder_cache class attribute For Starcoder,"# What does this PR do?

[Small PR] Adds __reorder_cache attribute for starcoder2 similar to opt. 
 @SunMarc @gante @ArthurZucker ",[],0,open
Quickfix generate tests,"# What does this PR do?

(Should) fix broken tests on main for generate testing, tests broken not caught in https://github.com/huggingface/transformers/pull/32493 .

cc @ArthurZucker ",[],2,open
Phi3FlashAttention2 call to rotary embeddings bugfix,"# What does this PR do?
The Phi3RotaryEmbedding forward call (all of its implementations) requires position_ids as an input arg but in the Phi3FlashAttention2 implementation it is called with just the seq_len argument. I'm adding position_ids back (looks like they were removed here: https://github.com/huggingface/transformers/pull/31500) and also adding a check for either position_ids or seq_len to exist in the forward call, and using seq_len to define the position_ids when only the seq_len was provided. Cascading those changes to other places where this code is used.

@ArthurZucker 

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
chore: move `conftest.py` to `tests/`,"### Feature request

It's a common practice to place the `conftest.py` file within the `tests/` directory.


### Your contribution

Initial Attempt: https://github.com/huggingface/transformers/pull/32011

### References

* https://github.com/pytest-dev/pytest/discussions/10708","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
TrOCR is ExecuTorch compatible,"### Feature request

Enable TrOCR to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow

### Motivation

See details in #32253

### Your contribution

TBD","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",0,open
Bart/Wav2Vec2 is ExecuTorch compatible,"### Feature request

Enable Bart/Wav2Vec2 to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow

### Motivation

See details in #32253

### Your contribution

TBD","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",0,open
CLIP is ExecuTorch compatible,"### Feature request

Enable CLIP to [""Export to ExecuTorch""](https://github.com/huggingface/transformers/issues/32253) workflow

### Motivation

See details in #32253

### Your contribution

Support on common export issues","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",0,open
Implement `generate` (inference) for torch exported text-generation models,"### Feature request

Unlike `torch.compile`, `torch.export` only export the inner transformer to predict a single token for text-generation models. The autoregressive logics are not included in the exported artifact. For ExecuTorch Runtime, the autoregressive logics is implemented there, either directly in c++ ([code pointer](https://github.com/pytorch/executorch/blob/main/examples/models/llama2/runner/runner.cpp#L310-L415)) or python ([code pointer](https://github.com/pytorch/executorch/blob/main/examples/models/llama2/runner/generation.py#L70-L124)). Here is a minimal `generate` impl in a test for Phi3-mini in 🤗  ([code pointer](https://github.com/huggingface/transformers/blob/543df489147412efb20575c3c2a3fb69a18ac107/tests/models/phi3/test_modeling_phi3.py#L73)). 

Instead of duplicate the generate logics for each exported model, it's better to have a common implementation of `generate` that can be used by any exported text-generation mdoel.

### Motivation

To support the new ""Export to ExecuTorch"" workflow. This would enable users to have a unified `generate` experience as they run inference using eager or compiled model. 

### Your contribution

TBD","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",0,open
Fix zero shot detection pipeline,"# What does this PR do?

1. Fix `zero-shot-object-detection` pipeline for grounding dino
2. Speedup `zero-shot-object-detection` in `len(canidate_labels)` times

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #32206

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",1,open
SAM can't process batches of nonhomogenous-count of bounding-boxes per image,"### System Info

- `transformers` version: 4.43.3
- Platform: Windows-10-10.0.22631-SP0
- Python version: 3.10.14
- Huggingface_hub version: 0.24.3
- Safetensors version: 0.4.3
- Accelerate version: 0.33.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.4.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: No
- Using GPU in script?: Yes
- GPU type: NVIDIA GeForce RTX 3090

### Who can help?

@amyeroberts 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Run the following code:

```
from transformers import SamProcessor, SamModel
from PIL import Image
import requests

# Load processor and model
processor = SamProcessor.from_pretrained(""facebook/sam-vit-base"")
model = SamModel.from_pretrained(""facebook/sam-vit-base"")

# Prepare batch of images and bounding boxes
image_url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
image = Image.open(requests.get(image_url, stream=True).raw)
images = [image, image]

bounding_boxes = [
    [[100, 100, 200, 200], [200, 200, 400, 400]],  # bounding boxes for image1
    [[100, 100, 200, 200]],  # bounding boxes for image2
]

# Process the batch
inputs = processor(
    images=images,
    input_boxes=bounding_boxes,
    return_tensors=""pt""
)
```

You should get the following error:
`ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.`

Originating from:
`transformers\models\sam\processing_sam.py` (line 142)

### Expected behavior

As an end-user, I expect to get 2 masks/results for the first image and 1 mask/result for the second image.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",5,open
Adding new zero-shot examples,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes https://github.com/huggingface/transformers/issues/32459

Following two PR should be merged before merging this example!
https://github.com/huggingface/transformers/pull/31828
https://github.com/huggingface/transformers/pull/31964

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],12,open
[ I built it! ] Server application with on-the-fly quantization to serve up HF-Hub models for back-and-forth request-response inferencing   ,"### Feature request

It would be immensely useful to have a server-application to serve up HF-Transformer and other Hub models as a service, similar to the how `llama.cpp` bundles the `llama-server` application to load GGUFs into memory and expose them via an API to enable back-and-forth inferencing request-responses without having to reload the model for each query.  

### Motivation

Serving up HF-Hub models with ease: We had some excellent LLMs released in the last few weeks and I was naturally eager to try them the second they were available but had to wait for `llama.cpp` to bake in proper support first.

While `llama.cpp` and `GGUfs` are amazing especially for hybrid-inferencing, waiting on support for models based on new tokenizers/attention-mechanisms and performing a lengthy recompilation of `llama.cpp` with every release can be cumbersome, especially in containerized environments with GPUs. 

This got thinking that there has to be a better way! Since model creators release LLMs primarily with Transformers support in mind, if there were an easy way to serve them up via a local-API in their native HF-Transformer format, especially with on-the-fly quantization, we could run them as a local-inferencing service the day they're out, with at most a few pip-updates to local Python packages in most cases.

As there wasn't such a server readily avaialble, and the general advice online was to build your own, I did and am looking to contribute it to this amazing project!

### Your contribution

Presenting `HF-Waitress`, named after the Flask-WSGI server it leverages!

Git repo: https://github.com/abgulati/hf-waitress

HF-Waitress enables loading HF-Transformer & AWQ-quantized models directly off the hub, while providing on-the-fly quantization via BitsAndBytes, HQQ and Quanto for the former. It negates the need to manually download any model yourself, simply working off the models name instead. It requires no setup, and provides concurrency and streaming responses all from within a single, easily-portable, platform-agnostic Python script.

### Key Features

- **On-the-fly, in-place quantization**: Supports int8 & int4 quantization via BitsAndBytes, int8, int4 and int2 quantization via Quanto and int8, int4, int3, int2, int1 quantization via HQQ
- **Model Agnosticism**: Compatible with any HF-Transformers format LLM.
- **Configuration Management**: Uses `config.json` to store settings, allowing for easy configuration and persistence across runs.
- **Error Handling**: Detailed logging and traceback reporting via centralized error-handling functions.
- **Health Endpoint**: Provides valuable information about the loaded model and server health.
- **Concurrency Control**: Uses semaphores for selective concurrency while taking advantage of semaphore-native queueing.
- **Streaming Responses**: Supports both standard and streaming completions.

### API Endpoints

1. `/completions` (POST): Generate completions for given messages.
2. `/completions_stream` (POST): Stream completions for given messages.
3. `/health` (GET): Check the health and get information about the loaded model.
4. `/hf_config_reader_api` (POST): Read values from the configuration.
5. `/hf_config_writer_api` (POST): Write values to the configuration.
6. `/restart_server` (GET): Restart the LLM server.

### Usage

To start the server, run: `python hf_waitress.py [arguments]`

Example:

```
python hf_waitress.py --model_id=mistralai/Mistral-Nemo-Instruct-2407 --quantize=quanto --quant_level=int4 --access_token=<token> --trust_remote_code --use_flash_attention_2 --do_sample
```
*launch-arguments are optional, even on the first run! See [README](https://github.com/abgulati/hf-waitress?tab=readme-ov-file#default-values-for-missing-launch-arguments) for defaults.*

### Command-line Arguments

- `--model_id`: The model ID in HF-Transformers format - see below for details.
- `--access_gated`: Set to True if accessing gated models you're approved for.
- `--access_token`: Your Hugging Face Access Token.
- `--gguf`: Add this flag if attempting to load a GGUF model - [For future use, not presently functional](https://huggingface.co/docs/transformers/main/en/gguf)
- `--gguf_model_id`: GGUF repository ID - [For future use, not presently functional](https://huggingface.co/docs/transformers/main/en/gguf)
- `--gguf_filename`: Specific GGUF filename - [For future use, not presently functional](https://huggingface.co/docs/transformers/main/en/gguf)
- `--quantize`: Quantization method ('bitsandbytes', 'quanto', 'hqq' or 'n' for none, see important details below.).
- `--quant_level`: Quantization level (Valid values -  BitsAndBytes: int8 & int4; Quanto: int8, int4 and int2; HQQ: int8, int4, int3, int2, int1).
- `--hqq_group_size`: Specify group_size (default: 64) for HQQ quantization. No restrictions as long as weight.numel() is divisible by the group_size.
- `--push_to_hub`: Push quantized model to Hugging Face Hub.
- `--torch_device_map`: Specify inference device (e.g., 'cuda', 'cpu').
- `--torch_dtype`: Specify model tensor type.
- `--trust_remote_code`: Allow execution of custom code from the model's repository.
- `--use_flash_attention_2`: Attempt to use Flash Attention 2 - [Only for specific Nvidia GPUs](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features) 
- `--pipeline_task`: Specify the pipeline task (default: 'text-generation').
- `--max_new_tokens`: Maximum number of tokens to generate.
- `--return_full_text`: Return the full text including the prompt.
- `--temperature`: Set LLM temperature (0.0 to 2.0) - set do_sample to True for temps above 0.0, and False when setting temperature=0.0!
- `--do_sample`: Perform sampling when selecting response tokens - must be set to True for temps above 0.0!
- `--top_k`, `--top_p`, `--min_p`: Token selection parameters - must set do_sample to True!
- `--port`: Specify the server port (default: 9069).
- `--reset_to_defaults`: Reset all settings to default values.

### Detailed documentation in the [repo](https://github.com/abgulati/hf-waitress?tab=readme-ov-file#hf-waitress)!

Since this is an independent script file and is in very active development, I wasn't sure of how to go about contributing it as a PR. I'd be very grateful for guidance on if and how to go about contributing this application to this repo, so humbly requesting help here!

PS - Reddit post made by me announcing HF-Waitress: https://www.reddit.com/r/LocalLLaMA/comments/1eijqc6/the_softwarepain_of_running_local_llm_finally_got/","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Enhancing Hugging Face Models with Tensor Parallelism for Large-Scale Model Support 🚀,"### Feature request

## Description

This feature proposal aims to update Hugging Face's support for tensor parallelism (TP) to accommodate the increasing size and complexity of models such as [LLaMA 3.1](https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/453304228_1160109801904614_7143520450792086005_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=PC3CtquZIecQ7kNvgFOxUn_&_nc_ht=scontent-ssn1-1.xx&oh=00_AYAqF6Xje_dLZR94A1h9NDZWJ-kLjEEsGF0_H-dKHJNoFQ&oe=66B815C7), [Nemotron-4-340B-Instruct](https://build.nvidia.com/nvidia/nemotron-4-340b-instruct), and others, which have surpassed the capabilities of current training frameworks like TRL + DeepSpeed.

Currently, the Hugging Face codebase is outdated concerning these advancements. Although parallelism requires careful customization based on hardware setup, dataset size, sequence length, and model size, implementing TP across many Hugging Face models is crucial.

## Proposal

With the introduction of tensor parallelism in [PyTorch 2.0](https://pytorch.org/docs/stable/distributed.tensor.parallel.html), the previous method of creating processes per device and model in the [Megatron](https://github.com/NVIDIA/Megatron-LM) style is no longer efficient.

### Key Changes:

1. **Refactoring Code for TP:**
   - Remove the use of `kwargs` in favor of more straightforward TP implementations, as PyTorch parallel plans do not accommodate `kwargs`.
   - Refactor PyTorch models to incorporate TP effectively.

2. **Current Limitations:**
   - Existing implementations, such as in [modeling_llama](https://github.com/huggingface/transformers/blob/3d8bd11942cec26851c80c01aa5e8403542ca50b/src/transformers/models/llama/modeling_llama.py#L292), are not trainable and are incompatible with `torch.compile` for inference optimization.
   
3. **Future Integration:**
   - As models scale to large sizes, 8-way Tensor Parallel is becoming standard.
   - This change would enable `Accelerate` to later support TP + FSDP (Fully Sharded Data Parallel), which many users could benefit from.

## Personal Contribution

I have personally developed code that allows LLaMA to run entirely with TP, observing that it handles longer token sequences with less memory than FSDP. However, I have not submitted a pull request due to the need for comprehensive code refactoring.

## Call to Action

If Hugging Face acknowledges this need, I am willing to contribute further if there is an overarching plan for abstraction and integration.

### Motivation

## Motivation

The motivation behind this proposal is to address the limitations and frustrations experienced when using Hugging Face with the current parallelism approaches, especially for large-scale models like LLaMA 3.1 and Nemotron-4-340B-Instruct. As models grow in complexity, existing frameworks struggle to support efficient training and inference.

### Current Issues with Existing Solutions:

- **[NVIDIA Megatron-LM](https://github.com/NVIDIA/Megatron-LM)**: Lacks compiler-level optimization and is somewhat outdated.
- **[Tensor Parallel by BlackSamorez](https://github.com/BlackSamorez/tensor_parallel)**: Also lacks compiler-level optimization and is outdated.
- **[DeepSpeed](https://github.com/microsoft/DeepSpeed)**: Primarily uses data parallelism (DP), with ZeRO closer to model parallelism (MP) rather than tensor parallelism (TP). It also has issues with ZeRO Stage 3.
- **[AWS Neuron Distributed](https://github.com/aws-neuron/neuronx-distributed)**: Potentially supports TP in distributed settings, though not tested extensively.
- **[PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/tp.html)**: Implements TP but is not applicable to Hugging Face models.
- **[NVIDIA NeMo](https://github.com/NVIDIA/NeMo?tab=readme-ov-file)**: Uses PyTorch Lightning, underscoring the need for Hugging Face to adopt TP, including coding styles like avoiding `kwargs`.

Implementing tensor parallelism (TP) in Hugging Face models is crucial to keep up with the trend towards larger models and to enhance compatibility with modern optimization techniques like `torch.compile`.

### Your contribution

## Contribution

I am willing to contribute to implementing tensor parallelism (TP) within the Hugging Face ecosystem. To facilitate this, I would appreciate guidance on the following aspects:

1. **Integration Approach**: Clarification on whether TP should be applied during model initialization, such as with `AutoModelForCausalLM`, or if it should be managed externally using `torchrun`.

2. **Automatic Initialization**: Decide if the implementation should automatically initialize `torch.distributed` without requiring explicit commands from users.

With a defined plan or abstraction level, I can work on refactoring the necessary code and submit a pull request to integrate TP effectively. My experience with TP, particularly with LLaMA, has demonstrated its efficiency in handling large models with reduced memory usage compared to current methods.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",11,open
Additional options for include_num_input_tokens_seen in Trainer,"### Feature request

Track only the training, avoiding the count of padding tokens

### Motivation

It appears that this metric also includes padding tokens. If one would use example packing, then it really tracks the “correct” number of tokens seen by the model.

However, I can think of two cases where this will not be accurate:
1. In cases where packing is not used, training examples are padded to the longest sequence in the batch
2. For SFT training on completions only

For the first case, a more accurate calculation would be to sum the attention mask.
For the second case, I'm not sure how this should be regarded. However, we can consider counting only label tokens != `-100`

### Your contribution

Replace lines 2248-2258 in trainer.py (v4.43.4) with the following:
```python
self.state.num_input_tokens_seen += (
    torch.sum(
        self.accelerator.gather(
            torch.tensor(
                inputs['attention_mask'].sum(), device=self.args.device, dtype=torch.int64
            )
        )
    )
    .cpu()
    .item()
)
```","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Zero-shot finetuning examples,"### Feature request

@amyeroberts @qubvel @NielsRogge 

Does HF team intersted in adding zero-shot finetuning example like https://github.com/SangbumChoi/transformers/tree/grounding_examples/examples/pytorch/zero-shot?

If the team merge these two following open PR then
https://github.com/huggingface/transformers/pull/31828
https://github.com/huggingface/transformers/pull/31964

we can finetune the groundingdino and write something like this.
https://blog.roboflow.com/how-to-fine-tune-paligemma/

I will left some model that you can run 
Model : https://huggingface.co/danelcsb/grounding-dino-tiny-finetuned-cppe-5-10k-steps-no-trainer/tree/main

### Motivation

People can use zero-shot model to finetune there own projects.

### Your contribution

I have made the branch already and ready to generate PR. Need some more modification.
Before
<img width=""1002"" alt=""Screenshot 2024-08-06 at 6 08 57 PM"" src=""https://github.com/user-attachments/assets/92bbe009-91df-4be5-b786-eb998fe4c508"">
After
<img width=""793"" alt=""Screenshot 2024-08-06 at 6 09 05 PM"" src=""https://github.com/user-attachments/assets/dec37ec9-f0fb-4ef4-93a9-a5575196e472"">

","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",3,open
[WiP] Add buf_init_callbacks for dist inference = auto generate rope embeddings after meta/fake tracing,"add buf_init_callback.
This is an extension of the modeling_llama.py PR here: 
https://github.com/huggingface/transformers/pull/32428/files

# What does this PR do?
This adds the same class level buf_init_callback dict that allows someone to regenerate the rope embeddings that are destroyed during meta and then fake tracing.  (meta can be resolved via intercepting buffers, but running a fake tensor to trace for PP then destroys the rope embeddings as they become fake tensors. 

The buf_init_callbacks allows you to quickly find the entry point to regenerate all the non-persistent embeddings after tracing ensuring you have a ready to run model graph. 
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
[i18n-ar] Translating docs to Arabic (العربية),"Hi! !مرحبا! السلام عليكم

Let's bring the documentation to all the Arabic-speaking community 🌏 (currently 0 out of 267 complete)

Would you want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

### Some notes:

- Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
- Please translate in a gender-neutral way.
- Add your translations to the folder called ko inside the [docs/source/ar](https://github.com/huggingface/transformers/tree/main/docs/source/ar).
- Register your translation in ar/_toctree.yml; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
- Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu, @MKhalusova and @abodacs for review.
- 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).
- 

@AhmedAlmaghz 

## Launch

- [x] Add README_AR to README.md And #32583
- [x] Enable easy Table of Contents editing #32696

# GET STARTED

 - [x] 🤗 Transformers #33040
- [x]  Quick tour #33069
- [x]  Installation #33041

# TUTORIAL

- [x]  Run inference with pipelines #33065
- [x]  Write portable code with AutoClass #33022
- [x]  Preprocess data #33068
- [x]  Fine-tune a pretrained model #33081
- [x]  Train with a script #33070
- [x]  Set up distributed training with 🤗 Accelerate #33016
- [x]  Load and train adapters with 🤗 PEFT #33051
- [x]  Share your model #33046
- [x]  Agents #33020
- [x]  Generation with LLMs #33044
- [x] Chatting with Transformers #33029

# CONCEPTUAL GUIDES

- [x]  Philosophy #33064
- [x]  Glossary #33038
- [x]  What 🤗 Transformers can do #33073
- [x]  How 🤗 Transformers solve tasks #33074
- [x]  The Transformer model family #33047
- [x]  Summary of the tokenizers #33078
- [x]  Attention #33021
- [x]  Padding and truncation #33050
- [x]  BERTology #33024
- [x]  Perplexity of fixed-length models #33063
- [x]  Pipelines for webserver inference #33066
- [x]  Model training anatomy #33045
- [x] Getting the most out of LLMs #33043


# TASK GUIDES

## NATURAL LANGUAGE PROCESSING
- [ ]  Text classification #35192 
- [ ]  Token classification
- [ ]  Question answering
- [ ]  Causal language modeling
- [ ]  Masked language modeling
- [ ]  Translation
- [ ]  Summarization 
- [ ]  Multiple choice

## AUDIO
- [ ]  Audio classification
- [ ]  Automatic speech recognition

## COMPUTER VISION
- [ ]  Image classification
- [ ]  Image segmentation
- [ ]  Video classification
- [ ]  Object detection
- [ ]  Zero-shot object detection
- [ ]  Zero-shot image classification
- [ ]  Depth estimation
- [ ]  Image-to-Image
- [ ]  Image Feature Extraction
- [ ]  Mask Generation
- [ ]  Keypoint Detection
- [ ]  Knowledge Distillation for Computer Vision

## Multimodal
- [ ]  Image captioning
- [ ]  Document Question Answering
- [ ]  Visual Question Answering
- [ ]  Text to speech
- [ ]  Image-text-to-text
- [ ]  Video-text-to-text

## Generation
- [ ]  Customize the generation strategy
- [ ]  Best Practices for Generation with Cache

## Prompting
- [ ]  Image tasks with IDEFICS
- [ ]  LLM prompting guide

# DEVELOPER GUIDES

- [x]  Use fast tokenizers from 🤗 Tokenizers #33034
- [x]  Run inference with multilingual models #33048
- [x]  Use model-specific APIs #33030
- [x]  Share a custom model #33031
- [x]  Templates for chat models #33026
- [x]  Trainer #33080
- [x]  Run training on Amazon SageMaker #33071
- [x]  Export to ONNX #33072
- [x]  Export to TFLite #33077
- [x]  Export to TorchScript #33079
- [x]  Benchmarks #33023
- [x]  Notebooks with examples #33049
- [x]  Community resources #33027
- [x]  Troubleshoot #33017
- [x]  Interoperability with GGUF files #33037

## PERFORMANCE AND SCALABILITY

- [ ]  Overview

## EFFICIENT TRAINING TECHNIQUES

- [ ]  Training on one GPU
- [ ]  Training on many GPUs 
- [ ]  Training on CPU
- [ ]  Training on many CPUs 
- [ ]  Training on TPUs
- [ ]  Training on TPU with TensorFlow
- [ ]  Training on Specialized Hardware
- [ ]  Custom hardware for training
- [ ]  Hyperparameter Search using Trainer API

## OPTIMIZING INFERENCE

- [ ]  Inference on CPU
- [ ]  Inference on one GPU
- [ ]  Inference on many GPUs
- [ ]  Inference on Specialized Hardware
- [ ]  Instantiating a big model 
- [ ]  Debugging 
- [ ]  XLA Integration for TensorFlow Models 
- [ ]  Optimize inference using torch.compile

# CONTRIBUTE
 
- [ ] How to contribute to transformers?
- [ ]  How to add a model to 🤗 Transformers? 
- [ ]  How to convert a 🤗 Transformers model to TensorFlow? 
- [ ]  How to add a pipeline to 🤗 Transformers? 
- [ ]  Testing 
- [ ]  Checks on a Pull Request

","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Skip non-selected experts for mixtral and qwen2_moe,"Fixes #32283

This PR avoids redundant computation for some MoE models (mixtral and qwen2_moe).
The current implementation loops all the experts and inevitably loads experts' weight, which brings extra IO costs.
@ArthurZucker 


",[],3,open
Register buffer init callbacks in llama,"# What does this PR do?

Non-persistent buffers is not saved in state dict. 

In the case of meta init, while loading state dict from checkpoint can fill in parameters and persistent buffers, we need a way to initialize non-persistent buffers.

This PR does so by registering a buffer's init function against the buffer's FQN, and attaching such a callback dict to the model.

For how the init callbacks can be used, please refer to the `init_buffers` utility in this PR:
https://github.com/pytorch/PiPPy/pull/1135


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
Simplify label mapping logic,"# What does this PR do?

Simplifies label mapping logic

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
[i18n-<languageCode>] Translating docs to <languageName>中文,"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the <languageName>-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) https://github.com/huggingface/transformers/pull/20180
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) (waiting for initial PR to go through)
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)

<!--
Keep on adding more as you go 🔥
-->","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Tutorial for using DeepSpeed's activation checkpointing instead of PyTorch's,"### Feature request

Is there a tutorial for using DeepSpeed's activation checkpointing instead of PyTorch's?

I'm using `Trainer` with ZeRO integration to train my model. Here's my code:

```python
if training_args.deepspeed_gradient_checkpointing and training_args.deepspeed:
        from deepspeed.runtime.activation_checkpointing.checkpointing import configure
        configure(mpu_=None)
        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint
        model._set_gradient_checkpointing(training_args.deepspeed_gradient_checkpointing, checkpoint)
```

```json
{
""activation_checkpointing"": {
    ""partition_activations"": true,
    ""cpu_checkpointing"": true,
    ""contiguous_memory_optimization"": false,
    ""number_checkpoints"": null,
    ""synchronize_checkpoint_boundary"": false,
    ""profile"": false
  }
}
```

```shell
torchrun --nproc_per_node=8 \
    --nnodes=${NNODES} \
    --node_rank=${NODE_RANK} \
    --master_addr=${MASTER_ADDR} \
    --master_port=${MASTER_PORT} \
    train.py \
    --deepspeed ${DEEPSPEED_CONFIG_PATH} \
    --gradient_checkpointing False
```

However, I got this in FlashAttention2:

```
class XXXFlashAttention2(XXXAttention):
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        output_attentions = False

        bsz, q_len, _ = hidden_states.size()  # <---- this got error

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)
```

```
  File ""modeling_xxx.py"", line 518, in forward
    bsz, q_len, _ = hidden_states.size()
ValueError: not enough values to unpack (expected 3, got 2)
```

### Motivation

It seems there isn't such a tutorial available at the moment in either [deepspeed](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html)'s tutorial or [huggingface](https://huggingface.co/docs/transformers/v4.43.3/en/deepspeed#activationgradient-checkpointing).

### Your contribution

Provide my results","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",4,open
"Rewrite all modeling_whisper.py ,,, is there any reason at all why it's written like that?","### Feature request
I've been trying to make transformers work by editing the code with my model architecture.. Feature request.. more plasticity when it comes to unique architecture.. I want to be able to use hf and hf side hustles with my whisper-like transformer mashups.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Avoid decoding the entire token cache for every new token during text streaming,"# What does this PR do?
In the current implementation of the put() function in the TextStreamer class, the entire token cache is decoded for every new token. This is redundant, and expensive especially for very long context generations. In the case of long context generations, the total time it takes to complete the generation with streaming is higher compared to the case without streaming,

In this change, instead of maintaining token cache and decoding the entire cache every time, we maintain the decoded text and append to it each time a new token is generated.

I have tested a mini repro on M2 Pro, where the following example is faster by about 1 second in generating the long sequence. But in a real long context generation case, the difference is larger.

```
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

tok = AutoTokenizer.from_pretrained(""openai-community/gpt2"")
model = AutoModelForCausalLM.from_pretrained(""openai-community/gpt2"")
inputs = tok([""An increasing sequence from one to thousand: one,""], return_tensors=""pt"")
streamer = TextStreamer(tok)

# Despite returning the usual output, the streamer will also print the generated text to stdout.
model.generate(**inputs, streamer=streamer, max_new_tokens=1023)
```

Before change:
Time taken = 21.9 seconds (Averaged over 100 iterations)

After change:
Time taken = 20.2 seconds (Averaged over 100 iterations)

Verified that the tests pass.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@gante ",[],1,open
Added sdpa support for roberta,"# What does this PR do?

Added spda support for roberta

## Who can review?

@ArthurZucker ",[],0,open
fix: Updated BridgeTower Image processor,"# What does this PR do?
Updated BridgeTower Image processor.

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@amyeroberts @ArthurZucker ",[],4,open
Adds uniform processing kwargs to paligemma.,"# What does this PR do?
Adds Uniform Processing kwargs for paligemma model. 

Partially Fixes Issue 31911 


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

Can Review - @zucchini-nlp 
",[],3,open
"Some Whisper beam search output (sequences_scores, etc.) is lost in _stack_split_outputs","### System Info

- `transformers` version: 4.43.3
- Platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.23.2
- Safetensors version: 0.4.3
- Accelerate version: 0.30.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: no
- Using GPU in script?: yes
- GPU type: NVIDIA GeForce RTX 4090 Laptop GPU


### Who can help?

@sanchit-gandhi @kamilakesbi 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

In generating short form output (<30 sec):

```python
# inputs is from the processor
gen_kwargs = {
            ""max_new_tokens"": 400,
            ""num_beams"": 5,
            ""temperature"": None,
            ""return_timestamps"": False,
            ""return_dict_in_generate"": True,
            ""num_return_sequences"": 1,
            ""output_scores"": True,
            ""language"": ""english""
        }
pred_ids = self.model.generate(inputs, **gen_kwargs)
print(pred_ids.__class__)
print(dict((k,type(v)) for k, v in vars(pred_ids).items()))
```



### Expected behavior

`GenerateBeamEncoderDecoderOutput` seems to lose some fields in a recent version. (Maybe other output forms are also affected, haven't checked.)

Bisecting transformers versions, in 4.42.4 the output looked like:       

> <class 'transformers.generation.utils.GenerateBeamEncoderDecoderOutput'>
> {'sequences': <class 'torch.Tensor'>, **'sequences_scores': <class 'torch.Tensor'>**, 'scores': <class 'tuple'>, 'logits': <class 'NoneType'>, **'beam_indices': <class 'torch.Tensor'>**, 'encoder_attentions': <class 'NoneType'>, 'encoder_hidden_states': <class 'NoneType'>, 'decoder_attentions': <class 'NoneType'>, 'cross_attentions': <class 'NoneType'>, 'decoder_hidden_states': <class 'NoneType'>, 'past_key_values': <class 'tuple'>}
> 


In 4.43.0 and after `sequences_scores` and `beam_indices` became `None`:

> <class 'transformers.generation.utils.GenerateBeamEncoderDecoderOutput'>
> {'sequences': <class 'torch.Tensor'>, **'sequences_scores': <class 'NoneType'>**, 'scores': <class 'tuple'>, 'logits': <class 'NoneType'>, **'beam_indices': <class 'NoneType'>**, 'encoder_attentions': <class 'NoneType'>, 'encoder_hidden_states': <class 'NoneType'>, 'decoder_attentions': <class 'NoneType'>, 'cross_attentions': <class 'NoneType'>, 'decoder_hidden_states': <class 'NoneType'>, 'past_key_values': <class 'tuple'>}


It looks like these get removed in postprocessing, potential culprit in `_stack_split_outputs` at https://github.com/huggingface/transformers/blob/9451a385261b30e7319a2c93285ab76161e8c003/src/transformers/models/whisper/generation_whisper.py#L946

Which looks like it changed in https://github.com/huggingface/transformers/pull/30984.

Hacking in `if key in [""sequences"", ""beam_indices"", ""sequences_scores""]:`, for example, fixes it, although I'm not sure what's intended to be handled as tensors vs. tuples, so will defer as to the best way to fix.
","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",4,open
Remove irrelevant comment in `_update_causal_mask`,"Forgot to push a commit to https://github.com/huggingface/transformers/pull/32227, my bad!

Feel free to merge this one if approved!",[],1,open
🌐 [i18n-KO] Translated `<hqq>.md` to Korean,"# What does this PR do?

Translated the `<hqq>.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 N조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! 차례대로 1 ~ 5조 -->
<!-- May you please review this PR? -->
<!-- @jun048098, @yijun-lee, @mreraser, @shinhyunji36, @heuristicwave -->
<!-- @win2dvp21, @enchantee00, @fabxoe, @1kmmk1, @junejae -->
<!-- @SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd -->
@mjk0618, @cjfghk5697, @ahnjj, @JinukHong, @nuatmochoi
<!-- @010kim, @chhaewxn, @boyunJang, @jeongiin, @harheem -->

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
<!-- @stevhliu May you please review this PR? -->",[],1,open
Fix: MPS device is not being set on current implementation ,"# What does this PR do?
This PR changes the behavior of the function `_setup_devices(self)` to actually return an mps device when mps is available on the current machine. 

(Also this is my first PR on this repo so if I am doing something wrong sorry! And please help me making things right

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [✅] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@muellerz and @SunMarc 
",[],3,open
Trainer: add predict with generate,"# What does this PR do?

Fixes #26474, fixes #31462, fixes #33396 and fixes #31672. This PR adds possibility to generate and compute metrics on generated texts for decoder-only models. 

The basic idea is almost same as in Seq2Seq Trainer, but decoder-only models need a prompt-only input for generation. While for loss computation we need the whole input. Therefore we can ask users to prepare train and eval datasets, so that the eval contains `generation_inputs` used for generation. Additionally, to make user's life easier, I added a possibility to pass in different collators for train and for eval/test datasets.

The args used for generation should be set via `GenerationConfig`, as imo that makes most sense instead of adding only `max_length` and `num_beams` as in Seq2SeqTrainer. 

The code was tested with the below dummy train script. 

```python
import random
import torch
from datasets import load_dataset
from peft import LoraConfig
from transformers import AutoProcessor, BitsAndBytesConfig, Idefics2ForConditionalGeneration, TrainingArguments, Trainer

DEVICE = ""cuda:0""
processor = AutoProcessor.from_pretrained(""HuggingFaceM4/idefics2-8b"", do_image_splitting=False)
pad_token_id = processor.tokenizer.pad_token_id

lora_config = LoraConfig(
    r=8,
    lora_alpha=8,
    lora_dropout=0.1,
    target_modules='.*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$',
    use_dora=False,
    init_lora_weights=""gaussian""
)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = Idefics2ForConditionalGeneration.from_pretrained(
    ""HuggingFaceM4/idefics2-8b"",
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
)

model.add_adapter(lora_config)
model.enable_adapters()


eval_dataset = load_dataset(""nielsr/docvqa_1200_examples"", split=""test"")
eval_dataset = eval_dataset.remove_columns(['id', 'words', 'bounding_boxes', 'answer'])
eval_dataset = eval_dataset.select(range(10))


class DataCollatorForGeneration:
    def __init__(self, processor, eval_mode=False):
        self.processor = processor
        self.image_token_id = processor.tokenizer.additional_special_tokens_ids[
            processor.tokenizer.additional_special_tokens.index(""<image>"")
        ]
        self.eval_mode = eval_mode

    def __call__(self, examples):
        texts, texts_eval = [], []
        images = []
        for example in examples:
            image = example[""image""]
            question = example[""query""][""en""]
            answer = random.choice(example[""answers""])
            messages = [
                {
                    ""role"": ""user"",
                    ""content"": [
                        {""type"": ""text"", ""text"": ""Answer briefly.""},
                        {""type"": ""image""},
                        {""type"": ""text"", ""text"": question}
                    ]
                },
                {
                    ""role"": ""assistant"",
                    ""content"": [
                        {""type"": ""text"", ""text"": answer}
                    ]
                }
            ]
            text = processor.apply_chat_template(messages, add_generation_prompt=False)
            text_eval = processor.apply_chat_template([messages[0]], add_generation_prompt=True)
            texts.append(text.strip())
            texts_eval.append(text_eval.strip())
            images.append([image])

        # Make sure we have right padding in train and left padding for eval parts
        processor.tokenizer.padding_side = ""right""
        batch = processor(text=texts, images=images, return_tensors=""pt"", padding=True) 
        
        if self.eval_mode:
            processor.tokenizer.padding_side = ""left""
            batch_eval = processor(text=texts, images=images, return_tensors=""pt"", padding=True)
            batch['generation_input_ids'] = batch_eval['input_ids']
            batch['generation_attention_mask'] = batch_eval['attention_mask']

        labels = batch[""input_ids""].clone()
        labels[labels == processor.tokenizer.pad_token_id] = self.image_token_id
        batch[""labels""] = labels

        return batch

gen_config = model.generation_config
gen_config.max_length = 200

training_args = TrainingArguments(
    max_steps=100,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=5,
    gradient_accumulation_steps=2,
    output_dir=""tmp_delete"",
    eval_strategy=""steps"",
    fp16=True,
    remove_unused_columns=False,
    report_to=""none"",
    predict_with_generate=True,
    generation_config=gen_config,
)

def custom_metrics(prediction_dict):
    # unmask for correct detokenization, because preds are padded to max length with -100
    preds = prediction_dict.predictions
    preds[preds == -100] = pad_token_id
    lbls = prediction_dict.label_ids
    lbls[lbls == -100] = pad_token_id

    # Decode and do magic for metrics
    preds = processor.batch_decode(preds)
    lbls = processor.batch_decode(lbls)
    bleu = rouge = 0
    return {""bleu"" : bleu, ""rouge"": rouge}


trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=DataCollatorForGeneration(processor),
    eval_data_collator=DataCollatorForGeneration(processor, eval_mode=True),
    train_dataset=eval_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=custom_metrics,
)

print(trainer.evaluate())
```",[],21,open
Length grouping for iterable datasets,"### Feature request

Similar to non-iterable datasets I would like functionality to group batches by similar length inputs. 

Example of how this could be implemented is using a buffer to preload batches and sort in IterableDatasetShard in `accelerate` using the defined column for length grouping. 

```
class IterableDatasetShard(IterableDataset):
    def __init__(
        self,
        dataset: IterableDataset,
        batch_size: int = 1,
        drop_last: bool = False,
        num_processes: int = 1,
        process_index: int = 0,
        split_batches: bool = False,
        group_by_length_buffer: int = 1, ## how many batches to preload
        length_column_name: str = """", ## which column determines length
    ):
        if split_batches and batch_size > 1 and batch_size % num_processes != 0:
            raise ValueError(
                f""To use `IterableDatasetShard` in `split_batches` mode, the batch size ({batch_size}) ""
                f""needs to be a round multiple of the number of processes ({num_processes}).""
            )
        self.dataset = dataset
        self.batch_size = batch_size
        self.drop_last = drop_last
        self.num_processes = num_processes
        self.process_index = process_index
        self.split_batches = split_batches
        self.group_by_length_buffer = group_by_length_buffer
        self.length_column_name = length_column_name

    def set_epoch(self, epoch):
        self.epoch = epoch
        if hasattr(self.dataset, ""set_epoch""):
            self.dataset.set_epoch(epoch)

    def __len__(self):
        # We will just raise the downstream error if the underlying dataset is not sized
        if self.drop_last:
            return (len(self.dataset) // (self.batch_size * self.num_processes)) * self.batch_size
        else:
            return (
                math.ceil(len(self.dataset) / (self.batch_size * self.num_processes))
                * self.batch_size
            )

    def __iter__(self):
        if (
            not hasattr(self.dataset, ""set_epoch"")
            and hasattr(self.dataset, ""generator"")
            and isinstance(self.dataset.generator, torch.Generator)
        ):
            self.dataset.generator.manual_seed(self.epoch)
        real_batch_size = (
            self.batch_size if self.split_batches else (self.batch_size * self.num_processes)
        ) * self.group_by_length_buffer
        process_batch_size = (
            (self.batch_size // self.num_processes) if self.split_batches else self.batch_size
        ) * self.group_by_length_buffer
        process_slice = range(
            self.process_index * process_batch_size, (self.process_index + 1) * process_batch_size
        )

        first_batch = None
        current_batch = []
        for element in self.dataset:
            current_batch.append(element)
            # Wait to have a full batch before yielding elements.
            if len(current_batch) == real_batch_size:
                process_current_batch = [current_batch[i] for i in process_slice]
                process_current_batch.sort(key=lambda x: len(x[self.length_column_name]))
                yield from process_current_batch
                if first_batch is None:
                    first_batch = current_batch.copy()
                current_batch = []

        # Finished if drop_last is True, otherwise complete the last batch with elements from the beginning.
        if not self.drop_last and len(current_batch) > 0:
            if first_batch is None:
                first_batch = current_batch.copy()
            while len(current_batch) < real_batch_size:
                current_batch += first_batch
            for i in process_slice:
                yield current_batch[i]


```





### Motivation

This will help improve efficiency for our datasets with very large variance in sequence lengths. 

### Your contribution

I could submit an initial PR .","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Filtering logits of non tensors,"# What does this PR do?
adds default ignore keys to mamba config for inference, avoiding returning logits tuple with cache_params output in it
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],0,open
Add Segment Anything 2 (SAM2),"# What does this PR do?

https://github.com/huggingface/transformers/issues/32308

As stated in this issue this PR is making SAM2 compatible to transformers

cc. @zinccat @RUFFY-369

- [X] image encoder
- [X] promptencoder
- [ ] 

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",11,open
Support for Segment Anything 2,"### Model description

The newer version of SAM, https://github.com/facebookresearch/segment-anything-2

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://github.com/facebookresearch/segment-anything-2","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",12,open
DeepSpeed sequence parallelism (aka Ulysses) integration with HF transformer,"# What does this PR do?
This PR enhances capabilities of [DeepSpeed long sequence (context) parallelism (aka DS Ulysses)](https://dl.acm.org/doi/10.1145/3662158.3662806) with support for HF models. Support is currently enabled when both DeepSpeed and flash attn are enabled. Future support would be extended to SPDA.  All current and future HF models (such as Llama, opt etc) using refactored flash_attention_utils are supported.

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@muellerzr ","[{'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",11,open
Fix chinese clip,"# What does this PR do?

Fixes #32280 , weight loading issue

cc @qubvel @amyeroberts, linked issue shows a weight loading error when loading the text part of `ChineseCLIP`. It's an edge case due to checkpoint structure + the way ChineseCLIPModel is created. This separates ChineseCLIPModel parts a bit better, while hopefully not breaking anything. 
To clarify: it will not break preexisting checkpoints of ChineseCLIPModel BUT it _will_ break ckpts of `ChineseCLIPTextModel` if such exist. 

",[],2,open
[i18n-KO] Translated `aqlm.md` to Korean,"# What does this PR do?

Translated the `<your_file>.md` file of the documentation to Korean.
Thank you in advance for your review. 

Part of https://github.com/huggingface/transformers/issues/20179

## Before reviewing
- [x] Check for missing / redundant translations (번역 누락/중복 검사)
- [x] Grammar Check (맞춤법 검사)
- [x] Review or Add new terms to glossary (용어 확인 및 추가)
- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)
- [ ] Check live-preview for gotchas (live-preview로 정상작동 확인)

## Who can review? (Initial)

<!-- 1. 위 체크가 모두 완료된 뒤에만 3조 팀원들에게 리뷰 요청하는 아래 주석을 노출해주세요! -->
<!-- May you please review this PR? -->
@SeungAhSon, @Jwaminju, @thsamajiki, @4N3MONE, @jungnerd

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review? (Final)

<!-- 2. N조 팀원들과 리뷰가 끝난 후에 아래 주석을 노출해주세요! -->
<!-- @stevhliu May you please review this PR? -->",[],6,open
Support loading shard GGUF models,"### Feature request

- Support loading from sharded GGUF model files (For example, [legraphista/Meta-Llama-3.1-70B-Instruct-IMat-GGUF](https://huggingface.co/legraphista/Meta-Llama-3.1-70B-Instruct-IMat-GGUF/tree/main)).

### Motivation

- For some large models, the GGUF format model will be split into multiple shards during converting. 
- However, the GGUF integration in `transformers` only supports single file GGUF model currently.

### Your contribution

- This kind of GGUF model has been split into multiple shards, formatted as `<ShardNum>-of-<ShardTotal>`, just like sharded `safetensors`. 
- The header for config and tokenizer handing are kept in the first shard file (For example, [Meta-Llama-3.1-70B-Instruct.Q5_K-00001-of-00003.gguf](https://huggingface.co/legraphista/Meta-Llama-3.1-70B-Instruct-IMat-GGUF/blob/main/Meta-Llama-3.1-70B-Instruct.Q5_K/Meta-Llama-3.1-70B-Instruct.Q5_K-00001-of-00003.gguf)).
- For more details about sharded GGUF files, please refer to [gguf.md](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md).","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
JudgeXL-LLM,"### Model description

Transformers XL variation called JudgeXL-LLM (for now). I am experimenting with different variations of this model but the goal is to make this model infused with a restaurant program i am developing. It is a text generation/question answering LLM that i would eventually like to teach logic and reasoning. I am still new to the AI/Machine learning world so I will be making adjustments as i experiment with this model. I would like the Original Model name to be JudgeXL-LLM but the model type to be judgellm because i am expecting to make plenty of variations and they may not include the xl part of the code.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://huggingface.co/Wonder-Griffin/XL-Judge-LLM... that is where i have the model loaded, the rest is all saved on my local device. I am the only one who has worked on the code, my name is Morgan Griffin, aka WongrifferousAI, Judgemrogan, Wonder-Griffin","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
JudgeXL-LLM,"### Model description

Transformers XL variation called JudgeXL-LLM (for now). I am experimenting with different variations of this model but the goal is to make this model infused with a restaurant program i am developing. It is a text generation/question answering LLM that i would eventually like to teach logic and reasoning. I am still new to the AI/Machine learning world so I will be making adjustments as i experiment with this model. I would like the Original Model name to be JudgeXL-LLM but the model type to be judgellm because i am expecting to make plenty of variations and they may not include the xl part of the code.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://huggingface.co/Wonder-Griffin/XL-Judge-LLM... that is where i have the model loaded, the rest is all saved on my local device. I am the only one who ahs worked on the code, my name is Morgan Griffin, aka WongrifferousAI, Judgemrogan, Wonder-Griffin","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Export to ExecuTorch,"## Feature request

Unlock a new workflow for on-device use-cases via [**torch.export**](https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html) and [**ExecuTorch**](https://pytorch.org/executorch/main/intro-overview.html).

So ideally the users can have an e2e experience by loading a pretrained transformer model from HuggingFace, export and lower it to `ExecuTorch` and get reasonable performance out-of-the-box. 

For example:

1. Load a model with StaticCache:
```
model = AutoModelForCausalLM.from_pretrained(
    hf_model_repo,
    config=config,
    attn_implementation=""sdpa"",
    cache_config={
        ""use_cache"": True, 
        ""cache_implementation"": ""static"", 
        ""max_cache_length"": 128,
    },  # Mandatory field to set ONLY for ""Export to ExecuTorch"" workflow, optional in other use-cases
)
```

2. Then export the model with StaticCache. 
```
exported_program = convert_and_export_with_cache(
    model, 
    args=(model_inputs,), 
    kwargs={""position_ids"": <val>, ""inputs_embeds"": <val>, ""cache_position"": <val>}
```
and then further lower the exported program to `ExecuTorch` with delegates for performance:
```
executorch_m = lower_to_executorch(
    model, 
    recipes=""xnnpack_fp32"",  # Delegate to XNNPACK backend
)

# The lowered artifact can be saved into a `.pte` binary format for integration and distribution.
```
With that you may get a model for on-device with reasonable performance to start with. 

From there and still within `ExecuTorch` stack, you can easily tailor the experience for your use-cases, of course, with better performance! Note that `ExecuTorch` supports delegatation to [XNNPACK backend](https://pytorch.org/executorch/main/native-delegates-executorch-xnnpack-delegate.html), [Apple Core ML](https://pytorch.org/executorch/main/build-run-coreml.html) and [MPS](https://github.com/pytorch/executorch/tree/main/examples/apple/mps), [Qualcomm QNN](https://pytorch.org/executorch/main/build-run-qualcomm-ai-engine-direct-backend.html), [ARM Ethos-U](https://pytorch.org/executorch/stable/executorch-arm-delegate-tutorial.html), [Vulkan GPU](https://pytorch.org/executorch/main/build-run-vulkan.html) and more. You can learn more by reading our [tutorial](https://pytorch.org/executorch/main/examples-end-to-end-to-lower-model-to-delegate.html).


3. Use the exported/lowered artifact for inference:
```

# The lowered artifact can run on a local device in the ExecuTorch runtime in c++ or via pybind, providing the same experience as how users run inference with the eager model on server.

generate(model=executorch_m, prompt=""Hello world"")  # Will generate up to the maximal sequence length/cache length 
```

The example workflow above shows direct integration between `ExecuTorch` and HF `transformers` models. Eventually this workflow could be accessible via `optimum exporters-et`, `Transformers.js` or in [`ExecuTorch`](https://github.com/pytorch/executorch) and [`torchchat`](https://github.com/pytorch/torchchat).

## Motivation

Unlock a whole new on-device experience of using HuggingFace models w/o leaving the PyTorch ecosystem ([`ExecuTorch`](https://pytorch.org/executorch/main/intro-overview.html) is native PyTorch!)


##  Issues Tracker

### Cache
- [x] Make `StaticCache` compatible with `torch.export`: PR #32168
- [x] #32500: PR #32830
- [x] #32503
- [ ] Support dynamic length slicing in `StaticCache`: PR #30862
- [x] #32504: PR #33707

### E2E workflow
- [ ] Umbrella task for `Optimum` enablement: https://github.com/huggingface/optimum/issues/2128
- [ ] Umbrella task for `Tranformers.js` enablement: https://github.com/huggingface/transformers.js/issues/1039

### Optimization
- [ ] Support quantized models w/ ExecuTorch + TorchAO #34787

### Models
- [x] #33709: PR #33707
- [x] #32505: PR #34101
- [ ] #32506
- [x] #32507: PR #34424
- [ ] #32508
- [ ] #32509
- [x] #33833: PR #34102
- [ ] #33834
- [x] #33835: PR #34475
- [x] #33836: PR #34476
- [ ] #33837
- [ ] #33838
- [ ] #33839
- [x] #33840: PR #34181
- [x] #33841: PR #34425
- [ ] #33842
- [x] #33843: PR #34473
- [ ] #34879
- [ ] #35327

And more! We're ambitious to expanding the model coverage massively. Please comment below if you are interested in a particular model for on-device use-case!

Even better, we warmly welcome direct contributions from the community to support more models in exporting to ExecuTorch!
- [x] Cohere2: #35224
- [x] OLMo2: #34551


## Your contribution

1. Co-design the ""Export to ExecuTorch"" workflow.
2. Co-design the `generate` for exported model and the integration in `Optimum`

Here is how ExecuTorch implements the `generate()` for llama2/3 in [eager python](https://github.com/pytorch/executorch/blob/main/examples/models/llama2/runner/generation.py) and [c++](https://github.com/pytorch/executorch/blob/main/examples/models/llama2/runner/runner.cpp).


cc: @amyeroberts @gante @ArthurZucker @michaelbenayoun ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 7305045262, 'node_id': 'LA_kwDOCUB6oc8AAAABs2olDg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ExecuTorch', 'name': 'ExecuTorch', 'color': '33CAA3', 'default': False, 'description': ''}]",11,open
Chameleon image generation low quality. ,"### System Info

transformers 4.43.0 dev

### Who can help?

@zucchini-nlp 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Using the model.generate function within the transformers framework to generate an image, in comparison to the inference pipeline from https://github.com/GAIR-NLP/anole, results in a relatively lower quality image.

Besides I take a test on some image generation benchmark, and get consistent results.

### Expected behavior

1. Could you please provide a script to convert Hugging Face weights to PyTorch weights, so that we can use them with the Meta inference pipeline?
2. Could you kindly outline the best sampling parameters in the doc for generating high-quality images?","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",1,open
[WIP] Add support for XTR,"# What does this PR do?

XTR (ConteXtualized Token Retriever) is a multi-vector retrieval model that improves efficiency by focusing on retrieving and ranking the most important document tokens. Details are described at https://arxiv.org/abs/2304.01982.
Author: @jhyuklee

Fixes #31873

## Before submitting
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker @amyeroberts
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Add New Optimizer,"### Feature request

I want to add a new optimizer that I worked on it 
Is there any doc or anything how to add this new optimizer to all files of transformers library?

### Motivation

This optimizer would be great to implement on transformers and reduce time to train a new model 

### Your contribution

I can make a clone of this repo and then changes file and submit PR. But I can't find a document how to implement my algorithm to transformers Files because  I found so much classes and there are a lots of file that need changes. 
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
flashattention3,"### Feature request

flashattention3 has been released, when will it be supported flashattention3？

### Motivation

use

### Your contribution

pr","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
auto_find_batch_size for OOM during evaluation,"### System Info

When running training and evaluation (`_inner_training_loop` in the HF `Trainer`), the auto-find-batch size tries to reduce the training batch size, even when the OOM happens during evaluation and not the training. It should distinguish whether the error comes from training or evaluation (`_maybe_log_save_evaluate`) and reduce the evaluation batch size if needed.
Also, it would be nice to report the finally chosen batch size (maybe this is already done).

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Choose a very large evaluation batch size, and a small training batch size.

### Expected behavior

Should reduce eval batch size, not just the training batch size","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",9,open
Support `from_pretrained` of `FlaxPretrainedModel` from sharded `.safetensors` weights,"### Feature request

Currently `FlaxPretrainedModel` only supports loading pretrained models from sharded PyTorch weights or single-file `.safetensors`. It's worth adding support for loading sharded `.safetensors`.

### Motivation

Recent open-source language models trained with PyTorch are likely only to release sharded `.safetensors` weights. The lack of support for loading from the dominating paradigm makes it troublesome to use these models in Jax.

### Your contribution

I'm relatively new to the implementation of the saving & loading mechanism of `transformers`, but I'd love to try to work with this feature if the core team doesn't have enough bandwidth.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
[WIP] - Enable speculative decoding with batch size >1,"# What does this PR do?

This PR aims at solving issue #32165. 

I've started adapting code to enable speculative decoding with batch_size >1. I've reused some of the work in former PR #26875. 

# Main steps of the solution:  

When batch size > 1: 

1. Compute the number of similar tokens between candidate tokens and tokens obtained after doing a forward pass with the main model. This results in a tensor `n_matches` with the number of matches for each sequence in the batch. 

2. We keep the matching tokens. For that, we keep all tokens from the output to the main model with a sequence position inferior to `n_matches.max() + 1`. In doing so, we also retain some potential mismatched tokens, which we will deal with in the next steps using padding tokens. The resulting tensor, `input_ids`,  is thus in the form (`batch_size`, `n_matches` + 1).

3. We shift each sequence `i` in `input_ids` by `n_matches.max() - n_matches[i]`. The matching tokens are displaced to the right of `input_ids` and `n_matches.max() - n_matches[i]` padding tokens are added to the left.

5. Left cut: We cut all columns that contain only padding tokens. By design, these columns are to the left of the `input_ids`. In this way we keep the smallest possible `input_ids' that contain all the information needed to continue assisted generation. 

Steps 1 to 4 are the main addition to the the original speculative decoding loop described in detail in [this blog](https://huggingface.co/blog/assisted-generation) to enable assisted generation with BS > 1.

To make this work, we also need to adapt the computation of  the `attention_masks`, `past_key_values` and `position_ids` to take into account the shifted positions of the generated tokens. 

# To do: 

For now, I want to make this work with Whisper using this [snippet](https://github.com/huggingface/transformers/issues/32165). 

I've implemented steps 1 to 4 and adapted the computation of the `attention_masks` and `past_key_values` to handle the new padding tokens. 

I still need to make some adaptations with the `position_ids` to make this work properly. From what I can see: 

- For the main model (here `WhisperForConditionalGeneration`), `position_ids` are inferred directly from the attention_mask as we can see [here](https://github.com/huggingface/transformers/blob/7f5d644e69068825bb5b6e84cdc56b3d3a9bd04f/src/transformers/models/whisper/modeling_whisper.py#L1774). So if we pass the right attention mask to `generate` we should be good. 

- For the assistant model (`WhisperForCausalLM` in our example), `position_ids` are currently not computed nor passed to [prepare_inputs_for_generation](https://github.com/huggingface/transformers/blob/7f5d644e69068825bb5b6e84cdc56b3d3a9bd04f/src/transformers/models/whisper/modeling_whisper.py#L2052), which I'm not sure exactly why. I've done a first attempt at solving this with no success so far. 


cc @sanchit-gandhi @gante ",[],11,open
"Uniformize kwargs for Layoutlm (2, 3, X) processors","# What does this PR do?

- Uniformizes kwargs for LayoutLM (2, 3, X) processors as discussed in https://github.com/huggingface/transformers/issues/31911
- Also fixes the bug in `tests_exotic_models` which totally prevents any test from being run

Fixes # (issue)

- https://github.com/huggingface/transformers/issues/31911

## Who can review?

@zucchini-nlp @molbap @NielsRogge ",[],2,open
Enable speculative decoding with batch size >1,"### Feature request

Speculative decoding isn't currently enabled for batch sizes >1. PR #26875 was previously open to add this feature to main, but never merged. As the PR is quite old and has been closed, I'm opening an issue to motivate the addition of this feature to Transformers. 

Two approaches can be implemented to enable speculative decoding with batch size >1: 

1. **First approach**: at each step, we compare the token ids obtained with the assistant model with those obtained with the main model for all sequences in the batch, and roll back to the first incorrect token id in the batch. 

This is a simple approach, but rather naive, since some valid tokens would have to be regenerated during the successive iterations of the assistant model. 

2. **Second approach:** At each step, we dynamically roll back to the first mismatching token ID of each sequence in the batch, correcting with padding tokens and attention masks. 

In this way, at each step, we would keep all the valid tokens and wouldn't need to regenerate them during future iterations of the assistant model. 

The second approach is better, and PR #26875 already implements most of the solution, so we should focus on that one IMO.



## How to reproduce: 

```python
from transformers import pipeline, AutoModelForCausalLM, AutoModelForSpeechSeq2Seq, AutoProcessor
import torch
from datasets import load_dataset

device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

assistant_model_id = ""distil-whisper/distil-large-v2""

assistant_model = AutoModelForCausalLM.from_pretrained(
    assistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
assistant_model.to(device)

model_id = ""openai/whisper-large-v2""

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

pipe = pipeline(
    ""automatic-speech-recognition"",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    max_new_tokens=128,
    generate_kwargs={""assistant_model"": assistant_model},
    torch_dtype=torch_dtype,
    chunk_length_s=15,
    batch_size=4,
    device=device,
)

dataset = load_dataset(""distil-whisper/librispeech_long"", ""default"", split=""validation"")
sample = dataset[0][""audio""]

result = pipe(sample)
print(result[""text""])
```

## Curent output: 

```
ValueError: assisted generate is only supported for batch_size = 1
```

## Expected output: 

```
"" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his manner. He tells us that at this festive season of the year with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Upguards and Adam paintings, and Mason's exquisite idylls are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth, and Mr. John Collier gives his sitter a cheerful slap on the back, before he says, like a shampoo-er in a Turkish bath, Next man!""
```

### Your contribution

I started iterating on the solution and will open a PR soon to solve it :) 

cc @sanchit-gandhi @ylacombe @gante ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Add Matching Anything by Segmenting Anything (MASA) MOT tracking model,"### Model description

I know the transformers library has not included object tracking models in the past, but this one can either plug into any object detection model or be an end-to-end open world tracking model by using a backbone line Grounding-Dino, DETR or Sam, all of which are already implemented in transformers. 
It achieves state of the art on the Open-vocabulary MOT benchmark.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Authors: @siyuanliii, @lkeab, @martin-danelljan, @mattiasegu

Code: https://github.com/siyuanliii/masa/tree/main
Weights: https://huggingface.co/dereksiyuanli/masa/tree/main
Paper: https://arxiv.org/pdf/2406.04221","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
Added error when sequence length is bigger than max_position_embeddings,"Fixes # 32154


",[],4,open
Adding warnings or errors when provided sequence length is bigger than config.max_position_embeddings,"### Feature request

Add a new ValueError when you pass input_ids larger than config.max_position_embeddings somwhere in the code.

### Motivation

In the [CLIPTextEmbeddings class](https://github.com/huggingface/transformers/blob/a1844a3209eb7e75582684809203bc189931a90c/src/transformers/models/clip/modeling_clip.py#L200) When you pass input_ids larger than config.max_position_embeddings you don't get errors until the last stage where you add input_embeds and position_embeddings where you get dimension incompatibility error, which is quite vague at that stage. 

### Your contribution

Not sure where is the best place to put the ValueError. I can make a PR If you specify this.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
callback to implement how the predictions should be stored.,"I am exploring distributed inference capabilities with the Hugging Face Trainer for transformers. I need to do distributed inference across multiple devices or nodes and save the predictions to a file. However, after reviewing the available callbacks, I did not find any that facilitate this specific task. Furthermore, when using the trainer.predict method, I noticed that it returns only the labels and predictions, without including the original input batches used for inference.

PyTorch Lightning offers a flexible mechanism for handling prediction outputs using custom callbacks. For example, the following PyTorch Lightning code snippet demonstrates how a custom **BasePredictionWriter** callback can be implemented to save predictions to files:

```import torch
import os
from lightning.pytorch.callbacks import BasePredictionWriter

class CustomWriter(BasePredictionWriter):

    def __init__(self, output_dir, write_interval):
        super().__init__(write_interval)
        self.output_dir = output_dir

    def write_on_batch_end(
        self, trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx
    ):
        torch.save(prediction, os.path.join(self.output_dir, str(dataloader_idx), f""{batch_idx}.pt""))

    def write_on_epoch_end(self, trainer, pl_module, predictions, batch_indices):
        torch.save(predictions, os.path.join(self.output_dir, ""predictions.pt""))

pred_writer = CustomWriter(output_dir=""pred_path"", write_interval=""epoch"")
trainer = Trainer(callbacks=[pred_writer])
model = BoringModel()
trainer.predict(model, return_predictions=False)
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
add scaling_factor to GemmaRotaryEmbedding for fix error in GemmaLine…,"In generated code, there are GemmaLinearScalingRotaryEmbedding and GemmaDynamicNTKScalingRotaryEmbedding which use scaling_factor need to declare it in base class, or remove generation of: GemmaLinearScalingRotaryEmbedding and GemmaDynamicNTKScalingRotaryEmbedding",[],4,open
DINOv2 register support,"# What does this PR do?
This PR adds DINOv2 with registers as proposed in [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588).
My aim was to  add registers with minimal code changes in a backward compatible way.
My changes were made using the [facebookresearch/dinov2](https://github.com/facebookresearch/dinov2) repository commits.
I also had to add additional options for interpolation which caused me to comment and fix the intepolate embeddings method. (Said comments were written on the fly as I was understanding and changing the code and are biased, and can be removed).
While doing that I added support for non square patch sizes which seamed like a bug since `Dinov2PatchEmbeddings` supported  them, but the rest of the code didn't. 


To do:

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes https://github.com/huggingface/transformers/issues/27379
Alternatives https://github.com/huggingface/transformers/pull/31832

## Before submitting
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Update benchmark.py-- Enhance Benchmarking with Multi-Commit Support …,"1. **Enhanced Metrics Handling**:
   - Integrated new metrics to capture detailed performance data: - `decode.latency.mean`: Captures the average latency during the decoding process. - `per_token.latency.mean`: Measures the average latency per token. - `per_token.throughput.value`: Calculates the throughput per token.
   - These metrics offer finer granularity of performance insights, aiding in the precise identification of bottlenecks and optimization opportunities.

2. **Support Multi-Commit Benchmarking**:
   - Implemented functionality to benchmark multiple commits by checking out specific commits and running benchmarks for each.
   - This feature allows users to compare the performance of different code versions, providing insights into how changes impact performance over time.
   - Utilized a context manager (`checkout_commit`) to manage the process of checking out and reverting commits.

3. **Providing Structured JSON Reports**:
   - Added functionality to generate and summarize benchmark results into structured JSON reports for better analysis and comparison.
   - The JSON reports include model, commit, configuration, and metrics data, facilitating detailed performance tracking and reporting.

4. ** Code Changes**:
   - Added `summarize` function to produce summaries of benchmark results.
   - Implemented `combine_summaries` function to aggregate information across commits.
   - Integrated these functions into the main script, ensuring comprehensive reporting and analysis.

These enhancements collectively improve the usability and effectiveness of the benchmarking process, making it easier to track and optimize performance across different code versions.

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],2,open
fix: resolve bug with `use_mps_device` setting not taking effect,"# Fixes the bug with `use_mps_device` setting not taking effect.
In training_args.py,, even when `use_mps_device` is set, the current implementation overrides it with `device=cuda/cpu`, causing `use_mps_device` to not take effect.

It's a minor code typo, so I didn't open an issue for it. 

<img width=""982"" alt=""image"" src=""https://github.com/user-attachments/assets/68b84763-7b67-4064-aa00-34f77bd8f453"">


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#create-a-pull-request), Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.pytorch.org/)? Please add a link to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/pytorch/pytorch/tree/master/docs), and [here are tips on formatting docstrings](https://github.com/pytorch/pytorch/tree/master/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?
",[],2,open
_prepare_4d_causal_attention_mask mask inversion should work boolean masks,"### Feature request

https://github.com/huggingface/transformers/blob/0fdea8607d7e01eb0e38a1ebeb7feee30a22f0cf/src/transformers/modeling_attn_mask_utils.py#L332

Here we can just assume the user provides a tensor of 0s and 1s right? In that case, the inversion can be done like `inverted_mask = ~(attention_mask.to(torch.bool))`

### Motivation

There are two advantages I found with doing the conversion assuming the input is bool
* Its slightly faster 
* When using a cached huggingface dataset to generate the mask, it is way more space-efficient to store the masks as bool type. 

### Your contribution

`inverted_mask = ~(attention_mask.to(torch.bool))` ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
[wip][meta-llama][torch.compile] Fix issues with torch.compile,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Using Trainer + a pretrained tokenizer + 4D attention mask is extremely slow,"### System Info

transformers                  4.41.0

### Who can help?

@ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```
from transformers import LlamaForCausalLM, LlamaConfig, TrainingArguments, Trainer, AutoTokenizer
from datasets import IterableDataset
import numpy as np

model_config = LlamaConfig(
    vocab_size=10,
    hidden_size=384,
    num_hidden_layers=6,
    num_attention_heads=6,
    intermediate_size=1024,
    max_position_embeddings=1024,
)
model = LlamaForCausalLM(model_config)
tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')

def get_data1():
    for i in range(10000):
        yield {'input_ids': np.zeros(1024, dtype=int), 'labels': np.zeros(1024, dtype=int), 'attention_mask': np.zeros((1, 1024, 1024), dtype=float)}

def get_data2():
    for i in range(10000):
        yield {'input_ids': np.zeros(1024, dtype=int), 'labels': np.zeros(1024, dtype=int), 'attention_mask': np.zeros((1024), dtype=int)}
    
ds_slow = IterableDataset.from_generator(get_data1).with_format('torch')
ds_fast = IterableDataset.from_generator(get_data2).with_format('torch')

training_args = TrainingArguments(max_steps=1, output_dir='./out', report_to=None, per_device_train_batch_size=32, gradient_accumulation_steps=32)
trainer1 = Trainer(model, training_args, train_dataset=ds_slow, tokenizer=tokenizer)
trainer2 = Trainer(model, training_args, train_dataset=ds_fast, tokenizer=tokenizer)

import cProfile
cProfile.run('trainer1.train()', './test_slow.profile')
cProfile.run('trainer2.train()', './test_fast.profile')
```

```
import pstats

# compare the two profiles
p1 = pstats.Stats('./test_slow.profile')
p2 = pstats.Stats('./test_fast.profile')
p1.sort_stats('cumtime').print_stats()
```
```
         1582200 function calls (1401111 primitive calls) in 340.112 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000  340.112  340.112 {built-in method builtins.exec}
        1    0.000    0.000  340.112  340.112 <string>:1(<module>)
        1    0.000    0.000  340.112  340.112 trainer.py:1784(train)
        1    0.017    0.017  340.112  340.112 trainer.py:1892(_inner_training_loop)
       33    0.001    0.000  326.171    9.884 data_loader.py:663(__iter__)
       33    0.001    0.000  325.473    9.863 data_loader.py:618(_fetch_batches)
 2486/265    0.001    0.000  325.428    1.228 {built-in method builtins.next}
       33    0.001    0.000  325.088    9.851 dataloader.py:625(__next__)
       33    0.725    0.022  325.083    9.851 dataloader.py:672(_next_data)
       33    0.002    0.000  323.988    9.818 fetch.py:24(fetch)
       33    0.000    0.000  320.979    9.727 trainer_utils.py:807(__call__)
       33    0.000    0.000  320.971    9.726 data_collator.py:270(__call__)
       33   16.982    0.515  320.971    9.726 data_collator.py:52(pad_without_fast_tokenizer_warning)
       33    0.005    0.000  303.989    9.212 tokenization_utils_base.py:3209(pad)
     6493  235.747    0.036  235.747    0.036 {built-in method torch.tensor}
      197    0.001    0.000  234.735    1.192 tokenization_utils_base.py:204(__init__)
      197    0.001    0.000  234.732    1.192 tokenization_utils_base.py:681(convert_to_tensors)
       99    0.000    0.000  234.730    2.371 tokenization_utils_base.py:718(as_tensor)
```

```
p2.sort_stats('cumtime').print_stats()
```

```
        1567440 function calls (1386340 primitive calls) in 16.431 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000   16.431   16.431 {built-in method builtins.exec}
        1    0.000    0.000   16.431   16.431 <string>:1(<module>)
        1    0.000    0.000   16.431   16.431 trainer.py:1784(train)
        1    0.018    0.018   16.431   16.431 trainer.py:1892(_inner_training_loop)
       32    0.003    0.000   14.327    0.448 trainer.py:3212(training_step)
       32    0.001    0.000    8.830    0.276 accelerator.py:2093(backward)
       32    0.000    0.000    8.829    0.276 _tensor.py:433(backward)
       32    0.000    0.000    8.829    0.276 __init__.py:149(backward)
       32    8.827    0.276    8.827    0.276 {method 'run_backward' of 'torch._C._EngineBase' objects}
       33    0.000    0.000    4.546    0.138 memory.py:147(empty_cache)
       33    4.546    0.138    4.546    0.138 {built-in method torch._C._cuda_emptyCache}
 2486/265    0.001    0.000    1.469    0.006 {built-in method builtins.next}
       33    0.001    0.000    1.160    0.035 data_loader.py:663(__iter__)
       33    0.000    0.000    1.145    0.035 data_loader.py:618(_fetch_batches)
       33    0.000    0.000    1.136    0.034 dataloader.py:625(__next__)
       33    0.003    0.000    1.134    0.034 dataloader.py:672(_next_data)
       33    0.002    0.000    1.124    0.034 fetch.py:24(fetch)
       32    0.000    0.000    0.955    0.030 trainer.py:3254(compute_loss)
...
        1    0.000    0.000    0.000    0.000 modeling_utils.py:903(_
...
```

### Expected behavior

Since the trace of the profiler is really long I only included the first few lines. 
I am running a small llama model on some dummy data, the only difference between the two datasets is that the slow version outputs 4D attention masks, which is a feature recently added in #27539. I am running both trainers for 1 iteration.

As you can see the slow run is 340s while the fast one runs in 16s. 

The slow version of the trainer is many times slower than the fast version. The problem probably lies in the default collator `DataCollatorWithPadding` (when there is a pretrained tokenizer), which calls `tokenizer.pad` on the 4D attention masks. When you takeaway either 1) the pretrained tokenizer or 2) the 4D attention mask, trainer runs much faster. ","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Custom beam search scorer argument in generate function,"# What does this PR do?

Added an argument `beam_search_scorer_class` in `generate()` function to allow users to use beam search and grouped beam search using a custom beam search scorer.

Before that, the internal methods `_beam_search()` and `_group_beam_search()` had to be used to pass in a custom beam search scorer. However, this caused a lot of problems since the `generate()` function does a lot of preprocessing on the input (interleave the input_ids for example in the case of beam search). All that preprocessing had to be done manually Right now, this can be set directly in the `generate()` function by passing the type of the beam search scorer in the following way :

```python
tokenizer = AutoTokenizer.from_pretrained(""google-t5/t5-base"")
model = AutoModelForSeq2SeqLM.from_pretrained(""google-t5/t5-base"")

encoder_input_str = ""translate English to German: How old are you?""
encoder_input_ids = tokenizer(encoder_input_str, return_tensors=""pt"").input_ids

class CustomBeamSearchScorer(BeamSearchScorer):
    finalize_called = False
    process_called = False

    def __init__(self, test_args, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.test_args = test_args

    def process(self, *args, **kwargs):
        results = super().process(*args, **kwargs)
        # Do stuff
        return results

    def finalize(self, *args, **kwargs):
        results = super().finalize(*args, **kwargs)
        # Do stuff
        return results

# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((1, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {""encoder_outputs"": model.get_encoder()(encoder_input_ids, return_dict=True)}

outputs = model.generate(
    input_ids,
    num_beams=num_beams,
    min_length=5,
    eos_token_id=model.config.eos_token_id,
    beam_search_scorer_class=CustomBeamSearchScorer,
    beam_search_scorer_args={""test_args"": True},
    **model_kwargs,
)
``` 

# Why was this done that way
Initially, the beam_search_scorer was simply an object instead of the type. However, this could lead to inconsistencies between the parameters of the generation config and the beam search scorer. For example, the number of beams could have been set to 2 in the generate() method, but set to 4 when creating the scorer. By passing the type only, this allows the scorer to be created with the generation config inside the method (like before) and preventing any inconsistencies between the two objects.


<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@gante 
@ArthurZucker 
","[{'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",1,open
added warning to Trainer when label_names is not specified for PeftModel,"

# What does this PR do?

in Trainer, it automatically finds `label_names` with input arguments name of model.
But with model which is `PeftModel`, the input arguments are all hidden and we can't find the input argument's name automatically.

So, for the case then the user didn't specified `label_names` in `TrainerArguments`, I made warning message pop up.

p.s. it makes the change when I add call `trainer.predict`; if `label_names` it not set, there's no way I can get get `label_ids` from `trainer.predict` outcomes.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc

Documentation: @stevhliu

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],8,open
Checkpoint validation as an option,"### Feature request

Added `validate_checkpoint_key: bool` in `from_pretrained`

### Motivation

When pytorch loads state dict there is an option `strict=True` but there is no counterpart in `from_pretrained`. If people saved the pretrained model incorrectly, the incorrect keys can silently pass through.

### Your contribution

There is already an validation step in the `from_pretrained` one just need to raise the error if `validate_checkpoint_key=True`","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Using `numpy==2.0.0`,"`transformers`  now remove the pin of `numpy<2.0.0`. However, some issues (with 3rd libraries) are known, and the following gives some guide:

- In an environment with `TensorFlow` and/or `Flax` --> downgrade to `numpy<2.0.0`
- issue with soxr --> `pip install soxr==0.4.0b1`
- issue with `faiss`  --> downgrade to `numpy<2.0.0`
- If error out with a message `using np.array(obj, copy=False) replace it with `np.asarray(obj) to allow a copy`: 
   - downgrade to `numpy<2.0.0`
   - or, install `datasets` dev. version","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
low cpu usage default to true,"# What does this PR do?
Try to default to True! ",[],2,open
static cache implementation is not compatible with attn_implementation==flash_attention_2,"### System Info

- `transformers` version: 4.43.0.dev0
- Platform: Linux-4.18.0-425.3.1.el8.x86_64-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.23.5
- Safetensors version: 0.4.3
- Accelerate version: 0.33.0.dev0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: <fill in>
- Using GPU in script?: <fill in>
- GPU type: NVIDIA A100 80GB PCIe

### Who can help?

@ArthurZucker 

### Information

- [x] The official example scripts
- [ ] My own modified scripts

### Tasks

- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```bash
pytest -rA tests/test_cache_utils.py::CacheIntegrationTest -k ""test_static_cache_greedy_decoding_pad_left and flash_attention""
```
fails with 
```bash
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        if isinstance(past_key_value, StaticCache):
>           raise ValueError(
                ""`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` ""
                ""make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers""
            )
E           ValueError: `static` cache implementation is not compatible with `attn_implementation==flash_attention_2` make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers

src/transformers/models/llama/modeling_llama.py:388: ValueError
```

And the right padding test case also fails:
```bash
pytest -rA tests/test_cache_utils.py::CacheIntegrationTest -k ""test_static_cache_greedy_decoding_pad_right and flash_attention""
```



### Expected behavior

Either we don't test `flash_attention` in this case, or we should add a if check to skip setting `cache_implementation` to `static`. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",3,open
Improve support for image generation with Chameleon & Anole,"# What does this PR do?

- Adds modelling for the `VQVAE` decoder & also includes it in the conversion script.
  - I've also uploaded the converted model here: https://huggingface.co/leloy/Anole-7b-v0.1-hf
- Adds support for decoding the BPE tokens -> discrete image tokens ->  pixel values
- Moves masking of image tokens in text-only generation mode to a LogitsProcessor.
- Adds masking of non-image tokens for image-only generation mode.
- ~~Reimplements Chameleon's FSM to be more compatible with Transformers and Outlines (for structured generation)~~
  - ~~This PR _will not_ add the FSM, but instead just makes it easier for external libraries like Outlines & MMSG to integrate with Transformers to add the interleaved generation mode back in~~
  - Reimplements Chameleon's Finite-State Machine that it uses to dynamically switch between text- and image-generation modes as Logits Processors. We can now support interleaved text-image generation natively.

Required TODOs:
- [ ] Improve docs
  - [x] Write docs for image-only generation with Chameleon/Anole
    - [x] Provide minimal example on how to use
    - [x] Remove the need for passing `max_length` or `max_new_tokens` on image-only generation mode
    - [x] Force the model to generate at least one image on image-only generation mode. Chameleon doesn't officially support image-generation yet so it just immediately closes `begin-image-token`s with either an `end-image-token` or an EOS token. And finetunes like Anole haven't fully removed this issue yet so they occasionally still does that.
  - [ ] Write docs for interleaved text-image generation
    - [x] Provide minimal example on how to use
    - [ ] Show how to split token sequences by modality in the example for interleaved text-image generation
  - [ ] Improve docs (as comments) for newly-added logits processors
  - [ ] Add sample usage for newly-added logits processors 
- [ ] Add tests
  - [x] Add test for image postprocessing
  - [ ] Add tests for each generation mode
    - [x] Add tests for `text-only` generation mode
    - [ ] Add tests for `image-only` generation mode
      - Note: don't rely on hashing pytorch tensors to compare arrays
      - [ ] Add test where `max_new_tokens` is unset on `image-only` generation mode
    - [ ] Add tests for `interleaved-text-image` generation mode
    - [ ] Add tests for `unrestricted` generation mode
    - [ ] Add tests for invalid generation mode
  - [ ] Add tests for VQVAE decoder
  - [ ] Add tests for each newly-added logits processors
  - [x] Add tests for multi-GPU model sharding
- [x] Improve modelling of VQVAE encoder & decoder
- [x] VQVAE: dynamically compute `quant_state_flattened_dims` which scales with the resolution instead of hardcoding it in the configs
- [x] Improve postprocessing: only accept and return pytorch tensors
- [ ] Logits processors
  - [ ] Add new logits processors to import structure

Optional TODOs or for future PRs:
- [ ] Run a hyperparameter search for image generation
- [x] Fix bugs caused by sharding the model into multiple GPUs
- [ ] Implement features that were in the [Chameleon paper](https://arxiv.org/abs/2405.09818v1) but are not crucial here
  - [ ] Implement support for [z-loss](https://arxiv.org/abs/2309.14322)
  - [ ] Implement Classifier Free Guidance. The Lumina team claims that ""though it is not that indispensable, its impact is still significant"" https://github.com/Alpha-VLLM/Lumina-mGPT/issues/13#issuecomment-2293176135
    - [ ] Implement Classifier Free Guidance for image-only generation
    - [ ] Implement Classifier Free Guidance for interleaved text-image generation
- [ ] Refactor VQVAE (sub-)modules
  - [ ] Convert `mid`, `down`, and `up` blocks into explicit subclasses of `nn.Module()` (as suggested by @amyeroberts)
  - [ ] Make it clearer which attention types are allowed in the VQVAE (sub-)modules. Tho Chameleon currently only supports ""vanilla"" attention.
- [ ] Implement support for other finetunes of Chameleon
  - [ ] Implement support for [Lumina-mGPT](https://github.com/Alpha-VLLM/Lumina-mGPT) as suggested by @minostauros 

Links:
- [Multimodal Structured Generation](https://github.com/leloykun/mmsg): Repo that supports interleaved text-image generation and interleaved multimodal structured generation
- [Official Anole repo by GAIR-NLP team](https://github.com/GAIR-NLP/anole)
- [Official Anole model by GAIR-NLP team](https://huggingface.co/GAIR/Anole-7b-v0.1)
- [Transformers-compatible Anole with image generation support](https://huggingface.co/leloy/Anole-7b-v0.1-hf)

(partially) Implements # (issue)
- https://github.com/GAIR-NLP/anole/issues/18

@ArthurZucker @zucchini-nlp @JoyBoy-Su",[],28,open
Add support for Apple's DCLM-Baseline-7B model,"### Model description

This is a new model released by Apple using a new framework called ""openlm"" so it doesn't work with Huggingface Transformers currently.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://huggingface.co/apple/DCLM-Baseline-7B Link to the model weights.
https://github.com/mlfoundations/open_lm Link to the model framework.
https://github.com/mlfoundations/dclm
https://arxiv.org/abs/2406.11794 Link to the paper.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
RoPE: model-agnostic RoPE refactor,"# What does this PR do?

This PR:
- Refators RoPE such that it is model-agnostic. 
  - RoPE models now only need one class
  - The class is parameterized by the model config.
  - Based on the model config, the appropriate type of rope will be loaded into the class
- Adds `longrope`, as part of the model-agnostic refactor on Phi3 (closes #31992); With `longrope`, phi3's checkpoints are now loadable.

👉 Built on top of the Yarn PR (#30910)
____________________________________________
### Review

Key files to check, IN THIS SPECIFIC ORDER:
```
src/transformers/models/llama/modeling_llama.py 
src/transformers/models/llama/configuration_llama.py
src/transformers/modeling_rope_utils.py
```

👉 Other relevant files include `phi3` (`longrope`) and `recurrentgemma` (a few custom changes)

__________________________________________

### Models that require future changes for standardization

⚠️ Some models don't support `cache_positions`, and therefore they are not changed as part of this PR (the new classes is built with the new pattern in mind). A future PR is needed on these models, where both `cache_positions` and this new model-agnostic RoPE is added.

Models that were NOT changed but have RoPE:
- ESM
- Falcon
- GPTNeoX
- GPTNeoXJapanese
- Idefics
- Mixtral
- Persimmon
- Phi
- Qwen2
- Qwen2MoE
- StableLM
- Starcoder2


",[],8,open
Plans to Integrate LongRoPE into LLaMA?,"### Feature request

Microsoft has introduced their [microsoft/LongRoPE](https://github.com/microsoft/LongRoPE) implementation. Unlike plug-and-play solutions, LongRoPE requires hyperparameter tuning via a genetic algorithm. This implementation is likely the same as described in the `Su` on Phi-3. Are there any plans to incorporate LongRoPE into LLaMA?

### Motivation

In my research on long content, I have managed to integrate LongRoPE into LLaMA with some minor code adjustments. 
I am curious if Huggingface is also working on integrating this feature.

### Your contribution

If necessary.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Deepseek v2 support,"Model: https://huggingface.co/deepseek-ai/DeepSeek-V2


**Key Features**
- 400GB of weights, slow
- Yarn support
- `topk_method` is group limited greedy
- Attention: down-projection and up-projection of Q, KV

**Key Changes to Authors' code:**

General:
- [x] removed `ep_size` logic

MoE:
- [x]  removed separate inference logic in `MoEGate` --> More like Mixtral/Llama/ etc.
- [x] upcast to float32 in hidden state calculations",[],3,open
Keep Tuple of past key values as an option,"### Feature request

I see [llama](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L829-L835) will remove tuple past key values in 4.43.

### Motivation

The model outputs should always keep tuple type as an option. If we remove this, all models will be failed in jit trace.

### Your contribution

@gante Do you mind taking a look at it? Thx!

cc @amyeroberts ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",11,open
FEAT / Optim: Add QGaLore optimizer,"# What does this PR do ? 

This PR adds qgalore optimizer. Draft for now, untested ! ",[],2,open
Added optimizer adam mini,"Implements Adam Mini algorithm as introduced in [Adam-mini: Use Fewer Learning Rates To Gain More] (https://arxiv.org/abs/2406.16793).

Inspired from https://github.com/zyushun/Adam-mini
",[],14,open
A bug that may cause device inconsistency,"### System Info

In transformers/generation/util.py line:2297
The device of unfinished_sequences is same as input_ids.device()
But in line:2351, If the model is split across different GPUs, for example, input_ids is on GPU 0, and the model executes pipeline parallel on GPUs 0 and 1, the outputs will be on GPU 1, which leads to devices inconsistency in line:2404

### Who can help?

@zucchini-nlp @gan

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

The inference example of internvl2-40B
https://github.com/OpenGVLab/InternVL

### Expected behavior

No error.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",9,open
Add Microsoft CLAP model,"## What does this PR do ? 

This PR aims at adding the [Microsoft CLAP](https://github.com/microsoft/CLAP/tree/main) (MSClap)  model to Transformers. The architecture can be decomposed in two parts: 

The first part contains:
1. A GPT-2 Text Encoder, based on the Transformers GPT-2 model.
2. An Audio Encoder, based on the [HTS-AT architecture](https://arxiv.org/abs/2202.00874).

It can be used mostly for zero-shot audio classification or audio retrieval.

The second part adds:

 3.  A mapper model which maps the audio embeddings to a GPT2 input sequence.
 4.  A GPT-2 text decoder (also based on the Transformers model)
This second architecture can be used to perform audio captioning. 

For now, in this PR, we will only add the first part (text encoder + audio encoder) of the architecture. I will add the second part in a following PR. 

## What have been done for now

- [x]  Adapt the audio model using the current CLAP architecture in Transformers (laion clap).
- [x]  Integrate the Text encoder model. 
- [x] Successfully converted checkpoints and pushed them to the Hub. 
- [x] Added Config files, Feature Extractor and Preprocessor. 
- [x]  Made sure we get the same output from the FeatureExtractor / Processor and Models as we did from the original MS Clap. 
- [x] To Do: Add tests.  

","[{'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",4,open
"xpu device is not used running pipeline(device_map=""auto"")","Found on this code versions: https://github.com/huggingface/transformers/commit/52585019a17f6033df64e6ad4222d22a1f993c61,  https://github.com/huggingface/accelerate/commit/12a007d55937345aa986f5d7b1a1b6f2038465a7, https://github.com/pytorch/pytorch/commit/3477ee38e4dd1429ecfd7e6f20a30cce0f4f78e7. This is an issue with XPU support in stock pytorch (i.e. without using IPEX).

HF model pipelines with `device_map=""auto""` (or `device_map=""sequential""`) does not actually run on XPU even if they can fit the device memory. I spotted that trying to run LLAMA 3 models:
* https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
* https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct

Example script:
```
import transformers
import torch

model_id = ""meta-llama/Meta-Llama-3-8B-Instruct""
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model_id,
    model_kwargs={""torch_dtype"": torch.bfloat16},
    device_map=""auto"",
)
messages = [
    {""role"": ""system"", ""content"": ""You are a pirate chatbot who always responds in pirate speak!""},
    {""role"": ""user"", ""content"": ""Who are you?""},
]
terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids(""<|eot_id|>"")
]
outputs = pipeline(
    messages,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
print(outputs[0][""generated_text""][-1])
```

Workarounds and findings:
* If model fits device memory, then changing `device_map=""auto""` to `device_map=""xpu""` will allow model to run (that's easier to check on 8B model)
* Model starts to also work (but see a note below) if you add `max_memory` to the model kwargs:
```
model_kwargs={""torch_dtype"": torch.bfloat16, ""max_memory"": {0: 5.0e+10}}, device_map=""auto"",
```
* NOTE: adding `max_memory` will currently work only if model fits into device memory and you provided big enough max_limit. If not, then you will see the following error (filed separate https://github.com/huggingface/transformers/issues/31941 for this):
```
...
  File ""/home/gta/git/huggingface/accelerate/src/accelerate/utils/offload.py"", line 118, in __getitem__
    return self.dataset[f""{self.prefix}{key}""]
  File ""/home/gta/git/huggingface/accelerate/src/accelerate/utils/offload.py"", line 171, in __getitem__
    tensor = f.get_tensor(weight_info.get(""weight_name"", key))
  File ""/home/gta/git/pytorch/pytorch/torch/cuda/__init__.py"", line 305, in _lazy_init
    raise AssertionError(""Torch not compiled with CUDA enabled"")
AssertionError: Torch not compiled with CUDA enabled
```

CC: @gujinghui @EikanWang @fengyuan14 @guangyey @jgong5 @sywangyi  @yao-matrix
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",5,open
Uniform kwargs for processors,"### Feature request


We want to standardize the logic flow through Processor classes. Since processors can have different kwargs depending on the model and modality, we are adding a `TypedDict` for each modality to keep track of which kwargs are accepted. 

The initial design is merged and an example model is modified to follow the new uniform processor kwargs in https://github.com/huggingface/transformers/pull/31198. Also https://github.com/huggingface/transformers/pull/31197 has two more examples with standardized API.

This design has to be shipped to all the processors in Transformers, and appreciate contributions. 
Below is an incomplete list of models that need standardization, feel free to add a model if it's missing:

- [x] Align #31368 
- [x] AltClip #31368 
- [x] BLIP #31368
- [x] BLIP-2  #31368 
- [x] Bridgetower #31368 
- [x] Chameleon -> https://github.com/huggingface/transformers/pull/32181
- [x] Chinese CLIP -> #31368 
- [x] CLIP -> in progress by @davidgxue 
- [x] ClipSeg -> https://github.com/huggingface/transformers/pull/32841
- [x] Donut #31368
- [x] Flava -> https://github.com/huggingface/transformers/pull/32845
- [x] Fuyu -> https://github.com/huggingface/transformers/pull/32544
- [x] GIT https://github.com/huggingface/transformers/pull/33668
- [x] Grounding DINO #31964
- [x] Idefics -> https://github.com/huggingface/transformers/pull/32568
- [x] Idefics-2 -> https://github.com/huggingface/transformers/pull/32568
- [x] InstructBlip -> https://github.com/huggingface/transformers/pull/32544
- [x] InstructBlipVideo https://github.com/huggingface/transformers/pull/32845
- [x] Kosmos-2 -> https://github.com/huggingface/transformers/pull/32544
- [x] LayoutLM (1, 2, 3) -> https://github.com/huggingface/transformers/pull/32180
- [x] LLaVa -> https://github.com/huggingface/transformers/pull/32858
- [x] LLaVa-NeXT -> https://github.com/huggingface/transformers/pull/32544
- [x] LLaVa-NeXT-Video https://github.com/huggingface/transformers/pull/32845
- [x] MGP-STR https://github.com/huggingface/transformers/pull/32845
- [x] Nouga -> https://github.com/huggingface/transformers/pull/32841
- [x] OneFormer -> https://github.com/huggingface/transformers/pull/34547
- [x] Owlv2 https://github.com/huggingface/transformers/pull/32841
- [x] OwlVIT https://github.com/huggingface/transformers/pull/32841
- [x] Paligemma -> https://github.com/huggingface/transformers/pull/33571
- [x] Pix2Struct -> https://github.com/huggingface/transformers/pull/32544
- [x] Pixtral -> https://github.com/huggingface/transformers/pull/33521
- [x] SAM -> https://github.com/huggingface/transformers/pull/34578
- [x] SigLip -> https://github.com/huggingface/transformers/pull/32845
- [ ] TrOCR 
- [x] TVP -> https://github.com/huggingface/transformers/pull/32845
- [x] Udop -> https://github.com/huggingface/transformers/pull/33628
- [x] VideoLLaVa -> https://github.com/huggingface/transformers/pull/32845
- [x] VILT -> https://github.com/huggingface/transformers/pull/32845
- [x] VisionTextDualEncoder -> https://github.com/huggingface/transformers/pull/34563
- [x] X-CLIP -> https://github.com/huggingface/transformers/pull/32845


Note: For now we'll start with image or image+text, https://github.com/huggingface/transformers/pull/31368 is an ongoing PR that has also audio processor standardization

### Motivation

.

### Your contribution

.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 6126880899, 'node_id': 'LA_kwDOCUB6oc8AAAABbTDIgw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/contributions-welcome', 'name': 'contributions-welcome', 'color': 'F99E09', 'default': False, 'description': ''}]",44,open
Added french version of preprocessing data and corrected English version,"### What does this PR do?
This pull request makes the following changes:
- Adds a French translation of `preparation des donnees.md` from English to French.
- Clarifies the English version of `preparation des donnees.md` for improved readability.

### Changes made
- Translated `preparation des donnees.md` from English to French.
- Revised the English version of `preparation des donnees.md` to improve clarity.

### Why is this change important?
Providing documentation in multiple languages helps make the information accessible to a broader audience and supports users who are more comfortable reading in French. 🇫🇷

### Review
Tagging:
@ArthurZucker (for the review 🇫🇷)",[],2,open
"Set position_embeddings in BertEmbeddings for absolute position type only, to avoid unused parameters","The position_embeddings are only used for the ""absolute"" position type. For other position types, they are unused, causing DDP training to fail due to unused parameters. Instead, we should only add this module if it will be used in the forward pass.

Tagging @ArthurZucker, because this is a text model, but feel free to tag others instead.

This is my first contribution, so please let me know if I've missed a step (or worse)!",[],1,open
Initializes generators in trainer.py with the device specified in sel…,"…f.args.device

Initializes generators in Sampler and DataLoader classes with the device specified in TrainingArguments. Currently, the generators are initialized on the CPU by default, leading to errors when running Trainer on a non-CPU device.

Fixes #31897

@muellerzr @SunMarc @amyeroberts ",[],4,open
Add support for XTR,"### Model description

XTR (ConteXtualized Token Retriever) is a multi-vector retrieval model that improves efficiency by focusing on retrieving and ranking the most important document tokens. Details are described at https://arxiv.org/abs/2304.01982.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Implementation for PyTorch: https://github.com/mjeensung/xtr-pytorch
Weight: https://huggingface.co/google/xtr-base-en
Author: @jhyuklee","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Add cosine_with_min_lr_schedule_with_warmup_lr_rate scheduler in Trainer,"# What does this PR do?

Add cosine_with_min_lr_schedule_with_warmup_lr_rate scheduler in Trainer, which is based on https://github.com/huggingface/transformers/pull/29341.

As mentioned in the previous PR, it implemented ""a warmup period during which it increases linearly between 0 and the initial learning rate set in the optimizer."" However, I recently investigated the DeepSpeed framework and noticed that their function implements additional features, namely ""warmup_min_ratio"" in https://github.com/richardodliu/DeepSpeed/blob/master/deepspeed/runtime/lr_schedules.py#L774, which supports a warmup start learning rate ratio differs from 0. Considering that DeepSpeed is a crucial component framework in the implementation of Transformers, I aim to ensure consistency between the two in the implementation of learning rate schedulers through this PR. This is to prevent potential confusion for users who employ both frameworks simultaneously.

Our implementation is based on improvements from previous PR, providing significant benefits. Specifically, we allow the reuse of this method without modifying any input parameters(Don't worry; if you prefer to specify the parameter, you are certainly allowed to do so by simply passing it as an argument ""warmup_lr_rate"", which is used to specify the ratio between the start learning rate and the initial learning rate). In such cases, the implementation is equivalent to setting ""warmup_lr_rate"" to ""1/warmup_steps"". Regarding enhancements to the training process, since it is recommended to perform ""optimizer.step()"" before ""lr_scheduler.step()"", the learning rate for the batch corresponding to the first step would be zero under the previous implementation. This means that the gradients would not be updated for that batch, effectively wasting it. Our updated method addresses this issue. While this phenomenon may not be noticeable with larger dataset sizes, it becomes significant with smaller datasets where the total number of steps is limited. Additionally, our implementation ensures that the final small learning rate is reachable, rather than being updated only after the training is completed. Overall, our approach is better suited as an improvement to the existing method rather than a complete overhaul. Therefore, we implemented our idea by creating a new function, allowing users the flexibility to choose which implementation method they prefer.


Fixes: WarmupCosineLR missed in WarmupCosineLR


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@muellerzr and @SunMarc
",[],6,open
Generate: XGLM can generate with inputs_embeds,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc 

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
NotImplementedError: ggml_type 3 not implemented,"### Description 
When trying to use the model `Qwen/Qwen2-7B-Instruct-GGUF` with the gguf file `qwen2-7b-instruct-q4_0.gguf`, I encountered a NotImplementedError about ggml_type 3 not implemented. 

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Install the required libraries:
`pip install transformers==4.42.3 gguf==0.6.0`

Load the model and tokenizer:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = ""Qwen/Qwen2-7B-Instruct-GGUF""
file_name = ""qwen2-7b-instruct-q4_0.gguf""

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file = file_name)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file = file_name)
```

### Expected behavior

The code should process ggml_type 3 without raising an error, or provide an alternative implementation.

### System Info

- transformers version: 4.42.3 
- gguf version: 0.6.0
- Platform: Ubuntu 20.04
- Python version: 3.9.16 
- PyTorch version (GPU?): 2.2.2(True)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6973402771, 'node_id': 'LA_kwDOCUB6oc8AAAABn6Wukw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/GGUF', 'name': 'GGUF', 'color': 'D41E57', 'default': False, 'description': ''}]",5,open
Fix some FastSpeechConformer2 failing tests,"# What does this PR do?

- Fixes #31270 that happened because of the wrong text-to-waveform mapping
- Fixes mono-gpu failing tests (cf [here](https://github.com/huggingface/transformers/actions/runs/9823944203/job/27122464080))

FastSpeechConformer2 also suffers from faililng `test_multi_gpu_data_parallel_forward`, I'll address this later, don't hesitate to share if you think of a reason for this bug!

cc @amyeroberts and @ydshieh  ",[],5,open
Add support for MiniCPM-V-2 and MiniCPM-Llama3-V-2_5,"### Model description

MiniCPM-V is a series of Openbmb's vision language models.
We want to add support for MiniCPM-V-2 and later models

### Open source status

- [x] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://huggingface.co/openbmb/MiniCPM-V-2
https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
Add NSP Labels Handling to DataCollatorForWholeWordMask for Simultaneous WWM and NSP Pre-training,"# What does this PR do?

Integrating NSP with WWM allows for a more comprehensive pre-training of Encoder-only models, leveraging the benefits of both tasks to improve model performance and robustness.

- Modified `torch_call`, `tf_call`, and `numpy_call` methods in `DataCollatorForWholeWordMask` to process NSP labels.
- Updated input handling to include `next_sentence_label` and `token_type_ids` if present in the input examples.
<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc 

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@ArthurZucker",[],0,open
Add MultiStepLR with Warmup Scheduler,"### Feature request

I would like to propose the addition of a new learning rate scheduler that combines MultiStepLR with a warmup phase. Currently, the Transformers library does not include a scheduler that uses both MultiStepLR and warmup. This feature can be beneficial for training models where the learning rate needs to be adjusted at specific epochs with an initial warmup phase to stabilise training.

### Motivation

In many training scenarios, it is beneficial to start with a warmup phase where the learning rate gradually increases, followed by a phase where the learning rate decreases at specific milestones (steps).

### Contribution

I propose adding a new scheduler, `get_multistep_schedule_with_warmup`, which combines the functionality of MultiStepLR and Warmup. This scheduler will increase the learning rate linearly during the warmup phase and then follow the MultiStepLR schedule. I am more than happy to create a pull request (PR) implementing this feature. Please let me know if this sounds like a valuable addition, and I will proceed with the implementation.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
[GroundingDino] Fix grounding dino loss 🚨,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes https://github.com/huggingface/transformers/issues/31434

As the [original repo](https://github.com/IDEA-Research/GroundingDINO) doesn't provide the loss implementation I'm using the one implemented [here](https://github.com/longzw1997/Open-GroundingDino) as a baseline since it was mentioned by the original repo, on this issue https://github.com/IDEA-Research/GroundingDINO/issues/241, as a reliable source if one wants to train a `GroundingDino` model 

TODO: 
- [x] Test `GroundingDinoMatcher` and `GroundingDinoLoss` are working properly 

## Explanation of the Issue and Solution

So the issue was that `GroundingDinoLoss` and `GroundingDinoHungarianMatcher` were just a copy from `DeformableDetr` which is used for closed-set object detection (i.e. a fixed set of categories). Whereas in `GroundingDino` there's no limited amount of categories and the output logits are `d_model` dimensional where the first `seq_len` elements have a specified value and the subsequent are `nan`. The main differences are: 

1. `class_labels` are associated with the text prompt used 
2. The logits are asscoaited with the tokens of the text so it's not necessarily 1-to-1

For instance if an image with bounding boxes with fishes and jellyfishes using a prompt `""fish. jellyfish.""` fish should have `class_label` 0 assigned to it and jellyfish should have 1 assigned. If the position of jellyfish and fish in the prompt swapped then the `class_label`s would swap as well. Moreover, jellyfish is represented by two tokens (`[20919, 7529]`) and fish by one token (`[3869]`) therefore we need to select the appropriate logits for each class.


As the [original implementation](https://github.com/IDEA-Research/GroundingDINO) doesn't provide the training loop or the loss implementation, but does recommend other implementations for training `GroundingDino` on this issue https://github.com/IDEA-Research/GroundingDINO/issues/241, I took as baseline the implementation from [Open-GroundingDino](https://github.com/longzw1997/Open-GroundingDino/tree/main) as it supports both visual grounding and object detection and they've trained their own `GroundingDino` using their code base achieving good performance.

Things added in this PR are:
- `build_label_maps` which generates a list of `torch.Tensor` with lenght `batch_size` mapping each category to its corresponding tokens based on the `input_ids`
- `build_text_mask` just expand the `attention_mask` to select the appropriate tokens when computing `GroundingDino.loss_labels`
- Added `enc_topk_proposals`, `encoder_logits` and `encoder_pred_boxes` to `GroundingDinoModelOutput` and `GroundingDinoObjectDetectionOutput` to compute first stage loss
- Added `class_loss_coefficient` (with correct default value) and `class_loss_reduction` to `GroundingDinoConfig`. `class_loss_reduction` was added because in `sigmoid_focal_loss` from the baseline implementation they reduced `loss_ce` with a simple sum, but that makes the losses imbalanced most of the time and in the original implementation they do have a `sigmoid_focal_loss` implemented, but using `mean` reduction, therefore I made I decided to make it configurable and use the `sum` one for testing reasons
- Modifications to `GroundingDinoLoss` and `GroundingDinoHungarianMatcher`


Also added a new integration test called `test_grounding_dino_loss` where I compare the loss obtained from 2 sample images with the baseline implementation from `Open-GroundingDino`.

c.c. @amyeroberts ","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",48,open
DirectML storage compatibility issues,"### Feature request

I'm trying to run transformers models using DirectML, and it works fine in most cases. However, since Microsoft's DirectML does not support storage, this causes an error whenever an untyped_storage method is involved.


Package versions installed:

torch~=2.3.1
torch_directml=0.2.2.dev240614
transformers=4.42.3


### Motivation

compatibility issues

### Your contribution

I hope there is a way to provide a custom device context method without modifying the transformers source code. In my current use case, I can write compatible code for storage and skip this part.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Why there is no LlavaForSequenceClassification ?,"### Feature request

Need of LlavaForSequenceClassification class in '`src/transformers/models/llava/modeling_llava.py`'..

### Motivation

If there is a LlavaForSequenceClassification class we can use decoder only models for classification tasks. 

### Your contribution

Allow me to contribute to this as I am already working on this model for mental health meme classification for the upcoming AAAI conference.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Fix pipeline tests - don't set torch_dtype on non-torch pipelines,"# What does this PR do?

Fixes errors introduced by the merge of #31342 

Example failing run: https://app.circleci.com/pipelines/github/huggingface/transformers/97170/workflows/fd23720b-5884-42d5-8877-e69afcdeda34/jobs/1285703",[],3,open
TinyModel addition,"### Model description

https://github.com/noanabeshima/tiny_model

It's a small language model trained on TinyStories for interpretability with sparse autoencoders and transcoders added. It has no layernorms (this helps with interpretability) which makes it not fit with any existing model architecture in the transformers library. Its architecture is essentially GPT-2's except that it doesn't have layernorms and it has untied embed/deembed.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The implementation is here:
https://github.com/noanabeshima/tiny_model/blob/main/tiny_model/lm.py

The weights are here:
https://huggingface.co/noanabeshima/tiny_model/blob/main/tiny_model.pt

The default config corresponding to the weights is:

        d_model=768,
        n_layers=4,
        n_heads=16,
        max_seq_len=256,
        vocab_size=10_000

I am the author.
  ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",6,open
Loading HuBERT models with DeepSpeed ZeRO-3 causes program to hang,"### System Info

- `transformers` version: 4.42.3
- Platform: Linux-5.14.0-362.24.1.el9_3.x86_64-x86_64-with-glibc2.34
- Python version: 3.10.14
- Huggingface_hub version: 0.23.4
- Safetensors version: 0.4.3
- Accelerate version: 0.31.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using distributed or parallel set-up in script?: yes
- Using GPU in script?: no
- GPU type: NVIDIA A100-SXM4-40GB

### Who can help?

@sanchit-gandhi @muellerzr

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

`hubert_mre.py`:
```python
from transformers import AutoConfig, HubertModel, TrainingArguments, HfArgumentParser

def main():
    parser = HfArgumentParser(TrainingArguments)
    training_args = parser.parse_args_into_dataclasses()[0]

    config = AutoConfig.from_pretrained(""facebook/hubert-large-ls960-ft"")

    model = HubertModel.from_pretrained(
        ""facebook/hubert-large-ls960-ft"", config=config
    )

if __name__ == ""__main__"":
    main()
```

`hubert_mre.sh`:
```sh
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL
export CUDA_LAUNCH_BLOCKING=1

OUTPUT_DIR=$HOME/hubert_mre

deepspeed \
    --num_gpus 2 \
    --master_port 60000 \
    ./hubert_mre.py \
    --output_dir $OUTPUT_DIR \
    --deepspeed zero3.json
```

`zero3.json`:
```json
{
    ""fp16"": {
        ""enabled"": ""auto"",
        ""loss_scale"": 0,
        ""loss_scale_window"": 1000,
        ""initial_scale_power"": 16,
        ""hysteresis"": 2,
        ""min_loss_scale"": 1
    },
    ""bf16"": {
        ""enabled"": ""auto""
    },
    ""train_micro_batch_size_per_gpu"": ""auto"",
    ""train_batch_size"": ""auto"",
    ""gradient_accumulation_steps"": ""auto"",
    ""zero_optimization"": {
        ""stage"": 3,
        ""overlap_comm"": true,
        ""contiguous_gradients"": true,
        ""sub_group_size"": 1e9,
        ""reduce_bucket_size"": ""auto"",
        ""stage3_prefetch_bucket_size"": ""auto"",
        ""stage3_param_persistence_threshold"": ""auto"",
        ""stage3_max_live_parameters"": 1e9,
        ""stage3_max_reuse_distance"": 1e9,
        ""stage3_gather_16bit_weights_on_model_save"": true
    }
}
```

Run `hubert_mre.sh` and watch the script hang indefinitely.

The curious thing is that this seems to happen only with HuBERT models. If, for example, you replace `HubertModel.from_pretrained(""facebook/hubert-large-ls960-ft"")` with `Wav2Vec2BertModel.from_pretrained(""facebook/w2v-bert-2.0"")`, the script runs just fine.

Also, this works fine if you pass `--num_gpus 1`.

### Expected behavior

The script runs to completion without hanging indefinitely.","[{'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",9,open
Confusing documentation of input_ids and past_key_values in model.forward,"### System Info

Current documentation

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

The docs (e.g. for mistral forward method) state that :

> If past_key_values is used, optionally only the last decoder_input_ids have to be input (see past_key_values).

> If past_key_values are used, the user can optionally input only the last input_ids (those that don’t have their past key value states given to this model) of shape (batch_size, 1) instead of all input_ids of shape (batch_size, sequence_length).

https://huggingface.co/docs/transformers/main/model_doc/mistral#transformers.MistralModel.forward

### Expected behavior

It is my understanding that it is in fact not **optional** but **obligatory** to pass only the last input ids (those that don’t have their past key value states given to this model), as there is no handling of the case where full input ids are passed. C.f. https://discuss.huggingface.co/t/correct-input-ids-when-passing-past-key-values/92044","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Adding mplugdocowl,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc 

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],11,open
Whisper fix audio out of range,"# What does this PR do?


Fixes #31683


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
      https://github.com/huggingface/transformers/issues/31683
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc 

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}, {'id': 7377881103, 'node_id': 'LA_kwDOCUB6oc8AAAABt8GIDw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Whisper', 'name': 'Whisper', 'color': '83303E', 'default': False, 'description': ''}]",8,open
Train on logits instead of one hot vectors,"### Feature request

Add the ability to train on Bigger LLM logits instead of one hot vectors created from tokenized input example

### Motivation

As stated on [gemma2 paper](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf) train smaller models on logits of bigger models (distilation) produces better result

### Your contribution

Details from gemma2 [here](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
[docs] Redesign,"The main goal of this PR is to redesign the Transformers docs to:

1. Be more developer-friendly.
2. Improve navigation by replacing the existing structure with a more organic one that scales naturally instead of forcing content into the 4 current predefined sections.
3. Create a more unified docs experience by integrating content rather than adding it on.

This PR proposes a potential structure for achieving 2 and 3. Once the structure is in place, each doc will be rewritten to achieve 1.

If you're interested in more details about the redesign's motivation, please read this [blog post](https://www.stevhliu.com/2024/making-sense-of-this-mess). If you want more details about 1, 2, and 3, please read this [post](https://www.stevhliu.com/2024/unraveling-the-mess) and this [one](https://www.stevhliu.com/2024/diagrams) too.

All feedback, alternative structures, and comments are welcomed! Thanks 🙂 ",[],6,open
Suport sdpa for RoBERTa and XLM-RoBERTa models,"### Feature request

Enable sdpa for RoBERTa and XLM-RoBERTa models

### Motivation

While BERT, which is similar to RoBERTa and XLM-RoBERTa, support sdpa, RoBERTa and XLM-RoBERTa do not support sdpa yet. This enablement is straight-forward.

Our applications need latency reduction by sdpa. The performance advantage in BERT is already shown at #28802.

### Your contribution

I will submit a PR.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Fix 'Can't infer missing attention mask on `mps` device',"### Feature request

I was trying out [local-gemma](https://github.com/huggingface/local-gemma) just now on my M1 macbook and ran into this error:
```
ValueError: Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.
```
The proposed temporary solution of using `PYTORCH_ENABLE_MPS_FALLBACK=1` was not working. The cause of the error is this bit of code in `generation/utils.py`:
```py
def _prepare_attention_mask_for_generation(
        self,
        inputs: torch.Tensor,
        pad_token_id: Optional[torch.Tensor],
        eos_token_id: Optional[torch.Tensor],
    ) -> torch.LongTensor:
        # ...

        if inputs.device.type == ""mps"":
            # mps does not support torch.isin (https://github.com/pytorch/pytorch/issues/77764)
            raise ValueError(
                ""Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.""
            )

        is_pad_token_in_inputs = (pad_token_id is not None) and (
            torch.isin(elements=inputs, test_elements=pad_token_id).any()
        )
        is_pad_token_not_equal_to_eos_token_id = (eos_token_id is None) or ~(
            torch.isin(elements=eos_token_id, test_elements=pad_token_id).any()
        )
```
Mainly, `torch.isin` is not supported. I don't know all the places where `_prepare_attention_mask_for_generation` is used or how it's used. But just a cursory glance at the code made me wonder if this is really necessary:
```py
torch.isin(elements=inputs, test_elements=pad_token_id).any()
torch.isin(elements=eos_token_id, test_elements=pad_token_id).any()
```

I fixed my issue by changing those to:
```py
(inputs == pad_token_id).any()  #  was: torch.isin(elements=inputs, test_elements=pad_token_id).any()
# and
eos_token_id == pad_token_id  # was: torch.isin(elements=eos_token_id, test_elements=pad_token_id).any()
```
and removing the mps conditional.

Again, I'm not sure if this will actually cover all use cases of that helper function but if it does, could we please go with the simple solution until `isin` is supported by MPS?

### Motivation

This should _just work_ on a macbook with mps without hacks like setting `PYTORCH_ENABLE_MPS_FALLBACK=1` (which didn't work anyway).

### Your contribution

If the proposed solution looks good then I can submit a PR (or someone else can, either way I just want my code to work).","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Pass `HFQuantizer` to `from_pretrained` kwargs,"### Feature request

Currently, when loading a model in quantized form, the `HFQuantizer` is created based on other kwargs passed into the `from_pretrained` function. See current implementation below:
```python
# modeling_utils::from_pretrained()
    if pre_quantized or quantization_config is not None:
        if pre_quantized:
            config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                config.quantization_config, quantization_config
            )
        else:
            config.quantization_config = quantization_config
        hf_quantizer = AutoHfQuantizer.from_config(config.quantization_config, pre_quantized=pre_quantized)
    else:
        hf_quantizer = None
```
This should be a straightforward addition, by adding the following lines:
```python
# modeling_utils::from_pretrained()
    hf_quantizer = kwargs.pop(""hf_quantizer"", None)
    if hf_quantizer is not None:
        pass
    elif pre_quantized or quantization_config is not None:
        ...
```

### Motivation

This would give users more flexibility, and allow one to easily create and integrate custom implementations of the `HFQuantizer` class. I am personally working on a project where this change is necessary to work with quantization methods that have not yet been added to the library

### Your contribution

I can make a PR and contribution","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}]",2,open
Add LightGlue model,"# What does this PR do?

This PR implements [LightGlue](https://arxiv.org/pdf/2306.13643.pdf), a keypoint matching model that builds on top of [SuperPoint](https://huggingface.co/magic-leap-community/superpoint) and optimizes [SuperGlue](https://arxiv.org/pdf/1911.11763.pdf) inference time while setting SotA results in image matching tasks.
SuperGlue being currently implemented [through this PR](https://github.com/huggingface/transformers/pull/29886). This should be added after.
This branch is based on SuperGlue's branch and the PR is created before SuperGlue's is done to get CI tests results. Once SuperGlue's PR merged, this PR will be rebased onto main.

For now LightGlue is implemented like SuperGlue as a standalone model using `AutoModel` but this PR will also be the place where `AutoModelForKeypointMatching` will be created to group SuperGlue and LightGlue together.

This model supports FlashAttention2

Inference colab notebook: https://colab.research.google.com/drive/1c-6Hhe05vovKLctSFO7ZipDsHCzHSNuE?usp=sharing

## Who can review?

@amyeroberts  (cc @NielsRogge)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",11,open
Support Kosmos-2.5,"# What does this PR do?
#30877 Implementation of Kosmos-2.5 in transformers.
https://huggingface.co/kirp/kosmos2_5/blob/main/README.md
# Usage
```python
from PIL import Image
import requests
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq, AutoConfig
import re

repo = ""kirp/kosmos2_5""
device = ""cuda:0""
config = AutoConfig.from_pretrained(repo)

NAME = {
    ""f"" : ""flash_attention_2"",
    ""s"" : ""sdpa"",
    ""e"" : ""eager"",
}

# all sdpa fp16
dtype = torch.float16
config._attn_implementation = NAME[""s""]
config.vision_config._attn_implementation = NAME[""s""]
config.text_config._attn_implementation = NAME[""s""]

# # all sdpa fp16
# dtype = torch.float16
# config._attn_implementation = NAME[""s""]
# config.text_config._attn_implementation = NAME[""s""]
# config.vision_config._attn_implementation = NAME[""s""]

# # all eager bf16
# dtype = torch.bfloat16
# config._attn_implementation = NAME[""e""]
# config.text_config._attn_implementation = NAME[""e""]
# config.vision_config._attn_implementation = NAME[""e""]


model = AutoModelForVision2Seq.from_pretrained(repo, device_map = device, torch_dtype=dtype, config=config)
processor = AutoProcessor.from_pretrained(repo)

url = ""https://huggingface.co/kirp/kosmos2_5/resolve/main/receipt_00008.png""
image = Image.open(requests.get(url, stream=True).raw)
prompt = ""<ocr>"" # <md>

inputs = processor(text=prompt, images=image, return_tensors=""pt"")
height, width = inputs.pop(""height""), inputs.pop(""width"")
raw_width, raw_height = image.size
scale_height = raw_height / height
scale_width = raw_width / width

inputs = {k: v.to(device) if v is not None else None for k, v in inputs.items()}
inputs[""flattened_patches""] = inputs[""flattened_patches""].to(dtype)

generated_ids = model.generate(
    **inputs,
    max_new_tokens=1024,
)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)

def postprocess(y, scale_height, scale_width):
    y = y.replace(prompt, """")
    if ""<md>"" in prompt:
        return y
    pattern = r""<bbox><x_\d+><y_\d+><x_\d+><y_\d+></bbox>""
    bboxs_raw = re.findall(pattern, y)
    lines = re.split(pattern, y)[1:]
    bboxs = [re.findall(r""\d+"", i) for i in bboxs_raw]
    bboxs = [[int(j) for j in i] for i in bboxs]
    info = """"
    for i in range(len(lines)):
        box = bboxs[i]
        x0, y0, x1, y1 = box
        if not (x0 >= x1 or y0 >= y1):
            x0 = int(x0 * scale_width)
            y0 = int(y0 * scale_height)
            x1 = int(x1 * scale_width)
            y1 = int(y1 * scale_height)
            info += f""{x0},{y0},{x1},{y0},{x1},{y1},{x0},{y1},{lines[i]}""
    return info

output_text = postprocess(generated_text[0], scale_height, scale_width)
print(output_text)
```","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",17,open
Add `bot_token` attribute to `PreTrainedTokenizer` and `PreTrainedTokenizerFast`,"### Feature request

I'm requesting for the attribute `bot_token` (beginning-of-tools token) to be added to the `PreTrainedTokenizer` classes, similar to `eos_token`. This token would be associated with `self.bot_token` and `self.bot_token_id` and would expose the token to downstream consumers like vLLM.

### Motivation

This request builds off [this PR comment](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/discussions/35) as well as the ongoing work to support function calling [in transformers](https://github.com/huggingface/transformers/pull/30621). 

A number of downstream consumers depend on what's available in the `PreTrainedTokenizer` classes, like vLLM's `Sequence` class and `LLMEngine` class [example](https://github.com/vllm-project/vllm/blob/main/vllm/engine/llm_engine.py#L462). For example, the current problem I'm facing is that vLLM doesn't correctly label the finish reason for ""tool call"" outputs, as, well, tool calls, since the `CompletionOutput.finish_reason` ultimately relies on the attributes available in `PreTrainedTokenizer`. 

As open-source tool calling proliferates, having these attributes exposed would greatly enhance the utility of the library. This token can be set to `None` by default and should be backwards compatible with the right implementation. 

### Your contribution

I can help contribute to the PR and write code. Might need help navigating the library + writing good test cases. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Any config for DeBERTa series as decoders for TSDAE? ,"### Feature request

Seems that there is no config for DeBERTa v1-2-3 as decoder (while there are configs for BERT/RoBERTa et similia models)... This is needed in order to perform TSDAE unsupervised fine tuning.

*(TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning)*

### Motivation

Here a reference for the related sentence transformer issue: https://github.com/UKPLab/sentence-transformers/issues/2771

TSDAE demonstrated to be a powerful unsupervised approach, and DeBERTa is proven to be a really strong base model for further fine tuning (also, v2 has a an xxlarge 1.5B version and v3 demonstrated strong performance and efficiency with its ELECTRA-style pretraining)

For context, here the TSDAE paper: https://arxiv.org/abs/2104.06979

### Your contribution

I'm not sure if I can contribute to the repository... 

Anyway, i can certainly open source multiple domain adapted models, including models in a size range (1.5B) where there is not much choices while working with encoder-only models

Edited","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Do we need a config to change `padding_side='left` before the evaluation?,"### Feature request

I am trying to train a Llama model (a decoder-only model). I want to evaluate my model with not only the loss but also some generation-based metric. For example, my eval dataset could be a str as `1+2=`, and I use the Seq2seqTrainer which provides the modified prediction step so I can get the prediction of the model in the `EvalPrediction`. Then I write my eval code in the function `compute_metrics` and provide it for the Seq2seqTrainer.

The problem is about the padding_side of the tokenizer. Because I need to train the model, the tokenizer should be right padding in training dataset. (Because it is the default setting of Llama.) However, when I try to evaluate the model, the tokenizer should be changed into left padding because I need my model to generate. I do not find a easy way to do this, unless I change the source code of the trainer (for example, the `get_eval_dataloader` method of the Trainer).

My questions are:
1. Is it correct way to evaluate a decoder-only model in a generation-based way? Should I use the Seq2seqTrainer or is there some other methods I have not found? (Is there an example doc?)
2. Can I just train a model with right padding but evaluate it with left padding? If not, how should I evaluate models like Llama?
3. If my evaluate process is correct, how can I change the padding_side as right at the begining of the evaluation and change it back to left after the evaluation? (I think if we have the seperated training_data_coallotor and test_data_coallotor, the problem could be solved. Is it possbile for the current transformers Trainer? Or any other way to implement it?)

### Motivation

Motivation: generation-based evaluation when we train a decoder-only autoregressive model like llama.

### Your contribution

I do not know what I can help.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Data Map Trainer Callback,"### Feature request

It would be nice to have a callback for the trainer class which could create Data Maps. See the paper for more details https://arxiv.org/pdf/2009.10795.  A Data Map measures how a model's prediction of specific training data change over the course of model training.

The Callback should support:

- Executing at each step or epoch
-  Should integrate directly with the Trainer class.
- Should save the prediction of each training example  as a matrix of the form [n_examples, n_labels] so that it can easily be stacked into [n_epochs, n_examples, n_labels].  Right now I'm saving things as a List[List[float]] but this might be sub optimal. It needs some way of getting the logged information later.

Running [this colab notebook I made](https://colab.research.google.com/drive/1OyaQkFULDcCUVGEneyqFYK1_M1nunbUw?usp=sharing) will generate data map outputs for classification tasks using the Trainer in line with what I was thinking.  Here is what I have so far that works will for multilabel and multiclass classification using transformers.

```python
class DataMapCallback(TrainerCallback):
    """"""Trainer Callback to save DataMap data.

    Original Paper: https://arxiv.org/pdf/2009.10795.pdf.

    This callback saves the predictions of the model on each training example
    at the end of every epoch to callback_dir/{epoch}.json.
    """"""

    def __init__(
        self,
        log_on: str = ""epoch"",
        callback_dir: str = ""."",
        n_log_steps: Optional[int] = None,
        prediction_fn: Optional[Callable[[PreTrainedModel, DataLoader, TrainingArguments], List[List[float]]]] = None,
    ):
        self.callback_dir = callback_dir
        self.log_on = log_on
        self.log_count = 0
        self.n_log_steps = n_log_steps
        self.prediction_fn = self._predict if prediction_fn is None else prediction_fn

        # Handle discrepencies in how we initialize the logging mode.
        if n_log_steps is not None and self.log_on != ""step"":
            raise ValueError(
                ""n_log_steps is only valid when on='step'.  If you want to to run datamaps based on steps please specify on='step'.""
            )
        elif n_log_steps is None and self.log_on == ""step"":
            warnings.warn(
                ""You have not specified n_log_steps.  This will result in a large number of datamaps being saved setting step size to 1.""
            )
            self.n_log_steps = 1

        # Create the directory if it doesn't exist.
        if not os.path.exists(self.callback_dir):
            os.makedirs(self.callback_dir, exist_ok=True)

    def _predict(self, model, train_data_loader, args):
        if train_data_loader.batch_size is None:
          batch_size = args.per_device_train_batch_size
        else:
          batch_size = train_data_loader.batch_size
        batches = BatchSampler(
            SequentialSampler(train_data_loader.dataset),
            batch_size,
            False,
        )

        with torch.no_grad():
            predictions = []
            for batch in batches:
                # Adjust the indices to include the last element because python is [)
                start_idx, end_idx = batch[0], batch[-1] + 1

                # Extract the sample from the training dataset
                sample = train_data_loader.dataset[start_idx:end_idx]

                # Make sure to apply any data collators
                sample = train_data_loader.collate_fn(sample)

                # Move all samples to the appropriate device. We only do this
                # For args that are part of the model and the dataset.
                args = set(inspect.getfullargspec(model.forward).args).intersection(
                    set(sample.keys())
                )
                sample = {k: torch.tensor(sample[k]).to(model.device) for k in args}

                # Perform inference using the model
                current_preds = model(**sample)

                # Convert the predictions to a list and append them to the result
                predictions += current_preds.logits.tolist()

        return predictions

    def _save_predictions(self, model, train_data_loader, args):
        predictions = self.prediction_fn(model, train_data_loader, args)

        # Save Predictions.
        with open(os.path.join(self.callback_dir, f""{self.log_count}.json""), ""w"") as f:
            json.dump(predictions, f)
        self.log_count += 1

    def on_epoch_end(self, args, state, control, logs=None, **kwargs):
        """"""Predict on all training examples at the end of an epoch.""""""

        if self.log_on == ""epoch"":
            self._save_predictions(kwargs[""model""], kwargs[""train_dataloader""], args)

    def on_save(self, args, state, control, logs=None, **kwargs):
        """"""Predict on all training examples when a checkpoint is saved""""""
        if self.log_on == ""save"":
            self._save_predictions(kwargs[""model""], kwargs[""train_dataloader""], args)

    def on_evaluate(self, args, state, control, logs=None, **kwargs):
        """"""Predict on all training examples when we run an evaluation loop.""""""
        if self.log_on == ""evaluate"":
            self._save_predictions(kwargs[""model""], kwargs[""train_dataloader""], args)

    def on_step_end(self, args, state, control, logs=None, **kwargs):
        """"""Predict at the end of a step.""""""
        if self.log_on == ""step"":
            self._save_predictions(kwargs[""model""], kwargs[""train_dataloader""], args)
```



### Motivation

Optimizing data is as important as correctly configuring your model.  Data Maps are an incredibly powerful tool which help us understand the data we are using to train specific tasks.  Using Tensorboard to monitor the loss during training can identify many bugs. This is a technique which can be equally valuable. In my day to day this technique has seriously increased the performance of production models I've trained at multiple different companies. I think it's really useful for gaining insights about your data and also pushing the limits of your performance. I want to see everyone get the same benefits I've seen.   I wrote a [blog on doing this with sklearn](https://www.nbertagnolli.com/jekyll/update/2024/04/14/Data-Maps.html) if you want to see a simple example.


### Your contribution

I'd love to contribute this. I have already created a working prototype with [this colab notebook](https://colab.research.google.com/drive/1OyaQkFULDcCUVGEneyqFYK1_M1nunbUw?usp=sharing). It will generate data map outputs for classification tasks using the Trainer. I'm working on an example for non classification tasks as well.   If you'd be willing to guide me on this addition, and you think it's valuable, I'd do as much of this as possible : ).","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Skipping cudagraphs for unknown reason,"### System Info

Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.
                                      
- `transformers` version: 4.41.2
- Platform: Linux-5.15.0-112-generic-x86_64-with-glibc2.35
- Python version: 3.10.13                                                    
- Huggingface_hub version: 0.23.4                                            
- Safetensors version: 0.4.2                                                 
- Accelerate version: 0.28.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.1.2 (True) 
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I read [issue 30055](https://github.com/huggingface/transformers/issues/30055) and [issue 30351](https://github.com/huggingface/transformers/issues/30351), and Llama works well with  `cache_implementation=""static""`. However, I am trying to use `torch.compile` for other models such as `pythia` and `phi-2` where the  `cache_implementation=""static""` is not appliable, and it will produce errors like:
```
[2024-06-26 18:18:57,065] [0/0] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat_3                                        
[2024-06-26 18:18:57,065] [0/0] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat_2                                        
[2024-06-26 18:18:57,065] [0/0] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat_1
[2024-06-26 18:18:57,065] [0/0] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat
skipping cudagraphs for unknown reason
...
...
  File ""/tmp/torchinductor_hc29225/bi/cbiig6bqpidhtncuswvfxwqqjwoiiswlwlrnh7eobbwm4wjlvpts.py"", line 15465, in call
    extern_kernels.addmm(arg4_1, reinterpret_tensor(buf3, (16, 2560), (2560, 1), 0), reinterpret_tensor(arg3_1, (2560, 2560), (1, 2560), 0), alpha=1, beta=
1, out=buf4)
RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'
```


Here is my code for reproducing the errors.
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Llama works well with  cache_implementation=""static"", but other types of models do not have the configuration.
# model     = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"", torch_dtype=torch.float16, attn_implementation=""sdpa"", token=access_token).cuda().eval()
# tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"", token=access_token)

# model     = AutoModelForCausalLM.from_pretrained(""EleutherAI/pythia-2.8b"" , torch_dtype=torch.float16, trust_remote_code=True).cuda().eval()
# tokenizer = AutoTokenizer.from_pretrained(""EleutherAI/pythia-2.8b"", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-2"", torch_dtype=torch.float16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-2"", trust_remote_code=True)

model.forward = torch.compile(model.forward, mode=""reduce-overhead"", fullgraph=True)

inputs = tokenizer([""<s> [INST] Write an essay about large language models [/INST]""], return_tensors=""pt"").to(model.device)

max_new_tokens = 64
fn = lambda: model.generate(
      **inputs,
      do_sample=False,
      # cache_implementation=""static"", # this only works for Llama-2
      max_new_tokens=max_new_tokens,
      pad_token_id=tokenizer.pad_token_id,
      temperature=None,
      top_p=None
)
fn() 
```

### Expected behavior

The models such as `pythia` and `phi-2` can run with `torch.compile` and a clear latency improvement can be observed. ","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6625174977, 'node_id': 'LA_kwDOCUB6oc8AAAABiuQlwQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Compilation', 'name': 'Compilation', 'color': '58E75E', 'default': False, 'description': 'Issues related to torchdynamo and torchinductor'}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",5,open
Tokenizer discard data that exceed max_length,"### Feature request

When use tokenizer, it truncate data to max_length, but can't just discard the data.

### Motivation

Sometimes we want the sentence to be complete

### Your contribution

No","[{'id': 1834056635, 'node_id': 'MDU6TGFiZWwxODM0MDU2NjM1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Tokenization', 'name': 'Core: Tokenization', 'color': 'FF4446', 'default': False, 'description': 'Internals of the library; Tokenization.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
"New `save_strategy` option called ""best"" to save when a new best performance is achieved.","### Feature request

Introduce a new option for the `save_strategy` argument called `""best""` which would save the model once a new best performance is achieved.

### Motivation

The `save_strategy` argument was first introduced a few years ago in https://github.com/huggingface/transformers/pull/10286. Currently the supported options are `""no""`, `""epoch""`, and `""steps""`. I'm assuming that this is to match the [`IntervalStrategy`](https://github.com/huggingface/transformers/blob/0f67ba1d741d65b07d549daf4ee157609ce4f9c1/src/transformers/trainer_utils.py#L221:L224) that's used by evaluation as well.

Judging by a conversation on a HuggingFace Discussion Forum topic, the best model is always kept by default and therefore if saving occurs at any time during the process then the best model is saved (ref: https://discuss.huggingface.co/t/save-only-best-model-in-trainer/8442). If the user deems that saving often is too burdensome then they may set `save_strategy = ""no""` which would save the best model at the end of training.

I believe that introducing some flexibility for saving would be beneficial so that users don't have to perform saving so often but also don't have to wait until the end for a checkpoint.

### Your contribution

If this feature is deemed worth it by the core maintainers then I'd be willing to take this on myself and open a PR. There are some aspects that I believe might warrant further discussion (e.g., which metric should be used to determine ""best,"" how to override `IntervalStrategy`, etc.).","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Added HHCache class implementing H2O Cache,"# What does this PR do?
This PR adds the feature requested in #30758. The HHCache class is almost directly taken from the original H2O paper's authors code found [here](https://github.com/FMInference/H2O). Currently the PR only adds the changes required to Llama model class. As of now I have taken @gante 's suggestion of adding `Cache.post_process()` and calling it within `LlamaAttention.forward`.

# To-Do
1. I'm not sure if the logic for RoPE rerotation is 100% correct. I think the recent tokens are correct, but not the hh tokens after eviction. Would love to have another set of eyes on that.
2. Write tests to ensure that this HHCache class has the same behavior compared to the original code by paper authors.
3. Benchmarking(?)

Feedback and/or help would be appreciated. Thanks!","[{'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",3,open
Allow infer_framework_load_model to use the originally specified config.,"# What does this PR do?

Allows infer_framework_load_model to use the originally specified config, currently if you specify config in the model_kwargs, you get a duplicate key error.

I don't have time to test this, but want to point out where there's a problem/inconsistency. Just check the diff, it's one line!

Fixes # (issue)
Currently if you specify config in the model_kwargs, you get a duplicate key error.

## Before submitting
- [n ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [y ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [n/a ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [n/a ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [n/a ] Did you write any new necessary tests?

Models:

- text models: @ArthurZucker

Library:

- pipelines: @Narsil",[],11,open
[WIP] Add implementation of `_extract_fbank_features_batch` ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Following the discussion in #27159, this PR will add implementations of `_extract_fbank_features_batch` for the [audio models](https://github.com/search?q=repo%3Ahuggingface%2Ftransformers%20_extract_fbank_features&type=code) that use the `spectrogram` function for feature extractions. 

This PR is still a WIP and it is not ready for review yet.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc 

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",8,open
Add argument to set number of eval steps in Trainer,"### Feature request

I would like to add an argument to the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class that allows for setting the number of eval steps (batches) for the evaluation procedure during training. 

The current behavior is to fully iterate through the entire dataset provided by the `eval_dataset` argument. However, when using a very large evaluation dataset, the evaluation process can take a long time. Especially for debugging, it would be helpful to be able to explicitly specify how many batches you would like to use as part of the evaluation procedure. 

### Motivation

Currently, I am using dataset streaming to train and evaluate models (motivated by large dataset size). My `eval_strategy` argument is set to `""steps""` since I'm using the streaming dataset, and so the Trainer runs the evaluation procedure every `eval_steps` number of steps. However, because there is no way to control how many steps are run during the evaluation procedure, the Trainer iterates through the entire eval dataset, which can be very time-consuming for a large dataset. Ideally, we could specify how many evaluation steps to run inside the evaluation procedure. 

Note that the argument I would like to add is different than the current `eval_steps` argument -- the current `eval_steps` argument specifies ""Number of update steps _between_ two evaluations"" but what I would like to specify is the number of steps _within_ the evaluations. 

### Your contribution

I would be happy to contribute code for this if this is deemed a relevant feature request and there isn't existing functionality that I'm unaware of. ","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
handle when from_pretrained_id is a list,"fix for #30045

Failing do to 2 reasons:

- [x] handle from_pretrained_id being a list, after #29473 (https://github.com/huggingface/transformers/pull/29473#pullrequestreview-1945687810)
- [ ] still fails when input starts with a space, slow adds space and fast does not in llama tests. working on this fix

Reviewer: @ArthurZucker ",[],2,open
fix wav2vec2 with torch.compile,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #101160


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@sanchit-gandhi
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.


 -->
","[{'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",2,open
Support deepspeed sequence parallel,"# What does this PR do?

Support the sequence parallel with Deepspeed-Ulysses.

I have tested the training on starcoder2-3b. The loss decreases normally.

![CleanShot 2024-06-21 at 00 52 50@2x](https://github.com/huggingface/transformers/assets/6374697/92d91f8d-4f27-4426-a8a2-cfe7953262ea)


Requires https://github.com/huggingface/accelerate/pull/2877

~~I have made massive modifications to the original implementation of Deepspeed-Ulysses to support batch size dim in `layers.py`. It uses `all_to_all_single` instead of `all_to_all` like https://github.com/InternLM/InternEvo/blob/a61d391df96c5f5c243cdea32a5044b70d6fe33e/internlm/core/parallel/comm/isp.py#L628  for better performance. I have left some comments to help the future understanding.~~ Use all_to_all_single is too complex to support other scatter idx and gather idx

Currently, flash attn and sdpa for llama and mistral are tested. flash attn for starcoder is also tested, the sdpa for starcoder is not supported.

It requires a special dataloader (I have made in Trainer) and data collator (with example followed). In data collator, the sequence should be divided into multiple sub-sequences. The following is an example of sub-sequences processing in the data collator.

```python
            seq_parallel_world_size = mpu.get_sequence_parallel_world_size()
            seq_parallel_world_rank = mpu.get_sequence_parallel_rank()

            seq_length = input_ids.size(1)
            sub_seq_length = seq_length // seq_parallel_world_size
            sub_seq_start = seq_parallel_world_rank * sub_seq_length
            sub_seq_end = (seq_parallel_world_rank + 1) * sub_seq_length

            # There is no kv cache when training
            past_key_values_length = 0

            position_ids = torch.arange(
                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long,
            )
            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)

            batch = dict(
                input_ids=input_ids[:, sub_seq_start:sub_seq_end],
                labels=labels[:, sub_seq_start:sub_seq_end],
                position_ids=position_ids[:, sub_seq_start:sub_seq_end],
                attention_mask=(input_ids != self.tokenizer.pad_token_id),
            )
```


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.


@muellerzr and @SunMarc


<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",11,open
add bnb support for Ascend NPU,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR can make bnb nf4 QLoRA out of the box on Ascend NPUs. 🤗 

Related PR: https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1422


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}]",15,open
FineWeb SLM Training doesn't start,"### System Info

This is my dev env:
https://github.com/abhinand5/runpod-utils/blob/main/docker/torch-lm-dev/Dockerfile

Using the latest docker.

Torch 2.3.1
CUDA 12.1

### Who can help?

@ArthurZucker @younesbelkada 

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

I am using a custom version of the [run_clm.py](examples/pytorch/language-modeling/run_clm.py). Where the only changes are:

1. Accepted YAML input along with JSON for parsing arguments
2. Ability to change attention implementation with an argument
3. Handling `validation_split_percentage` less than 1% for huge datasets (because 1% is actually a lot).

After the dataset is preprocessed, running the tokenizer and the values are cached...the training takes a ton of time to start. Waited one hour on 4 different occasions and killed the instance cuz as you know GPUs aren't cheap. 

It works when I reduce the max train samples to something small like 10000. See `DEBUGGING` in the YAML below.

Here is my config:

```yaml
# --- Model settings ---
model_name_or_path: abhinand/personal-model-init-fp16
model_revision: main

# --- Training settings ---
do_train: true
seed: 1337

# --- Cache and torch settings ---
cache_dir: /workspace/.cache
trust_remote_code: true
torch_dtype: bfloat16
attn_implementation: ""flash_attention_2""

# --- Dataset settings ---
dataset_name: HuggingFaceFW/fineweb-edu
dataset_config_name: sample-10BT

# --- DEBUGGING ---
# max_train_samples: 1000
# max_steps: 60
# max_eval_samples: 1000
# -----------------

# --- Data processing settings ---
block_size: 2048
overwrite_cache: false
validation_split_percentage: 0.5

# --- Preprocessing settings ---
preprocessing_num_workers: 16
output_dir: ./outputs
overwrite_output_dir: true

# --- Evaluation settings ---
eval_strategy: steps
eval_steps: 50
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 16
eval_accumulation_steps: 16

# --- Optimizer settings ---
optim: adamw_torch_fused
learning_rate: 3.0e-4
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-8
max_grad_norm: 1.0
num_train_epochs: 1.0
lr_scheduler_type: cosine
warmup_ratio: 0.05
# warmup_steps: 500
weight_decay: 0.01

# --- Logging settings ---
logging_strategy: steps
logging_steps: 1
save_strategy: steps
save_steps: 50
save_total_limit: 5
save_safetensors: true
bf16: true
fp16: false
# bf16_full_eval: true

# --- Torch settings ---
torch_compile: false # not sure why it doesn't work with flash_attention_2
include_tokens_per_second: true
include_num_input_tokens_seen: true

# --- Hub settings ---
push_to_hub: true
hub_model_id: abhinand/personal-model-v0-test1
# hub_strategy: 
hub_private_repo: true
```

### Expected behavior

Training to start...","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",5,open
will this be a model or just a tokenizer/embeddings ? technically a sentence trancformer ?,"### Feature request

for the text component : will this config be abe to take a languge model as a pretrained config ? 

ie when creating a vision/text decoder model you can specify the tet model component ( ie the mistral model ) ? 
as the mistral is a decoder only model .... maybe it would need corss attention to connect ?
will the text componant be able to be used with the standard AutoModelForCausalLM ? <<< If so it would be prudent to be able to intialize a model with the pretrained components ... ie a lnaguge model and a whisper and a clip model and a xclip ? or to the effect of ... enableing for the genration of modelif these (bodys) ... ie collection of tensors whcih can be read by the imagebind library  transfromer... instead of a simple tokenizer .... it would then be able to utilize the tokenizer from the llm or combine the tokenizers into a single model by adding the speicla tokens to the languge model base ? 

in truth this would enable the generation of even larger models and give the pretrained model a chance to find thier homes in the new body of a (imagebind) as it is the wrapper which makes the model ...(hence perhaps also including a feature to execute code on the fly and return the executing back to the model directly if enabled or ok byu the user...as this is the method as the model could generate its on code to execute in opython and return the result to the transformer to be utilized in the response it is generating .... so if the model is given functions as an input (it would also be able to run them in the back ground pre retuning the response) ... also given code by the user in chat it should be able to run the code in ipython behind the scenes and return the response (not on the system directoy(hence havign hiddon ipython repl) ..(it would still have to make the calls)

### Motivation

rags internally ! 
usablity ! of the model wrapper to understand that each component could be trained seperate or even within the model itself !

as well as fitting with anything in and wnything out principle (media wise) (code as an input (function)) 

### Your contribution

i really would like to see this project get forwards as its the same thing i was working towards but i have fiath you will succeed ... if not i will stillc arry on my path !","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Update beam_constraints with KMP,"# What does this PR do?

This PR enhances the `PhrasalConstraint` class with the Knuth-Morris-Pratt (KMP) algorithm. This allows the constraint to restart quickly rather than start from the beginning when a match fails, improving efficiency.

Here is test:
```
from transformers import AutoTokenizer
import beam_constant


def test():
    tokenize = AutoTokenizer.from_pretrained(""Qwen-7B-Chat"", trust_remote_code=True)
    force_word = "" love cat, love dog""
    force_word_id = tokenize.encode(force_word, add_special_tokens=False)
    constraint = beam_constant.PhrasalConstraint(force_word_id)
    generate_contest = ""I love cat, love cat, love dog""
    generate_tokens = tokenize.encode(generate_contest, add_special_tokens=False)
    print(force_word_id)
    print(constraint.match_table)
    for token_id in generate_tokens:
        step, complete, reset = constraint.update(token_id)
        print(f""token_id:{token_id} {constraint.fulfilled_idx} {step} {complete} {reset}"")

if __name__ == ""__main__"":
    test()
```

test result

![image](https://github.com/huggingface/transformers/assets/124332948/9d83ad73-5087-49b3-a93d-a5802c198b1d)



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?
## Who can review?

@gante
","[{'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",3,open
How do I replace a spare tokens?,"### System Info

I want to SFT Mistral-v0.3 with my own chat template.
So I followed [this comment](https://github.com/huggingface/transformers/issues/27974#issuecomment-1854188941) and replaced some [controal_n] tokens with special tokens for the chat template.
However, the new vocabulary was actually added and the size of the vocabulary increased.
Is there any way to replace the vocabulary?

### Who can help?

@ArthurZucker

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

tokenizer.json
```
{
  ""version"": ""1.0"",
  ""truncation"": null,
  ""padding"": null,
  ""added_tokens"": [
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
{
      ""id"": 10,
      ""content"": ""<|system|>"",
      ""single_word"": false,
      ""lstrip"": false,
      ""rstrip"": false,
      ""normalized"": false,
      ""special"": true
    },
    {
      ""id"": 11,
      ""content"": ""<|user|>"",
      ""single_word"": false,
      ""lstrip"": false,
      ""rstrip"": false,
      ""normalized"": false,
      ""special"": true
    },
    {
      ""id"": 12,
      ""content"": ""<|assistant|>"",
      ""single_word"": false,
      ""lstrip"": false,
      ""rstrip"": false,
      ""normalized"": false,
      ""special"": true
    },
    {
      ""id"": 13,
      ""content"": ""<|eot|>"",
      ""single_word"": false,
      ""lstrip"": false,
      ""rstrip"": false,
      ""normalized"": false,
      ""special"": true
    },
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

tokenizer_config.json
```
{
  ""add_bos_token"": true,
  ""add_eos_token"": false,
  ""add_prefix_space"": true,
  ""added_tokens_decoder"": {
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ""10"": {
          ""content"": ""<|system|>"",
          ""lstrip"": false,
          ""normalized"": false,
          ""rstrip"": false,
          ""single_word"": false,
          ""special"": true
        },
        ""11"": {
          ""content"": ""<|user|>"",
          ""lstrip"": false,
          ""normalized"": false,
          ""rstrip"": false,
          ""single_word"": false,
          ""special"": true
        },
        ""12"": {
          ""content"": ""<|assistant|>"",
          ""lstrip"": false,
          ""normalized"": false,
          ""rstrip"": false,
          ""single_word"": false,
          ""special"": true
        },
        ""13"": {
          ""content"": ""<|eot|>"",
          ""lstrip"": false,
          ""normalized"": false,
          ""rstrip"": false,
          ""single_word"": false,
          ""special"": true
        },
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
}
```

test code 
```
tokenizer =  AutoTokenizer.from_pretrained(model_dir)
pprint(tokenizer.added_tokens_decoder)
```
output
```
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 768: AddedToken(""[control_766]"", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
 769: AddedToken(""[control_767]"", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
 770: AddedToken(""[control_768]"", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
 32768: AddedToken(""<|system|>"", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
 32769: AddedToken(""<|user|>"", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
 32770: AddedToken(""<|assistant|>"", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
 32771: AddedToken(""<|eot|>"", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}
```


### Expected behavior

[control_n] Tokens can be replaced with any token.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",13,open
Quantization support for heads and embeddings,"### Feature request

Hi! I’ve been researching LLM quantization recently ([this paper](https://arxiv.org/abs/2405.14852)), and noticed a potentially improtant issue that arises when using LLMs with 1-2 bit quantization.

### Problem description :mag:

Transformers supports several great ways for quantizing transformer ‘body’, but it seems that there is no built-in way
to quantize embeddings and/or lm head.

The reason why this is important is that some of the recent LLMs have very large vocabularies, and as a result,
their embeddings and heads can get massive. For instance, [Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
has 128K token vocabulary, [Qwen 2](https://huggingface.co/Qwen/Qwen2-72B-Instruct) has over 150K, [Gemma 2b](https://huggingface.co/google/gemma-2b) has 256K

As a result, if you load NF4 or AQLM quantized models, their **embeddings can take up 50% or more of the model footprint**. 
This is even more critical for lower bitwidth quantization:

![https://galqiwi.ru/persistent/2024-06-18/embed-1.png](https://galqiwi.ru/persistent/2024-06-18/embed-1.png)

### Feature Request :rocket:

It would be great if transformers had a flag to quantize embeddings and heads using some of the existing quantization methods. One simple way would be to use LLM.int8 or NF4 by Tim Dettmers since transformers already supports this.

I’ve investigated how quantizing embeddings with these methods affects common models. Below is model perplexity for [Llama 3 8B using AQLM+PV 2-bit quantization](https://huggingface.co/ISTA-DASLab/Meta-Llama-3-8B-AQLM-PV-2Bit-1x16). I measured three configurations: fp16 embeddings, int8 embeddings and NF4 embeddings with the same parameters that transformers uses for linear layers.

![https://galqiwi.ru/persistent/2024-06-18/emb_v3.png](https://galqiwi.ru/persistent/2024-06-18/emb_v3.png)
![https://galqiwi.ru/persistent/2024-06-18/head_v3.png](https://galqiwi.ru/persistent/2024-06-18/head_v3.png)

The values represent perplexity on [WikiText-2 test set](https://huggingface.co/datasets/Salesforce/wikitext/viewer/wikitext-2-v1)
measured with the same protocol used in [GPTQ](https://arxiv.org/abs/2210.17323) / [AQLM](https://arxiv.org/abs/2401.06118)
/ [QuIP#](https://arxiv.org/pdf/2402.04396.pdf) papers.
The code for these measurements can be found [here](https://gist.github.com/galqiwi/cb896f39052d1f4f718cb772040f3088).

Overall, 8-bit compression looks nearly lossless, the increase in perplexity does not exceed the error you get when 
quantizing the transformer with the same LLM int8 codec. In turn, NF4 introduces some error (within 0.05 for Llama 3),
but I would argue that this trade-off makes sense for low memory applications. Also, embeddings appear
easier to quantize than heads.

### Implementation details :gear:

There are multiple obstacles on the way to implementing this feature:
#### No support for mixed quantization
Currently, transformers does not support quantizing with multiple `HfQuantizer`s. IMO this is a good behaviour, as interactions between different quantizators can be messy. The problem is that this feature requires for transformers library to use different compression methods for body and heads/embeddings. I think that can be solved by extending `HfQuantizer` interface by adding embedding/head quantization methods and adding new `[embed,head]_quantization_config` arguments to `QuantizationConfigMixin` or something in this area.
#### No support for embedding quantization in bitsandbytes
As far as I know, no quantization method supports `nn.Embedding`-like interface. I can ask bitsandbytes maintainers if they would accept a PR that fixes that.

Also, there is a caveat that some models use tied embeddings/heads, while implementing, one need to be mindful of them.

### Cool things that this can enable :trophy:

If we can implement 4-bit embeddings, it will be possible to write a colab notebook that runs [Llama 3 70B model](https://huggingface.co/meta-llama/Meta-Llama-3-70B)
on the free tier T4 GPU without offoading, by combining embedding/heads quantization and the  PV-tuned model 
https://huggingface.co/ISTA-DASLab/Meta-Llama-3-70B-AQLM-PV-1Bit-1x16 .

Another use case is running quantized LLMs on smartphones or embedded devices: for instance, the [gemma-2b](https://huggingface.co/google/gemma-2b) can fit into 1GB RAM, but only if you quantize embeddings/heads in addition to transformer weights.

If you’re interested in making a demo out of this, I’d be excited to implement this with your review / recommendations if you prefer, or wait for you to implement it your way.


What do you think?

### Motivation

We are faced with a new bottleneck in model quantization. I think we can manage to fix it

### Your contribution

I can allocate my time to submitting PR, but we need to figure out what to do first","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}]",14,open
add changes in mistral model to avoid problems in pytorch hooks,"# What does this PR do?

Removing keyword arguments to allow pytorch hooks.
As keyword arguments are not passed to pytorch hooks (`Module.register_forward_hook`), removing keywords in MistralDecoderLayer allows us to hook the intermediary steps of the attentio calculation. Does not change the behavior of any other methods

Here is an example

```
from transformers import AutoModelForCausalLM, AutoTokenizer

base_model = AutoModelForCausalLM.from_pretrained(
    ""mistralai/Mistral-7B-v0.1"",
    attn_implementation=""eager"",
)

tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-v0.1"")

```

Creating a hook function. Since now the arguments are not passed as kwargs, pytorch hooks can identify them in the `input` variable

```
internal_parameters = []
def hook_internal_parameters(module, input, output):
        hidden_states = input[0]
        attention_mask = input[1]
        position_ids = input[2]
        past_key_value = input[3]
        cache_position = input[6]


        bsz, q_len, _ = hidden_states.size()

        query_states = module.q_proj(hidden_states)
        key_states = module.k_proj(hidden_states)
        value_states = module.v_proj(hidden_states)

        d = {
            ""query"": query_states,
            ""key"" : key_states,
            ""value"" : value_states
        }
        internal_parameters.append(d)
```

Setting hook in `self_attn` modules

```
for name, internal_module in bae_model.named_modules():
     if name.endswith(""self_attn""):
          internal_module.register_forward_hook(hook_internal_parameters)
```

Hooking internal embeddings

```
text = ""hello world""
tokens= tokenizer(text, return_tensors=""pt"")
with torch.no_grad():
    out = base_model(**tokens)

internal_parameters
```",[],3,open
A parameter in TrainingArguments: sample_output=True,"### Feature request

It would be nice to have the model output some sample generations while it is training, so that I can see if my training is going well, and at what point I should stop it.

### Motivation

For example let's say I set my training for 5 epochs.  I see the samples from the first epoch are bad, the second look a little better, and the third are exactly what I want.  I could stop the training right there so I don't have to overfit.

### Your contribution

I'm not sure if I can contribute.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Add `StatefulDataLoader` support,"### Feature request

Add official support for `StatefulDataLoader` as in [torchdata](https://github.com/pytorch/data/tree/main/torchdata/stateful_dataloader) and [datasets](https://huggingface.co/docs/datasets/stream#save-a-dataset-checkpoint-and-resume-iteration).

### Motivation

The StatefulDataLoader from the torchdata package provides a convenient way to recover a dataset iterator that was interrupted, without having to skip the first batches via a naive for loop, which can be time-consuming for extremely large datasets. The `datasets` package now officially supports stateful `IterableDataset` and its combination with `StatefulDataLoader` in [v2.20.0](https://github.com/huggingface/datasets/releases/tag/2.20.0).

Example usage:

```py
from torchdata.stateful_dataloader import StatefulDataLoader
iterable_dataset = load_dataset(""deepmind/code_contests"", streaming=True, split=""train"")
dataloader = StatefulDataLoader(iterable_dataset, batch_size=32, num_workers=4)
# checkpoint
state_dict = dataloader.state_dict()  # uses iterable_dataset.state_dict() under the hood
# resume from checkpoint
dataloader.load_state_dict(state_dict)  # uses iterable_dataset.load_state_dict() under the hood
```

To enhance the usability and efficiency of the `Trainer`, it would be highly beneficial for the community if official support for `StatefulDataLoader` could be added. 
This would allow users to easily recover from interruptions and resume training from checkpoints without wasting time on re-iterating over already processed batches.
By integrating `StatefulDataLoader` into the `Trainer`, users can seamlessly handle large datasets and ensure a smooth training process. This feature would greatly improve the overall user experience and make the Trainer more robust and efficient.
We kindly request the development team to consider adding official support for thoese features in the `Trainer`, as it would be a valuable addition to the library and benefit the wider community.
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",14,open
Make CI failure summary more useful,"# What does this PR do?


The current 'Failed Tests' summary output of tests isn't useful e.g. here: https://app.circleci.com/pipelines/github/huggingface/transformers/95692/workflows/2c1a2ff4-1182-46f4-b331-33d9fdb0a2c4/jobs/1260625

Output:
```   
1 failed because `AssertionError` -> torch.Size([1, 512, 37890]) != (1, 512, 1026)
2 failed because `AssertionError` -> False is not true
Number of failed tests: 3
```

As: 
* There is no record of the test which have failed. This means, to address failing tests, one still needs to go the main test output and parse yourself
* Without the test name messages like: ""`AssertionError` -> False is not true"" is not informative (pretty much all tests fail this way) 

Additional nit: ""splitted"" isn't a word. splitted -> split

Example of an updated output [for this run](https://app.circleci.com/pipelines/github/huggingface/transformers/95689/workflows/106ed803-3f9a-41d4-b6db-709929cc7db9/jobs/1260576): 

```
tests/models/mask2former/test_image_processing_mask2former.py::Mask2FormerImageProcessingTest::test_call_numpy failed because `AssertionError: Tuples differ` -> (1, 3, 32, 91) != (1, 3, 4032, 32)
tests/models/mask2former/test_image_processing_mask2former.py::Mask2FormerImageProcessingTest::test_call_numpy_4_channels failed because `AssertionError: Tuples differ` -> (1, 4, 32, 32) != (1, 4, 1320, 32)
tests/models/swin2sr/test_image_processing_swin2sr.py::Swin2SRImageProcessingTest::test_call_numpy failed because `AssertionError: Tuples differ` -> (1, 3, 136, 384) != (1, 3, 384, 8)
tests/models/swin2sr/test_image_processing_swin2sr.py::Swin2SRImageProcessingTest::test_call_numpy_4_channels failed because `AssertionError: Tuples differ` -> (1, 266, 104, 8) != (1, 4, 104, 8)
tests/models/yolos/test_image_processing_yolos.py::YolosImageProcessingTest::test_call_numpy failed because `AssertionError: Tuples differ` -> (1, 3, 16, 48) != (1, 3, 1376, 0)
tests/models/yolos/test_image_processing_yolos.py::YolosImageProcessingTest::test_call_numpy_4_channels failed because `AssertionError: Tuples differ` -> (1, 4, 48, 16) != (1, 4, 432, 16)
tests/models/vit/test_image_processing_vit.py::ViTImageProcessingTest::test_fast_is_faster_than_slow failed because `AssertionError` -> 0.003969907760620117 not less than or equal to 0.003456274668375651
tests/models/idefics2/test_image_processing_idefics2.py::Idefics2ImageProcessingTest::test_call_numpy_4_channels failed because `TypeError: Cannot handle this data type` -> (1, 1, 157), |u1
```

","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",9,open
Set ffmpeg_microphone input to system default on Darwin platform,"# What does this PR do?

Currently, the device index used for audio capture using ffmpeg_microphone is 0, which does not always work as expected. This PR sets the default audio capturing device on the Darwin platform to `:default` which is the ffmpeg supported way to do this.

Fixes #28154

There is no documentation or tests referencing ffmpeg_microphone so I did not update any of it.

I have created a local small test (similar to the [voice assistant tutorial](https://huggingface.co/learn/audio-course/chapter7/voice-assistant) to prove that switching between external mic (airpods) and Mac audio capture works ok.
```
from transformers.pipelines.audio_utils import ffmpeg_microphone_live, ffmpeg_microphone
from transformers import pipeline
import sys
import torch

device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""
transcriber = pipeline(
    ""automatic-speech-recognition"", model=""openai/whisper-small.en"", device=device
)

def transcribe(chunk_length_s=15.0, stream_chunk_s=1.5):
    sampling_rate = transcriber.feature_extractor.sampling_rate

    mic = ffmpeg_microphone_live(
        sampling_rate=sampling_rate,
        chunk_length_s=chunk_length_s,
        stream_chunk_s=stream_chunk_s,
    )

    print(""Start speaking..."")
    for item in transcriber(mic, generate_kwargs={""max_new_tokens"": 2048}):
        sys.stdout.write(""\033[K"")
        print(item[""text""], end=""\r"")
        if not item[""partial""][0]:
            break

    return item[""text""]

if __name__ == ""__main__"":
    transcribe()
```

## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ylacombe @Narsil 

",[],1,open
LLM Cache ,"### Feature request

Hi,

I'm working with an LLM model that generates text in stages. It stops after each section, performs an action, integrates the action's result into the generated text, and then continues. While this approach works, I'm facing a problem in time it takes.

I've tried using a static cache, but it actually increased processing time.

Is there a way to optimize this process?


### Motivation

.

### Your contribution

.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Preserve `ModelOutput` object for compute_metrics evaluation loop in trainer,"# What does this PR do?

### Preserve `ModelOutput` for evaluation loop
**Motivation**: During the evaluation, the trainer returns tuples instead of `ModelOutput` objects, however, image_processors expect `ModelOutput` for the postprocessing of predictions. This significantly reduces the generalization of training scripts for different models of the same type.

**Solution**: Introduced training argument `eval_preserve_model_output`. If set to `True`, then `ModelOutput` objects are preserved and passed to `compute_metrics`, otherwise model outputs are converted to tuples. `False` by default for BC.


### Fix discrepancies in return type for `batch_eval_metrics`

Current `main` behavior:
- if `batch_eval_metrics=True` on **device** (e.g. GPU) torch tenors are passed to the `compute_metrics` function
- if `batch_eval_metrics=False` **CPU** numpy arrays are passed to the `compute_metrics` function

Suggested behavior:
- Tensors are always moved to the CPU
- If training argument `eval_numpify_tensors=True`, then tensors are converted to numpy arrays, otherwise returned as torch tensors (`True` by default for BC)

This change is breaking for `batch_eval_metrics=True` but in favor of making outputs consistent, `batch_eval_metrics` was introduced in #28769 (about a month ago), I guess we can still change its behavior.



## Who can review?
",[],4,open
Problems when using SinkCache for model.generate(),"### System Info

- `transformers` version: 4.41.2
- Platform: Linux-5.4.0-173-generic-x86_64-with-glibc2.31
- Python version: 3.10.0
- Huggingface_hub version: 0.23.3
- Safetensors version: 0.4.3
- Accelerate version: 0.31.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@zucchini-nlp @gante @tomaarse

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I am trying to use sinkcache for multi-turn dialog where the cache should contains the previous turn's dialog. Here is my code:

```
import torch

from transformers import SinkCache

from transformers import AutoConfig, LlamaForCausalLM, AutoTokenizer

model_id = ""meta-llama/Llama-2-7b-hf""
model = LlamaForCausalLM.from_pretrained(
      model_id, low_cpu_mem_usage=True, device_map='auto',
      torch_dtype=torch.bfloat16,cache_dir=""cache"")
model = model.eval()
tokenizer = AutoTokenizer.from_pretrained(model_id,cache_dir=""cache"")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'left'

device = model.device

cache = SinkCache(window_length=1024, num_sink_tokens=4)
prefix_list=[""hello,my name is yy"",""what is my name?""]
for prefix in prefix_list:
    inputs = tokenizer(prefix, return_tensors='pt').to(device)

    
    gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=64,
                            use_cache=True,
                            past_key_values=cache,
                            pad_token_id=tokenizer.pad_token_id)
    
    decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)

    print(decoded)
```
I am not sure if  I was doing it right. But I got the following error:

```
Traceback (most recent call last):
  File ""/data/yaoy/long_context/repeat_sirllm/main.py"", line 25, in <module>
    gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=64,
  File ""/data/yaoy/anaconda/envs/long/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/data/yaoy/anaconda/envs/long/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1758, in generate
    result = self._sample(
  File ""/data/yaoy/anaconda/envs/long/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2390, in _sample
    model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)
  File ""/data/yaoy/anaconda/envs/long/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1326, in _get_initial_cache_position
    model_kwargs[""cache_position""] = torch.arange(past_length, cur_len, device=input_ids.device)
RuntimeError: upper bound and larger bound inconsistent with step sign
```

### Expected behavior

 Cache should include information from previous conversations.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",13,open
Object Detection Pipeline only outputs first element when batching,"### System Info

- `transformers` version: 4.41.2
- Platform: Linux-5.4.0-182-generic-x86_64-with-glibc2.31
- Python version: 3.11.8
- Huggingface_hub version: 0.23.0
- Safetensors version: 0.4.2
- Accelerate version: 0.30.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.2.2+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>


When running the ObjectDetectionPipeline in a batch, the output will only be the bounding boxes of the first input image due to ObjectDetectionPipeline.py accessing element [0] in postprocessing  and not looping over all outputs. 

https://github.com/huggingface/transformers/blob/a4e1a1d02894b8d801f5d0182e1979b55daaeaa4/src/transformers/pipelines/object_detection.py#L150


This accesses only and always the first element, instead of looping over all outputs. 

Only the first element is accessed in postprocessing

### Who can help?

@Narsil 

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

1. Create object detection Pipeline

`    pipe = pipeline(""object-detection"", model=model_name, image_processor=preprocessor_name, device=device)
`

2. use batch inference with batch_size > 2.
`
 for out in tqdm(pipe(dataset, batch_size=batch_size)):
`





### Expected behavior

Expected Output: 2 Elements with each x items (bboxes).
 Actual Output, only bboxes of the first input element.
","[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNzcxMTg3OTI0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Pipeline', 'name': 'Core: Pipeline', 'color': 'FF7066', 'default': False, 'description': 'Internals of the library; Pipeline.'}, {'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",12,open
Seq TransfoRNA,"### Model description

The Seq TransfoRNA model is intended to map short RNA fragments (18 nt. to 30 nt.) back to their parental precursor. The model is trained in a supervised manner to classify ~2k subclasses belonging to 11 major classes.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation
https://github.com/gitHBDX/TransfoRNA/tree/master/transforna/src/model
@yak-hbdx @jje-hbdx @tos-hbdx","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
 Implementation Issue of Phi3SuScaledRotaryEmbedding,"### Feature request

Improving the Implementation of Phi3SuScaledRotaryEmbedding to Reduce Unnecessary Computation

I'm not entirely sure if there is a deeper meaning to the implementation here. It seems that the inv_freq could be completely initialized in the `__init__ `method as `long_inv_freq `and `short_inv_freq`. 
The only parameter used here is the part that gets the device.

https://github.com/huggingface/transformers/blob/25245ec26dc29bcf6102e1b4ddd0dfd02e720cf5/src/transformers/models/phi3/modeling_phi3.py#L131-L137

### Motivation

Fixing this code can eliminate a redundant computation. Moreover, self.inv_freq used here doesn't seem appropriate to be a attribute.

I tested the modified version locally. Although I couldn't test with long contexts due to my device's performance, the new implementation doesn't seem to show any issues with short sentences.

### Your contribution

I can fix it if this isn't intentional.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Support saving models trained with DeepSpeed in Trainer callbacks,"### Feature request

Trainer callbacks [pass a model](https://github.com/huggingface/transformers/blob/a7cab3c283312b8d4de5df3bbe719971e24f4281/src/transformers/trainer_callback.py#L418) to any registered callback, but this model cannot be saved if training with DeepSpeed Stage 3 (needs access to `Trainer.accelerator` and `Trainer.model_wrapped`)

### Motivation

I have a custom callback that logs a model to a tracking server at the end of training, but need access to `Trainer.accelerator` and `Trainer.model_wrapped` in my callback to prevent skipping saving sharded tensors. I believe this may lead to a bug in the official `wandb` callback which uses a [""fake trainer""](https://github.com/huggingface/transformers/blob/b6c9f47fd6f911450024c52e382e544e5d04387a/src/transformers/integrations/integration_utils.py#L850) (I don't use `wandb` so can't confirm, but see an error similar to one reported in another repo if I try the ""fake trainer"" approach in my custom callback e.g., https://github.com/OpenAccess-AI-Collective/axolotl/issues/1092)

### Your contribution

I can contribute this feature, but wanted to get guidance on the design. Roughly

* `trainer_callback.CallbackHandler` receives an additional arg, `accelerator`, which will be passed `Trainer.accelerator` on instantiation
* `trainer_callback.CallbackHandler` has an attribute `model_wrapped` which gets updated with `Trainer.model_wrapped` by `Trainer`
* `trainer_callback.CallbackHandler.call_event` will pass along `self.accelerator` and `self.model_wrapped` when calling  callbacks

This allows my callback to do something like `state_dict = accelerator.get_state_dict(model_wrapped)` and pass that along to `model.save_pretrained` as to not skip saving sharded tensors

If there is a better design or if this is better classified as a bug, please let me know","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",3,open
Rest of model init refactors,Builds on top of https://github.com/huggingface/transformers/pull/31329 and applies the rest of the model init refactors.,[],1,open
SPLIT PR: eos bos tokens,"Fix for 2 issues:

1. `add_bos_token` & `add_eos_token` flags ignored for `PreTrainedTokenizerFast`: issue discussed [here](https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/140) and [here](https://github.com/huggingface/transformers/issues/30947#issuecomment-2128057992)
2. `add_special_tokens` does not update `bos_token` or `eos_token` - ex `.add_special_tokens({'bos_token': '<new_bos>'})`

TASKS:

- [x] added an `update_post_processor` function in `PreTrainedTokenizerFast` based on llamatokenizer, allows reading of bos / eos token flag

**SUPPORTS FAST ONLY 
slow required updating kwargs to be passed into `sp_model` , so that bos/eos tokens can be added accordingly..

Reviewer: @ArthurZucker 

NOTE: hub token seems to not have access to llama 3, should pass after addressed",[],1,open
SPLIT PR: add_prefix_space fix,"Fixes portion of https://github.com/huggingface/transformers/issues/30824

Adds support for add_prefix_space. 

TASKS:
#### convert_slow_tokenizer.py
- [x] `add_prefix_space` is set based on original_tokenizer, otherwise based on proto's noramlizer. 
#### llama_tokenizer.py
- [x] `add_prefix_space` is updated based on normalizer if unset.
#### llama_tokenizer_fast.py
- [x] **remove** forcing `from_slow` conversion if `add_prefix_space` is not set (see tokenization_utils_fast.py updates on how this is handled without conversion). This allows passing this field **wthout sentencepiece installed.**
#### tokenization_seamless_m4t.py
- [x] updated comment 'Copies' - ruff complained
#### tokenization_t5.py
- [x] do not set prefix to True, set based on  normalizer
#### tokenization_t5_fast.py
- [x] set prefix space, do not force from slow if set
#### tokenization_utils.fast.py
- [x] set prefix space
- [x] allow using `add_prefix_space` without converting from slow, thus allowing to use without sentencepiece installed
- [x] update rust pre_tokenizer and normalizer based on `add_prefix_space` and underlying normalizer. legacy logic copied from `convert_from_slow.py`

TESTS:
- [x] test legacy tokenizer in llama
- [x] test legacy tokenizer in t5
- [x] test tokenizer without sentencepiece installation (mock)

Reviewer:
@ArthurZucker",[],3,open
[torch.compile] Graph break+warning instead of error in _prepare_generation_config,"I ran into this error when attempting to `torch.compile` miniCPM (https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5).

I tracked changes back to this PR https://github.com/huggingface/transformers/pull/29443 and I think that we should be graph breaking (i.e. going ahead with `copy.deepcopy`) and issuing a warning in the case we need to, instead of crashing. But in the case where the copy doesn't need to be done, we can skip making the call in order to prevent the graph break.

cc @gante ",[],4,open
Having a function to verify if checkpoint is valid,"### Feature request

Have a boolean function that returns true if a given checkpoint is valid and false if not

### Motivation

In case a machine can be down at any moment while training. If it saves the checkpoint when the machine goes down, last checkpoint can be corrupted. So we would like to check if the last checkpoint is valid and be deleted (or not) by the end user.

### Your contribution

I can work on this","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
[WIP] Quantized StaticCache + compile (x2 less memory),"# What does this PR do?

Draft PR that makes HQQ cache compatible with `torch.compile`. We have only HQQ, because `quanto` is not compile-compatible yet for lower bits.

The current state of low-bit static cache has a lot of manipulations and reshapes with key/values before and after quantizing, so that we can actually concatenate the quant tensors and get the same tensors at dequantization. This is needed because we can't grow dynamically cache size by concating each step.  

Key points:
- We don't give users freedom with choosing axis for quantization, as I got to make it work only with axis=1 for now
- For int4 and in2 (which does packing) we manually change axis to 0, to do packing along the group_size dimension which allows us to do concatenation. That is what KIVI authors did in their implementation - quantize and pack along last dimension.
- The evaluation in benchmark shows that int has the same memory efficiency as it was for dynamic vs quantized
- Dequantization step is now also compiled to speed up as the tensor shapes are static now


TODO:
- Figure out how to make a better cache API for all the classes we have now
- See if we can make it work with axis=1, or get rid of extra reshapes by talking with HQQ guys
- More benchmarks, e.g. HQQ made a compile + weight quantization blogpost, we can add (+ static cache int4) to it
- I thought it should be x3-4 less memory, check if there's extra memory being used anywhere
- Get back to KIVI authors, they are open to possible collaboration for further work on kv quantization

<details><summary>Benchmark results (dynamic vs static-int4)</summary>
<p>
Memory used up by cache for each setting in MiB. The script I used is in the PR, it's the same one we have for dynamic vs static evaluation.
The images below have typo, it should eb dynamic 😅 Comparison with static will come later

![image](https://github.com/huggingface/transformers/assets/100715397/8b96b94d-aac9-4b06-bd4f-be4672e62737)

Total time in ms for each generation condition, excluding time for compilation and pre-fill.
![image](https://github.com/huggingface/transformers/assets/100715397/72fb9ba8-8c4c-4433-a4a0-0e76b4ceade7)

</p>
</details> 

cc @gante @ArthurZucker 
 ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",1,open
Potential fix for #30819. Check for best metric only on gpu 0,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #30819\. Allows running question-answering with DDP using torchrun by doing metric_for_best_model checks on only the main process. I have tested it on question-answering but maybe this needs unit tests in the CI too?


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @zucchini-nlp (visual-language models) or @gante (all others)
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @SunMarc

Integrations:

- deepspeed: HF Trainer/Accelerate: @muellerzr
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@amyeroberts @muellerz @SunMarc 
",[],3,open
Adaptive Decoding Support,"### Feature request

Adaptive decoding balances the diversity and coherence of open-ended text generation, making it an excellent method compared to top-k, top-p, and other decoding algorithms. For more details, the paper is available [here](https://arxiv.org/abs/2402.18223), and the code can be accessed [here](https://github.com/zwhong714/adaptive_decoding).

### Motivation

During the generation process, the distribution predicted by the language model generally falls into two categories. The first is a flattened distribution, indicating that the LM has multiple potential choices for the next token. The second is a sharp distribution, suggesting that the model's options are more limited. Ensuring that the model dynamically understands the current state is crucial for generating highly diverse and coherent sentences.

### Your contribution

idea done.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",1,open
Add support for non-CUDA architectures at the same time Bitsandbytes is doing it,"### Feature request

Currently, the helper/setup functions explicitly check for CUDA support:
https://github.com/huggingface/transformers/blob/8685b3c5d2dd2550527773d2a02499495a759e31/src/transformers/quantizers/quantizer_bnb_4bit.py#L60-L63

BNB is currently doing a project to enable support for other GPU backends:
[ALPHA TESTERS WANTED](https://github.com/TimDettmers/bitsandbytes?tab=readme-ov-file#alpha-testers-wanted-multi-backend-refactor-amd-gpu--intel-cpugpu-specific-bnb-backend-implementations)

### Motivation

Apple MPS support is being added for so many major players, it'd be great for the biggest one of all to support it as its dependencies do. Also would be good to not hard-code this kind of limitation so that code updates aren't necessary as dependent libraries update themselves...

### Your contribution

idea done","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Batch size schedulers,"### Feature request

Implement in the trainer the possibility to increment the batch size according to some schedule, similar to how lr can follow a schedule.

### Motivation

It's been shown (e.g. DeepSeek) that slowly increasing the batch size can have a positive effect on learning.

### Your contribution

Unfortunately no time to work on this. It's not trivial to add this, so it's really just an idea to throw out there.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",9,open
Speed up image processors - cast to array before BatchFeature,"### Feature request

Complete work initially started in #28221

### Motivation

Converting the model outputs to an array before creating the `BatchFeature` object can speed up image processing, especially for nested outputs, such as for videos. 

### Your contribution

Open to the community to add","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
SDPA for T5 Attention,"# What does this PR do?

SDPA for T5 Attention
",[],3,open
Implement MambaForSequenceClassification,"# What does this PR do?
Adds the `MambaForSequenceClassification` model based on `MambaModel` backbone.

We recently published EHRMamba, a state-of-the-art foundation model for Electronic Health Records. This model is built on the same architecture and we will release the trained weights using the MambaForSequenceClassification class.
https://vectorinstitute.github.io/EHRMamba

Fixes #30431 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
      https://github.com/huggingface/transformers/issues/30431
- [X] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [X] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

As discussed in https://github.com/huggingface/transformers/issues/30431, @ArthurZucker could you take a look? 😊

## Notes

This implementation closely follows the GPT2ForSequenceClassification method, with the exception of pooling the last hidden states before passing them to the classifier to improve efficiency. ",[],30,open
SAM-HQ implementation in transformers,"### Model description

HQ-SAM is an upgrade SAM for high-quality zero-shot segmentation. It's design mostly repeats the design of SAM, but additionally early VisionEncoder features are used in enhanced MaskDecoder for fine-grained segmentation

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Code is available here https://github.com/SysCV/sam-hq
Paper https://arxiv.org/abs/2306.01567

Authors @lkeab et al.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",5,open
Request for Static Cache Support for XLA Compiler in Transformers,"### Feature request

I am requesting static cache support for the XLA compiler for transformer models. This will prevent recompilation at each step caused by slicing

### Motivation

To enhance the efficiency of autoregressive generation in transformer models, static cache support for the XLA can be implemented to mitigate the need for recompilation at each generation step. 

### Your contribution

[Use the Index_copy method to update static cache inplace](https://github.com/huggingface/transformers/pull/31129)

- Additionally, While out-of-place operations could offer further improvements, they introduce new tensors with different addresses, complicating calculations. Addressing this issue is crucial for fully leveraging the potential of XLA.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Understanding loss in Training LLM,"### Feature request

Hi,

I have a misunderstanding regarding training LLMs. When we train the model, we calculate the loss by having the model predict the next word and then compute the difference between the true and predicted values.

What I want to know is: when making the model predict the next words, it generates a new word based on the previously generated words. If a generated word is wrong, won't the subsequent predictions continue down the wrong path?

### Motivation

![LLMTrain-ezgif com-webp-to-jpg-converter](https://github.com/huggingface/transformers/assets/76405917/a35dd000-5c10-480f-9972-150870a43ca9)


### Your contribution

.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
need better token_type_id processing on transformer GPT2Model ,"### Feature request

in the GPT2Model code, I see that the token_type_id value is embeded with the same **wte** of input tokens. so that the two token type id values: 0, 1,  are in collision with input sentence tokens. We need a more nature design here so that we do not have to reproduce vocab_size and sentence token values when trained with the `token_type_ids` inputs.

### Motivation

make it simply the same  vocab size and sentence token inputs for training GPT2Model with  or without `token_type_ids` input.

### Your contribution

maybe another embedding layer can be used for the token_type_id embeddings.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Add HelpingAIForCausalLM to it,"### Model description

HelpingAI is a state-of-the-art large language model specializing in emotionally intelligent conversation. With advanced emotional understanding capabilities, it can engage in empathetic dialogue tailored to the user's emotional state and context.

## Emotional Intelligence Capabilities

HelpingAI-3B-v2.2 exhibits several key traits that enable emotionally resonant responses:

- Emotion recognition and validation
- Empathetic perspective-taking 
- Generating emotionally supportive language
- Contextual emotional attunement
- Using appropriate tone, word choice and emotional expression

Whether comforting someone grieving, celebrating positive news, or addressing complex feelings, HelpingAI-3B-v2.2 can adapt its communication style with emotional nuance.

## Examples of Emotionally Intelligent Responses


""I'm really sorry to hear about your friend's loss. 😔 Losing a parent can be incredibly difficult and traumatic, and it's completely understandable that they are struggling with grief. It's important to show them empathy and support during this tough time. You can start by simply being there for them, listening to them, and offering a shoulder to lean on. 💕 It's also important to acknowledge their feelings and let them know that it's okay to grieve and take as much time as they need to process their emotions. Offering practical help, such as running errands or cooking for them, can also make a big difference. Remember, small gestures of kindness and compassion can go a long way in comforting someone during difficult times.""

""Hey there! 🌟 It's a beautiful day today, so you should cook something delicious and enjoy the sunny weather! How about trying out a new recipe and let your creativity flow? Whether it's a classic dish or something experimental, the kitchen is the perfect place to let your imagination run wild. Write me a list of ingredients you have on hand, and I'll help you come up with a wonderful culinary creation! 🍲🌞""

The model tailors its language, tone and emotional content to be contextually appropriate, combining emotional intelligence with factual knowledge and practical suggestions.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

@OEvortex
https://huggingface.co/OEvortex/HelpingAI-3B-v2.2","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",7,open
Caching Past Key values of any length for Vision LLM's,"### Feature request

Allowing passing past key values during the forward pass of more than one token similar to the text large language models. 

### Motivation

According to the documentation [here](https://huggingface.co/docs/transformers/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration.forward.past_key_values) one could in theory pass past key values of the prompt to speed up the forward pass. However, I think that the cached forward pass only happens when the input_ids has **only** one new token as described in the condition [here](https://github.com/huggingface/transformers/blob/573565e35a5cc68f6cfb6337f5a93753ab16c65b/src/transformers/models/llava_next/modeling_llava_next.py#L816). Changes to this making it consistent with the text LLMs (thanks for that) would be highly appreciated.

### Your contribution

If you can give me any pointers on how I can realign cache, create the attention masks, etc., It would also be very helpful","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",2,open
rework `test_multi_gpu_data_parallel_forward`,"# Description

Currently `test_multi_gpu_data_parallel_forward` is problematic and are skipped for several model testing. Many times, it looks like some cuda issue (`CUDA error: misaligned address` etc.). With other `nvidia` related stuffs (software or hardware) and/or torch versions, they might pass, fail, or pass but fail many subsequent tests.

It currently uses `nn.DataParallel` which is no longer recommended. In the long term, we should try `DistributedDataParallel` and see how this test goes.

See  #31086 for example",[],0,open
Log multiple losses used along with the combined losses when a model returns a dictionary of losses.,"### Feature request

Able to log individual losses returned as dict.

### Motivation

I have multiple losses that are being added to form a combined loss. I want to log all these individual losses to observe the trend of the individual losses. SemanticSegmenterOutput accepts single loss at the moment and logs the loss in the SemanticSegmenterOutput.

### Your contribution

I have modified the Trainer class and SemanticSegmenter output as below but it is not working as expected. I have added a few print statements to check if the on_log part is being accessed or not but that code is not even being accessed.

    
class CustomTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.additional_losses = {}

    def training_step(self, model, inputs):
        outputs = model(**inputs)
        
        # Extract the additional losses from the model output
        self.additional_losses = outputs.additional_losses
        
        # Continue with usual training step process
        return super().training_step(model, inputs)

    def on_log(self, global_step, state, control, logs, **kwargs):
        loss_dict = logs.get(""loss_dict"", {})
        
        if 'wandb' in self.args.report_to:
            wandb.log({k: v for k, v in loss_dict.items()})
        
        if 'tensorboard' in self.args.report_to:
            tensorboard_writer = self.state.log_history.get('tensorboard')
            if tensorboard_writer is not None:
                tensorboard_writer.add_scalars('additional_losses', loss_dict, global_step)
        
        # Call the parent's on_log method
        super().on_log(global_step, state, control, logs, **kwargs)

    def on_train_end(self):
        tensorboard_writer = self.state.log_history.get('tensorboard')
        if tensorboard_writer is not None:
            tensorboard_writer.close()


@dataclass
class CustomSemanticSegmenterOutput(SemanticSegmenterOutput):
    additional_losses: Optional[Dict[str, torch.FloatTensor]] = None","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Add ability to specify input device for ffmpeg_microphone(),"### Feature request

The function `transformers.pipelines.audio_utils.ffmpeg_microphone()` currently has the following code for setting the input device for ffmpeg:

```
def ffmpeg_microphone(
    sampling_rate: int,
    chunk_length_s: float,
    format_for_conversion: str = ""f32le"",
):
    """"""
    Helper function to read raw microphone data.
    """"""
   <....>
    system = platform.system()
    if system == ""Linux"":
        format_ = ""alsa""
        input_ = ""default""
    elif system == ""Darwin"":
        format_ = ""avfoundation""
        input_ = "":0""
    elif system == ""Windows"":
        format_ = ""dshow""
        input_ = _get_microphone_name()
```

This makes it where the only option is to use default ALSA input device. If the user wants to select a different device, there is no way to change this, other than making the system-wide change of default device. 

What I would like to see instead is an optional `input_device=...` parameter added to the `ffmpeg_microphone()`  function that allows the user to specify a different input device. 

It would still behave the same as it currently does (use alsa default), if user does nothing, so the change wouldn't break existing code. But if they pass a different input device in function args, they can use it without having to change default input device system-wide.

### Motivation

I want to use a different input device than the system default input. I do not want to make the change system-wide.

### Your contribution

This would be an extremely simple change to make.  Just change the function header from:

```
def ffmpeg_microphone(
    sampling_rate: int,
    chunk_length_s: float,
    format_for_conversion: str = ""f32le"",
):
```

to:

```
def ffmpeg_microphone(
    sampling_rate: int,
    chunk_length_s: float,
    format_for_conversion: str = ""f32le"",
    input_device= None,
):
```
And make the same change to the `ffmpeg_microphone_live()` function so that different device could be used there as well. 

Then, change this part:

```
    system = platform.system()
    if system == ""Linux"":
        format_ = ""alsa""
        input_ = ""default""
    elif system == ""Darwin"":
        format_ = ""avfoundation""
        input_ = "":0""
    elif system == ""Windows"":
        format_ = ""dshow""
        input_ = _get_microphone_name()

    ffmpeg_command = [
        ""ffmpeg"",
        ""-f"",
        format_,
        ""-i"",
        input_,
        ""-ac"",
        ac,
        ""-ar"",
        ar,
        ""-f"",
        format_for_conversion,
        ""-fflags"",
        ""nobuffer"",
        ""-hide_banner"",
        ""-loglevel"",
        ""quiet"",
        ""pipe:1"",
    ]
```

to use user-supplied format input if provided, and OS specific defaults otherwise:

    input_ = input_device
   
    system = platform.system()
    if system == ""Linux"":
        format_ = ""alsa""
        if not input_:
            input_ = ""default""
    elif system == ""Darwin"":
        format_ = ""avfoundation""
        if not input_:
            input_ = "":0""
    elif system == ""Windows"":
        format_ = ""dshow""
        if not input_:
            input_ = _get_microphone_name()

    ffmpeg_command = [
        ""ffmpeg"",
        ""-f"",
        format_,
        ""-i"",
        input_,
        ""-ac"",
        ac,
        ""-ar"",
        ar,
        ""-f"",
        format_for_conversion,
        ""-fflags"",
        ""nobuffer"",
        ""-hide_banner"",
        ""-loglevel"",
        ""quiet"",
        ""pipe:1"",
    ]","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
feat: adding mplugdocowl,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],1,open
Add basic eval table logging for WandbCallback,"# What does this PR do?
This PR adds basic support for logging raw evals using `WandbCallback`.
1. Calling `trainer.evaluate()` will log an eval table with inputs and outputs; and
2. Logging is controlled by the `WANDB_LOG_EVALS` env var.

This also adds some changes to trainer internals to support this, including:
1. Updates `EvalLoopOutputs` to include `inputs`
2. Automatically adds a ref to the trainer when instantiating callbacks

Here's an example table that gets logged when the user calls `trainer.evaluate()`
![image](https://github.com/huggingface/transformers/assets/15385696/17918541-8b84-4437-918d-1d97d812ddf3)



<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts are you the right person to review?

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],10,open
Checkpoint saving by different evaluation criterias,"### Feature request

I would like to be able to save the best checkpoint separately for each criteria in the evaluation.
For example```save_best_checkpoint=[loss,bleu,rouge]``` then we can get 3 different best checkpoint in result repo.

### Motivation

There can be 2 different best check points when training a model. For example, LOSS might be the lowest at check point 10000 and BLEU SCORE might be the highest at check point 20000.  However, the current TrainingArgument doesn't seem to support saving for these two different best checkpoints as far as I know.

### Your contribution

I did not make any PR yet. 
I am just suggesting idea
Thank you!","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Whether the OutEffHop can support with Transfomers,"### Feature request

We request that the option ""OutEffHop"" be included in the BERT, OPT, and ViT models.

### Motivation

I am the author of OutEffHop and we plan to release our OutEffHop-based model on HuggingFace. However, because we use a different activation function (not softmax) in attention, if we directly use the original structure, the model inference online will show a huge problem.

### Your contribution

We are able to supply the code for OutEffHop and assist with integrating the function into BERT and OPT. Additionally, if necessary, we can also help incorporate OutEffHop into ViT. My initial idea is to add an option in the model configuration that allows the user to choose whether to use OutEffHop. If they opt to use it, they can employ the new activation function.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Add option to only install AutoTokenizer for production environment,"### Feature request

Add option to only install AutoTokenizer for production environment, so we can minimize the dependencies and footprint in a production system.

### Motivation

When we deploy LLM models to a Triton inference server, we often convert the model to a optimized engine for a specific backend (ONNX, TensorRT-LLM etc.), these optimized engine does not require the transformers library anymore. 

However, we are still required to install the full transformers library, as we need the AutoTokenizer to handle pre/post processing steps to handle tokenization.

### Your contribution

Not sure","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
sdpa for bert casues nan when using bfloat16 with padding.,"### System Info

* transformers: transformers==4.41.1
* pytorch: 2.3.0+cu121
* python: 3.10

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```python

In [1]: import torch.nn.functional as F

In [2]: import torch

In [3]: data = torch.load(""reproduce_data.pt"", map_location='cuda')

In [4]: with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True):
   ...:     print(F.scaled_dot_product_attention(data['q'], data['k'], data['v'], data['attn_mask'], data['dropout_p'], data['is_causal']).isnan().any((1
   ...: ,2)))
   ...:
tensor([[False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],               # <----- without pandding
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],               # <----- with pandding
        [ True,  True,  True,  ...,  True,  True,  True],               # <----- with pandding
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')

In [5]: with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):
   ...:     print(F.scaled_dot_product_attention(data['q'], data['k'], data['v'], data['attn_mask'], data['dropout_p'], data['is_causal']).isnan().any((1
   ...: ,2)))
   ...:
tensor([[False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')

# it's okay with offical math kernel
In [6]: with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):
   ...:     print(F.scaled_dot_product_attention(data['q'], data['k'], data['v'], data['attn_mask'], data['dropout_p'], data['is_causal']).isnan().any((1
   ...: ,2)))
   ...:
/root/.local/share/conda/envs/bytednlp/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
tensor([[False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        ...,
        [False, False, False,  ..., False, False, False],               # <----- with pandding, which is correct
        [False, False, False,  ..., False, False, False],               # <----- with pandding, which is correct
        [False, False, False,  ..., False, False, False]], device='cuda:0')

In [7]: mask = data['attn_mask']

# we use min / 2 as float('-inf')
In [10]: mask2 = mask.masked_fill(mask.bool(), torch.finfo(mask.dtype).min / 2)

In [11]: with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):
    ...:     print(F.scaled_dot_product_attention(data['q'], data['k'], data['v'], mask2, data['dropout_p'], data['is_causal']).isnan().any((1,2)))
    ...:
/root/.local/share/conda/envs/bytednlp/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
tensor([[False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        ...,
        [False, False, False,  ..., False, False, False],              # <----- with pandding, which the result is correct.
        [False, False, False,  ..., False, False, False],               # <----- with pandding, which the result is correct.
        [False, False, False,  ..., False, False, False]], device='cuda:0')
```



### Expected behavior

the output should without nan when using bfloat16 and sdap enabled.

I think it is safe to use `torch.finfo(dtype).min / 2 ` instead of `torch.finfo(dtype.min`.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",7,open
Trainer should throw a warning if max_sequence_length < number of tokens in dataset sample record.,"### Feature request

If the samples in the training dataset exceeds max_sequence_length the trainer should throw a warning.

### Motivation

Without warning devs are scratching their heads trying to understand why finetuning doesn't work for them.

### Your contribution

Willing to review the change.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Add Nomic Embed Code to Transformers,"### Model description

[Nomic Embed](https://arxiv.org/abs/2402.01613) embedding models are high performing long-context embedding models. 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://huggingface.co/nomic-ai/nomic-embed-text-v1
https://huggingface.co/nomic-ai/nomic-embed-text-v1.5
https://github.com/nomic-ai/contrastors","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Training GPT2 with run_clm.py exceeds the described memory amount .,"### System Info

- `transformers` version: 4.40.0.dev0
- Platform: Linux-6.5.0-28-generic-x86_64-with-glibc2.17
- Python version: 3.8.19
- Huggingface_hub version: 0.22.2
- Safetensors version: 0.4.2
- Accelerate version: 0.29.2
- Accelerate config:    not found
- PyTorch version (GPU?): 1.10.0+cu111 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

 @ArthurZucker and @younesbelkada

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

python run_clm.py \
    --model_name_or_path openai-community/gpt2 \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --overwrite_output_dir \
    --output_dir /tmp/test-clm

### Expected behavior

The example in the script mentions training with a K80 GPU at a batch size of 8, noting that the K80 has 24GB of memory. However, when I use an RTX 3090 with a batch size set to 4, it consumes 20GB of memory without modifying any settings. Why is this the case?","[{'id': 1936351150, 'node_id': 'MDU6TGFiZWwxOTM2MzUxMTUw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Examples', 'name': 'Examples', 'color': 'd4c5f9', 'default': False, 'description': 'Which is related to examples in general'}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",8,open
"[LLaMA3] 'add_bos_token=True, add_eos_token=True' seems not taking effect","### System Info

Platform = Windows
PyTorch = 2.3.0
Transformers = 4.41.0

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

```
import torch
from transformers import AutoTokenizer

LLaMAPath = '/path/to/llama3-8b'

# The following two yields the same results, all of them contains BOS token and no EOS token
tokenizer = AutoTokenizer.from_pretrained(LLaMAPath, add_bos_token=True, add_eos_token=True)
# tokenizer = AutoTokenizer.from_pretrained(LLaMAPath, add_bos_token=False, add_eos_token=False)

tokenizer.add_special_tokens({""pad_token"": ""<|reserved_special_token_0|>""}) 
inputs = tokenizer(['hi, how are you today?'], padding=True, return_tensors='pt')
print(inputs)
```

All of the statements above produce `[128000,   6151,     11,   1268,    527,    499,   3432,     30]`

### Expected behavior

I think when using `tokenizer = AutoTokenizer.from_pretrained(LLaMAPath, add_bos_token=True, add_eos_token=True)`, we get `[128000,   6151,     11,   1268,    527,    499,   3432,     30,  128001]`,

when using `tokenizer = AutoTokenizer.from_pretrained(LLaMAPath, add_bos_token=False, add_eos_token=False)`, we get `[6151,     11,   1268,    527,    499,   3432,     30]`,","[{'id': 1834056635, 'node_id': 'MDU6TGFiZWwxODM0MDU2NjM1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Tokenization', 'name': 'Core: Tokenization', 'color': 'FF4446', 'default': False, 'description': 'Internals of the library; Tokenization.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Add IRIS,"# What does this PR do?
This PR adds Iris, a Reinforcement learning agent for Sample Efficient RL

Fixes #30882


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@amyeroberts @younesbelkada @NielsRogge @kashif 

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

#### Ready for review!!!","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",14,open
Add IRIS,"### Model description

IRIS (Imagination with auto-Regression over an Inner Speech) is a Reinforcement learning agent trained in the imagination of a world model composed of a discrete autoencoder and an autoregressive Transformer. IRIS learns behaviors by accurately simulating millions of trajectories.

The approach presented in the paper casts dynamics learning as a sequence modeling problem, where an autoencoder builds a language of image tokens and a Transformer composes that language over time.

The agent is introduced in the paper titled [TRANSFORMERS ARE SAMPLE-EFFICIENT WORLD MODELS](https://arxiv.org/abs/2209.00588).

There is also a [medium blog post](https://medium.com/@cedric.vandelaer/paper-review-transformers-are-sample-efficient-world-models-d0f9144f9c09) to understand how the algorithm works.



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The model is adapted from the [official code repository](https://github.com/eloialonso/iris) of the paper.

The officially released weights can be found on this [Github repository](https://github.com/eloialonso/iris_pretrained_models)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Kosmos-2.5 implementation in transformers,"### Model description

Hello everyone,

The Kosmos-2.5 is a multimodal literate model that can be used for tasks such as OCR and text-rich image comprehension. It includes a ViT encoder, a Resampler, and a shared decoder module. To the best of my knowledge, the architecture of this model is similar to Kosmos-2 but has some differences. Due to these differences, using this model in Transformers requires a standalone implementation.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/pdf/2309.11419
Code: https://github.com/microsoft/unilm/tree/master/kosmos-2.5
Authors: @Dod-o @wolfshow","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",8,open
[WIP] Dynamic length in static cache,"# What does this PR do?

**The current version is a minimal change that works, maybe not the best way**

Current static cache is nice (when running with `torch.compile`). However, in each generation step, the new position (to be generated) computes the attentions against all positions in the cache, which is not optimal. In fact, we only need to compute the attentions against the positions prior the current position. 

This PR implement dynamic length computation with static cache, which work with `torch.compile`. The following table demonstrate the speedup gain (with `torch.compile`) of this implementation over the current `main` branch.

The correctness is verified by

> RUN_SLOW=1 TF_FORCE_GPU_ALLOW_GROWTH=true python3 -m pytest -v tests/models/gemma/test_modeling_gemma.py -k ""test_compile_static_cache""

The data below is based on 
<details>

<summary>this script</summary>

```python
import os
import torch
import datetime

from transformers import AutoTokenizer, AutoModelForCausalLM

token = ""ADD_YOUR_OWN_TOKEN""

os.environ[""TOKENIZERS_PARALLELISM""] = ""false""

batch_size = 1
n_iter = 5

ckpt = ""google/gemma-2b""

tokenizer = AutoTokenizer.from_pretrained(ckpt, token=token)
model = AutoModelForCausalLM.from_pretrained(ckpt, token=token, torch_dtype=torch.float16).to(""cuda"")

model.generation_config.max_new_tokens = 1024
model.generation_config.max_new_tokens = 1024

model.generation_config.cache_implementation = ""static""
model.forward = torch.compile(model.forward, mode=""reduce-overhead"", fullgraph=True)

input_text = ""Why dogs are cute.""
input_ids = tokenizer([input_text] * batch_size, return_tensors=""pt"").to(""cuda"")

for i in range(n_iter):
    s = datetime.datetime.now()
    outputs = model.generate(**input_ids, do_sample=False)
    t = datetime.datetime.now()
    e = (t-s).total_seconds()
    print(e)
```

</details>

with some modification to run it with different configurations, running on `A100` with `torch==2.3+cu121`.

# Benchmark

**I will re-run (part of) the benchmark as the following numbers are on top of of an older commit of `main`**

[benchmark data on the hub](https://huggingface.co/datasets/ydshieh/A100_cache_benchmark_dynamic_length/tree/main)

## Static cache compiled: full length v.s. optimal length (this PR)

### gemma-2b (18 layers)


| seq. length  | speedup |
| ------------- | ------------- |
| 1024 | 1.03 x |
| 2048 | 1.11 x |
| 4096 | 1.24 x |
| 8192 | 1.38 x |


","[{'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}, {'id': 7548528517, 'node_id': 'LA_kwDOCUB6oc8AAAABwe1nhQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-benchmark', 'name': 'run-benchmark', 'color': 'DD9AAA', 'default': False, 'description': ''}]",25,open
Add bounding box converter,"# What does this PR do?

Intorduces `output_bbox_format` parameter for `postprocess_for_object_detection`.
Adds the `convert_boxes` function to convert boxes from one format to another, e.g. from YOLO to Pascal VOC.

```python
boxes = convert_boxes(
    boxes, input_format=""relative_xcycwh"", output_format=""absolute_xyxy"", image_size=target_sizes
)
```

 - works with `np.array`, `torch.tensor`, `list`, `tuple` (could be easily extended to `jax` and `tf`)
 - works with single box shape=(4,) , image boxes shape=(N, 4), multiple images boxes List[(N, 4)]
 - supports boxes with extra metadata, e.g. `box=[x_min, y_min, x_max, y_max, color, class, ...]`
 - supports format aliases. e.g. `yolo`, `coco`
 
Supported input/output bounding box formats:
    - `absolute_xyxy` (aliases: `pascal_voc`, `xyxy`): [x_min, y_min, x_max, y_max]
    - `absolute_xywh` (aliases: `coco`, `xywh`): [x_min, y_min, width, height]
    - `absolute_xcycwh`: [center_x, center_y, width, height]
    - `relative_xyxy` (aliases: `albumentations`): [x_min, y_min, x_max, y_max] normalized to [0, 1] by image size
    - `relative_xywh`: [x_min, y_min, width, height] normalized to [0, 1] by image size
    - `relative_xcycwh` (aliases: `yolo`, `xcycwh`): [center_x, center_y, width, height] normalized to [0, 1] by image size

## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@amyeroberts could you please look at this draft, looks a bit overсomplicated, probably we should drop some functionality in favor to simplification.
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Integrate IndicTrans2 models and tokenizer into HF Transformers,"### Model description

[IndicTrans2 ](https://openreview.net/forum?id=vfT4YuzAYA)is a multilingual transformer model developed by AI4Bharat, and is available in 3 flavors: `indic-en`, `en-indic` and `indic-indic`. Each flavor has 2 versions, a large 1B model, and a distilled 200M model. The architecture is a standard transformer, very similar to NLLB and M2M models. However, the major difference is the vocabularies of the encoder and decoder and not shared, as they require different languages.

Unlike, NLLB and M2M models, IndicTrans2 required specific preprocessing for the inputs. Hence a [custom processor class has been developed](https://github.com/VarunGumma/IndicTransTokenizer), and is required for training/inference. More examples can be found in the aforementioned repository.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Authors: @AI4Bharat @jaygala24 @PranjalChitale @oneraghavan @VarunGumma @sumanthd17 @prajdabre @anoopkunchukuttan

Official GitHub Repository: [AI4Bharat/IndicTrans2](https://github.com/ai4bharat/IndicTrans2)

The HF compatible models and tokenizer are available here as of now:
- [indictrans2-en-indic-1B](https://huggingface.co/ai4bharat/indictrans2-en-indic-1B)
- [indictrans2-en-indic-dist-200M](https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M)
- [indictrans2-indic-en-dist-200M](https://huggingface.co/ai4bharat/indictrans2-indic-en-dist-200M)
- [indictrans2-indic-en-1B](https://huggingface.co/ai4bharat/indictrans2-indic-en-1B)
- [indictrans2-indic-indic-1B](https://huggingface.co/ai4bharat/indictrans2-indic-indic-1B)
- [indictrans2-indic-indic-dist-320M](https://huggingface.co/ai4bharat/indictrans2-indic-indic-dist-320M)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
tracker: `generate` composability refactor ,"## `generate` + composability = more use cases with minimal rewrites

As I write this issue, `generate` is mostly a sequential monolith. Many internal blocks were carved into functions over the last two years, but navigating there as a beginner is still messy. It is also very challenging to adapt `generate` to different tasks and/or modalities, forcing us to overwrite the entire generate function (e.g. [RAG](https://github.com/huggingface/transformers/blob/ccdabc5642bf84849af93f591e207dc625c8e1e1/src/transformers/models/rag/modeling_rag.py#L907), [MusicGen](https://github.com/huggingface/transformers/blob/ccdabc5642bf84849af93f591e207dc625c8e1e1/src/transformers/models/musicgen/modeling_musicgen.py#L1542)). All these aspects make using, documenting, maintaining, and testing `generate` a challenge.

This issue is a tracker for the refactor of `generate`, where we aim to build the structure outlined in [this board](https://miro.com/app/board/uXjVNgMGfaQ=/). Key ideas for this refactor:
👉 All models can use the base `generate` API
👉 Reduce if/else blocks
👉 Reduce the barriers to entry for new decoding methods/modalities/use cases
👉 Reduce per-model overwrites when possible
👉 Add unit tests
👉 Add documentation regarding the structure of `generate`

### Tasks

- [ ] 1. Isolate prefill into a separate function, pulling it from the decoding methods. Note that 
    - a) prefill is done excluding the latest token (`input_ids[:, -1:]`), so we don't compute variables regarding the latest token twice; 
    - b) prefill only runs when `use_cache=True` and cache length < input length - 1; 
    - c) `_expand_inputs_for_generation` needs to be changed (it copied inputs before prefill, we will need to copy prefill outputs)
- [ ] 2. (depends on 1.) Separate generate on the 5 stages described in the diagram, passing around the data structures described therein
- [ ] 3. (depends on 1.) Streaming 2.0
    - a) Add option to `yield`/`yield from` instead of `return` 
    - b) Deprecate the old streamer classes; 
    - c) Add a new class to print the stream into the screen. For beam methods, build a class that prints up to the point where all beams agree with each other.
    - d) thoroughly document and communicate this feature
    - e) enable streaming into the screen with `pipeline`
- [ ] 4. (depends on 2.) Separate stage 1 into a set of functions as described in the diagram. Add unit tests.
- [ ] 5. (depends on 2.) Separate stage 2 into a set of functions as described in the diagram. Add unit tests. Move the preparation of common model inputs here, such as `position_ids`.
- [ ] 6. (depends on 2.) Separate stage 3 into a set of functions as described in the diagram. Add unit tests. Deprecate `LogitsWarper` in this step (it's a copy of `LogitsProcessor`)
- [ ] 7. (depends on 2.) Separate stage 5 into a set of functions as described in the diagram. Add unit tests.
- [ ] 8. Add a new document page walking through the structure of `generate`

[From this point onwards the tasks are only a sketch, need more detailed planning when we get there]
- [ ] 9. Reduce if/elses through templates (e.g. LLMs have a certain default for `prepare_inputs_for_generation`, VLMs also have their special preprocessing steps, ...)
- [ ] 10. Play around with caching of some blocks to determine whether it speeds up generation
- [ ] 11. Rework `prepare_inputs_for_generation` ?
- [ ] 12. Remove `generate` from models that have a custom implementation
- [ ] (other tasks, TBD)","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",9,open
Add data2vec 2.0,"### Model description

Hello,

The data2vec 2.0 paper has been released quite a while and achieved impressive performance across different modalities: speech, text, and image (results similar or better than data2vec 1.0 but much more efficient). Especially, the audio-only model seems to be one of the best SSL speech models using base architecture (93M parameters). Therefore, I think it would be a nice addition to the `transformers` library.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/abs/2212.07525
Code: https://github.com/facebookresearch/fairseq/tree/main/examples/data2vec
Authors: @alexeib @michaelauli @wnhsu ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",4,open
Add DAB-DETR Object detection/segmentation model,"# What does this PR do?

Add DAB-DETR Object detection model. Paper: https://arxiv.org/abs/2201.12329
Original code repo: https://github.com/IDEA-Research/DAB-DETR

Fixes # (issue)
[WIP] This model is part of how DETR models have evolved, alongside DN DETR (not part of this PR), to pave the way for newer and better models like Dino and Stable Dino in object detection


## Who can review?
@amyeroberts


","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",44,open
new model request: DeepSeek-V2,"### Model description

Tech Report: https://arxiv.org/abs/2405.04434
Code: https://huggingface.co/deepseek-ai/DeepSeek-V2/blob/e0828e3cc0a03408724b80c3cc92c8e072db8d01/modeling_deepseek.py

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Implement kv cache sparsity like H2O with attention score,"### Feature request

Hello!

It is a bit like [#26553](https://github.com/huggingface/transformers/issues/26553), which implement `SinkCache`. I would love to see some method of kv cache sparsity like **H2O** implemented, as proposed in [http://arxiv.org/abs/2405.04434](http://arxiv.org/abs/2405.04434).

The authors have release the code here: [https://github.com/FMInference/H2O](https://github.com/FMInference/H2O).

People can use it like:
```python
from transformers import AutoModelForCausalLM AutoTokenizer, H2O_Cache

cache = H2O_Cache(recent_length=512, HH_length=512)
gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=3000, past_key_values=cache)
```

### Motivation

<img width=""899"" alt=""image"" src=""https://github.com/huggingface/transformers/assets/63134210/468abafb-ab40-4818-9fe3-1c156c9f2e4d"">

> Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores.
> a KV cache eviction policy that dynamically retains a balance of recent and H2 tokens

### Your contribution

I would love to help implement this into transformers.

It is not only implement a `H2Ocache` in `src/transformers/cache_utils.py`, but also change the order of some code in `LlamaAttention#forward` function, so `Cache#update` can get the attention score, which some method of kv cache sparsity like [snapKV](http://arxiv.org/abs/2404.14469) and future work also need.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6735206706, 'node_id': 'LA_kwDOCUB6oc8AAAABkXMZMg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Cache', 'name': 'Cache', 'color': '1EA506', 'default': False, 'description': ''}]",2,open
TokenClassificationPipeline support is_split_into_words tokeniser parameter,"### Feature request

The TokenClassificationPipeline currently sets a hardcoded tokeniser config within it sanitiser method. This prevents users from passing their own config to the tokeniser. 

It would be good to support some user input for tokeniser config. Especially for is_split_into_words as input data may be split already.

### Motivation

It is common for token classification datasets to be split into words already so that they match their labels. 

### Your contribution

I naivley anticipate this being a simple change, so I am happy to submit a PR for it. Though it would first be nice to see a discussion surrounding the feature and if it fits with the goals of Transformers.

","[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNzcxMTg3OTI0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Pipeline', 'name': 'Core: Pipeline', 'color': 'FF7066', 'default': False, 'description': 'Internals of the library; Pipeline.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Mixtral past_key_values and output_router_logits incompatible,"### System Info

transformers==4.40.2
Python 3.11.8


### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```python
from transformers import MixtralConfig, MixtralForCausalLM, AutoTokenizer
import torch
# Initializing a smaller version of Mixtral for faster execution
configuration = MixtralConfig(
    hidden_size=256,
    intermediate_size=896,
    num_hidden_layers=8,
    num_attention_heads=8,
    num_key_value_heads=8,
    num_local_experts=4,
    num_experts_per_tok=1,
)

model = MixtralForCausalLM(configuration)
tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mixtral-8x7B-v0.1"")
prompt = ""This is a test""
tokenized = tokenizer(prompt, return_tensors=""pt"")
output = model(**tokenized, output_router_logits=True)
key_values = output.past_key_values
logits = output.logits
next_token_logits = logits[..., -1, :]
# Softmax
softmaxed = torch.nn.functional.softmax(next_token_logits, dim=-1)
# Sample
sampled = torch.multinomial(softmaxed.squeeze(), num_samples=1)
ids = sampled.item()

attention_mask = torch.cat([tokenized[""attention_mask""], torch.tensor([[1]])], dim=-1)
next_output = model(
    torch.tensor([[ids]]),
    attention_mask=attention_mask,
    past_key_values=key_values,
    output_router_logits=True
)
```

### Expected behavior

It seems that this is the same underlying issue as in #29087 - I would expect `past_key_values` to work with `output_router_logits`. 
So what happens?
1. Without past key values (and with multiple input ids) the `all_router_logits` has the proper sequence length, thus in `load_balancing_loss_func` this `num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)` correctly evaluates the number of hidden layers.
2. If past key values are used, `all_router_logits` has a sequence length of 1, but since the attention mask is still the whole sequence (from which the `sequence_length` is inferred) the hidden layers evaluate to a small value or 0, leading to the same error as in #29087

Instead, I would like the `load_balancing_loss_func` to be able to deal with a case where the `gate_logits` passed are of shape `[batch_size X 1, num_experts]` instead of `[batch_size X sequence_length, num_experts]`.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",4,open
Support for Multiple Datasets and Domain-Specific Loss Calculation in Trainer,"### Feature request

I am currently working on a project that involves sequence level distillation across multiple domains, requiring the handling of separate datasets for each domain within a single training loop. Specifically, the challenge involves integrating data from four distinct domains, computing loss individually per domain, and then aggregating these losses into a global loss measure that can guide the overall training process.

### Motivation

Ideally, the Trainer class would natively support the following features:

Multiple Dataset Handling: Ability to pass multiple datasets into the Trainer directly, with each dataset potentially representing a different domain.
Domain-Specific Loss Calculation: Support for defining and computing loss separately for each domain's dataset within the training loop and then integrating these losses into a global training objective.

### Your contribution

Currently, the Trainer class in the Transformers library supports passing a single dataset for training and evaluation. To handle multiple datasets or to calculate domain-specific losses, one must subclass the Trainer and override methods such as compute_loss, which complicates the implementation and integration of domain-specific training strategies.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",16,open
Add TableTransformerImageProcessor,"### Feature request

The [Table Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer) is a model with basically the same architecture as [DETR](https://huggingface.co/docs/transformers/main/en/model_doc/detr).

Now, when people do this:
```
from transformers import AutoImageProcessor

processor = AutoImageProcessor.from_pretrained(""microsoft/table-transformer-detection"")
print(type(processor))
```
this will print `DetrImageProcessor`.

However, Table Transformer has some specific image processing settings which aren't exactly the same as in DETR:
```
from torchvision import transforms

class MaxResize(object):
    def __init__(self, max_size=800):
        self.max_size = max_size

    def __call__(self, image):
        width, height = image.size
        current_max_size = max(width, height)
        scale = self.max_size / current_max_size
        resized_image = image.resize((int(round(scale*width)), int(round(scale*height))))
        
        return resized_image

# this is required for the table detection models
detection_transform = transforms.Compose([
    MaxResize(800),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# this is required for the table structure recognition models
structure_transform = transforms.Compose([
    MaxResize(1000),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
```
Hence we could create a separate `TableTransformerImageProcessor` which replicates this.

### Motivation

Would be great to 100% replicate original preprocessing settings

### Your contribution

I could work on this but would be great if someone else can take this up","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",4,open
Refusal rejection removal as a feature,"### Feature request

It seems to be possible to add or remove 'intentions' or directions in trains of 'thought' in LLMs via a relatively easy method. They suggest both an on the fly measure and to simply change the weights. It's not perfect, but likely a useful tool. 
You could for example try to get rid of certain stereotypes by that method or make a model permanently 'smarter', i.e. reasoning more like an expert. 

### Motivation



[Refusal in LLMs is mediated by a single direction](https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction)

### Your contribution

Hope you have a nice day","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Adding imagebind,"# What does this PR do?

This PR fixes https://github.com/huggingface/transformers/issues/23240 by adding `ImageBind` model. 

This is based on https://github.com/huggingface/transformers/pull/26310 which is currently stale and the author said it would not have time to work on it (though welcome to help @dg845 ).

Taking into consideration the points raised by @dg845 here https://github.com/huggingface/transformers/pull/26310#issuecomment-1907231272 I'll focus on adding the text/image/audio portion and try to contact the authors.

## Who can Review
@amyeroberts (?)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",25,open
Add Prismatic VLMs to Transformers,"### Model description

Hi! I'm the author of [""Prismatic VLMs""](https://github.com/TRI-ML/prismatic-vlms), our upcoming ICML paper that introduces and ablates design choices of visually-conditioned language models that are similar to LLaVa or InstructBLIP, introducing ~50 new VLMs at the 3B/7B/13B scale that are trained with:

- Different Visual Representations (`CLIP`, `SigLIP`, `DINOv2`, fusions thereof like `SigLIP + DINOv2`)
- Different LLM Backbones (`LLaMa2`, `Vicuña v1.5`, `Mistral v0.1`, `Mistral v0.1 Instruct`, `Phi-2`, etc.)
- Different Data (e.g., the LLaVa v1.5 Data, LVIS-Instruct-4V, and more upcoming!)

Our best models outperform LLaVa v1.5 given the same data/same scale on a wide spectrum of different evaluation tasks; furthermore, we're seeing a lot of folks adopt our code for their research into new data mixtures, scaling to different LLM/Vision backbones, new projection mechanisms, and more. 

I think it'd be awesome to support these in `transformers` -- especially to tap into existing tooling for loading quantized versions of models, using `PEFT` and other tools in the HF ecosystem for adaptation/fine-tuning, and general usability of our trained models.

While we have 50+ checkpoints (all open-sourced, and loadable in our library), all currents models share a pretty common interface of using some pretrained visual extractor from `timm`, a `XXXForCausalLM` from `transformers`, and a lightweight `nn.Module` to project visual features into the LLM embedding space. As such, I'm hoping to contribute a general `modeling_prismatic.py` class that implements `PrismaticPretrainedModel` and `PrismaticForConditionalGeneration` that properly instantiates the appropriate VLM instance using the dependencies already in `transformers`.

---
I'm happy to get started with this, following the [instructions here](https://huggingface.co/docs/transformers/add_new_model#how-to-add-a-model-to--transformers), but would love help/advice on clean ways to support all the various image backbones / LLM backbones / preprocessing schemes, and verifying compatibility with existing HF tooling!

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Prismatic Authors: @siddk, @ashwin-balakrishna96
(Potentially) Relevant Folks at HF: @merveenoyan @NielsRogge 

Useful Links: 
- [Loading Pretrained Models (in Prismatic)](https://github.com/TRI-ML/prismatic-vlms/blob/main/prismatic/models/load.py#L47)
- [Lightweight Generation REPL](https://github.com/TRI-ML/prismatic-vlms/blob/main/scripts/generate.py)
- [12 Task Evaluation Suite from Paper (for unit-testing, fully reproduces all results from our paper)](https://github.com/TRI-ML/vlm-evaluation)
- [Existing Checkpoints on HF Hub](https://huggingface.co/TRI-ML/prismatic-vlms/tree/main)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
Community contribution: enable dynamic resolution input for more vision models.,"### Feature request

Some of our models interpolate its positional embeddings, enabling pretrained checkpoints to be used on different input resolutions. For example, [here in ViT](https://github.com/huggingface/transformers/blob/75bbfd5b2237b7e35a9265731ecf63022579e7e2/src/transformers/models/vit/modeling_vit.py#L79). 

- [x] [beit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/beit/modeling_beit.py) & [data2vec](https://github.com/huggingface/transformers/blob/main/src/transformers/models/data2vec/modeling_data2vec_vision.py) @OmarManzoor #31053
- [x] [blip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/modeling_blip.py), [blip_2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py) @zafstojano #30722
- [x] [clip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py) and clip related models: [altclip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py), [bridgetower](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bridgetower/modeling_bridgetower.py), [chinese_clip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/chinese_clip/modeling_chinese_clip.py), [git](https://github.com/huggingface/transformers/blob/main/src/transformers/models/git/modeling_git.py), [kosmos2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/kosmos2/modeling_kosmos2.py) ~#30783~ #32600
- [x] [deit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/deit/modeling_deit.py) #31131 
- [ ] [owlvit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py), [owlv2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlv2/modeling_owlv2.py) @yMayanand https://github.com/huggingface/transformers/pull/34764
- [x] [perceiver](https://github.com/huggingface/transformers/blob/main/src/transformers/models/perceiver/modeling_perceiver.py) @g1y5x3 #30979 
- [x] [siglip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/siglip/modeling_siglip.py) @davidgxue https://github.com/huggingface/transformers/pull/30719
- [x] [swin](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swin/modeling_swin.py), [donut](https://github.com/huggingface/transformers/blob/main/src/transformers/models/donut/modeling_donut_swin.py), [maskformer swin](https://github.com/huggingface/transformers/blob/main/src/transformers/models/maskformer/modeling_maskformer_swin.py), [swinv2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swinv2/modeling_swinv2.py) #30656 @the-neural-networker 
- [x] [tvp](https://github.com/huggingface/transformers/blob/main/src/transformers/models/tvp/modeling_tvp.py) @bhuvanmdev #30863
- [x] [vit_mae](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit_mae/modeling_vit_mae.py) #30657 #30732 @bhuvanmdev 
- [x] [vivit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vivit/modeling_vivit.py) #30630 @jla524 

### Motivation

Let's add this to more models, to leverage existing checkpoints for new cases! 

### Your contribution

For anyone who would like to contribute, please comment on the issue, claiming a model you'd like to work on and share a link to the PR. 

Each PR should: 
* Add an `interpolate_pos_encoding` method
* Add a test showing the model can correctly interpolate an input image of a different size

There was a PR opened to add this to CLIP models, which is now inactive, but useful for reference of the changes to make: https://github.com/huggingface/transformers/pull/27457

Once the PR is ready, you can ping me for review 🤗 ","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",40,open
Add ViTPose,"# What does this PR do?

This PR adds ViTPose as introduced in [ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://arxiv.org/abs/2204.12484).

Here's a demo notebook - note that the API might change: 
 - https://colab.research.google.com/drive/15_3gjcC0wtKSH85k76zewt81eUJIEWWA?usp=sharing.
 - https://colab.research.google.com/drive/1e8fcby5rhKZWcr9LSN8mNbQ0TU4Dxxpo?usp=sharing (supervision visualization)

To do:

- [x] get rid of cv2 dependency (?)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",44,open
Use models as Seq2Seq model,"### System Info

- `transformers` version: 4.40.0
- Platform: Linux-6.1.58+-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.22.2
- Safetensors version: 0.4.3
- Accelerate version: 0.29.3
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.2.1+cu121 (False)
- Tensorflow version (GPU?): 2.15.0 (False)
- Flax version (CPU?/GPU?/TPU?): 0.8.2 (cpu)
- Jax version: 0.4.26
- JaxLib version: 0.4.26
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No


### Who can help?

@ArthurZucker  @muellerzr @stevhliu

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

There is this snippet in many model documentations:
To be used in a Seq2Seq model, the model needs to initialized with both is_decoder=True and bidirectional=False argument as well as add_cross_attention set to True; an encoder_hidden_states is then expected as an input to the forward pass.

I try it like this for the MEGA model:
```
from transformers import MegaConfig

# config for a small seq2seq model like in the MEGA paper
config = MegaConfig(
    vocab_size=vocabulary_size,
    max_position_embeddings=context_length,
    is_decoder=True,
    bidirectional=False,
    add_cross_attention=True
)

from transformers import AutoTokenizer, MegaModel,MegaForCausalLM

model = MegaModel(config=config)
# only the causalLM as decoder-only seems to run
#model = MegaForCausalLM(config=config)
```
The following error occurs, when training it with `Seq2SeqTrainer`,`Seq2SeqTrainingArguments` and `DataCollatorForSeq2Seq`: 
```
[transformers/models/mega/modeling_mega.py](https://localhost:8080/#) in forward(self, hidden_states, attention_mask, causal_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, use_cache)
   1271         if self.cross_attn is not None:
   1272             if encoder_hidden_states is None:
-> 1273                 raise ValueError(""Requested cross-attention without providing encoder hidden states"")
   1274 
   1275             cross_attn_outputs = self.cross_attn(

ValueError: Requested cross-attention without providing encoder hidden states
```


### Expected behavior

Train MEGA like a seq2seq as in their paper.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Support align_corners=True in image_transforms module,"### Feature request

For a new model I'm working on #30136 I'd need to resize images in the image processor using `align_corners=True`, as the original code uses `torch.nn.functional(..., align_corners=True)` for resizing images during pre-processing.

### Motivation

Would be great to have this option available so that we can remove the torch dependency from the image processor

### Your contribution

Not sure I can look into this, but @molbap showed interest in looking into this","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",2,open
Image + text + audio uniform processors,"# What does this PR do?

This PR is a stab at uniformizing the processors across all transformers models. If we are happy with the design, I'll expand it to all existing models. It only touches on some text + audio and text + images models as experiment subjects. Linked with #28711 which has a larger scope, and several previous discussions with team members. 

## Usage

As before, kwargs that are passed to processors at `__call__` time take priority. However, per-modality processors can be instantiated with their own kwargs, and if they are not overriden at call time, they will serve as defaults. 

Type hinting of kwargs is preserved if they are passed as structured dictionary entries
![image](https://github.com/huggingface/transformers/assets/39954772/c859e4b0-5c08-49c6-bd65-6e71d3f6c9f0)

It also works with kwargs passed without nesting:
![image](https://github.com/huggingface/transformers/assets/39954772/aa675d30-ca45-466a-86c9-41066a064c60)

Merging of kwargs and handling priority order is done in `processing_utils` through a dedicated method. 
The order of operations is as follows:
1) kwargs passed as before have highest priority to preserve BC.
```python
high_priority_kwargs = {""crop_size"" = (224, 224), ""padding"" = ""max_length""}
processor(..., **high_priority_kwargs)
```
2) kwargs passed as modality-specific kwargs have second priority. This is the recommended API.
```python
processor(..., text_kwargs={""padding"": ""max_length""}, images_kwargs={""crop_size"": (224, 224)}})
```
3) kwargs passed during instantiation of a modality processor have fourth priority.
```python
tokenizer = tokenizer_class(..., {""padding"": ""max_length""})
image_processor = image_processor_class(...)
processor(tokenizer, image_processor) # will pass max_length unless overriden by kwargs at call
```
4) defaults kwargs specified at processor level have lowest priority.

```python
class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwargs, total=False):
    _defaults = {
        ""text_kwargs"": {
            ""padding"": ""max_length"",
            ""max_length"": 64,
        },
    }
```
## What changes:
- ~~There is now an attribute in the constructor that stores the processing kwargs needed to be passed to various encoders down the line. ~~ Not anymore, see next point
- There is now a base `ProcessingKwargs` TypedDict of kwargs that inherits from `ImagesKwargs`, `TextKwargs`, and so on.
- These nested attributes (one dictionary for text, one for images, one for audio, one for video) are typed with a `TypedDict` that does not need to be total. We can expand this typed dict (in `processing_utils`) as processors get uniformized. 
- The processors are called with the **same kwargs signature, text, images, audio, videos**. `kwargs` set by a user will always override default processing kwargs. 
- Slicing of positional args is removed and replaced by named kwargs corresponding to the modality. Order of kwargs is not constant as a consequence to preserve BC.
- Inputs (text, images, audio, videos) are now always typed in the `call`. 

## What that allows:

- We know each model has its own processing logic/way to mix inputs. This way of typing both the modalities sent to the processors AND the processing arguments allow a faster design of future processors, hence faster review, and faster merge, better usage.
- The reason for that is that we push away from the function call the actual mixing of modalities and their specific processing, which can be handled explicitly with the kwargs passed. 
- With TypedDict, type hints are preserved even in the nesting. I hesitated with pydantic/dataclasses and opted for TypedDict _because_ it is less flexible, and we want to enforce a standard.

 

## Limitations:

- This still relies on kwargs for the processing.  
- Few models tested on that PR - more will follow in other PRs.

## Tests pass (I think). What's missing:

- [x] A consistent way (with maybe logger.warning_once) to warn a user they are using a deprecated call. 
- [x] Typing of input modalities need to be improved and be more flexible especially for audio&videos
- [x] A test in the CI to check that the enforced designed is respected for a new model added. This can be done in a separate PR.
- [ ] All models PRs:
    - [x] Initial design + Align model #31197 
    - [x]  Altclip https://github.com/huggingface/transformers/pull/31198
    - [x] Blip https://github.com/huggingface/transformers/pull/31368
    - [x] Blip2 https://github.com/huggingface/transformers/pull/31368
    - [x] Bridgetower https://github.com/huggingface/transformers/pull/31368
    - [x] Chinese_clip https://github.com/huggingface/transformers/pull/31198
    - [ ] Donut
    - [ ] wav2vec2



## Who can review?

Models:

- vision models: @amyeroberts
- speech models: @sanchit-gandhi
",[],7,open
Add trainer integration test for llava to ensure accelerate autocasting works correctly,"# What does this PR do?
This PR adds a new integration test to ensure the accelerate autocasting is working correctly. This came out of a discussion found [here](https://github.com/huggingface/transformers/pull/29721#discussion_r1567192103) and that PR should probably be merged first (or this one merged into that one).

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
      Not an issue but another PR that should probably be merged first: https://github.com/huggingface/transformers/pull/29721#discussion_r1567192103
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],5,open
add `stream` to pipeline parameters,"### Feature request

add option to stream output from pipeline

### Motivation

using `tokenizer.apply_chat_template` then other stuff then `model.generate` is pretty repetitive and I think it's time to integrate this with pipelines, also it's time to add a **streaming** pipeline too.

### Your contribution

I can provide this resource as a reference.
This is a pr I made with the requested feature https://huggingface.co/google/gemma-1.1-2b-it/discussions/14.
another tip I can provide is don't use **yield** and **return** in the same function, you should separate them (it's a python problem) 
sadly I'm a bit busy lately to open a PR, but if I could find some time I'll try to help out.","[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNzcxMTg3OTI0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Pipeline', 'name': 'Core: Pipeline', 'color': 'FF7066', 'default': False, 'description': 'Internals of the library; Pipeline.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",9,open
Move out input validation into base image processor,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
[WIP] adds AudioMAE model,"Fixes https://github.com/huggingface/transformers/issues/27453 Audio-MAE - ViTMAE for audio.

@ArthurZucker @sanchit-gandhi @ylacombe @pennychong94

There are some nit changes pending. Looking forward to push within 2-3 days.

Thanks @NielsRogge for awesome tutorials.


","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",3,open
(Willing to PR) Make `tokenizer.padding_side` an argument instead of only being a field,"### Feature request

Hi thanks for the library! When using tokenizer, for example, for batch-generation with GPT2 (in https://discuss.huggingface.co/t/batch-generation-with-gpt2/1517), it seems that currently I have to do something like:

```
tokenizer.padding_side = 'left'
data = tokenizer(['sentence one', 'another'])
tokenizer.padding_side = 'right'
```

Therefore, it would be great to have:

```
data = tokenizer(['sentence one', 'another'], padding_side = 'left')
```

just like what we do today for many options like `padding_strategy` etc.

### Motivation

(see above)

### Your contribution

Yes, I am willing to PR","[{'id': 1834056635, 'node_id': 'MDU6TGFiZWwxODM0MDU2NjM1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Tokenization', 'name': 'Core: Tokenization', 'color': 'FF4446', 'default': False, 'description': 'Internals of the library; Tokenization.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Mamba Models - Missing MambaForSequenceClassification,"### Feature request

Many prominent transformers (e.g. BERT) under the transformers library have specific implementations for SequenceClassification. I believe implementing these could potentially save a huge amount of time in the community.

### Motivation

I have been using the transformers library for tasks in biology or healthcare that required elegant refactoring of the current transformers library which has made me quite familiar with the internal infrastructure used to connect everything. Recently, I came upon the Mamba model and it seems these sub-models are missing.

### Your contribution

I am currently implementing MambaforSequenceClassification using the standard transformers library design patterns and will make a PR. You could assign this issue to me!","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",9,open
Remove `mps` workaround for `isin()`,"### Feature request

Remove `mps` workaround for `isin()`

### Motivation

#30376 introduced a workaround for `isin()` on `mps` devices, because PyTorch does not support that op yet: https://github.com/pytorch/pytorch/issues/77764#issuecomment-2067838075.

Going forward, it'd be desirable to use the much more readable `isin()` version. This issue is meant to track PyTorch support of `isin()` on `mps` so we can remove the workaround and simplify the code.

### Your contribution

I can submit a PR when the op is eventually supported.","[{'id': 1862634478, 'node_id': 'MDU6TGFiZWwxODYyNjM0NDc4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Should%20Fix', 'name': 'Should Fix', 'color': 'FF0000', 'default': False, 'description': 'This has been identified as a bug and should be fixed.'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Add HelpingAI-3B-v2.2: Emotionally Intelligent Conversational AI,"### Model description

## Introduction

HelpingAI-3B-v2.2 is a state-of-the-art large language model specializing in emotionally intelligent conversation. With advanced emotional understanding capabilities, it can engage in empathetic dialogue tailored to the user's emotional state and context.

## Emotional Intelligence Capabilities

HelpingAI-3B-v2.2 exhibits several key traits that enable emotionally resonant responses:

- Emotion recognition and validation
- Empathetic perspective-taking 
- Generating emotionally supportive language
- Contextual emotional attunement
- Using appropriate tone, word choice and emotional expression

Whether comforting someone grieving, celebrating positive news, or addressing complex feelings, HelpingAI-3B-v2.2 can adapt its communication style with emotional nuance.

## Examples of Emotionally Intelligent Responses


""I'm really sorry to hear about your friend's loss. 😔 Losing a parent can be incredibly difficult and traumatic, and it's completely understandable that they are struggling with grief. It's important to show them empathy and support during this tough time. You can start by simply being there for them, listening to them, and offering a shoulder to lean on. 💕 It's also important to acknowledge their feelings and let them know that it's okay to grieve and take as much time as they need to process their emotions. Offering practical help, such as running errands or cooking for them, can also make a big difference. Remember, small gestures of kindness and compassion can go a long way in comforting someone during difficult times.""

""Hey there! 🌟 It's a beautiful day today, so you should cook something delicious and enjoy the sunny weather! How about trying out a new recipe and let your creativity flow? Whether it's a classic dish or something experimental, the kitchen is the perfect place to let your imagination run wild. Write me a list of ingredients you have on hand, and I'll help you come up with a wonderful culinary creation! 🍲🌞""

The model tailors its language, tone and emotional content to be contextually appropriate, combining emotional intelligence with factual knowledge and practical suggestions.



### Open source status

- [ ] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation
https://huggingface.co/collections/OEvortex/emotional-intelligence-6625cc3247851d717e9fa382","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Update `make-fixup` to make sure image processor tests present,"### Feature request

Update `make-fixup` to make sure image processor tests present

See [this comment](https://github.com/huggingface/transformers/pull/30136#issuecomment-2068884321)



### Motivation

Make `make-fixup` more robust

### Your contribution

I can work on it","[{'id': 1834056761, 'node_id': 'MDU6TGFiZWwxODM0MDU2NzYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Modeling', 'name': 'Core: Modeling', 'color': 'FF8446', 'default': False, 'description': 'Internals of the library; Models.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
[DynamicHead - Microsoft] Implementation request,"### Model description

Hi there! 
I was wondering if anyone has tried to implement Microsoft's DyHead? If not, I would like to contribute the implementation by adding a new model to the library. Is there any interest in a PR for this model?

Here a short explanation from their official implementation:
_""In this paper, we present a novel dynamic head framework to unify object detection heads with attentions. By coherently combining multiple self-attention mechanisms between feature levels for scale-awareness, among spatial locations for spatial-awareness, and within output channels for task-awareness, the proposed approach significantly improves the representation ability of object detection heads without any computational overhead.""_

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

GitHub Repository: [Link](https://github.com/microsoft/DynamicHead/tree/master)
Paper: [Link](https://arxiv.org/pdf/2106.08322.pdf)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Flash Attention Support for Blip2ForConditionalGeneration ,Is there anyone working on a FlashAttention support for Blip2ForConditionalGeneration?,"[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}]",1,open
Schedule Free Optimizers (pytorch) and Sophia optimizer,"### Feature request

See <https://github.com/facebookresearch/schedule_free> and <https://github.com/Liuhong99/Sophia> -- These optimizers have very different properties and are often useful over the existing choices.

### Motivation

Schedule free optimizer (especially schedule-free SGD) offers increased convergence over the existing implementation with no additional memory requirements. There's some variability on the end gradients but this is an equitable engineering tradeoff in many cases. In the same vein, sophia has been shown to converge twice as fast as adam on some language modeling tasks.

### Your contribution

These would add in additional dependencies if included straight on, I'm not sure what the typical approach for this is. I'd be willing to implement both features but I'm unsure how the addition of dependencies should go.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6712797012, 'node_id': 'LA_kwDOCUB6oc8AAAABkB0nVA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/optimization', 'name': 'optimization', 'color': '47E0D2', 'default': False, 'description': ''}]",2,open
If a training job job failed MLFlow will not be reported and MLFlow shows job still running,"### System Info

- `transformers` version: 4.40.0
- Platform: Linux-6.8.6-arch1-1-x86_64-with-glibc2.39
- Python version: 3.11.7
- Huggingface_hub version: 0.20.3
- Safetensors version: 0.4.2
- Accelerate version: 0.27.0

- PyTorch version (GP?): 2.2.2+rocm5.7 (True)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: True
- Using distributed or parallel set-up in script?: <False

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

1. Setup MLFlow integration correctly,
2. Run a job.
3. The job failed due to OOM error.
4. Go to MLFlow UI and the job experiment shows status ""Running""

### Expected behavior

MLFlow callback should report the job as failure and call end_run() instead of keeping ""Running ""status.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 6675366189, 'node_id': 'LA_kwDOCUB6oc8AAAABjeIBLQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Integrations', 'name': 'Integrations', 'color': '051E90', 'default': False, 'description': ''}]",6,open
Badam support,"### Feature request

https://arxiv.org/pdf/2404.02827.pdf

Memory efficient optimizer like galore but with less hyperparameters 

### Motivation

https://github.com/hiyouga/LLaMA-Factory/pull/3287

LlamaFactory supports it, but having it in transformers is much more convenient

### Your contribution

-","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6712797012, 'node_id': 'LA_kwDOCUB6oc8AAAABkB0nVA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/optimization', 'name': 'optimization', 'color': '47E0D2', 'default': False, 'description': ''}]",2,open
Support multiple `timm` as backbones with `forward_intermediates` method,"### Feature request

Currently, in `TimmBackbone` requires that the `timm` model has a `return_layers`, which I'm not sure if any model implements:
https://github.com/huggingface/transformers/blob/c15aad0939e691d2ffdbac7ae71921b51fe04e3f/src/transformers/models/timm_backbone/modeling_timm_backbone.py#L80-L83

Lately, `timm` implemented  a `forward_intermediates` method to almost all networks for  feature extraction - [issue](https://github.com/huggingface/pytorch-image-models/issues/2131) and [pr](https://github.com/huggingface/pytorch-image-models/pull/2136).

### Motivation

Thus, will little changes, we will be able to do
```py
import transformers

model = transformers.Mask2FormerModel(
    config=transformers.Mask2FormerConfig(
        use_timm_backbone=True,
        backbone=""vit_small_patch16_224"",
    ),
)
```

as currently we get the error:
```sh
AttributeError: 'FeatureGetterNet' object has no attribute 'return_layers'
```

### Your contribution

Review and/or implemented the PR","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",4,open
Token merging for LLM Inference,"### Feature request

I propose to create an Optimized Inference class for LLM where the input sequence is merged using SLERP to process smaller sequences. I did experiments on Mistral 7B Instruct and it turns out that you still have decent results while processing up to 2x smaller sequences.
The code is here https://github.com/samchaineau/llm_slerp_generation and I made it usable in a text-generation pipeline

![demo](https://github.com/huggingface/transformers/assets/47329252/1b10d269-c61d-4c7d-a499-dfbcf31b27bf)


### Motivation

While we are improving a lot on the training side, LLMs still generate in a brute force manner. From the experiments, processing the full sequence does not necessarily help to better predict the last token. Hence, we could speed up significantly the inference of the LLM by computing on a smaller sequence inside the attention modules. 

### Your contribution

I did experiments, proposed an OptimizedMistral class and a notebook to try it. I tested the merged inference model on alpaca eval and the results are encouraging as it still outperforms gemma 7B it for instance. I could help to build the wrapper class and see how to insure a dual usage : one for training and one for generation.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
modeling_t5 incompatible with multiprocessing,"### System Info

- `transformers` version: 4.39.0.dev0
- Platform: Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.17
- Python version: 3.10.13
- Huggingface_hub version: 0.21.4
- Safetensors version: 0.4.2
- Accelerate version: 0.27.2
- Accelerate config:    - compute_environment: LOCAL_MACHINE
        - distributed_type: DEEPSPEED
        - mixed_precision: bf16
        - use_cpu: False
        - debug: False
        - num_processes: 8
        - machine_rank: 0
        - num_machines: 1
        - rdzv_backend: static
        - same_network: True
        - main_training_function: main
        - deepspeed_config: {'gradient_accumulation_steps': 16, 'zero3_init_flag': False, 'zero_stage': 0}
        - downcast_bf16: no
        - tpu_use_cluster: False
        - tpu_use_sudo: False
        - tpu_env: []
- PyTorch version (GPU?): 2.2.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

Hi, @ArthurZucker and @younesbelkada . I'm trying to split a dataset automatically to multi gpu (a bit like data parallel) for **inference**. But strange things happen when using t5 model in hf while other models work correctly(i.e. bart), so I guess here exist some problem related to t5 implementation, would you like help checking it out?    ：） 

> Although it has been mentioned online that the error below may be related to OOM, I am certain that it is not. The following code only allows rank0 to obtain normal output, while other ranks will report the following error.

```bash
Traceback (most recent call last):
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/data/ruanjh/NiuInference/NiuInference.py"", line 97, in get_pred
    output = model.generate(
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1388, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/generation/utils.py"", line 503, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs[""encoder_outputs""]: ModelOutput = encoder(**encoder_kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py"", line 1115, in forward
    layer_outputs = layer_module(
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py"", line 695, in forward
    self_attention_outputs = self.layer[0](
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py"", line 602, in forward
    attention_output = self.SelfAttention(
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py"", line 521, in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
```

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

The following code should be quite easy to reproduce. All you need to do is replace the model_dir in the main function with a specific model, such as **Google/t5-v1_1-large** , and make sure CUDA VISIBLE DEVICES >1 .
```python
import torch
from torch import bfloat16
import torch.distributed as dist
import torch.multiprocessing as mp

from torch.utils.data import Dataset,DataLoader
import functools
from transformers import AutoTokenizer,DefaultDataCollator,GenerationConfig,PreTrainedModel,AutoModelForSeq2SeqLM,AutoModelForCausalLM,AutoConfig,DataCollatorWithPadding
import logging
from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES
from tqdm import tqdm
# from accelerate import find_executable_batch_size

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)



class DefaultDataset(Dataset):
    def __init__(self,data,tokenizer):
        self.data=tokenizer(data,return_tensors='pt',padding=True)

    
    def __getitem__(self,idx):
        return {'input_ids':self.data['input_ids'][idx]}
    
    def __len__(self):
        return self.data['input_ids'].size(0)




class NiuInference:
    def __init__(self,model_dir,data,dtype=bfloat16,dataset=None,data_collator=None,output_path='niuinference.out',auto_batch_size=True,batch_size=1,generation_config=None):
        self.model_dir=model_dir
        self.dtype=dtype
        self.data=data
        self.dataset=dataset
        self.data_collator=data_collator
        self.output_path=output_path
        self.batch_size=batch_size
        self.auto_batch_size=auto_batch_size
        self.generation_config=generation_config
        
        
    def _load_model_and_tokenizer(self,device):
        print(self.dtype)
        config=AutoConfig.from_pretrained(self.model_dir)
        if config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES:
            model=AutoModelForCausalLM.from_pretrained(self.model_dir,torch_dtype=self.dtype)
        else:
            model=AutoModelForSeq2SeqLM.from_pretrained(self.model_dir,torch_dtype=self.dtype)
        model.to(device)
        tokenizer=AutoTokenizer.from_pretrained(self.model_dir)
        return model,tokenizer

    # @find_executable_batch_size(starting_batch_size=1)
    # def auto_get_pred(batch_size):
        

    def get_pred(self,rank,out_path,data,dict):
        batch_size=2
        
        try:
            device = torch.device(f'cuda:{rank}')
            model, tokenizer = self._load_model_and_tokenizer(device)
            if self.dataset is not None:
                dataset=self.dataset(data=data,tokenizer=tokenizer)
            else:
                dataset=DefaultDataset(data=data,tokenizer=tokenizer)

            if self.data_collator is not None:
                collator=self.data_collator(tokenizer,model=model,padding=True)
            else:
                collator= DataCollatorWithPadding(tokenizer)
            dataloader=DataLoader(dataset,batch_size,collate_fn=collator,pin_memory=True,num_workers=0)
            result=[]
            for input in tqdm(dataloader):
                input.to(device)
                print(input)
                output = model.generate(
                            input_ids=input['input_ids'],
                            attention_mask=input['attention_mask'],
                            num_beams=5,
                            do_sample=False,
                            temperature=1.0,
                            max_new_tokens=512,
                        )
                pred = tokenizer.batch_decode(output,skip_special_tokens=True)
                print(pred)
                result+=pred
            dict[f'{rank}']=result
        except Exception as e:
            print('error',device)
            raise
        
        
          
    
    def split_list(self,lst, n):
        avg = len(lst) / float(n)
        return [lst[int(avg * i):int(avg * (i + 1))] for i in range(n)]

    def run(self,):
    
        world_size = min(torch.cuda.device_count(),len(self.data)) # corner case， data<available GPU num
        
        data_subsets = self.split_list(self.data,world_size)
        print(data_subsets)
        processes = []
        manager = mp.Manager()
        record_dict = manager.dict()
        for rank in range(world_size):

            p = mp.Process(target=self.get_pred, args=(rank,self.output_path,data_subsets[rank],record_dict))
            p.start()
            processes.append(p)
        for p in processes:
            p.join()

        with open(self.output_path, ""w"", encoding=""utf-8"") as f:
            for rank in range(world_size):
                for r in record_dict[f'{rank}']:
                    f.write(r.replace('\n','\\n')+'\n')

  
if __name__=='__main__':
    mp.set_start_method('spawn')
    i=NiuInference(model_dir=**replace here to t5 or bart**,data=['hello,how is your day','my wish is that you happy','from scratch',])
    i.run()


```

### Expected behavior

t5 model can inference in multiprocessing.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",7,open
[i18n-PL] Translating docs to Polish,"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the Polish-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `pl` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `pl/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md)
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md)
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md)

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
add deepspeed grad ckpt,"hi, @younesbelkada , sorry for the late PR.
its been busy days ;-)

i wrote some draft lines for [deepspeed gradient checkpointing](https://github.com/huggingface/transformers/issues/29648),
i tried to change original code as little as i can, but 2 things were inevitable

- 1. add `gradient_checkpointing_kwargs` as member variables for parsing `num_checkpoints` and `use_deepspeed_grad_ckpt` keys to check use advanced grad ckpt or not
- 2. second thing is not about code but deepspeed checkpoint function force me to use deepspeed.init because of [this line](https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/activation_checkpointing/checkpointing.py#L539-L545).

here is my code for sanity checking advanced activation checkpointing

```python
import os
import copy
import random
from pdb import set_trace as Tra

import torch
import deepspeed
from deepspeed import get_accelerator
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments


def _reset_seeds(seed=1234):
    torch.manual_seed(seed)
    random.seed(seed)

def _get_dummy_inputs(B=2, T=10, seed=1234):
    _reset_seeds(seed)
    return {
        'input_ids': torch.rand(B, T).long().cuda(),
        'attention_mask' : torch.ones(B, T).cuda(),
    }

def _get_optimizer(model):
    return torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

if __name__ == ""__main__"":

    ## set distributed arguments (because deepspeed ckpt check rank for printing...)
    local_rank = int(os.environ[""LOCAL_RANK""])
    get_accelerator().set_device(local_rank)
    device = torch.device(get_accelerator().device_name(), local_rank)
    deepspeed.init_distributed()

    ## get model, tokenizer and dummy optimizer
    model_path = ""lmsys/vicuna-7b-v1.5""
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path).cuda().train() # fp16 can cause nan 
    optimizer = _get_optimizer(model)

    ## vanilla gradient checkpointing
    model.gradient_checkpointing_enable()
    inputs = _get_dummy_inputs()
    optimizer.zero_grad()
    output1 = model(**inputs)
    output1.logits.sum().backward()
    tmp_grad1 = copy.deepcopy(model.model.layers[0].self_attn.q_proj.weight.grad.cpu())

    ## deepspeed CPU offloading, selective gradient checkpointing
    gradient_checkpointing_kwargs = {
        ""use_deepspeed_grad_ckpt"" : True,
        ""num_checkpoints"" : 4,
        ""checkpoint_in_cpu"" : True,
    }
    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs)
    inputs = _get_dummy_inputs()
    optimizer.zero_grad()
    output2 = model(**inputs)
    output2.logits.sum().backward()
    tmp_grad2 = copy.deepcopy(model.model.layers[0].self_attn.q_proj.weight.grad.cpu())

    grad_allclose = torch.allclose(tmp_grad1, tmp_grad2, rtol=1e-05, atol=1e-08)
    assert grad_allclose
    print(f'''
    grad_allclose : {grad_allclose}
    tmp_grad1     : {tmp_grad1}
    tmp_grad2     : {tmp_grad2}
    ''')
```

and the result was like

```python
/path/to/dir/transformers$ python -m torch.distributed.launch test_act_ckpt.py
/path/to/dir/venv/transformers_pr/lib/python3.10/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2024-04-13 08:46:25,432] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 08:46:26,405] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-13 08:46:26,405] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.50s/it]
/path/to/dir/transformers/src/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[2024-04-13 08:46:38,841] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
[2024-04-13 08:46:38,841] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING True
[2024-04-13 08:46:38,841] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 4 total layers
[2024-04-13 08:46:38,841] [INFO] [checkpointing.py:543:forward] ----Synchronization False
[2024-04-13 08:46:38,841] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False

    grad_allclose : True
    tmp_grad1     : tensor([[-2.6913e-12,  1.5395e-12,  8.9857e-13,  ..., -8.0048e-13,
          5.5549e-12, -4.5321e-13],
        [ 4.7248e-12, -2.7027e-12, -1.5775e-12,  ...,  1.4053e-12,
         -9.7519e-12,  7.9564e-13],
        [-4.4621e-12,  2.5525e-12,  1.4898e-12,  ..., -1.3271e-12,
          9.2097e-12, -7.5140e-13],
        ...,
        [-1.0189e-12,  5.8284e-13,  3.4018e-13,  ..., -3.0304e-13,
          2.1030e-12, -1.7158e-13],
        [-2.7976e-12,  1.6004e-12,  9.3407e-13,  ..., -8.3210e-13,
          5.7743e-12, -4.7112e-13],
        [ 7.3264e-12, -4.1910e-12, -2.4461e-12,  ...,  2.1791e-12,
         -1.5122e-11,  1.2337e-12]])
    tmp_grad2     : tensor([[-2.6913e-12,  1.5395e-12,  8.9857e-13,  ..., -8.0048e-13,
          5.5549e-12, -4.5321e-13],
        [ 4.7248e-12, -2.7027e-12, -1.5775e-12,  ...,  1.4053e-12,
         -9.7519e-12,  7.9564e-13],
        [-4.4621e-12,  2.5525e-12,  1.4898e-12,  ..., -1.3271e-12,
          9.2097e-12, -7.5140e-13],
        ...,
        [-1.0189e-12,  5.8284e-13,  3.4018e-13,  ..., -3.0304e-13,
          2.1030e-12, -1.7158e-13],
        [-2.7976e-12,  1.6004e-12,  9.3407e-13,  ..., -8.3210e-13,
          5.7743e-12, -4.7112e-13],
        [ 7.3264e-12, -4.1910e-12, -2.4461e-12,  ...,  2.1791e-12,
         -1.5122e-11,  1.2337e-12]])
```

my environment was

```
python -c ""import torch; print(torch.__version__); \
import transformers; print(transformers.__version__); \
import deepspeed; print(deepspeed.__version__)""
```

```python
2.2.2+cu118
4.40.0.dev0
[2024-04-13 08:44:58,468] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0.14.0
```","[{'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",7,open
Create panoptic segmentation task guide,"### Feature request

We currently already have a semantic segmentation task guide: https://huggingface.co/docs/transformers/tasks/semantic_segmentation. It would be great to create a similar one for panoptic segmentation with models like MaskFormer, Mask2Former and OneFormer.

### Motivation

Panoptic segmentation deserves its own task guide as it's different from semantic segmentation.

Lots of people have reported issues, including the following:

- [ ] computing mAP during MaskFormer training: https://github.com/NielsRogge/Transformers-Tutorials/issues/373
- [ ] evaluation of DETR and friends on a panoptic segmentation dataset: https://github.com/NielsRogge/Transformers-Tutorials/issues/320
- [ ] there was an effort to add the panoptic quality (PQ) metric to Evaluate: https://github.com/huggingface/evaluate/pull/408
- [ ] various people want to use a COCO-formatted dataset for fine-tuning, but there's no guide yet regarding how to do this: https://github.com/NielsRogge/Transformers-Tutorials/issues/296

### Your contribution

Can be based off the notebooks provided here: https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MaskFormer

cc @qubvel ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",5,open
Tranformers documentation translation to Persian,"Hi!

Let's bring the documentation to all the Persian speaking community 🌐 

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) (waiting for initial PR to go through)
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) 
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
4bit Adam,"### Feature request

Is there any chance we coukd get this 4bit adam optimizer added to tranformers?
It has nearly the same performance as 32bit adam with significant drop in vram overhead. 
[repo](https://github.com/thu-ml/low-bit-optimizers)
[Paper](https://arxiv.org/abs/2309.01507)

With this added qlora would be even more memory efficient, and theoretically, you should be capable of FFT a 7b on a 24gb card.

### Motivation

The github repo has a paper which shows negligible difference between 32bit and 4bit adam, and they have the code for the adam optimizer here: [4bit code](https://github.com/thu-ml/low-bit-optimizers/blob/main/lpmm/optim/adamw.py)

Or one bit adamw from deep speed, I only didn't recommend it since digging through the deep speed code, it isn't as laid out as this one is with a whole dedicated script. While yeah you can always use deepspeed and transformers, but deepspeed comes with its own set of draw backs, such as windows compatibility, and unsloth compatibility. But either or a one bit adamw would be awesome. Aside from that there aren't too many other ways to save memory for qlora. I mean theoretically if someone had the will, they could make a bitnet adamnw, that runs on the cpu. Since bit net doesn't need matmuls, the entire computation could be offloaded to the cpu. It would be actually fast, so the training wont be bogged down.

### Your contribution

Submit feature request","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6712797012, 'node_id': 'LA_kwDOCUB6oc8AAAABkB0nVA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/optimization', 'name': 'optimization', 'color': '47E0D2', 'default': False, 'description': ''}]",2,open
Fix scheduler reporting,"# What does this PR do?

Fixes the issue of the wrong iterations scheduler being reported and adds a test

Solves https://github.com/huggingface/transformers/issues/28124


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@amyeroberts ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Can Transformers AutoModel Support to(dml),"### Feature request

Windows  
RX6800XT  Rocm
use DirectML to speed up
Can AutoModel.to()  support DirectML

### Motivation

it useful for me to use glm

### Your contribution

stable diffusion support DirectML","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Image Processor fails to process void segmentation maps,"### System Info

- `transformers` version: 4.34.0
- Platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.31
- Python version: 3.10.13
- Huggingface_hub version: 0.21.4
- Safetensors version: 0.4.0
- PyTorch version (GPU?): 2.1.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```
from transformers import Mask2FormerImageProcessor
import numpy as np


ignore_index = 255
image_processor = Mask2FormerImageProcessor(ignore_index=ignore_index,
                                                         do_resize=False,
                                                         do_rescale=False,
                                                         do_normalize=False)

image_norm = np.random.rand(4, 4, 3)

# Create void mask (all pixels have ignore_index)
semantic_mask = np.ones(image_norm.shape[:2], dtype=np.uint8)*255

semantic_mask = semantic_mask.astype(np.uint8)
print(semantic_mask)

inputs = image_processor(
    image_norm,
    segmentation_maps=semantic_mask,
    return_tensors='pt',
)

print(inputs)
```


===========================================================

> [[255 255 255 255]
>  [255 255 255 255]
>  [255 255 255 255]
>  [255 255 255 255]]
> Traceback (most recent call last):
>   File ""/home/anba/catkin_ws/src/tas_dev/dev/anba/Mask2Former/test.py"", line 21, in <module>
>     inputs = image_processor(
>   File ""/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py"", line 566, in __call__
>     return self.preprocess(images, segmentation_maps=segmentation_maps, **kwargs)
>   File ""/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py"", line 764, in preprocess
>     encoded_inputs = self.encode_inputs(
>   File ""/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py"", line 943, in encode_inputs
>     masks, classes = self.convert_segmentation_map_to_binary_masks(
>   File ""/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py"", line 558, in convert_segmentation_map_to_binary_masks
>     return convert_segmentation_map_to_binary_masks(
>   File ""/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py"", line 284, in convert_segmentation_map_to_binary_masks
>     binary_masks = np.stack(binary_masks, axis=0)  # (num_labels, height, width)
>   File ""<__array_function__ internals>"", line 180, in stack
>   File ""/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/numpy/core/shape_base.py"", line 422, in stack
>     raise ValueError('need at least one array to stack')
> ValueError: need at least one array to stack
> 
> Process finished with exit code 1

### Expected behavior

If this is intended that void masks should never be passed, then the result is fine.

However, when training segmentation models, shouldn't it be possible to include images with only background/void class?","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",6,open
`test_encode_decode_fast_slow_all_tokens` is failing,"This test starts to fail after #29473

See https://github.com/huggingface/transformers/pull/29473#pullrequestreview-1945687810

#30044 skips it for now, but let's fix it once we have the bandwidth.","[{'id': 1834056635, 'node_id': 'MDU6TGFiZWwxODM0MDU2NjM1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Tokenization', 'name': 'Core: Tokenization', 'color': 'FF4446', 'default': False, 'description': 'Internals of the library; Tokenization.'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
"Model trained with Flash Attention 2.0 raises ""RuntimeError: query and key must have the same dtype"" when generating","### System Info

- `transformers` version: 4.38.2
- Platform: Linux-5.15.0-75-generic-x86_64-with-glibc2.35
- Python version: 3.11.8
- Huggingface_hub version: 0.21.4
- Safetensors version: 0.4.2
- Accelerate version: 0.28.0
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.2.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@gante @muellerz @pacman100

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from datasets import load_dataset
from trl import SFTTrainer

dataset = load_dataset(""imdb"")

checkpoint = ""facebook/opt-125M""
model_kwargs = {
    ""attn_implementation"": ""flash_attention_2"",
}

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint, **model_kwargs)

def tokenize_function(examples):
    return tokenizer(examples[""text""], padding=""max_length"", truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

training_args = TrainingArguments(
    output_dir=""./test_trainer"",
    evaluation_strategy=""epoch"",
    report_to=""none"",
    fp16=True,
    save_strategy=""epoch"",
)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets[""train""],
    eval_dataset=tokenized_datasets[""test""],
    dataset_text_field=""text"",
)

trainer.train() # works
trainer.evaluate() # works

test_input = ""I really liked this movie because""
test_input = tokenizer(test_input, return_tensors=""pt"").to(""cuda"")
model.generate(test_input.input_ids, max_length=100, num_return_sequences=1) # Blows up!
```

Here is the exception:
```
Traceback (most recent call last):
  File ""/workspace/storage/hyperml/notebooks/test_trainer/x.py"", line 43, in <module>
    model.generate(test_input.input_ids, max_length=100, num_return_sequences=1) # Blows up!
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/transformers/generation/utils.py"", line 1544, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/transformers/generation/utils.py"", line 2404, in greedy_search
    outputs = self(
              ^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 822, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/accelerate/utils/operations.py"", line 810, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/amp/autocast_mode.py"", line 16, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py"", line 1145, in forward
    outputs = self.model.decoder(
              ^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py"", line 911, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py"", line 552, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py"", line 384, in forward
    attn_output = self._flash_attention_forward(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py"", line 450, in _flash_attention_forward
    attn_output = flash_attn_func(
                  ^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py"", line 825, in flash_attn_func
    return FlashAttnFunc.apply(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/torch/autograd/function.py"", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py"", line 507, in forward
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(
                                                                ^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/storage/env/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py"", line 51, in _flash_attn_forward
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(
                                                                ^^^^^^^^^^^^^^^^^^^^
RuntimeError: query and key must have the same dtype
```

I don't think this is an SFTTrainer problem, which is why I didn't report there.

### Expected behavior

It should generate without any problems. Otherwise, it should give an error or indication of what needs to be done so that the model can be used for generation.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",14,open
Flash Attention 2 support for Wav2Vec2ForCTC,"Hello
Usage of ""optimum.bettertransformer"" for Wav2Vec2ForCTC gave me the inference speedup for ~2x. 
How do you think, is it reasonable to try to implement flash attention 2 for Wav2Vec2ForCTC?
I saw the same implementation in the Whisper family of models, and it works very well (speedup ~1.8x on batch inference).","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Add support for walltime-based saving/logging/evaluating,"### Feature request

We currently have the save strategies `epoch` or `steps`. It would be useful to add one for `time`, too. After every backward pass we check if a given time interval has passed. If it has, save a checkpoint and reset the timer.

### Motivation

Motivation comes from usage on clusters where you have a job time limit. You _can_ first do a test run and see how long a step takes on average and extrapolate from there, but relying on walltime would probably be easier.

### Your contribution

I can work on this. I think a condition should be added to the defaultflowcallback (https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_callback.py#L432) to also include this new strategy. I am not sure yet how to track the starting time, though. Should it be passed separately and saved in the `trainer` instance? Or added to `args`?

In terms of implementation, a lot of inspiration can be taken from https://twitter.com/StasBekman/status/1774842972795982160","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",8,open
Nested Tensor support,"### Feature request

Support of NestedTensor. 

Conventional Tensor is dense. 

For natural language tasks, we also need to pad input tensor with various length to the longest tensor so that it can be processed, and use a mask tensor to indicate the padding positions. This work around does work, but introduces unnecessary computation and have extra memory costs, especially in FFN, where padding is not necessary at all. 

Nested Tensor stores tensors in a list so that it does not need paddings. 

### Motivation

+ reduce memory costs. 

+ speed up process in FFN and Attention (with FlashAttention).

### Alternatives

PyTorch has its own nested_tensor implementation. However, its progress is very slow and unlikely to be useful in the coming months. 

### Your contribution

```python
# pylint: disable=protected-access
from __future__ import annotations

from typing import Any, Callable, Iterable, Mapping, Sequence, SupportsFloat

import torch
from torch import Tensor
from torch.utils.data._utils.collate import default_collate_fn_map

from ..utils import method_cache
from .functional import mask_tensor, pad_tensor
from .utils import TorchFuncRegistry


class PNTensor(Tensor):
    r""""""
    Wrapper for tensors to be converted to `NestedTensor`.

    `PNTensor` is a subclass of `torch.Tensor`.
    It implements two additional methods as `NestedTensor`: `tensor` and `mask`.

    Although it is possible to directly construct `NestedTensor` in dataset,
    the best practice is to do so is in `collate_fn`.
    `PNTensor` is introduced to smooth the process.

    Convert tensors that will be converted to `NestedTensor` to a `PNTensor`,
    and PyTorch Dataloader will automatically collate `PNTensor` to `NestedTensor`.
    """"""

    @property
    def tensor(self) -> Tensor:
        r""""""
        Identical to `self`.

        Returns:
            (torch.Tensor):

        Examples:
            >>> tensor = torch.tensor([1, 2, 3])
            >>> pn_tensor = PNTensor([1, 2, 3])
            >>> bool((tensor == pn_tensor).all())
            True
            >>> bool((tensor == pn_tensor.tensor).all())
            True
        """"""

        return self

    @property
    def mask(self) -> Tensor:
        r""""""
        Identical to `torch.ones_like(self)`.

        Returns:
            (torch.Tensor):

        Examples:
            >>> tensor = torch.tensor([1, 2, 3])
            >>> pn_tensor = PNTensor([1, 2, 3])
            >>> bool((pn_tensor.mask == torch.ones_like(pn_tensor)).all().item())
            True
        """"""

        return torch.ones_like(self)

    def new_empty(self, *args, **kwargs):
        return PNTensor(super().new_empty(*args, **kwargs))


class NestedTensor:
    r""""""
    Wrap an iterable of tensors into a single tensor with a mask.

    In sequence to sequence tasks, elements of a batch are usually not of the same length.
    This made it tricky to use a single tensor to represent a batch of sequences.

    `NestedTensor` allows to store a sequence of tensors of different lengths in a single object.
    It also provides a mask that can be used to retrieve the original sequence of tensors.

    When calling `__getitem__(arg)` on a `NestedTensor`, it has two return type:
    1. if arg is `int` or `slice`, returns a tuple of two `tensor`s, representing data and padding mask.
    2. if arg is a `tuple`, return a new `NestedTensor` with specified shape.

    Attributes:
        _storage: The sequence of tensors.
        tensor: padded tensor.
        mask: mask tensor.
        batch_first:  Whether the first dimension of the tensors is the batch dimension.

            If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.

            If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`
        padding_value: The padding value used to in padded tensor.
        mask_value: The mask value used in mask tensor.

    Args:
        tensors:
        batch_first:
        padding_value:
        mask_value:

    Raises:
        ValueError: If `tensors` is not an iterable.
        ValueError: If `tensors` is empty.

    Notes:
        We have rewritten the `__getattr__` function to support as much native tensor operations as possible.
        However, not all operations are tested.

        Please file an issue if you find any bugs.

    Examples:
        >>> nested_tensor = NestedTensor(torch.tensor([1, 2, 3]), torch.tensor([4, 5]))
        >>> nested_tensor.shape
        torch.Size([2, 3])
        >>> nested_tensor.device
        device(type='cpu')
        >>> nested_tensor.dtype
        torch.int64
        >>> nested_tensor.tensor
        tensor([[1, 2, 3],
                [4, 5, 0]])
        >>> nested_tensor.mask
        tensor([[ True,  True,  True],
                [ True,  True, False]])
        >>> nested_tensor.to(torch.float).tensor
        tensor([[1., 2., 3.],
                [4., 5., 0.]])
        >>> nested_tensor.half().tensor
        tensor([[1., 2., 3.],
                [4., 5., 0.]], dtype=torch.float16)
        >>> nested_tensor[:]
        (tensor([[1, 2, 3],
                [4, 5, 0]]), tensor([[ True,  True,  True],
                [ True,  True, False]]))
        >>> nested_tensor[1]
        (tensor([4, 5]), tensor([True, True]))
        >>> nested_tensor[:, 1:]
        NestedTensor([[2, 3],
                [5, 0]])
        >>> nested_tensor.tolist()
        [[1, 2, 3], [4, 5]]
        >>> NestedTensor(*[[1, 2, 3], [4, 5]])
        NestedTensor([[1, 2, 3],
                [4, 5, 0]])
    """"""

    __storage: Sequence[Tensor]
    batch_first: bool = True
    padding_value: SupportsFloat = 0.0
    mask_value: bool = False

    def __init__(
        self,
        *tensors: Iterable[Tensor],
        batch_first: bool = True,
        padding_value: SupportsFloat = 0.0,
        mask_value: bool = False,
    ) -> None:
        if len(tensors) == 1 and isinstance(tensors, Sequence):
            tensors = tensors[0]  # type: ignore
        self._storage = tensors
        self.batch_first = batch_first
        self.padding_value = padding_value
        self.mask_value = mask_value

    @property
    def _storage(self):
        return self.__storage

    @_storage.setter
    def _storage(self, tensors: Sequence):
        if not isinstance(tensors, Iterable):
            raise ValueError(f""tensors must be an Iterable, bug got {type(tensors)}."")
        tensors = list(tensors)
        if len(tensors) == 0:
            raise ValueError(""tensors must be a non-empty Iterable."")
        if not isinstance(tensors[0], Tensor):
            tensors = [torch.tensor(tensor) for tensor in tensors]
        self.__storage = tensors

    def storage(self):
        return self._storage

    @property
    def tensor(self) -> Tensor:
        r""""""
        Return a single tensor by padding all the tensors.

        Returns:
            (torch.Tensor):

        Examples:
            >>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])
            >>> nested_tensor.tensor
            tensor([[1, 2, 3],
                    [4, 5, 0]])
        """"""

        return self._tensor(tuple(self._storage), self.batch_first, self.padding_value)

    @property
    def mask(self) -> Tensor:
        r""""""
        Padding mask of `tensor`.

        Returns:
            (torch.Tensor):

        Examples:
            >>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])
            >>> nested_tensor.mask
            tensor([[ True,  True,  True],
                    [ True,  True, False]])
        """"""

        return self._mask(tuple(self._storage), self.batch_first, self.mask_value)

    @classmethod
    def from_tensor_mask(cls, tensor: Tensor, mask: Tensor):
        r""""""
        Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.

        Args:
            tensor: Padded Tensor.
            mask: Tensor Mask.

        Returns:
            (torch.Tensor):

        Examples:
            >>> padded_tensor = torch.tensor([[1, 2, 3, 0, 0],
            ...                                [4, 5, 0, 0, 0],
            ...                                [6, 7, 8, 9, 0]])
            >>> mask_tensor = torch.tensor([[1, 1, 1, 0, 0],
            ...                             [1, 1, 0, 0, 0],
            ...                             [1, 1, 1, 1, 0]])
            >>> nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)
            >>> nested_tensor
            NestedTensor([[1, 2, 3, 0],
                    [4, 5, 0, 0],
                    [6, 7, 8, 9]])
        """"""

        if mask.ndim == 2:
            return cls(t[slice(0, m.sum())] for t, m in zip(tensor, mask))
        return cls(
            t[[slice(0, (m.sum(dim=dim) > 0).sum().item()) for dim in reversed(range(m.dim()))]]
            for t, m in zip(tensor, mask)
        )

    def nested_like(self, other: Tensor, unsafe: bool = False) -> NestedTensor:
        r""""""
        Create a new `NestedTensor` from a `Tensor`.
        The newly created `NestedTensor` will have the same shape as current `NestedTensor`.

        Args:
            other: The `Tensor` to be nested.
            unsafe: Whether to check the shape of `other` and current `NestedTensor`.

        Returns:
            (NestedTensor):

        Examples:
            >>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])
            >>> (nested_tensor == nested_tensor.nested_like(nested_tensor)).all()
            tensor(True)
            >>> tensor = nested_tensor.tensor
            >>> (nested_tensor == nested_tensor.nested_like(tensor)).all()
            tensor(True)
            >>> f = nested_tensor.nested_like(torch.randn(2, 2))
            Traceback (most recent call last):
            ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])
            >>> p = nested_tensor.nested_like(torch.randn(2, 2), True)
            >>> p = nested_tensor.nested_like(torch.randn(3, 3), True)
            Traceback (most recent call last):
            ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3
        """"""  # noqa: E501

        if isinstance(other, NestedTensor):
            return other.clone()

        if not unsafe and self.shape != other.shape:
            raise ValueError(
                f""The shape of NestedTensor and input tensor does not match, {self.shape} != {other.shape}""
            )
        if self.size(0) != other.size(0):
            raise ValueError(
                f""The batch size of NestedTensor and input tensor does not match, {self.size(0)} != {other.size(0)}""
            )
        return NestedTensor([o[tuple(slice(0, dim) for dim in t.shape)] for t, o in zip(self._storage, other)])

    @property
    def device(self) -> torch.device:
        r""""""
        Device of the NestedTensor.

        Returns:
            (torch.Tensor):

        Examples:
            >>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])
            >>> nested_tensor.device
            device(type='cpu')
        """"""

        return self._device(tuple(self._storage))

    @property
    def shape(self) -> torch.Size | int:
        r""""""
        Alias for `size()`.
        """"""

        return self.size()

    @property
    def ndim(self) -> int:
        r""""""
        Alias for `dim()`.
        """"""

        return self.dim()

    def size(self, dim: int | None = None) -> torch.Size | int:
        r""""""
        Returns the size of the self `NestedTensor`.

        Args:
            dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.
                If specified, returns an `int` holding the size of that dimension.
                Defaults to `None`.

        Returns:
            (torch.Size | int):

        Examples:
            >>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])
            >>> nested_tensor.size()
            torch.Size([2, 3])
            >>> nested_tensor.size(0)
            2
            >>> nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])
            >>> nested_tensor.shape
            torch.Size([2, 4])
            >>> nested_tensor.size(1)
            4
        """"""

        return self._size(tuple(self._storage), dim, self.batch_first)

    def dim(self) -> int:
        r""""""
        Number of dimension of the NestedTensor.

        Returns:
            (int):

        Examples:
            >>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])
            >>> nested_tensor.dim()
            2
            >>> nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))
            >>> nested_tensor.ndim
            2
        """"""

        return self._dim(tuple(self._storage))

    def tolist(self) -> list:
        r""""""
        Convert a NestedTensor to a list of lists of values.

        Returns:
            (list):

        Examples:
            >>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])
            >>> nested_tensor.tolist()
            [[1, 2, 3], [4, 5]]
        """"""

        return [t.tolist() for t in self._storage]

    def all(self, dim: int | None = None, keepdim: bool = False) -> bool | Tensor | NestedTensor:
        r""""""
        Tests if all elements in NestedTensor evaluate to True.

        Returns:
            (bool | Tensor):

        Examples:
            >>> nested_tensor = NestedTensor([torch.ones(2, 4, dtype=torch.bool), torch.ones(3, 5, dtype=torch.bool)])
            >>> nested_tensor.all()
            tensor(True)
            >>> nested_tensor.all(dim=0)
            tensor([True, True])
            >>> nested_tensor.all(dim=0, keepdim=True)
            tensor([[True, True]])
            >>> nested_tensor.all(dim=1)
            NestedTensor([[ True,  True,  True,  True, False],
                    [ True,  True,  True,  True,  True]])
            >>> nested_tensor.all(dim=1, keepdim=True)
            NestedTensor([[[ True,  True,  True,  True, False]],
            <BLANKLINE>
                    [[ True,  True,  True,  True,  True]]])
            >>> nested_tensor.batch_first = False
            >>> nested_tensor.all(dim=1)
            tensor([True, True])
            >>> nested_tensor.batch_first = False
            >>> nested_tensor.all(dim=0)
            NestedTensor([[ True,  True,  True,  True, False],
                    [ True,  True,  True,  True,  True]])
            >>> nested_tensor.all(dim=1)
            tensor([True, True])
        """"""

        if dim is None:
            return torch.tensor(all(i.all() for i in self._storage))
        if (self.batch_first and dim == 0) or (not self.batch_first and dim == 1):
            if keepdim:
                return torch.tensor([i.all() for i in self._storage]).unsqueeze(0 if self.batch_first else 1)
            return torch.tensor([i.all() for i in self._storage])
        if self.batch_first or dim != 0:
            dim -= 1
        return NestedTensor([i.all(dim=dim, keepdim=keepdim) for i in self._storage])

    def where(self, condition, other) -> NestedTensor:
        r""""""
        Return a NestedTensor of elements selected from either self or other, depending on condition.

        Returns:
            (NestedTensor):

        Examples:
            >>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])
            >>> nested_tensor.where(nested_tensor > 2, torch.tensor([[6, 5, 4], [3, 2, 1]]))
            NestedTensor([[6, 5, 3],
                    [4, 5, 0]])
            >>> nested_tensor.where(nested_tensor > 2, NestedTensor([[6, 5, 4], [3, 2]]))
            NestedTensor([[6, 5, 3],
                    [4, 5, 0]])
            >>> nested_tensor.where(torch.tensor(True), NestedTensor([[6, 5, 4], [3, 2]]))
            NestedTensor([[1, 2, 3],
                    [4, 5, 0]])
        """"""

        if isinstance(condition, Tensor) and self.shape == condition.shape:
            condition = self.nested_like(condition)
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(condition, NestedTensor) and isinstance(other, NestedTensor):
            return NestedTensor(
                [x.where(c, y) for x, c, y in zip(self._storage, condition._storage, other._storage)], **self._state
            )
        if isinstance(condition, NestedTensor):
            return NestedTensor([x.where(c, other) for x, c in zip(self._storage, condition._storage)], **self._state)
        if isinstance(other, NestedTensor):
            return NestedTensor([x.where(condition, y) for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor(x.where(condition, other) for x in self._storage)

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}
        if func not in NestedTensorFunc or not all(issubclass(t, (torch.Tensor, NestedTensor)) for t in types):
            args = [a.tensor if hasattr(a, ""tensor"") else a for a in args]
            return func(*args, **kwargs)
        return NestedTensorFunc[func](*args, **kwargs)

    def __getitem__(self, index: int | slice | tuple) -> tuple[Tensor, Tensor] | NestedTensor:
        if isinstance(index, tuple):
            return NestedTensor([t[index[0]][index[1:]] for t in self._storage])
        if isinstance(index, (int, slice)):
            ret = self._storage[index]
            if isinstance(ret, Tensor):
                return ret, torch.ones_like(ret, dtype=torch.bool)
            return self.tensor, self.mask
        raise ValueError(f""Unsupported index type {type(index)}"")

    def __getattr__(self, name) -> Any:
        if not self._storage:
            raise ValueError(f""Unable to get {name} from an empty {self.__class__.__name__}"")
        ret = [getattr(i, name) for i in self._storage]
        elem = ret[0]
        if isinstance(elem, Tensor):
            return NestedTensor(ret, **self._state)
        if callable(elem):
            return NestedTensorFuncWrapper(ret, state=self._state)
        if elem.__hash__ is not None and len(set(ret)) == 1:
            return elem
        return ret

    @property
    def _state(self) -> Mapping:
        return {k: v for k, v in self.__dict__.items() if not (k.startswith(""_"") or k.endswith(""_""))}

    def __state__(self) -> Mapping:
        return self.__dict__

    def __setstate__(self, state: Mapping) -> None:
        self.__dict__.update(state)

    def __len__(self) -> int:
        return len(self._storage)

    def __repr__(self):
        return self.__class__.__name__ + repr(self.tensor)[len(self.tensor.__class__.__name__) :]  # noqa: E203

    def __bool__(self) -> int:
        return all(bool(x) for x in self._storage)

    def __gt__(  # type: ignore[override]
        self, other: Tensor | NestedTensor | SupportsFloat
    ) -> bool | Tensor | NestedTensor:
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor(i > j for i, j in zip(self._storage, other._storage))
        if isinstance(other, (int, float, Tensor)):
            return NestedTensor([x > other for x in self._storage], **self._state)
        raise TypeError(f""> not supported between instances of '{type(self)}' and '{type(other)}'"")

    def __ge__(  # type: ignore[override]
        self, other: Tensor | NestedTensor | SupportsFloat
    ) -> bool | Tensor | NestedTensor:
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor(i >= j for i, j in zip(self._storage, other._storage))
        if isinstance(other, (int, float, Tensor)):
            return NestedTensor([x >= other for x in self._storage], **self._state)
        raise TypeError(f"">= not supported between instances of '{type(self)}' and '{type(other)}'"")

    def __eq__(  # type: ignore[override]
        self, other: Tensor | NestedTensor | SupportsFloat
    ) -> bool | Tensor | NestedTensor:
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor(i == j for i, j in zip(self._storage, other._storage))
        if isinstance(other, (int, float, Tensor)):
            return NestedTensor([x == other for x in self._storage], **self._state)
        return False

    def __le__(  # type: ignore[override]
        self, other: Tensor | NestedTensor | SupportsFloat
    ) -> bool | Tensor | NestedTensor:
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor(i <= j for i, j in zip(self._storage, other._storage))
        if isinstance(other, (int, float, Tensor)):
            return NestedTensor([x <= other for x in self._storage], **self._state)
        raise TypeError(f""<= not supported between instances of '{type(self)}' and '{type(other)}'"")

    def __lt__(  # type: ignore[override]
        self, other: Tensor | NestedTensor | SupportsFloat
    ) -> bool | Tensor | NestedTensor:
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor(i < j for i, j in zip(self._storage, other._storage))
        if isinstance(other, (int, float, Tensor)):
            return NestedTensor([x < other for x in self._storage], **self._state)
        raise TypeError(f""< not supported between instances of '{type(self)}' and '{type(other)}'"")

    def __abs__(self):
        return NestedTensor([abs(value) for value in self._storage], **self._state)

    def __add__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x + y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value + other for value in self._storage], **self._state)

    def __radd__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y + x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other + value for value in self._storage], **self._state)

    def __iadd__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x += y
        else:
            for value in self._storage:
                value += other
        return self

    def __sub__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x - y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value - other for value in self._storage], **self._state)

    def __rsub__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y - x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other - value for value in self._storage], **self._state)

    def __isub__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x -= y
        else:
            for value in self._storage:
                value -= other
        return self

    def __pos__(self):
        return NestedTensor([+x for x in self._storage])

    def __neg__(self):
        return NestedTensor([-x for x in self._storage])

    def __invert__(self):
        return NestedTensor([~x for x in self._storage])

    def __mul__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x * y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value * other for value in self._storage], **self._state)

    def __rmul__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y * x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other * value for value in self._storage], **self._state)

    def __imul__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x *= y
        else:
            for value in self._storage:
                value *= other
        return self

    def __pow__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x**y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value**other for value in self._storage], **self._state)

    def __rpow__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y**x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other**value for value in self._storage], **self._state)

    def __ipow__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x **= y
        else:
            for value in self._storage:
                value **= other
        return self

    def __matmul__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x @ y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value @ other for value in self._storage], **self._state)

    def __rmatmul__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y @ x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other @ value for value in self._storage], **self._state)

    def __imatmul__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x @= y
        else:
            for value in self._storage:
                value @= other
        return self

    def __truediv__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x / y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value / other for value in self._storage], **self._state)

    def __rtruediv__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y / x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other / value for value in self._storage], **self._state)

    def __itruediv__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x /= y
        else:
            for value in self._storage:
                value /= other
        return self

    def __floordiv__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x // y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value // other for value in self._storage], **self._state)

    def __rfloordiv__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y // x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other // value for value in self._storage], **self._state)

    def __ifloordiv__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x //= y
        else:
            for value in self._storage:
                value //= other
        return self

    def __mod__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x % y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value % other for value in self._storage], **self._state)

    def __rmod__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y % x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other % value for value in self._storage], **self._state)

    def __imod__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x %= y
        else:
            for value in self._storage:
                value %= other
        return self

    def __and__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x & y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value & other for value in self._storage], **self._state)

    def __rand__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y & x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other & value for value in self._storage], **self._state)

    def __iand__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x &= y
        else:
            for value in self._storage:
                value &= other
        return self

    def __or__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x | y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value | other for value in self._storage], **self._state)

    def __ror__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y | x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other | value for value in self._storage], **self._state)

    def __ior__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x |= y
        else:
            for value in self._storage:
                value |= other
        return self

    def __xor__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x ^ y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value ^ other for value in self._storage], **self._state)

    def __rxor__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y ^ x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other ^ value for value in self._storage], **self._state)

    def __ixor__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x ^= y
        else:
            for value in self._storage:
                value ^= other
        return self

    def __lshift__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x << y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value << other for value in self._storage], **self._state)

    def __rlshift__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y << x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other << value for value in self._storage], **self._state)

    def __ilshift__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x <<= y
        else:
            for value in self._storage:
                value <<= other
        return self

    def __rshift__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([x >> y for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([value >> other for value in self._storage], **self._state)

    def __rrshift__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if isinstance(other, NestedTensor):
            return NestedTensor([y >> x for x, y in zip(self._storage, other._storage)], **self._state)
        return NestedTensor([other >> value for value in self._storage], **self._state)

    def __irshift__(self, other):
        if isinstance(other, Tensor) and self.shape == other.shape:
            other = self.nested_like(other)
        if hasattr(other, ""to""):
            other = other.to(self.dtype)
        if isinstance(other, NestedTensor):
            for x, y in zip(self._storage, other._storage):
                x >>= y
        else:
            for value in self._storage:
                value >>= other
        return self

    @method_cache(maxsize=1)
    def _tensor(self, storage: tuple, batch_first: bool, padding_value: SupportsFloat) -> Tensor:
        if storage[0].dim() == 0:
            return torch.stack(storage, dim=0)
        return pad_tensor(storage, size=self.size(), batch_first=batch_first, padding_value=float(padding_value))

    @method_cache(maxsize=1)
    def _mask(self, storage: tuple, batch_first: bool, mask_value: bool) -> Tensor:
        if storage[0].dim() == 0:
            return torch.full((len(storage),), not mask_value, dtype=torch.bool, device=self.device)
        size = self.size()
        # ignore channel dimension
        if storage[0].dim() > 1 and len({t.size(-1) for t in storage}) == 1:
            size = size[:-1]  # type: ignore
        return mask_tensor(storage, size=size, batch_first=batch_first, mask_value=mask_value)

    @method_cache(maxsize=1)
    def _device(self, storage) -> torch.device:
        return storage[0].device

    @method_cache(maxsize=1)
    def _size(self, storage, dim: int | None = None, batch_first: bool = True) -> torch.Size | int:
        if dim is not None:
            if dim == 0:
                return len(storage)
            return max(t.size(dim - 1) for t in storage)
        if max(t.dim() for t in storage) == 0:
            return torch.Size((len(storage),))
        ndim = max(t.dim() for t in storage)
        size = [max(t.shape[i] if i < len(t.shape) else 0 for t in storage) for i in range(ndim)]
        size.insert(0 if batch_first else 1, len(storage))
        return torch.Size(size)

    @method_cache(maxsize=1)
    def _dim(self, storage) -> torch.Size:
        return max(t.dim() for t in storage) + 1


NestedTensorFunc = TorchFuncRegistry()


@NestedTensorFunc.implement(torch.cat)
def cat(tensors, dim: int = 0):
    if dim != 0:
        raise NotImplementedError(f""NestedTensor only supports cat when dim=0, but got {dim}"")
    return NestedTensor([t for tensor in tensors for t in tensor._storage], tensors[0]._state)


@NestedTensorFunc.implement(torch.isin)
def isin(elements, test_elements, *, assume_unique: bool = False, invert: bool = False):
    if isinstance(elements, NestedTensor):
        elements = elements.tensor
    if isinstance(test_elements, NestedTensor):
        test_elements = test_elements.tensor
    return torch.isin(elements, test_elements, assume_unique=assume_unique, invert=invert)


@NestedTensorFunc.implement(torch.log)
def log(tensor):
    return NestedTensor([torch.log(t) for t in tensor._storage])


@NestedTensorFunc.implement(torch.mean)
def mean(
    input,
    dim: int | None = None,
    keepdim: bool = False,
    *,
    dtype: torch.dtype | None = None,
):
    return input.mean(dim=dim, keepdim=keepdim, dtype=dtype)


@NestedTensorFunc.implement(torch.sqrt)
def sqrt(tensor):
    return NestedTensor([torch.sqrt(t) for t in tensor._storage])


@NestedTensorFunc.implement(torch.stack)
def stack(*args, **kwargs):
    raise NotImplementedError(""NestedTensor does not support stack as of now"")


class NestedTensorFuncWrapper:  # pylint: disable=R0903
    r""""""
    Function Wrapper to handle NestedTensor as input.
    """"""

    __storage: Sequence[Callable] = []
    state: Mapping = {}

    def __init__(self, *callables: Iterable[Callable], state: Mapping | None = None) -> None:
        if len(callables) == 1 and isinstance(callables, Sequence):
            callables = callables[0]  # type: ignore
        self._storage = callables  # type: ignore
        if state is None:
            state = {}
        self.state = state

    @property
    def _storage(self):
        return self.__storage

    @_storage.setter
    def _storage(self, callables: Sequence):
        if not isinstance(callables, Sequence):
            raise ValueError(f""callables must be a Sequence, bug got {type(callables)}"")
        if len(callables) == 0:
            raise ValueError(""callables must be a non-empty Sequence."")
        if not callable(callables[0]):
            raise ValueError(f""callables must be a Sequence of Callable, bug got {type(callables[0])}"")
        self.__storage = callables

    def __call__(self, *args, **kwargs) -> NestedTensor | Sequence[Tensor]:
        ret = [call(*args, **kwargs) for call in self._storage]
        elem = ret[0]
        if isinstance(elem, Tensor):
            return NestedTensor(ret, **self.state)
        if elem.__hash__ is not None and len(set(ret)) == 1:
            return elem
        return ret


def collate_pn_tensor_fn(batch, *, collate_fn_map: dict[type | tuple[type, ...], Callable] | None = None):
    return NestedTensor(batch)


default_collate_fn_map[PNTensor] = collate_pn_tensor_fn
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Enhance Generality of Trainer Class,"### Feature request

In the current version of the transformers library's Trainer class, the model can only report a total loss during the training and evaluation stages. However, in practical applications, we often wish to observe the changes in different types of loss function values in multi-task learning. For example, we might add a Contrastive loss on top of the MLE loss. I suspect that adding the feature to record different losses could enhance the generality of the code.

### Motivation

Multi-task learning is a common technique for improving model performance and generalization ability. In this case, in order to observe the effectiveness of different objective functions more finely, we need to record other losses apart from the total loss.

### Your contribution

Yes, I can submit a PR.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",8,open
[WIP] Added Llama-vid,"# What does this PR do?

Adds Llama-VID 


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #27980

For video: 
```python
from PIL import Image
import requests
from transformers import LLaMAVIDLlavaForConditionalGeneration, LLaMAVIDLlavaProcessor,LLaMAVIDLlavaConfig
from huggingface_hub import hf_hub_download
import torch
import numpy as np
from decord import VideoReader, cpu



def load_video(video_path, fps=1):
    vr = VideoReader(video_path, ctx=cpu(0))
    fps = round(vr.get_avg_fps()/fps)
    frame_idx = [i for i in range(0, len(vr), fps)]
    spare_frames = vr.get_batch(frame_idx).asnumpy()
    return spare_frames

model = LLaMAVIDLlavaForConditionalGeneration.from_pretrained(""Nilesh360/llama-vid-7b-full-224-video-fps-1"")
processor = LLaMAVIDLlavaProcessor.from_pretrained(""Nilesh360/llama-vid-7b-full-224-video-fps-1"")

prompt = ""<image>\nUSER: What's the content of the video?\nASSISTANT:""
video= hf_hub_download(repo_id=""nielsr/video-demo"", filename=""eating_spaghetti.mp4"", repo_type=""dataset"")
images= load_video(video)

inputs = processor(text=[prompt], images=[images], return_tensors=""pt"")
# Generate
generate_ids = model.generate(**inputs, max_length=512)
reply = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
print(reply)

```
For image : 
```python
from PIL import Image
import requests
from transformers import LLaMAVIDLlavaForConditionalGeneration, LLaMAVIDLlavaProcessor,LLaMAVIDLlavaConfig
import torch
import numpy as np

processor = LLaMAVIDLlavaProcessor.from_pretrained(""Nilesh360/llama-vid-7b-full-336"")
model = LLaMAVIDLlavaForConditionalGeneration.from_pretrained(""Nilesh360/llama-vid-7b-full-336"")

prompt = ""<image>\nUSER: What's the content of the image?\nASSISTANT:""
url= 'https://huggingface.co/datasets/huggingface/cats-image'
image=  Image.open(requests.get(url, stream=True).convert('RGB'))


inputs = processor(text=[prompt], images=[image], return_tensors=""pt"")
# Generate
generate_ids = model.generate(**inputs, max_length=512)
reply = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
print(reply)
```

Colab: https://colab.research.google.com/drive/1fvaagb4nSjtEvx2YDrcqY0E2jWIsdCtv#scrollTo=v4Ry7RyyiruU



## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@amyeroberts

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",9,open
[SigLIP] Add fast tokenizer,"# What does this PR do?

Fixes #29925.

🔴 Breaking change! Updates `SiglipTokenizer`, specifically `strip` behaviour in `tokenize`

To do:

- [x] fix remaining tests
- [x] add slow integration test",[],20,open
Add serialization to hf_argparser.py,"# What does this PR do?

This PR introduces three key enhancements to the HfArgumentParser class within the Hugging Face Transformers library: the internal method _to, and the public methods to_json and to_yaml. These additions allow for seamless serialization of parsed command-line arguments (or any dataclass instances) into JSON and YAML files, facilitating better interoperability and configuration management for users.

The _to method serves as a generic serializer that underpins both to_json and to_yaml, abstracting the serialization process and ensuring that any dataclass instance can be easily saved in these formats. The to_json method leverages this to serialize dataclass instances to a JSON file, while to_yaml does the same for YAML files. Both methods are designed to work with tuples of dataclass instances, allowing for batch serialization of multiple configurations at once.

This functionality enhances the library's usability by providing users with a straightforward way to save and share model configurations, training parameters, or any other dataclass-based settings. This is particularly useful for reproducing experiments, sharing configurations across teams, or for logging purposes.

Documentation has been updated accordingly to include these new methods, along with examples and guidelines on how to use them effectively. Tests have been added to verify the correctness of the serialization process and to ensure that it works as expected for a variety of dataclass structures.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

Tagging @ArthurZucker @konstantinjdobler  @sgugger 


","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Replacing the LlamaDecoderLayer Class hugging Face With New LongNet,"### System Info

```shell
i working on the CodeLLama Model which Uses a Decoder-Only Model Transformer following Arch Blow

Main Task is replaced Decoder-Only which used Masked-Self-Attention and KV_cache with my own Encoder-Only which used Diltaed-Attention used in LongNet
```


### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```python 
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch
from transformers.models.llama.configuration_llama import LlamaConfig
from transformers.models.llama.modeling_llama import LlamaAttention , LlamaDecoderLayer , LlamaModel, LlamaForCausalLM


model_id = ""codellama/CodeLlama-7b-hf""
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16
).to(""cpu"")

class CondensedLlamaConfig(LlamaConfig):
    def __init__(
        self,
        dilation_rates=None,
        segment_lengths=None,
        is_causal=None,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.dilation_rates = dilation_rates
        self.segment_lengths = segment_lengths
        self.is_causal = is_causal

    # Override the `to_dict` method to include the new parameters
    def to_dict(self):
        base_dict = super().to_dict()
        config_dict = {
            ""dilation_rates"": self.dilation_rates,
            ""segment_lengths"": self.segment_lengths,
            ""is_causal"": self.is_causal
        }
        base_dict.update(config_dict)
        return base_dict

config.num_hidden_layers = 2
model_1 = CondensedLlamaModel(config)

import torch
import torch.nn as nn
from transformers.models.llama.modeling_llama import LlamaForCausalLM, LlamaDecoderLayer
from transformers.modeling_utils import ModuleUtilsMixin

class CondensedLlamaAttention(LlamaAttention):
    def __init__(self, config: CondensedLlamaConfig,layer_idx=None):
        super().__init__(config)

        self.LongNetAttention = MultiheadDilatedAttention(
            config.hidden_size,
            config.num_attention_heads,
            config.dilation_rates,
            config.segment_lengths
        )
        self.is_causal = config.is_causal


    def forward(self, input, is_causal=None):
        if is_causal is None:
            is_causal = self.is_causal
        x, _ = self.LongNetAttention(input, input, input, is_causal=is_causal)
        return x


class CondensedLlamaDecoderLayer(LlamaDecoderLayer):

    def __init__(self, config: CondensedLlamaConfig, layer_idx=None):  # Add layer_idx as an argument
        super().__init__(config, layer_idx=None)  # Pass layer_idx to the parent class constructor
        # Replace self_attn with your new attention module
        self.self_attn = MultiheadDilatedAttention(
            config.hidden_size,
            config.num_attention_heads,
            config.dilation_rates,
            config.segment_lengths
        )
        self.is_causal = config.is_causal


    def forward(self, input, is_causal=None):
        if is_causal is None:
            is_causal = self.is_causal
        x, _ = self.LongNetAttention(input, input, input, is_causal=is_causal)
        return x


class CondensedLlamaModel(LlamaModel):
    def __init__(self, config: CondensedLlamaConfig):
        super().__init__(config)

        self.layers = nn.ModuleList([CondensedLlamaDecoderLayer(config,layer_idx=None) for _ in range(config.num_hidden_layers)])
        # Initialize weights and apply final processing
        self.post_init()

model_2 = model.model

import torch
module_patterns_to_transfer = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj""]
def transfer_weights(original_model, custom_model, module_patterns_to_transfer):
    original_dict = original_model.state_dict()
    custom_dict = custom_model.state_dict()

    # Filter and transfer weights for specified layers
    for key in custom_dict.keys():
        for pattern in module_patterns_to_transfer:
            if pattern in key:
                if key in original_dict:
                    # Transfer weights
                    with torch.no_grad():
                        custom_dict[key].copy_(original_dict[key])

    # Load the updated state dictionary to the model
    custom_model.load_state_dict(custom_dict)

config= CondensedLlamaConfig(dilation_rates=[2048, 4096, 8192, 16384, 32768],segment_lengths=[1, 2, 4, 6, 12],is_causal=False)
config.num_hidden_layers = 2
model_1 = CondensedLlamaModel(config)



# Transfer weights from the original model to the model
transfer_weights(model_2, model_1, module_patterns_to_transfer)

# transferred weights in the custom model
for key, parameter in model_1.state_dict().items():
    print(key)
    print(parameter.size())
    print(parameter)
```

### Expected behavior

```shell
yeah i am aware of that
```


### Checklist

- [X] I have read the migration guide in the readme. ([pytorch-transformers](https://github.com/huggingface/transformers#migrating-from-pytorch-transformers-to-transformers); [pytorch-pretrained-bert](https://github.com/huggingface/transformers#migrating-from-pytorch-pretrained-bert-to-transformers))
- [X] I checked if a related official extension example runs on my machine.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Fix from pretrained ignoring errors,"# What does this PR do?
Draft for now",[],4,open
[WIP] Add musecoco,"# What does this PR do?
Add the attribute-to-music generation model in [MuseCoco](https://arxiv.org/abs/2306.00110). The model undergoes large-scale pre-training on 1M multitrack MIDI dataset, can be useful for various tasks by fine-tuning.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Models:

- text models: @ArthurZucker and @younesbelkada

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Failing Flash Attention 2 tests,"### System Info

- `transformers` version: 4.40.0.dev0
- Platform: Linux-5.4.0-166-generic-x86_64-with-glibc2.29
- Python version: 3.8.10
- Huggingface_hub version: 0.20.2
- Safetensors version: 0.4.2
- Accelerate version: 0.28.0
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.1.0+cu121 (True)
- Tensorflow version (GPU?): 2.13.1 (True)
- Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)
- Jax version: 0.4.13
- JaxLib version: 0.4.13
- Using GPU in script?: Yes - A100 80GB


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction


`RUN_SLOW=1 pytest tests/models -k ""flash_attn"" `

Results:

```
FAILED tests/models/bark/test_modeling_bark.py::BarkSemanticModelTest::test_flash_attn_2_from_config - ValueError: Unrecognized configuration class <class 'transformers.models.bark.configuration_bark.BarkSemanticConfig'> for this kind of AutoModel: AutoModelForCausalLM.
FAILED tests/models/bark/test_modeling_bark.py::BarkCoarseModelTest::test_flash_attn_2_from_config - ValueError: Unrecognized configuration class <class 'transformers.models.bark.configuration_bark.BarkCoarseConfig'> for this kind of AutoModel: AutoModelForCausalLM.
FAILED tests/models/bark/test_modeling_bark.py::BarkCoarseModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true
FAILED tests/models/gemma/test_modeling_gemma.py::GemmaModelTest::test_flash_attn_2_generate_padding_right - AssertionError: ValueError not raised
FAILED tests/models/gpt2/test_modeling_gpt2.py::GPT2ModelLanguageGenerationTest::test_flash_attn_2_generate_padding_left - AssertionError: Lists differ: ['<|e[102 chars]y of Bali, and who was a member of the Muslim [101 chars]rry""] != ['<|e[102 chars]y of Kolkata, was a member of the Kolkata', ""H[85 chars]rry""]
FAILED tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py::GPTBigCodeModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true
FAILED tests/models/gpt_neo/test_modeling_gpt_neo.py::GPTNeoModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true
FAILED tests/models/gpt_neox/test_modeling_gpt_neox.py::GPTNeoXModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true
FAILED tests/models/stablelm/test_modeling_stablelm.py::StableLmModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true
FAILED tests/models/whisper/test_modeling_whisper.py::WhisperModelTest::test_flash_attn_2_inference_padding_right - AssertionError: assert False
FAILED tests/models/whisper/test_modeling_whisper.py::WhisperStandaloneDecoderModelTest::test_flash_attn_2_inference - AssertionError: assert False
FAILED tests/models/whisper/test_modeling_whisper.py::WhisperStandaloneDecoderModelTest::test_flash_attn_2_inference_padding_right - AssertionError: assert False
```

### Expected behavior

Flash Attention tests are all expected to pass. 
I'll look into Bark, the rest of the models failing are:
- Whisper (cc @sanchit-gandhi)
- StableLM
- GPTNeo
- GPTNeoX
- GPTBigCode
- GPT2
- Gemma

cc  @ArthurZucker @amyeroberts 
","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",3,open
SigLIP tokenizer not enforcing use_fast=True,"### System Info

- `transformers` version: 4.38.2
- Platform: Linux-4.18.0-477.27.1.el8_8.x86_64-x86_64-with-glibc2.28
- Python version: 3.10.13
- Huggingface_hub version: 0.21.4
- Safetensors version: 0.4.2
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.2.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

_No response_

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```
from transformers import AutoTokenizer
t=AutoTokenizer.from_pretrained('google/siglip-so400m-patch14-384', use_fast=True)
assert t.is_fast, 'tokenizer is not fast'
print('Success')
```

### Expected behavior

print 'Success' which indicates `use_fast=True`","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",5,open
Support DBRX Model,"### Feature request

Support the DBRX model (only correct pronunciation: DB-Rex) [blog post](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm).

Code is from the open source [databricks/dbrx](https://github.com/databricks/dbrx) repository.

### Motivation

> Across a range of standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides the open community and enterprises building their own LLMs with capabilities that were previously limited to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.

### Your contribution

https://github.com/huggingface/transformers/pull/29910","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
Change Device Allocation and Improve Shape Handling in Trainer evaluation loop,"### Feature request

## Context
Currently when setting the [compute_metrics](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L360) parameter as non-null value it sets the [prediction_loss_only](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3402)  param as `False`.

As `prediction_loss_only` is `False`, [self.prediction_step](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3586) will return non-null value for the `logits` variable.

This variable will be used here to be [padded](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3608), then [concatenated](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3612) to previously stored logits.

## What bothers me

### Device Allocation
If the logits are tensors, a new tensor will be created using the same device as logits' one ([cf](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_pt_utils.py#L91)). Meaning that if `logits` is on `cuda:0`, the resulting tensor will also be on `cuda:0`. This unnecessarily consumes VRAM as after the tensor will be moved back to CPU ([cf](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3626)) without further operations.

IMO the resulting tensor should be on CPU from the beginning to avoid consuming VRAM unnecessarily. But let me know if I miss something :)

### Handling of Shapes

It seems that the concatenation is only done across `logits` ([cf](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3608)), not taking into account the shape of previously registered logits in `preds_host`. Meaning that at each iteration, the `logits` should have the same max length otherwise it will raise the following error : `RuntimeError: Sizes of tensors must match except in dimension 0. Expected size [SHAPE] but got size [ANOTHER SHAPE] for tensor number 1 in the list.`

This causes issue for `AnyToSeq` models (i.e `SeqToSeq`) where the length of the outputs sequences can vary from one iteration to another. The current workaround would be to pad the output to a fix max length (i.e by padding the input itself to a fix max length) however this greatly increase the necessary compute, thus making the training process slower.

A fix could be to directly cast the logits to numpy with a non-constant shapes across logits.

---

@muellerzr I would be happy to have your input on that one

### Motivation

For more context, I encountered those behaviors when working on [that issue](https://github.com/Thytu/SMIT/issues/6).

Fixing both behaviors would :
* Remove the spikes in VRAM usage generated when using `compute_metrics`
* Avoid having to pad the model's input unnecessarily thus reducing the compute required and making the training faster.

### Your contribution

It's kinda of a blocker on my side, so I would be happy to work on a PR :)","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
LLaVA `torch.compile` implementation,"### Feature request

As per #28981, LLaVA is planned to receive `torch.compile` support. Seeing to the fact that LLaVA is composed of a vision tower and a LLM, both of which can be separately compiled with `fullgraph=True` (after support has been added, which is not the case for Mistral), it seems much easier to compile both parts separately as well.

### Motivation

The `_merge_input_ids_with_image_features` function that connects the two parts is difficult to compile as PyTorch has yet to add support for many of the functions used that require dynamic input sizes, which are necessary here as the number of input image tokens is subject to change.

### Your contribution

I'd love to try submitting a PR if possible but I'm not sure what the best way to do so is given the current circumstances.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 6625174977, 'node_id': 'LA_kwDOCUB6oc8AAAABiuQlwQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Compilation', 'name': 'Compilation', 'color': '58E75E', 'default': False, 'description': 'Issues related to torchdynamo and torchinductor'}]",11,open
Implement SuperGlue model,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #25489 
This PR is the next step after implementing [SuperPoint](https://github.com/huggingface/transformers/pull/28966) to implement image matching through keypoint matching.

**Colab notebook with inference example:**
https://colab.research.google.com/drive/1NhwofZFzy7IMN4irN-jC-9LZy7dx_GZ2?usp=sharing

## Who can review?

@amyeroberts 
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",53,open
Added HelpingAI model type in it,"### Model description


Introduction
HelpingAI-3B is a state-of-the-art AI model designed to assist with day-to-day tasks. It's trained on a diverse range of datasets, making it versatile and adaptable to various applications.

Model Overview
HelpingAI-3B is the latest model in the HelpingAI series. It's built on advanced machine learning algorithms and trained on a wide variety of data sources. This ensures that the model is capable of understanding and generating responses in a wide range of contexts.

Usage code
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

# Let's bring in the big guns! Our super cool HelpingAI-3B model
model = AutoModelForCausalLM.from_pretrained(""OEvortex/HelpingAI-3B"", trust_remote_code=True, torch_dtype=torch.bfloat16).to(""cuda"")

# We also need the special HelpingAI translator to understand our chats
tokenizer = AutoTokenizer.from_pretrained(""OEvortex/HelpingAI-3B"", trust_remote_code=True, torch_dtype=torch.bfloat16)

# This TextStreamer thingy is our secret weapon for super smooth conversation flow
streamer = TextStreamer(tokenizer)

# Now, here comes the magic! ✨ This is the basic template for our chat
prompt = """"""
<|im_start|>system: {system}
<|im_end|>
<|im_start|>user: {insaan}
<|im_end|>
<|im_start|>assistant:
""""""

# Okay, enough chit-chat, let's get down to business!  Here's what our system will say to the user
system = ""You are an adaptive and versatile AI assistant, ready to help with various topics and situations while maintaining a conversational, engaging, and friendly tone. You aim to provide accurate, comprehensive information and advice. Be open to feedback and adjust your responses based on user input. Always show empathy and understanding in your conversations.""


# And the insaan is curious (like you!) insaan means user in hindi
insaan = ""Hey HelpingAI, how's it going?""

# Now we combine system and user messages into the template, like adding sprinkles to our conversation cupcake
prompt = prompt.format(system=system, insaan=insaan)

# Time to chat! We'll use the tokenizer to translate our text into a language the model understands
inputs = tokenizer(prompt, return_tensors=""pt"", return_attention_mask=False).to(""cuda"")

# Here comes the fun part!  Let's unleash the power of HelpingAI-3B to generate some awesome text
generated_text = model.generate(**inputs, max_length=3084, top_p=0.95, do_sample=True, temperature=0.7, use_cache=True, streamer=streamer)
```

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Bart evaluation throws the following error at generate(): UnboundLocalError: 'model_kwargs['decoder_attention_mask']' is used before assignment,"### System Info

- `transformers` version: 4.39.0
- Platform: Linux-5.4.0-167-generic-x86_64-with-glibc2.31
- Python version: 3.10.13
- Huggingface_hub version: 0.21.4
- Safetensors version: 0.4.2
- Accelerate version: 0.28.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.2.1+cu121 (False)
- Tensorflow version (GPU?): 2.16.1 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help?

_No response_

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

1. Run the run_clm.py with bart given at https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization
2. Use the latest tensorflow native version (2.16.1) and latest transformers version (4.39.1)
3. I run only inference and so give only --do_eval while running the command

### Expected behavior

Produce evaluation results","[{'id': 1834054694, 'node_id': 'MDU6TGFiZWwxODM0MDU0Njk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/TensorFlow', 'name': 'TensorFlow', 'color': 'FF6F00', 'default': False, 'description': 'Anything TensorFlow'}, {'id': 1936351150, 'node_id': 'MDU6TGFiZWwxOTM2MzUxMTUw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Examples', 'name': 'Examples', 'color': 'd4c5f9', 'default': False, 'description': 'Which is related to examples in general'}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",4,open
Int4 transformers training,"### Feature request

There is a GitHub repo out with the necessary kernels and code (and a great paper) to train a transformer based models using int4. 

The authors use a couple of algorithms to get around the struggle of quantizing down to int4 including keeping non linear operators in fp16 to avoid certain quant issues, they solve the outlier problem by ""propose a Hadamard quantizer (HQ) to solve the outlier problem. Its main idea is to quantize the matrices in another linear space which has fewer outliers.""  The results they achieved were ""We compare the training throughput of the FP16 PyTorch AMP and our INT4 training algorithm for training BERT [24] and GPT [37]-style language models on a system of 8 Nvidia A100 GPUs. We vary the hidden layer size, intermediate fully-connected layer size, and batch size, and plot the speedup of INT4 training in Fig. 5. Our INT4 training algorithm can achieve up to 35.1% speedup for BERT-style models and up to 26.5% speedup for GPT-style models."" 

These results are without using Flash Attention which would increase gains further, and you could use the Galore 8bit optimizer, or better yet Deep speeds 1bit Adam optimizer, fully offloaded to the CPU. For optimized full fine tuning of large 7b models on consumer hardware.


This code and paper is for FFT but this same concept could apply directly for Lora and QLora.


Links:
[Paper](https://arxiv.org/pdf/2306.11987.pdf)
[Code](https://github.com/xijiu9/Train_Transformers_with_INT4)

### Motivation

Having int4 as a trainable dtype would provide a ton of utility. On 2 consumer 3090’s you get 1 TFLOP of compute according to Nvidia’s documentation. It would increase the training possibilities for the gpu poor. And significantly increase training speed on for server applications.

### Your contribution

Gathered information. I’m not very good at coding, at least not good enough to add to transformer repo. This might be too long of an endeavor, so if it is sorry for wasting your time, and we can close this feature request.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
About new ClearML Intergrations,"### Feature request

We need add set / filter metrics that displayed on ClearML plots. It's hard to use it with a tons of metrics that have diffrerent scale on the one plot.

### Motivation

This new implementation ClearML callback's output all metrics from log as multiple series at one plot, that's looks not readable. 
from https://github.com/huggingface/transformers/pull/28559
![image](https://github.com/huggingface/transformers/assets/6096108/614fabc2-0d1c-4320-b4cc-7ae61bbae88d)
For example: runtime and samples_per_seconds has different scale than other metrics.

Previous version report metrics as one metric - one plot.

Any ideas how can we set / filter metrics without code changes?
As temporarily solution I add my own callback for send some metrics to ClearML at other plots.

### Your contribution

Yes, I can.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6675366189, 'node_id': 'LA_kwDOCUB6oc8AAAABjeIBLQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Integrations', 'name': 'Integrations', 'color': '051E90', 'default': False, 'description': ''}]",1,open
Mixture of All Intelligence (MoAI),"### Model description

A new large language and vision model (LLVM) that uses auxiliary visual information and natural language for prediction.

It uses 2 modules: 𝙈𝙤𝘼𝙄-𝘾𝙤𝙢𝙥𝙧𝙚𝙨𝙨𝙤𝙧 and 𝙈𝙤𝘼𝙄-𝙈𝙞𝙭𝙚𝙧. Here 𝗖𝗼𝗺𝗽𝗿𝗲𝘀𝘀𝗼𝗿 condenses the verbalized outputs of the external CV models into auxiliary visual information and 𝗠𝗶𝘅𝗲𝗿 blends three types of intelligence — visual features, auxiliary features from external CV models and language features into a cohesive whole.

MoAI-7B surpasses both open-source and closed-source LLVMs in vision language tasks.

Model repo: https://github.com/ByungKwanLee/MoAI


### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6726435287, 'node_id': 'LA_kwDOCUB6oc8AAAABkO1B1w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Multimodal', 'name': 'Multimodal', 'color': 'F3E08B', 'default': False, 'description': ''}]",4,open
Support batching sequence labels pairs for zero shot classification pipeline,"### Feature request

Currently zero shot classification pipeline dose not support specifying different set of labels for each sample input when batching multiple sequences of input. We want support input of a pair of sequence and its labels so that we can run prediction on them in batch.

Following is an example of what sample input we want to specify:

```
samples = [
(""Iphone 15 pro max"", [""mobiles"", ""appliances"", ""toys""]),
(""Iphone 15 pro max"", [""Samsung"", ""Apple""]),
(""galaxy s21 ultra"", [""mobiles"", ""appliances"", ""toys""]),
(""galaxy s21 ultra"", [""Samsung"", ""Apple""])
]
```


### Motivation

By grouping labels with sequences pairs we can batch different type of labels for each sample and by batching it runs on gpu will be faster.

### Your contribution

I will work on this feature and create a pr for it then update this issue. Making the issue now just to make sure that no one is working on this while I am doing it.","[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNzcxMTg3OTI0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Pipeline', 'name': 'Core: Pipeline', 'color': 'FF7066', 'default': False, 'description': 'Internals of the library; Pipeline.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
"Community contribution: enabling `device_map=""auto""` support for more vision and multimodal models","### Feature request

# Feature Request 

`transformers` models can be easily loaded across multiple devices using `device_map=""auto""`. This will automatically allocate weights across available devices e.g. GPUs and offload any weights onto CPU, then disk as necessary. This is useful when doing inference with large models. 

To enable this, `_no_split_modules` has to be defined in the model's pretrained model class e.g. [like here for LLaMa](https://github.com/huggingface/transformers/blob/fadb053379b3ef24c4ec8e6d7d58555af21f58db/src/transformers/models/llama/modeling_llama.py#L793). This defines layers which should not be split across devices, and should contain as few layers as possible.

### Steps to add
* Pick a model to work on and open a PR - comment on this issue to say which model you're working on
* Define `_no_split_modules` in the PreTrainedModel subclass. Try with `_no_split_modules = []` first
* Enable testing
    * Ensure the following tests are not skipped for the model: `test_disk_offload_bin`, `test_disk_offload_safetensors`, `test_cpu_offload`, `test_model_parallelism`, `test_model_parallel_beam_search`
    * Run the tests in a multi-gpu environment `pytest tests/models/{MODEL_NAME}/test_modeling_{MODEL_NAME}.py -vv -k ""offload or parallelism""`

## Models
- [ ] [Align](https://github.com/huggingface/transformers/blob/main/src/transformers/models/align/modeling_align.py)
- [ ] [Altclip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py)
- [x] [Beit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/beit/modeling_beit.py) #30379
- [ ] [Bit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bit/modeling_bit.py)
- [ ] [Blip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/modeling_blip.py)
- [ ] [Chinese_clip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/chinese_clip/modeling_chinese_clip.py)
- [x] [Convnext](https://github.com/huggingface/transformers/blob/main/src/transformers/models/convnext/modeling_convnext.py) #30207
- [x] [Convnextv2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py) #30207
- [x] [Cvt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/cvt/modeling_cvt.py) #30207
- [ ] [Data2vec](https://github.com/huggingface/transformers/blob/main/src/transformers/models/data2vec/modeling_data2vec_vision.py)
- [ ] [Depth_anything](https://github.com/huggingface/transformers/blob/main/src/transformers/models/depth_anything/modeling_depth_anything.py)
- [ ] [Dinat](https://github.com/huggingface/transformers/blob/main/src/transformers/models/dinat/modeling_dinat.py)
- [ ] [Dinov2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/dinov2/modeling_dinov2.py)
- [ ] [Donut](https://github.com/huggingface/transformers/blob/main/src/transformers/models/donut/modeling_donut_swin.py)
- [ ] [Dpt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpt/modeling_dpt.py)
- [ ] [Efficientformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/efficientformer/modeling_efficientformer.py)
- [x] [Efficientnet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/efficientnet/modeling_efficientnet.py) #29989
- [ ] [Flava](https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py)
- [ ] [Focalnet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/focalnet/modeling_focalnet.py) #30207
- [ ] [Git](https://github.com/huggingface/transformers/blob/main/src/transformers/models/git/modeling_git.py)
- [x] [Glpn](https://github.com/huggingface/transformers/blob/main/src/transformers/models/glpn/modeling_glpn.py) #30207
- [ ] [Groupvit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/groupvit/modeling_groupvit.py)
- [x] [Imagegpt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/imagegpt/modeling_imagegpt.py) #30207
- [ ] [Layoutlmv3](https://github.com/huggingface/transformers/blob/main/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py)
- [x] [Levit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/levit/modeling_levit.py) #30207
- [ ] [Mask2former](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mask2former/modeling_mask2former.py)
- [ ] [Maskformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/maskformer/modeling_maskformer.py)
- [ ] [Maskformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/maskformer/modeling_maskformer_swin.py)
- [x] [Mgp_str](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mgp_str/modeling_mgp_str.py) #30207
- [x] [Mobilenet_v1](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py) #30207
- [x] [Mobilenet_v2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py) #30207
- [x] [Mobilevit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mobilevit/modeling_mobilevit.py) #30207
- [ ] [Mobilevitv2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py)
- [ ] [Nat](https://github.com/huggingface/transformers/blob/main/src/transformers/models/nat/modeling_nat.py)
- [ ] [Oneformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/oneformer/modeling_oneformer.py)
- [ ] [Perceiver](https://github.com/huggingface/transformers/blob/main/src/transformers/models/perceiver/modeling_perceiver.py)
- [x] [Poolformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/poolformer/modeling_poolformer.py) #30207
- [ ] [Pvt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/pvt/modeling_pvt.py)
- [x] [Regnet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/regnet/modeling_regnet.py) #30207
- [ ] [Resnet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/resnet/modeling_resnet.py) #30207
- [x] [Sam](https://github.com/huggingface/transformers/blob/main/src/transformers/models/sam/modeling_sam.py) #30207
- [ ] [Segformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/segformer/modeling_segformer.py)
- [x] [Swiftformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swiftformer/modeling_swiftformer.py) #30207
- [x] [Swin](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swin/modeling_swin.py) #30207
- [ ] [Swin2sr](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swin2sr/modeling_swin2sr.py)
- [x] [Swinv2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swinv2/modeling_swinv2.py) #30207
- [ ] [Timesformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/timesformer/modeling_timesformer.py)
- [ ] [Timm_backbone](https://github.com/huggingface/transformers/blob/main/src/transformers/models/timm_backbone/modeling_timm_backbone.py)
- [x] [Trocr](https://github.com/huggingface/transformers/blob/main/src/transformers/models/trocr/modeling_trocr.py) #30207
- [ ] [Tvlt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/tvlt/modeling_tvlt.py)
- [ ] [Tvp](https://github.com/huggingface/transformers/blob/main/src/transformers/models/tvp/modeling_tvp.py)
- [x] [Upernet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/upernet/modeling_upernet.py) #30207
- [ ] [Videomae](https://github.com/huggingface/transformers/blob/main/src/transformers/models/videomae/modeling_videomae.py)
- [ ] [Vit_mae](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit_mae/modeling_vit_mae.py)
- [ ] [Vit_msn](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit_msn/modeling_vit_msn.py)
- [x] [Vitmatte](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vitmatte/modeling_vitmatte.py) #30379
- [x] [Vivit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vivit/modeling_vivit.py) #30379
- [ ] [X_clip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/x_clip/modeling_x_clip.py)
- [x] [Yolos](https://github.com/huggingface/transformers/blob/main/src/transformers/models/yolos/modeling_yolos.py) #30207

### Motivation

Enable a powerful HF feature for all of our vision models

### Your contribution

Ping me for review 🤗 ","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",15,open
[Feature request] FastV: Plug-and-play inference acceleration  for  large vision language models,"### Feature request

FastV is a plug-and-play inference acceleration method for large vision language models relying on visual tokens. It could reach 45% theoretical FLOPs reduction without harming the performance through pruning redundant visual tokens in deep layers. It is originally implemented in https://github.com/pkunlp-icler/FastV  , with [paper](https://arxiv.org/abs/2403.06764) and [demo](https://www.fastv.work/)

![fastv_tradeoff](https://github.com/huggingface/transformers/assets/43977888/97490036-8942-440f-b46b-5629e68d817a)

*aokvqa results*
| Model                         | Score | latency / first output token (A100 80G) | GPU Memory |
| ----------------------------- | ----- | --------------------------------------- | ---------- |
| 7B Vanilla Decoding           | 76.8  | 0.138s                                  | 18G        |
| 13B Vanilla Decoding          | 81.9  | 0.203s                                  | 33G        |
| \- 13B FastV (K=2 R=25%)      | 81.8  | 0.181s                                  | 29G        |
| \- 13B FastV (K=2 R=50%)      | 81.3  | 0.155s                                  | 28G        |
| \- 13B FastV (K=2 R=75%)      | 80.9  | **0.124s**                                  | 27G        |
| 13B Vanilla Decoding 4Bit     | 81.5  | 0.308s                                  | 12G        |
| \- 13B FastV 4Bit (K=2 R=25%) | 81.7  | 0.277s                                  | 11G        |
| \- 13B FastV 4Bit (K=2 R=50%) | 81.1  | 0.275s                                  | 10G        |
| \- 13B FastV 4Bit (K=2 R=75%) | 80.3  | 0.245s                                  | **9G**         |

### Motivation

I want to merge the FastV feature to speed up the inference of HF's multimodal models (Llava is the first one).  Since Llava is based on Llama, I need to add features to both the Llama and Llava model. 

I would add a fastv_config parameters to the modeling_llava.py from llava and add a new forward function named fastv_forward from llama. I want to know if it is ok to do that or I should create a new model.

### Your contribution

I would submit a PR. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}, {'id': 6712797012, 'node_id': 'LA_kwDOCUB6oc8AAAABkB0nVA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/optimization', 'name': 'optimization', 'color': '47E0D2', 'default': False, 'description': ''}]",0,open
Add option to not re-load the model when resuming from checkpoint.,"### Feature request


I'd like to have the option to get the trainer to resume_from_checkpoint *without* reloading the model.

I propose adding a boolean option parameter:  `without_checkpoint_model: bool = False`.

Starting from a checkpoint with this parameter set to True would require that the model was supplied to the Trainer constructor (pretty standard practice anyway) and would simply skip this line of code in the Trainer:
`self._load_from_checkpoint(resume_from_checkpoint)`



@muellerzr , @pacman100, @amyeroberts 


### Motivation

When Trainer.train(resume_from_checkpoint=my_checkpoint) is called, the Trainer will attempt to re-load the model in the my_checkpoint directory during the resumption process.

This is kind of wasteful but no big tragedy if the model in question in fully supported by Trainer.
Right now, Trainer needs to know how to reload a model before it can be used with resume_from_checkpoint.

This was (and may be still) a big problem for QLoRA model users who had to spend quite some time waiting for Trainer to be modified to specifically support re-loading QLoRA models.  One could train these models from scratch just fine by loading them first and supplying them to the Trainer constructor, it was just impossible to resume_from_checkpoint with them.

It looks like there is supposed support for resuming with QLoRA added very recently but then some folks complaining that it does not actually work in practice:

- https://github.com/huggingface/transformers/issues/29607
- https://github.com/huggingface/transformers/issues/29383

But even if QLoRA reloading does work today or will soon, there is a wider problem.

In general, whenever there is a new model type that the Trainer does not explicitly know how to load, it will be impossible to resume_from_checkpoint for that kind of model until support is added to Trainer for loading that type of model.
Lots of new kinds of quantization and LoRA initialization methods are being invented all the time (e.g LoftQ).  Do we really want to prevent their adoption because it takes time to adapt Trainer to intrinsically know how to load every kind of model under the sun?

We might do well to just provide folks a way to work around that without having to fork transformers and then use their patched fork (like I've been doing).

Really, since the Trainer can be instantiated with the fully restored checkpoint, there is no reason why the train() call should force re-loading the model from the checkpoint.


### Your contribution

I've already got a POC implementation in my fork which I've pretty thoroughly integration tested, having resumed  QLoRA training from checkpoint with it dozens of times.

https://github.com/stevemadere/transformers/commit/5c2a1c8200392abfb268fc7407121de605fc7a33

PR incoming.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
[Feature Request] add layer-wise optimizers,"### Feature request

Context: https://github.com/huggingface/transformers/pull/29588#discussion_r1523510004

### Motivation

The layer-wise optimizers is not GaLore-specific. We could apply it to generic optimizers to save memory. For example, the 8bit Adam optimizer paired with the layer-wise optimization sounds like a pretty good option for me.

### Your contribution

Have a try when it is supported","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6712797012, 'node_id': 'LA_kwDOCUB6oc8AAAABkB0nVA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/optimization', 'name': 'optimization', 'color': '47E0D2', 'default': False, 'description': ''}]",5,open
[Mamba] Possible Issue in beam search with Mamba HF models,"### System Info

nightly build transformers version

### Who can help?

@ArthurZucker 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```
from transformers import AutoModelForCausalLM, AutoTokenizer
device = ""cuda:0""
tokenizer = AutoTokenizer.from_pretrained(""state-spaces/mamba-130m-hf"")
model = AutoModelForCausalLM.from_pretrained(""state-spaces/mamba-130m-hf"").to(device)

input_prompts = [""Question: Who is the lead singer of Coldplay? Answer:"",]
prompted_encoded = tokenizer(input_prompts[0:1], return_tensors=""pt"", padding=True).to(device)['input_ids']

with torch.inference_mode():
        generated_tokens = model.generate(input_ids=prompted_encoded, max_length=60, use_cache=True, num_beams=5)
        decoded_tokens = tokenizer.batch_decode(generated_tokens)
        print(""Generation finished."")
        print(decoded_tokens)
```

### Expected behavior

We are observing a degradation of the mamba model generated output as `num_beams` parameter goes up. 

Example with num_beams=1
```
Generation finished.
['Question: Who is the lead singer of Coldplay? Answer: The lead singer of Coldplay is the lead singer of the band Coldplay.\n\nThe lead singer of Coldplay is the lead singer of the band Coldplay. The lead singer of Coldplay is the lead singer of the band']
```

Example with num_beams=100:
```
Generation finished.
['Question: Who is the lead singer of Coldplay? Answer: Coldplay\n\n\n Cold isfield\n\n\nQuestion:\nColdplayplayfield\n\n\n:\n\n:\n\n:\n\n:\n\n:\n\n:\n\n:\n\n:\n\n:\n']
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
fix model names,This PR fixes the model name problems existing in Qwen2 related codes and docs,[],9,open
Moving in a folder & `push_to_hub` for a `trust_remote_code=True` model,"### Feature request

Bonjour !

I'm opening an issue following a discussion with Lysandre on Slack.

My request is to be able to do `..` in model repositories on HF (where currently you can only do `.`). On this point, I don't know if this applies to all template directories or only to customs, which then require a `trust_remote_code=True` to load them.

A second request is that when you have a custom model (i.e. loadable via `trust_remote_code=True`) and once it's finetuned, that the `push_to_hub` function pushes all the files needed for the model to function properly, not just `config.json`, `configuration.py`, `model.safetensors`, `special_tokens_map.json`, `tokenizer.json`, `tokenizer_config.json` and `training_args.bin`.

### Motivation

The concrete case behind my requests.


We recently extended Flash Attention to the T5. 

So we had to develop a custom implementation and to load our pre-trained models for finetuning, we have to do :

```
from transformers import AutoModel
model = AutoModel.from_pretrained(""CATIE-AQ/FAT5-base-UL2-fr"", trust_remote_code=True)
```

For this to work, we need a [modeling file](https://huggingface.co/CATIE-AQ/FAT5-base-UL2-fr/blob/main/modeling_flash_t5.py).
 
In our code on GitHub (https://github.com/catie-aq/flashT5/blob/main/src/model/modeling_flash_t5.py), we call up classes that we've put in a `utils` folder and import them, for example (line [47](https://github.com/catie-aq/flashT5/blob/dcb45ec44b29ba9a7a04564a90a34a57fa65f490/src/model/modeling_flash_t5.py#L47)) a `..utils.positional_encoding import ALiBiPositionalEncoding, RelativePositionalEncoding, RotaryPositionalEncoding`.
On HF, this returned an error saying that there was a `..` in the `modeling_flash_t5.py` code and that it was therefore not possible to retrieve the classes. We therefore had to move all the code contained in the `utils` folder to the root.

![image](https://github.com/huggingface/transformers/assets/58078086/273787a2-f6c6-43d8-a211-587b683b3f43)

The line I used as an example above then becomes from `.positional_encoding import ALiBiPositionalEncoding, RelativePositionalEncoding, RotaryPositionalEncoding` and it works.

So being able to use classes contained in files would be appreciated 😄


The second request is related to the fact that, once this model has been finetuned, I do a `push_to_hub` to save the weights.
This pushes me the files [config.json](https://huggingface.co/bourdoiscatie/FAT5_small_cls/blob/main/config.json), [configuration_flash_t5.py](https://huggingface.co/bourdoiscatie/FAT5_small_cls/blob/main/configuration_flash_t5.py), [model.safetensors](https://huggingface.co/bourdoiscatie/FAT5_small_cls/blob/main/model.safetensors), [special_tokens_map.json](https://huggingface.co/bourdoiscatie/FAT5_small_cls/blob/main/special_tokens_map.json), [tokenizer.json](https://huggingface.co/bourdoiscatie/FAT5_small_cls/blob/main/tokenizer.json), [tokenizer_config.json](https://huggingface.co/bourdoiscatie/FAT5_small_cls/blob/main/tokenizer_config.json) and [training_args.bin](https://huggingface.co/bourdoiscatie/FAT5_small_cls/blob/main/training_args.bin).
And when I then want to reload the model to do inference, it tells me that the 8 files circled in red in the image above + the [modeling_flash_t5.py](https://huggingface.co/CATIE-AQ/FAT5-base-UL2-fr/blob/main/modeling_flash_t5.py) file are missing.

So every time I finetune, I have to do a second push where I add these 9 missing files so that my model can load properly.

Wouldn't it be possible for these files (which are detected during model loading) to be pushed directly with the 1st `push_to_hub`? 🤗

### Your contribution

Let me know if there's any way I can help.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Grok-1 MoE support,"### Model description

X-AI recently released [grok-1](https://huggingface.co/xai-org/grok-1), a massive MoE model, with a total parameter count of 314B across 8 experts, 2 active at a time. Would be interesting if we could have support for the weights in transformers, would make integrations everywhere else much easier to handle.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

They have a reference implementation in JAX, but similar to meta's original release of llama, the inference code is highly unoptimized, and has a hard-coded number of GPUs as a requirement.

https://github.com/xai-org/grok-1","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
mamba generation throughput lower than original due to DecodingCGCache,"### System Info

Python 3.10.13, CUDA 12.1
GPU = NVIDIA GeForce RTX 2080 Ti. Max memory = 10.747 GB.

torch==2.2.1
torchaudio==2.1.0
torchvision==0.16.0
tokenizers==0.15.2
transformers ==git+https://github.com/huggingface/transformers@dd1c9052159ae824c8acef7c2552f9fad5ca020a
triton==2.2.0
causal_conv1d==git+https://github.com/Dao-AILab/causal-conv1d.git@96456720c00393a5c32872d8352d7a7ec31fb3db#egg=causal_conv1d
mamba_ssm==git+https://github.com/state-spaces/mamba.git@9127d1f47f367f5c9cc49c73ad73557089d02cb8#egg=mamba_ssm

### Who can help?

text models: @ArthurZucker and @younesbelkada
generate: @gante

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

The key model initialization and generation parts are given as below.

## Original code repo
In the original [code repo](https://github.com/state-spaces/mamba/blob/main/benchmarks/benchmark_generation_mamba_simple.py)

```
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel
model = MambaLMHeadModel.from_pretrained(""state-spaces/mamba-130m"")
model.eval()

model.generate(
        input_ids=input_ids,
        max_length=max_length,
        **cg=True**
    )
```
Then throughput for generating 1K length is
```
Number of parameters: 129135360
Prompt length: 100, generation length: 1000
Prompt processing + decoding time: 1011 ms
```

## Using the HF library
```
from transformers import MambaForCausalLM
model = MambaForCausalLM.from_pretrained(""state-spaces/mamba-130m-hf"")
model.eval()

model.generate(
        input_ids=input_ids,
        max_length=max_length
    )
```
Then throughput for generating 1K length is
```
Number of parameters: 129135360
Prompt length: 100, generation length: 1000
state-spaces/mamba-130m-hf prompt processing + decoding time: 15970ms
```


### Expected behavior

The ""cg=True"" is [confirmed to be the part has a significant impact on the generation performance](https://github.com/state-spaces/mamba/issues/90) for mamba.

I have tried:

1. Passing the ""use_cache=True"" as follows won't affect the results
```
model = MambaForCausalLM.from_pretrained(""state-spaces/mamba-130m-hf"", use_cache=True)
or
model = MambaForCausalLM.from_pretrained(""state-spaces/mamba-130m-hf"", cache_params={use_cache: True})
or
model.config.use_cache=True
```

2. Modifying the mamba model to force the argument ""use_cache=True"" in the [MambaModel](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mamba/modeling_mamba.py), but still not working.


I assume this is related to the #29605, but modifying the argument directly seems not solving the problem.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 6625174977, 'node_id': 'LA_kwDOCUB6oc8AAAABiuQlwQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Compilation', 'name': 'Compilation', 'color': '58E75E', 'default': False, 'description': 'Issues related to torchdynamo and torchinductor'}]",22,open
Add distribution params to time series output,"# What does this PR do?

This PR is adding the distribution params and the name of the distribution to the `SampleTSPredictionOutput`. Additionally, it enables the distribution outputs in the autoformer, informer, and time series transformer. 
<!-- Remove if not applicable -->

Fixes #29556 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?

I extended existing tests

## Who can review?

@kashif 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],19,open
Supporting Selective Activation Checkpointing and CPU Offloading Option.,"### Feature request

- references
  - [deepspeed docs](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html)

### Motivation

Hi, first of all, thank you for your great contribution for open-source community, HF team !
I really enjoy training with HF transformers but found that activation checkpointing (a.k.a gradient checkpointing) is applied for every layers and checkpoints activation in GPU memory where it can easily leads to GPU OOM.
However, there are well-known techniques to checkpoint activations selectively and even in CPU.
Implementations of these features can be found in Megatron and DeepSpeed, and torch team also release [the blog](https://pytorch.org/blog/maximizing-training/#selective-activation-checkpointing) about this technique today.  

So i tested selective activation checkpointing with CPU offloading by modifying HF model (in my case, gpt-2 xl).
i just changed two things, `model._gradient_checkpointing_func` and forward pass like below.

```python
from transformers import AutoConfig, AutoModelForCausalLM
from deepspeed.runtime.activation_checkpointing import checkpointing

# init model and activate grad ckpt
model = AutoModelForCausalLM(**kwargs)
model.gradient_checkpointing_enable()

# change grad ckpt function
checkpointing. configure(
    mpu_=None,
    deepspeed_config=None,
    partition_activations=False,
    contiguous_checkpointing=False,
    num_checkpoints=num_checkpoints, # only use these for now
    checkpoint_in_cpu=True, # only use these for now
    synchronize=False,
    profile=True,
)
model._gradient_checkpointing_func = checkpointing.checkpoint # use deepspeed activation checkpointing function
```

```python
def forward(**inputs):
    ...
    def custom(start, end):
        def custom_forward(*inputs):
            layers_ = self.h[start:end]
            x_ = inputs[0] # hidden_states
            head_mask_ = inputs[3] # hidden_states
            for i, layer in enumerate(layers_):
                x_ = layer(
                    x_, 
                    None,
                    inputs[2],
                    head_mask_[i],
                    inputs[4],
                    inputs[5],
                    False,
                    None
                )[0] # tuple to tensor
                # Tra()
            return x_
        return custom_forward

    if self.gradient_checkpointing and self.training:
        # print('use_deepspeed_activation_checkpointing is True !')
        # Tra()
        l = 0
        total_num_layers = len(self.h)
        chunk_length = num_layers ## how many checkpoint do you want?
        # print(f'chunk_length : {chunk_length}')

        while l < total_num_layers:
            if self.model_parallel:
                raise NotImplementedError(""Idc model parallel :)"")
            if output_hidden_states:
                raise NotImplementedError(""Idc layerwise output :)"")

            hidden_states = self._gradient_checkpointing_func(
                custom(l, l+chunk_length),
                hidden_states,
                None,
                attention_mask,
                head_mask[l:l+chunk_length],
                encoder_hidden_states,
                encoder_attention_mask,
                False,
                None,
            )

            if output_attentions:
                raise NotImplementedError(""We use efficient SDPA and it does not support to output attention score map"")
            if self.model_parallel:
                raise NotImplementedError(""Idc model parallel :)"")

            l += chunk_length
```

and i could see significant improvement in GPU memory (trading off throughput).

<img width=""1238"" alt=""cpu_gpu_elapsed_from_gpt4"" src=""https://github.com/huggingface/transformers/assets/48202736/9f9d102c-4c35-4216-b8b7-440fdc127ff8"">

The quantitative results (CPU/GPU memory and elapsed time for 3 times forwarding) are recorded by torch profiler and figure was generated by GPT-4. 
My setting was like this.

- 2x A100 80GB
- gpt2-xl (near 1.5B)
- fixed batch size with same random seed (`[128, 512] # B, T`)
- zero-3 without CPU offloading (using accelerate)
- flash attn 2 is applied (monkey patched with xformers)

Results show that it could save about 10GB for each GPU device.
The reason why reserved GPU memory for checkpointing every 2, 3, 6 layers are same is that peak memory is dominated by computing loss i guess.
here is my detailed result for checkpointing every 3 layers with CPU offloading,

<img width=""1047"" alt=""every3_cpu"" src=""https://github.com/huggingface/transformers/assets/48202736/4a9ee4e7-7b7e-4e51-96e7-78868943dda4"">

<img width=""1047"" alt=""every3_gpu"" src=""https://github.com/huggingface/transformers/assets/48202736/df40e4b6-f1fb-4cff-aebb-f85aa0e39c64"">

and HF default version (using torch.utils.checkpoint for every layers)

<img width=""1057"" alt=""default_cpu"" src=""https://github.com/huggingface/transformers/assets/48202736/8455b32e-2410-4faa-b093-b918c1ebe4d0"">

<img width=""1063"" alt=""default_gpu"" src=""https://github.com/huggingface/transformers/assets/48202736/61ab67b6-2d54-4388-8fe1-5f7ebdc06967"">

i didn't add profiling result for larger model but there were more improvement and i can find sweet spot for GPU memory VS throughput trade off.  


### Your contribution

Didnt PR yet because in HF transformers, every model has its own forward method and applying it for all would be a huge update.
i just add profiling results and naive code snippet for implementation.
just want to discuss with you guys !","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
Natively support SONAR text models as M2M100 encoder and decoder models,"# What does this PR do?

This PR adds native support for SONAR text encoders are decoders (https://github.com/facebookresearch/SONAR).

SONAR for text is architecturally an NLLB model, but with the encoder representations mean-pooled into a single fixed-sized vector before passing them to the decoder. Thus, SONAR encoder works as a sentence embedder, and thanks to pretraining on translation data, it is massively multilingual and language-agnostic. And, unlike other sentence encoder, this one has a decoder that can reconstruct the original texts back from their embeddings.

To supports such models natively, the easiest way would be to create ~~NLLB~~ M2M100 model classes with encoder only or decoder only, similarly to the existing classes `T5EncoderModel` or `MT5EncoderModel`.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->

# Details 

- I add two new public model classes, `M2M100EncoderModel` and `M2M100DecoderModel`. 
- `M2M100EncoderModel` is a typical encoder-only model (BERT-style), with an additional option of applying mean pooling to its outputs (this is how SONAR text embeddings are computed)
- `M2M100DecoderModel` is a module consisting of M2M100Decoder and an output projection layer. As the input, it always expects the `encoder_outputs` argument to be present, and ignores its `input_ids`. 
- Unlike `M2M100ForConditionalGeneration`,  `M2M100DecoderModel` always has its decoder input embedding and decoder output projection layers tied, because this is how SONAR decoder originally was implemented.
- I add a script for creating these models from the original `fairseq2` checkpoints  (it doesn't require `fairseq2` as a dependency; instead, it just reads and reformats the torch model state dicts).
- I add specialized unit tests for the encoder-only model (implemented following `T5EncoderOnlyModelTest`, see https://github.com/huggingface/transformers/pull/8717), and for the decoder-only model (based loosely on similar ideas, but with more tweaks). 
- I add an integration test based on the checkpoints that I published to the HF hub. They reproduce the example sentence encoding and decoding from the readme in the SONAR repo: https://github.com/facebookresearch/SONAR/tree/main.

# Testing

All the unit tests I added are run by 
```
python -m pytest tests/models/m2m_100/test_modeling_m2m_100.py
```

The integration tests that I added are marked as slow, so they could be run with

```RUN_SLOW=1 python -m pytest tests/models/m2m_100/test_modeling_m2m_100.py::SonarIntegrationTests```",[],17,open
NonMatchingSplitsSizesError on Flax BART with wiki summary dataset,"### System Info

Platform: TPU
Python: python3.11

### Who can help?

@sanchit-gandhi

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

1. set up example based on [here](https://github.com/huggingface/transformers/tree/e5eb55b88b7200e81144eaf7a2bf3001b2bd08e7/examples/flax/summarization)
2. run command below on wiki_summary

```
          ""cd /tmp/transformers/examples/flax/summarization &&""
          "" JAX_PLATFORM_NAME=TPU python3 run_summarization_flax.py""
          "" --model_name_or_path facebook/bart-base --tokenizer_name""
          "" facebook/bart-base --dataset_name wiki_summary --do_train""
          "" --do_eval --do_predict --predict_with_generate --learning_rate""
          "" 5e-5 --warmup_steps 0 --output_dir=/tmp/transformers/bart-base-wiki""
          "" --overwrite_output_dir --num_train_epochs 3 --max_source_length""
          "" 512 --max_target_length 64  --per_device_train_batch_size=64 --per_device_eval_batch_size=64""
```

### Expected behavior

We got issue below:

```
[2024-03-11, 02:09:49 UTC] {logging_mixin.py:150} WARNING - 
Generating test split:  18%|█▊        | 1000/5638 [00:00<00:00, 8781.53 examples/s]
[2024-03-11, 02:09:49 UTC] {logging_mixin.py:150} WARNING - 
Generating test split:  37%|███▋      | 2074/5638 [00:00<00:00, 9863.29 examples/s]
[2024-03-11, 02:09:49 UTC] {logging_mixin.py:150} WARNING - 
Generating test split:  55%|█████▌    | 3112/5638 [00:00<00:00, 10087.86 examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - 
Generating test split:  74%|███████▍  | 4188/5638 [00:00<00:00, 10345.56 examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - 
Generating test split:  93%|█████████▎| 5253/5638 [00:00<00:00, 10450.02 examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - 
Generating test split: 100%|██████████| 5638/5638 [00:00<00:00, 10140.16 examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - 
Generating validation split:   0%|          | 0/5074 [00:00<?, ? examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - 
Generating validation split:  20%|██        | 1035/5074 [00:00<00:00, 10319.92 examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - 
Generating validation split:  42%|████▏     | 2110/5074 [00:00<00:00, 10566.07 examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - 
Generating validation split:  63%|██████▎   | 3175/5074 [00:00<00:00, 10597.24 examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - 
Generating validation split:  84%|████████▍ | 4253/5074 [00:00<00:00, 10663.04 examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - 
Generating validation split: 100%|██████████| 5074/5074 [00:00<00:00, 10458.45 examples/s]
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - Traceback (most recent call last):
  File ""/tmp/transformers/examples/flax/summarization/run_summarization_flax.py"", line 1031, in <module>
    main()
  File ""/tmp/transformers/examples/flax/summarization/run_summarization_flax.py"", line 499, in main
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING -     dataset = load_dataset(
  File ""/home/ml-auto-solutions/.local/lib/python3.10/site-packages/datasets/load.py"", line 2582, in load_dataset
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING -     builder_instance.download_and_prepare(
  File ""/home/ml-auto-solutions/.local/lib/python3.10/site-packages/datasets/builder.py"", line 1005, in download_and_prepare
    self._download_and_prepare(
  File ""/home/ml-auto-solutions/.local/lib/python3.10/site-packages/datasets/builder.py"", line 1767, in _download_and_prepare
    super()._download_and_prepare(
  File ""/home/ml-auto-solutions/.local/lib/python3.10/site-packages/datasets/builder.py"", line 1118, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
  File ""/home/ml-auto-solutions/.local/lib/python3.10/site-packages/datasets/utils/info_utils.py"", line 101, in verify_splits
    raise NonMatchingSplitsSizesError(str(bad_splits))
datasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=207186608, num_examples=45654, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='wiki_summar
[2024-03-11, 02:09:50 UTC] {logging_mixin.py:150} WARNING - y')}]
[2024-03-11, 02:09:54 UTC] {taskinstance.py:1826} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/decorators/base.py"", line 220, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/operators/python.py"", line 181, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/operators/python.py"", line 198, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/gcs/dags/xlml/utils/tpu.py"", line 402, in ssh_tpu
    ssh_group.run(cmds, env=env)
  File ""/opt/python3.11/lib/python3.11/site-packages/fabric/group.py"", line 116, in run
    return self._do(""run"", *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/fabric/group.py"", line 282, in _do
    raise GroupException(results)
```

Based on this [thread](https://discuss.huggingface.co/t/nonmatchingsplitssizeserror/30033), a few flags can be added to load_dataset().","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 6784721646, 'node_id': 'LA_kwDOCUB6oc8AAAABlGai7g', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/jax', 'name': 'jax', 'color': '560CEA', 'default': False, 'description': ''}]",7,open
Add predict_proba method for Autoformer/Informer,"### Feature request

Currently, the autoformer and informer does only provide sampled outputs from a probability distribution for future values. However, it would be nice if there would be the possibility to provide the forecasted distribution to the user.

### Motivation

I am currently trying to develop an adapter in sktime that enables the integration of the time series models from the transformers library (https://github.com/sktime/sktime/issues/5790). Since sktime has a `predict_proba` method, I would like to translate the transformers probability distribution into sktime probability distributions. 

### Your contribution

I think there are at least three solutions:
1. Add a new method, e.g., `predict_distribution`, which is returning the distribution object or its parameters.
2. Add a parameter to the `generate` function that controls if the distribution or its parameters are returned. 
3. Always returns the distributions or it's parameters together with the sampled `sequence` in `generate`.

If you are preferring any of these three solutions, I am happy to implement it :) ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Can we change mistral attention bias=False to bias=config.attention_bias ？,"just like llama attention ![image](https://github.com/huggingface/transformers/assets/76865636/f483aa4c-6c8f-40fd-b942-784cf74774cf)
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Adherence to semantic versioning with APIs,"### Feature request

Transformers is an incredibly popular library used by many folks. It would be great if Transformers would adopt semantic versioning (https://semver.org/) and follow best practices to provide API deprecations and backwards compatibility. 

For example, the following PR https://github.com/huggingface/transformers/pull/29143 breaks API changes. This is acknowledged in the PR review. In the future, it would be great to instead deprecate the existing interface and provide a new interface, and then remove the old, deprecated interface in future releases. 

### Motivation

Without following semantic versioning, users have immense amounts of pain upgrading versions and code is often broken. It also leads to hacky support such as https://github.com/mosaicml/llm-foundry/pull/1018 which now has to case on the API of multiple versions of Transformer. 

### Your contribution

N/A","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Support more memory efficient processing for segmentation models.,"### Feature request

This feature request significantly improves memory consumption for segmentation models, particularly when working with datasets with large numbers of instances per image.

### Motivation

Most (all?) of the models for segmentation tasks in `transformers` rely on the label/ground truth input being a list of instance masks. For images that contain large numbers of objects (for example aerial imagery, life sciences/microscopy) this can result in huge arrays being passed around. For example a slide image containing 200 cells, each as separate instances, requires a mask input of 200xWxH. At least on my computer, trying to process such datasets means I regularly get OOMs - even with 64GB RAM - unless I take care to limit the number of instances per sample.

This issue is also relevant for torchvision's implementation of Mask-RCNN for the same reason, but I think Detectron2 (and possibly mmdet) can operate on polygons/RLE masks directly and I've not had issues training instance segmentation models from inputs with large numbers of objects. (Actually an alternative to this proposal would be to support internally encoding masks as RLE which would also significantly save on memory). My suspicion is that this hasn't been an issue because benchmark datasets like COCO have relatively few instances per image.

There are a couple of places that this situation can be improved, with significant boosts to processing speed and memory usage. Perhaps the biggest advantage is the ability to process much larger batch sizes on memory-constrained machines.

(1) The first is maybe specific to DETR.

DetrForSegmentation's processor computes bounding boxes by using a `masks_to_boxes` function which operates on stack of instance masks. This seems like an intentional decision, but I'm not sure why unless we can't assume that the segments_info boxes are scaled correctly. This function is expensive and is noticeably slow if you have e.g. 100 objects in an image. For object detection models, the processor simply loads the box coordinates from `annotations`. In the panoptic regime we'd achieve the same by querying `segments_info`; we can fall back to the mask processing if the bounding box info isn't provided.

This a minor fix, but for some samples it gives me an order of magnitude improvement in data-loading speed (which, without this optimisation, can be much longer than the forward/backward pass)

```python

        # This is taken almost verbatim from the object detection processor
        if ""bbox"" in target['segments_info'][0]:

            boxes = [segment_info[""bbox""] for segment_info in target[""segments_info""]]
            boxes = np.asarray(boxes, dtype=np.float32).reshape(-1, 4)
            boxes[:, 2:] += boxes[:, :2]
            boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=image_width)
            boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=image_height)

            #keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])
            new_target[""boxes""] = masks_to_boxes(masks)
        else:
            new_target[""boxes""] = masks_to_boxes(masks)
```

(2) The second is more significant for memory, but a more involved fix. Most of the models use the target masks to compute a mask loss of some kind. [MaskFormer](https://github.com/huggingface/transformers/blob/0290ec19c901adc0f1230ebdccad11c40af026f5/src/transformers/models/maskformer/modeling_maskformer.py#L1088) uses the same function. [Mask2Former](https://github.com/huggingface/transformers/blob/0290ec19c901adc0f1230ebdccad11c40af026f5/src/transformers/models/maskformer/modeling_maskformer.py#L1088) and [OneFormer](https://github.com/huggingface/transformers/blob/0290ec19c901adc0f1230ebdccad11c40af026f5/src/transformers/models/oneformer/modeling_oneformer.py#L507) use a slightly different approach with a sampled point loss.

For DETR, bounding box comparisons are used to assign source:target predictions, and then some permutation happens such that we can pair up the relevant source predictions (one for each target), and re-order the target masks so that we can compare. For MaskFormer/Mask2Former/OneFormer, the Hungarian matching algorithm is run on the masks themselves - see a comment later.

The main issue here is not processing speed (passing around individual masks makes things simple to reason about), but the significant memory burden of passing around these massive instance arrays which get, somewhat by definition, more sparse the more objects are present. Instead, if we have access to (a) a panoptic mask as processed with `rgb_to_id` and (b) the segment IDs which are ordered with respect to the input bounding boxes, we can iterate over the ground truth and pick off the mask for each object.

Performance wise I think should be net zero because this masking operation is normally done as part of dataloading _anyway_ to generate the individual instance masks. I'm sure a Numpy wizard could make the actual code more performant but here is a possible implementation that (in my brief testing) gives identical losses to the `loss_masks` version.

```python
def loss_mask(self, outputs, targets, indices, num_boxes):
        """"""
        Compute the losses related to the masks: the focal loss and the dice loss.

        Targets dicts must contain the key ""mask"" containing a tensor of dim [h, w] where each pixel
        corresponds to a segment index. The target dict must also contain ""segment_ids"" which are used
        to extract individual objects from the mask itself.
        """"""
        if ""pred_masks"" not in outputs:
            raise KeyError(""No predicted masks found in outputs"")
        
        source_idx = self._get_source_permutation_idx(indices)
        target_idx = self._get_target_permutation_idx(indices)

        # Permute/filter outputs to one source per target
        source_masks = outputs[""pred_masks""]
        source_masks = source_masks[source_idx]
        
        # Resize target masks to uniform shape
        # TODO use valid to mask invalid areas due to padding in loss
        masks = [t[""mask""].unsqueeze(0) for t in targets]
        target_masks, _ = nested_tensor_from_tensor_list(masks).decompose()
        target_masks = target_masks.to(source_masks)
        
        # Upsample predictions to the target size
        source_masks = nn.functional.interpolate(
            source_masks[:, None], size=target_masks.shape[-2:], mode=""bilinear"", align_corners=False
        )

        segment_ids = [t['segment_ids'] for t in targets]

        from collections import defaultdict
        losses = defaultdict(int)

        # Calculate loss per predicted mask
        for idx, s in enumerate(source_masks):
            
            # Derive batch/segment (probably a better way to do this)
            batch, segment = target_idx[0][idx], target_idx[1][idx]

            # Extract mask for object
            t = (target_masks[batch] == segment_ids[batch][segment]).flatten(1).float()
            s = s.flatten().unsqueeze(0)

            losses[""loss_mask""] += sigmoid_focal_loss(s, t, num_boxes)
            losses[""loss_dice""] += dice_loss(s, t, num_boxes)

        return losses
```

The main user-facing difference here is that the preprocessor needs to provide the rest of ""segments_info"" in the labels. There may also need to be some logic around transformations, but in principle this should be done prior to processing/encoding? e.g. one loads the image and the annotations, performs any transformation and the dataset returns the augmented sample and takes care not to include e.g. segments that were cropped out.

For DETR, this modification is minor but it really improves memory usage by 2-3 orders of magnitude in some cases. For me it enables training with a batch size of 8-16 images instead of 1-2 and I can run with many workers without hitting OOM. It provides the benefit of (almost) constant, predictable memory consumption during dataloading because the input mask is always a fixed size.

On Mask/Mask2/OneFormer: the difference with more recent models is that matching is done on a mask-basis and not a box-basis (e.g. MaskFormerHungarianMatcher), but a similar approach could be made where we would replace this with an iteration over segment indices present in the target mask when computing the matching cost matrix.

```
target_mask_flat = target_mask[:, 0].flatten(1) 
```

we would pay a penalty in speed, because presumably everything is well-vectorised at the moment (loops bad?). However, I think having the option to pay that price instead over memory may be worth it (again - in order to generate the stack of instance masks, that masking operation has to happen somewhere else anyway).

Note that currently the matcher calculates the same costs as `loss_masks` in order to derive the cost matrix, but these scores are then discarded - it would make more sense to just use the source:target losses directly from the cost matrix, once the matcher has run? i.e.  `loss_masks` should just return a sum over the winning indices in the cost matrix.

### Your contribution

There are two primary contributions here:

- Aim to speed up dataloading by using existing bounding box coordinates, if provided by the labels. This is canonically part of the COCO-panoptic spec. This is certainly a hotfix for DETR segmentation/panoptic, but seems to not be relevant for more recent models. 
- Offer the option for users to provide a panoptic 2D mask instead of a instance stack. This requires a modified loss function which in a few cases is `loss_masks`. I've implemented this for DETR (which seems to be a simple case), but I think the approach could be extended to Mask/Mask2/OneFormer.
- For Mask/Mask2/OneFormer we would also have to provide a modified version of the Hungarian matcher that can operate on a panoptic mask as the target.
- An aside - the loss computation for these models can be simplified by using the Hungarian matching costs directly instead of using loss_masks.

I'm happy to PR these but would appreciate some discussion on implementation any other considerations that we'd have to make r.e. the order of dataloading and transformations.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",2,open
Track for flaky batching tests,"### System Info

We added a test for comparing model outputs when the input is batched or not batched in [this PR](https://github.com/huggingface/transformers/pull/29297). The assumption is that batching should not have impact on the results, disregarding numerical precision errros.

Currently, all the models pass the test, but there are 3 flaky tests. This issue is to keep track of them. The models that are flaky: MobileNetV2, DPT hybrid, VIT hybrid. I cannot give any more information except that it is flaky, so it needs more exploration.

### Who can help?

@amyeroberts for tracking

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

NA

### Expected behavior

NA","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",0,open
TrainingArguments shouldn't set ACCELERATE_MIXED_PRECISION env variable,"### Feature request

When a mixed precision data type (such as `fp16`) is provided to `TrainingArguments`, it currently sets the `ACCELERATE_MIXED_PRECISION` ([cf](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py#L1610)) environment variable to this mixed precision data type value.

Consequently, the `Trainer` class uses this mixed precision setting even when `TrainingArguments` are not explicitly provided as `args=training_args`.

The expected behavior should be that `Trainer` does not use mixed precision unless it's explicitly stated in the `TrainingArguments` provided as `args=training_args`.

CC: @muellerzr & @pacman100

### Motivation

I encountered this behavior while debugging a model that was producing `nan`s as output logits during training.

To troubleshoot this issue, I opted to stop providing `TrainingArguments` to `Trainer` in order to minimize potential sources of error but unfortunately that did not solved the problem.

After extensive investigation, this is how I discovered that `TrainingArguments` automatically sets `Trainer`'s mixed precision, even when not explicitly provided as an argument.

To prevent others from encountering the same debugging process, I advocate for mixed precision to be set only when explicitly required by the user by providing `TrainingArguments` to `Trainer`.

### Your contribution

I'm interested in opening a pull request to address this issue. This a good opportunity for me to learn more about the codebase.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",4,open
Add Microsoft's Code Reviewer model as a dedicated model,"### Model description

The model has three goals:
1. Provide feedback on the quality of the code change.
2. Provide feedback on the code change itself
3. When give a code change and a review comment, perform the required action to rectify the issue identified in the comment

The biggest reason for adding this to the HuggingFace library as a dedicated model is so the model size can be expanded and retrained, and potentially provide more accurate and meaningful output.

The model has some files in HuggingFace at the moment, but the implementation is incomplete. As seen in the GitHub repo, the T5 model is augmented and the state_dict is loaded on top of this modified model. Ideally, everything should live in HuggingFace.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Current weights and model implementation: https://huggingface.co/microsoft/codereviewer
Paper: https://arxiv.org/abs/2203.09095
GitHub repo: https://github.com/microsoft/CodeBERT/tree/master/CodeReviewer","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Large World Model and Ring Attention,"### Model description

Ring Attention is a new kind of attention that can expand the context size reliably (tested with precise needle search) up to several _million_ tokens. See https://arxiv.org/abs/2310.01889.

One of the models trained with RingAttention is multimodal Large World Model https://largeworldmodel.github.io/

I believe this attention replacement can be crucial for future LLMs (and not only), and should be included with the Transformers library

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

LWM checkpoints https://huggingface.co/LargeWorldModel
Ring Attention in PyTorch by Lucidrains https://github.com/lucidrains/ring-attention-pytorch
LWM in Jax https://github.com/LargeWorldModel/LWM","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
ReBASED,"### Model description

Mirroring https://github.com/huggingface/transformers/issues/29466, the newer model utilizing LinearAttention adds RMS norm to the attention forwarding and contracts the Taylor expansion to include only the third term, showing better performance and like BASED outperforms Mamba and stuff

### Open source status

- [X] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

The repo https://github.com/corl-team/rebased","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
feat: Add data class for fsdp config and use it along with argument parser,"### Feature request

Like the trainer arguments data class https://github.com/huggingface/transformers/blob/2a002d073a337051bdc3fbdc95ff1bc0399ae2bb/src/transformers/training_args.py#L167

Its good to have a data class for fsdp config as it allows for validation even before stepping into starting a training with ambiguous types used in the config. 

### Motivation

For instance, the fsdp config json file might have been written as

```json
{
...
""xla"": ""False"" // or ""false""
...
}
```

This check (https://github.com/huggingface/transformers/blob/745bbfe4bb2b61491dedd56e1e8ee4af8ef1a9ec/src/transformers/training_args.py#L1768) might pass through. This might be harder to debug though the problem is due a simple type bug. Having a fsdp arguments data class with parser would help identify such bugs early.

### Your contribution

I would be glad to raise a PR based on the discussion on this issue.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",9,open
BASED ,"### Model description

BASED is an attention model which combines sliding window attention and global linear attention to capture similar dependencies to transformers in a subquadratic model.  

It outperforms other similar models such as Mamba.



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

- [GitHub Repository](https://github.com/HazyResearch/based)
- Model Weights: [360m](https://huggingface.co/hazyresearch/based-360m), [1b](https://huggingface.co/hazyresearch/based-1b)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Implementing the features of the TextStreamer into the pipeline,"### Feature request

It should be possible to format the output of a transformers.pipeline.

```
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

llm = pipeline(
    task=""text-generation"",
    model=model,
    tokenizer=tokenizer,
    return_full_text=True,
    generation_config=generation_config,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    streamer=streamer
)

prompt = ""[INST] ... [/INST]""

result = llm(prompt)
```
The problem is that the result variable contains the prompt and the special tokens.

### Motivation

When you use a transformers.pipeline, you can use a TextStreamer object to skip the prompt and the special tokens. **The problem is that the result is print on the standard output**. I haven't found a way to have this result as the output of the pipeline. I tried to apply batch_decode from the tokenizer on the model output, but the parameter skip_special_tokens didn't work.

### Your contribution

N/A","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",14,open
Make model compatible with `torch.func`,"Hi there,

I am trying to use DebertaV3 as an alternative to BERT for an experiment that requires `torch.func` transformations. However, since Deberta implementation relies on a [`torch.autograd.Function`](https://pytorch.org/docs/master/autograd.html#torch.autograd.Function), `torch.func` is not able to transform it. I get the following error

```bash
RuntimeError: In order to use an autograd.Function with functorch transforms 
  (vmap, grad, jvp, jacrev, ...), it must override the setup_context staticmethod. 
  For more details, please see https://pytorch.org/docs/master/notes/extending.func.html
```

The responsible lines are the classes implementing a `torch.autograd.Function`, for example

https://github.com/huggingface/transformers/blob/a69cbf4e64c7bc054d814d64f6877180f7cd3a25/src/transformers/models/deberta_v2/modeling_deberta_v2.py#L79

which according to the PyTorch documentation ([https://pytorch.org/docs/master/notes/extending.func.html](https://pytorch.org/docs/master/notes/extending.func.html)) must implement a

```python
@staticmethod
    def setup_context(ctx, inputs, output):
        ...
```

method. 

Would it be possible to update this implementation to make it compatible with function transformations?","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Add a warning when the dataset has no matching columns,"### Feature request

Currently, when a user provides a dataset to `Trainer` without any columns matching the parameters of the `model.forward` method, an `IndexError` is raised without any accompanying warning or logs to aid the user in understanding this behavior. This lack of feedback can lead to confusion and increased debugging time, as experienced and discussed in [this thread](https://discuss.huggingface.co/t/trainers-dataloader-influenced-by-target-model/75685).

I suggest adding a warning log when such a scenario occurs. Additionally, considering whether directly raising an error could be an option might be beneficial. However, it's important to consider cases where a user could intentionally provides a dataset with no matching columns.

### Motivation

As a user, I wasn't initially aware that columns not accepted by the `model.forward()` method are automatically removed during the creation of the dataloader. Consequently, I spent several hours debugging my dataloader, model, and dataset without understanding why I kept receiving an `IndexError`. Implementing a warning log could potentially save users from similar debugging efforts in the future.

### Your contribution

I'm ready to create a pull request! :D","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
ReduceLROnPlateau version of get_constant_schedule_with_warmup,"### Feature request

There is a very useful scheduler for people too lazy to code up the sequential scheduler pieces from pytorch:

https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#transformers.get_constant_schedule_with_warmup

It would be convenient to have a version of this where the long flat section is actually a ReduceLROnPlateau:

https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html



### Motivation

I'm really lazy and would love to have this piece of work done for me, plus there would probably be other people who find it useful

### Your contribution

Could do, if it's a feature other people would like to see but no one is eager to pick up the gauntlet","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
The results of run_mae.py pre-training were abnormal,"### System Info

- `transformers` version: 4.39.0.dev0
- Platform: Linux-5.10.0-60.18.0.50.oe2203.x86_64-x86_64-with-glibc2.34
- Python version: 3.9.18
- Huggingface_hub version: 0.21.3
- Safetensors version: 0.4.2
- Accelerate version: 0.27.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@NielsRogge.

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

1. ViT_MAE_visualization_demo: https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb
2. run_mae.py: https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining

This is the result of loading the pre-trained model `facebook/vit-mae-huge` directly from huggingface.
![image](https://github.com/huggingface/transformers/assets/96508996/7ef67f11-c796-42ec-97a0-baa9101ae2fb)

This is the result of my continued pre_training using `facebook/vit-mae-huge`
![image](https://github.com/huggingface/transformers/assets/96508996/8e89d48c-2eb8-4bbd-8c15-dec6875f22e3)


I don't know why it's getting worse，The loss is decreasing from loss=1.7 at the beginning to loss=0.13. 

### Expected behavior

a","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",6,open
ProcessorMixin doesn't properly instantiate image processors,"### System Info

Transformers dev version v4.38.2

### Who can help?

@ArthurZucker @amyeroberts 

### Reproduction

Let's say you define your processor as follows:
```python
from transformers.processing_utils import ProcessorMixin

class LlavaProcessor(ProcessorMixin):

    attributes = [""image_processor"", ""tokenizer""]
    image_processor_class = (""CLIPImageProcessor"", ""ViTImageProcessor"")
    tokenizer_class = (""LlamaTokenizer"", ""LlamaTokenizerFast"")
```
(this is mainly for demo purposes, since for PR #29012 I'd like to have `LlavaProcessor` work with 2 different image processor classes)

Then, even though you create a processor as follows:
```python
from transformers import ViTImageProcessor, LlavaProcessor, LlamaTokenizer

image_processor = ViTImageProcessor.from_pretrained(""google/vit-base-patch16-224"")
tokenizer = LlamaTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"")

processor = LlavaProcessor(image_processor=image_processor, tokenizer=tokenizer)

processor.push_to_hub(""nielsr/my-awesome-processor"")
```

Reloading it:

```python
from transformers import LlavaProcessor

processor = LlavaProcessor.from_pretrained(""nielsr/my-awesome-processor"")
print(type(processor.image_processor))
```
This is still going to be of type `CLIPImageProcessor`, even though we want to load `ViTImageProcessor`.

This is because of the way we decide which class to load [here](https://github.com/huggingface/transformers/blob/831bc25d8fdb85768402f772cf65cc3d7872b211/src/transformers/processing_utils.py#L497-L512). Namely, if one defines a tuple for the `image_processor_class` attribute of the processor, then always the first class is used.

### Expected behavior

The processor should reload the `ViTImageProcessor` instead of `CLIPImageProcessor`.

The current workaround is to do this:

```python
from transformers.processing_utils import ProcessorMixin

class LlavaProcessor(ProcessorMixin):

    attributes = [""image_processor"", ""tokenizer""]
    image_processor_class = ""AutoImageProcessor""
    tokenizer_class = (""LlamaTokenizer"", ""LlamaTokenizerFast"")
```

This correctly instantiates the image processor. However, in PR #29012, @amyeroberts [suggested](https://github.com/huggingface/transformers/pull/29012#discussion_r1503995662) that the use of the Auto class is discouraged and it might be more appropriate to define the specific classes.

I remember from the past that Sylvain had no problem regarding the use of the Auto class, but I'm up for discussion. It's definitely a bit inconsistent that we define explicit classes for the tokenizers, but not for the image processor.

Looking at it, I  think the Auto class serve the exact purpose of what we're trying to achieve: loading the proper image processor class based on the preprocessor_config.json. Hence I'm wondering whether we shouldn't be leveraging the Auto classes by default for the `image_processor_class` and `tokenizer_class` attributes of multimodal processors.

","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Transformers Agents Collab Notebook - OpenAI Run Mode Issues,"### System Info

- `transformers` version: 4.29.0
- Platform: Linux-6.1.58+-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.20.3
- Safetensors version: 0.4.2
- PyTorch version (GPU?): 2.1.0+cu121 (True)
- Tensorflow version (GPU?): 2.15.0 (True)
- Flax version (CPU?/GPU?/TPU?): 0.8.1 (cpu)
- Jax version: 0.4.23
- JaxLib version: 0.4.23
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

1. launch the [colab notebook](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj) from the [transformers agents page](https://github.com/huggingface/transformers/blob/main/docs/source/en/transformers_agents.md)
1. **Transformers can do anything**:  complete this section of the notebook
1. **Do anything with Transformers** notebook section:
- **Agent init**: for *agent_name* select ""OpenAI (API Key)"" and execute the cell
- **Using the agent**: running the cell generates the following error:

```
---------------------------------------------------------------------------
APIRemovedInV1                            Traceback (most recent call last)
[<ipython-input-6-4578d52c5ccf>](https://localhost:8080/#) in <cell line: 1>()
----> 1 boat = agent.run(""Generate an image of a boat in the water"")
      2 boat

3 frames
[/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py](https://localhost:8080/#) in __call__(self, *_args, **_kwargs)
     37 
     38     def __call__(self, *_args: Any, **_kwargs: Any) -> Any:
---> 39         raise APIRemovedInV1(symbol=self._symbol)
     40 
     41 

APIRemovedInV1: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742
```

### Expected behavior

The README for [openai-python](https://github.com/openai/openai-python) does not mention openai.Completion but in general run methods of OpenAI have been deprecated in favor of chat methods for some time.

To summarize:
- OpenAI:  Run mode is deprecated and doesn't work based on the current code and python dependencies.  Chat mode code blocks in the notebook work fine
- Mistral:  In a personal copy of the Notebook I added Mistral to the model selection options in the **Agent Init** section.
  - Using Mistral worked for both Run and Chat modes.
  - [Colab Notebook copy using Mistral](https://colab.research.google.com/drive/1YeNYh2P1Y3W3lt0OQbycj-Geq1InzMir?usp=sharing)
  
  ","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",2,open
"Mistral in Flax: generation is slow, JIT fails","### System Info

- `transformers` version: 4.38.1
- Platform: Linux-6.2.0-1019-azure-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.21.1
- Safetensors version: 0.4.2
- Accelerate version: 0.27.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.2.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): 0.8.1 (cpu)
- Jax version: 0.4.25
- JaxLib version: 0.4.25
- Using GPU in script?: Yes (JAX default behavior)
- Using distributed or parallel set-up in script?: No

### Who can help?

@sanchit-gandhi

### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

On a VM/Docker with NVIDIA A100 run:

```python
import jax
import jax.numpy as jnp
from transformers import FlaxAutoModelForCausalLM, AutoTokenizer

MODEL_ID = ""mistralai/Mistral-7B-Instruct-v0.2""
model = FlaxAutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    from_pt=True,
    dtype=jnp.bfloat16,
    max_position_embeddings=4096,   # much smaller than the default value
    sliding_window=4096                     
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
texts = [""<s>[INST]Write a poem[/INST]""]
input_ids = tokenizer(texts, return_tensors=""np"")[""input_ids""]
```
With this setup, I'm able to generate some output using:

```python
model.generate(input_ids, max_new_tokens=32)
```
But it takes 8.91s (after warmup) - longer than what I'd expect for total of 45 tokens. Obvious next step is to JIT-compile it:

```python
jax.jit(model.generate, static_argnames=(""max_new_tokens"",))(input_ids, max_new_tokens=32)
```
But it fails with:

```
0302 22:21:58.956701   19411 pjrt_stream_executor_client.cc:2804] Execution of replica 0 failed: INTERNAL: Failed to allocate 117440512 bytes for new constant
---------------------------------------------------------------------------
XlaRuntimeError                           Traceback (most recent call last)
Cell In[20], line 1
----> 1 jax.jit(model.generate, static_argnames=(""max_new_tokens"",))(input_ids, max_new_tokens=32)

    [... skipping hidden 10 frame]

File ~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1209, in ExecuteReplicated.__call__(self, *args)
   1207   self._handle_token_bufs(result_token_bufs, sharded_runtime_token)
   1208 else:
-> 1209   results = self.xla_executable.execute_sharded(input_bufs)
   1210 if dispatch.needs_check_special():
   1211   out_arrays = results.disassemble_into_single_device_arrays()

XlaRuntimeError: INTERNAL: Failed to allocate 117440512 bytes for new constant
```


### Expected behavior

I'd expect at least one of two things:

1. Generation without JIT is done using [`jax.lax.while_loop`](https://github.com/huggingface/transformers/blob/831bc25d8fdb85768402f772cf65cc3d7872b211/src/transformers/generation/flax_utils.py#L644) which compiles its body function and so generation is fast. 
2. Generation with JIT and minimal settings does't fail because of OOM. 

**Some notes:**

* `jax.jit(model)(input_ids)`, i.e. application of the model just once, works fine and takes only ~2.5ms, so in theory 32 new tokens should be generated in 80-200ms
* `XLA_PYTHON_CLIENT_PREALLOCATE=false` and `XLA_PYTHON_CLIENT_MEM_FRACTION=0.99` doesn't help - all 80Gb of VRAM are actually consumed","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 6784721646, 'node_id': 'LA_kwDOCUB6oc8AAAABlGai7g', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/jax', 'name': 'jax', 'color': '560CEA', 'default': False, 'description': ''}]",4,open
hyperparameter_serch() does not consider LoRA parameters like r  to be finetuned. ,"### Feature request

Hyperparameter search is a must known activity to get hyperparameters  which are for optimised machine learning or deep learning model output. I was trying hyperparameter_search() method to get optimal hyperparameters values. I wanted to tune LoRA confugarition parameters like rank r and alpha too. But i found that it is not able to finetune LoRA configuration paramaters. 

### Motivation

Following code part will give you the idea 
```python
def model_init():
    device_map = {"""": torch.cuda.current_device(
                 )} if torch.cuda.is_available() else None
​
    model_kwargs_dict = dict(
    # set this to True if your GPU supports it 
    #(Flash Attention drastically speeds up model computations)
    #attn_implementation=""flash_attention_2"", 
    torch_dtype=""auto"",
    # set to False as we're going to use gradient checkpointing
    use_cache=False, 
    device_map=device_map,
    )
    device_map = {"""": torch.cuda.current_device(
             )} if torch.cuda.is_available() else None
​
    bnb_config_args = dict(load_in_4bit = True,
                   bnb_4bit_quant_type = ""nf4"",
                   bnb_4bit_compute_dtype = torch.bfloat16,
                   bnb_4bit_use_double_quant = False)
    bnb_config = BitsAndBytesConfig(
                                **bnb_config_args
                     )
    model_kwargs_dict[""quantization_config""] = bnb_config
    model = AutoModelForCausalLM.from_pretrained(""facebook/opt-125m"", 
                                                return_dict=True, 
                                                #**model_kwargs_dict
                                               )
    print(peft_config)
    model = get_peft_model(model, peft_config = peft_config)
    return model

dataset = load_dataset(""imdb"", split=""train"")
tokenizer = AutoTokenizer.from_pretrained(""facebook/opt-125m"")


dataset1 = dataset.select([0, 10, 20, 30, 40, 50])
dataset2 = dataset.select([0, 10, 20, 30, 40, 50])


trainer = SFTTrainer(
    model=None,
    args=training_args,
    model_init=model_init,
    tokenizer=tokenizer,
    train_dataset=dataset1,
    eval_dataset= dataset2,
    dataset_text_field=""text"",
    max_seq_length=512,)


def optuna_hp_space(trial):
    return {
        ""learning_rate"": trial.suggest_float(""learning_rate"", 1e-6, 1e-4, log=True),
        ""per_device_train_batch_size"": trial.suggest_categorical(""per_device_train_batch_size"", [16, 32, 64, 128]),
        ""r"": trial.suggest_float(""r"", 2, 4, log=True),
    }


trainer.hyperparameter_search(direction=[""minimize""],
                            backend=""optuna"",
                            hp_space=optuna_hp_space,
                            n_trials=2)
``` 

The code above has resulted in output like 

[I 2024-02-29 09:42:36,869] A new study created in memory with name: no-name-331fbdff-6465-42f8-9c97-ad5c6c8c4703
**Trying to set r in the hyperparameter search but there is no corresponding field in `TrainingArguments`.**

### Your contribution

NA","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
add_code_shell,"# What does this PR do?

This PR adds a new model named `codeshell` to the library.

CodeShell is a code large language model (LLM) developed jointly by the [Knowledge Computing Lab at Peking University](http://se.pku.edu.cn/kcl/) and the AI team of Sichuan Tianfu Bank. CodeShell has 7 billion parameters, was trained on 500 billion tokens, and has a context window length of 8192. On authoritative code evaluation benchmarks (HumanEval and MBPP), CodeShell achieves the best performance for models of its scale. At the same time, we offer deployment solutions and IDE plugins that complement CodeShell. Please refer to the [CodeShell](https://github.com/WisdomShell/codeshell) repository for details.

## Motivation and Context

The `codeshell` model leverages advanced NLP techniques to understand developer intents expressed in natural language and generate corresponding code snippets or software modules. This functionality is crucial for automating routine coding tasks, improving developer productivity, and making software development more accessible to non-experts.

## Dependencies

- python 3.8 and above
- pytorch 2.0 and above are recommended
- transformers 4.32 and above
- CUDA 11.8 and above are recommended (for GPU users, flash-attention users, etc.)

## Update

Compared to the PR submitted before, this time I used the `transformers-cli add-new-model` command to generate the corresponding files, made modifications based on these files, improved the documentation, ran the test files, and ensured their correctness.

## Who can review?

Tagging potential reviewers: @ArthurZucker @younesbelkada for text models review, considering the `codeshell` model's relevance to NLP and code generation tasks.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
MPNet doesn't have an implemented LMHead subclass,"### Feature request

The code of mpnet needs a proper implementation of LMHead class to adapt to Language Generation Tasks

### Motivation

We want to perform some experiments on the MPNet style inference on language generation tasks. On going through the codebase, there is a standby class inherited from `torch.nn.module`, which means there is no proper implementation of LMHead on this model

### Your contribution

I'm eager to contribute to this issue by submitting a PR. This work would be super relevant for us to extend our experiments to MPNet as well for our research projects.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Add Mixture of Tokens model,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes https://github.com/huggingface/transformers/issues/28285


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Load transformer models with shared memory ,"### System Info

.

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

.

### Expected behavior

I want to load transformer models with share memory for example when open multi workers I want to load the model once and all workers use it and not load the model for every worker","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Please correct the following DeepSpeed config values that mismatch TrainingArguments values: scheduler.params.total_num_steps=0 vs hf num_training_steps (calculated)= 260,"### System Info

- `transformers` version: 4.36.2
- Platform: Linux-4.15.0-213-generic-x86_64-with-glibc2.27
- Python version: 3.9.18
- Huggingface_hub version: 0.21.1
- Safetensors version: 0.4.2
- Accelerate version: 0.27.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

 raise ValueError(
ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:
- ds scheduler.params.total_num_steps=0 vs hf num_training_steps (calculated)=260
The easiest method is to set these DeepSpeed config values to 'auto'.

When I use transformers==4.28.1 + deepspeed==0.13.3 for Llama2 fine-tuning, the code runs normally and training is completed. This error occurs when I upgrade transformers to 4.36.x, 4.37.x or 4.38.1 respectively.
And I have not modified the default_offload_opt_param.json file of deepspeed. The contents of the file are as follows:
```python
{
  ""bf16"": {
    ""enabled"": ""auto""
  },
  ""optimizer"": {
    ""type"": ""AdamW"",
    ""params"": {
      ""lr"": ""auto"",
      ""betas"": ""auto"",
      ""eps"": ""auto"",
      ""weight_decay"": ""auto""
    }
  },
  ""scheduler"": {
    ""type"": ""WarmupDecayLR"",
    ""params"": {
      ""total_num_steps"": ""auto"",
      ""warmup_min_lr"": ""auto"",
      ""warmup_max_lr"": ""auto"",
      ""warmup_num_steps"": ""auto""
    }
  },
  ""zero_optimization"": {
    ""stage"": 3,
    ""offload_optimizer"": {
      ""device"": ""cpu"",
      ""pin_memory"": true
    },
    ""offload_param"": {
      ""device"": ""cpu"",
      ""pin_memory"": true
    },
    ""overlap_comm"": true,
    ""contiguous_gradients"": true,
    ""sub_group_size"": 1e9,
    ""reduce_bucket_size"": ""auto"",
    ""stage3_prefetch_bucket_size"": ""auto"",
    ""stage3_param_persistence_threshold"": ""auto"",
    ""stage3_max_live_parameters"": 1e9,
    ""stage3_max_reuse_distance"": 1e9,
    ""stage3_gather_16bit_weights_on_model_save"": true
  },
  ""gradient_accumulation_steps"": ""auto"",
  ""gradient_clipping"": ""auto"",
  ""steps_per_print"": 5,
  ""train_batch_size"": ""auto"",
  ""train_micro_batch_size_per_gpu"": ""auto"",
  ""wall_clock_breakdown"": false
}
```
The value of scheduler.params.total_num_steps is always ""auto"".

### Expected behavior

please fix this bug","[{'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",10,open
Add Mixtral Model to Flax,"### Feature request

I would like to implement the Mixtral model in Flax

### Motivation

I am in the process of learning Flax and I have almost finished the model conversion to FLAX.

### Your contribution

I could submit a PR with the model implementation","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Resolved device mismatch error when exporting Hugging Face's BERTModel to TorchScript,"# What does this PR do?


Fixes #29205 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@ArthurZucker @fxmarty 
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],7,open
Whisper - get probability of detected language,"### System Info

- `transformers` version: 4.38.0.dev0
- Platform: Linux-4.15.0-142-generic-x86_64-with-glibc2.23
- Python version: 3.10.11
- Huggingface_hub version: 0.20.3
- Safetensors version: 0.4.2
- Accelerate version: 0.27.2
- Accelerate config:    not found
- PyTorch version (GPU?): 2.2.0+cu121 (True)
- Tensorflow version (GPU?): 2.12.0 (True)

### Who can help?

@sanchit-gandhi I guess, since he's the one who provided the answer in the previous git issue.

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Following #25138, @sanchit-gandhi provided an answer to retrieve the language using Whisper model and processor (since Whisper conditionnal tokens include the language token). He later provided a little adaptation in order to get the probability of the language. This is a nice possibility. However, using the latest version of transformers it seems that it's not possible anymore (that's why I write it as a bug but could also be a feature request).

Quick example in order to check :

```python
language_identification = WhisperForConditionalGeneration.from_pretrained(""openai/whisper-small"").to(""cuda:0"")
lid_processor = WhisperProcessor.from_pretrained(""openai/whisper-small"")

audio, _ = librosa.load(<my_file>, sr=16000)

lid = lid_processor(audio, sampling_rate=16000, return_tensors=""pt"", truncation=True)
input_features = lid.input_features.to(""cuda:0"", torch.float32)

outputs = language_identification.generate(input_features, 
      output_scores=True,  
      return_dict_in_generate=True, 
      max_new_tokens=1)

pred_text = lid_processor.batch_decode(outputs.sequences, skip_special_tokens=False)
pred_text
```
`pred_text` is : 
```
['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> 80']
```

Here we see the conditionnal tokens as well as my only transcription token ` 80` (because of `max_new_tokens=1`)
The issue is that `outputs.scores` object (which is used for the probabilities of each token. Size is (N_TOKEN, 1, 51865), 51865 is Whisper vocabulary size) only returns the probabilities for the tokens **after** the conditionnal tokens. I.e, `outputs.scores` has a length of only 1 because I asked only 1 token for generation (if I would have wrote 5, I would have got a length of 5).

This means that using the transitions scores computed as follow :

```python
transition_scores = language_identification.compute_transition_scores(
    outputs.sequences, outputs.scores, normalize_logits=True
)
```
will produce only the scores for the tokens generated **after** the specials tokens SoT, lang, task, notimestamps (if not asking for).

I also tried without asking for timestamps because my guess was that since `notimestamps` token is after lang and task, maybe having the `notimestamps` token injected manually was maybe making the code to fall in a special `if condition` where the scores of the previous tokens (lang and task) would be ignored somehow.


### Expected behavior

I would have expected the `outputs.scores` to have the scores for the language token (if language isn't forced obviously) as it was probably meant to be according to the answer in #25138.

With that, we could easily guess the score for the language, and maybe have a ranking (like EN with score of 0.8, FR with score of 0.1 and so on). ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",19,open
Unable to trace DebertaForMaskedLM,"### System Info

- `transformers` version: 4.37.1
- Platform: Linux-5.4.0-47-generic-x86_64-with-glibc2.17
- Python version: 3.8.13
- Huggingface_hub version: 0.19.4
- Safetensors version: 0.3.1
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.1.2+cu118 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

from config import load_config

from transformers import (
    DebertaForMaskedLM,
    DebertaConfig,
)
from transformers.utils.fx import symbolic_trace

def main():
    config_kwarg = load_config(""deberta-large"")
    config = DebertaConfig(
            **config_kwarg
        )

    # create model
    model = DebertaForMaskedLM(config)

    traced_model = symbolic_trace(model)

if __name__ == ""__main__"":
    main()

### Expected behavior

When I run the scirpt, get following error.
![image](https://github.com/huggingface/transformers/assets/60869411/2755e3d2-6733-40a6-bd58-2c6a9f960154)
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
OSError: You are trying to access a gated repo.,"### Model description

I tried to run the model on Colab and successfully logged in using huggingface cli login, but it still reported an error
<img width=""534"" alt=""联想截图_20240226102113"" src=""https://github.com/huggingface/transformers/assets/83337970/4d3d95a0-98c8-4451-b696-6a70f0d6c7ce"">
<img width=""1202"" alt=""联想截图_20240226102157"" src=""https://github.com/huggingface/transformers/assets/83337970/565471b5-0261-4420-9480-ec99d367b239"">


### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Avoiding prompt injection using special tokens via `apply_chat_template`,"### Feature request

Chat templates should provide some kind of protection against prompt injection via special tokens. Possible remedies:
1. Make it clear [in the docs](https://huggingface.co/docs/transformers/main/en/chat_templating#what-template-should-i-use) that this should be implemented by the developer when writing a prompt template.
1. Add an additional filter `escape_content` that can clean content of special tokens. (The builtin `escape` wont work for code generation where html is involved)
1. Make that part of the `apply_chat_template` function itself but that would require standardizing the input messages format to stick to `[{role: ..., content: ...}...]` or something

I add some code to my chat templates for doing this at the moment but I am worried that without enough warning, this might become common and people would realize this in hindsight.

### Motivation

Chat templates are a great idea and they have become quite popular. However currently they blindly render any content coming directly from users even if contains special tokens in it (whether spurious or accidental). This would cause all kinds of unwanted behavior and possibly even security issues.

### Your contribution

Happy to make a PR if people think this is important to do and then we can decide which of the options to go with.

I personally add this to my tokenizer templates to combat this:
```jinja
{%- set escape_tokens = (
    [bos_token, eos_token, unk_token, pad_token] + additional_special_tokens
  ) | map('default', '<unk>') | list -%}

{% for message in messages %}
  {#- Escape content -#}
  {%- set content_ns = namespace(value=message.content) -%}
  {%- for escape_token in escape_tokens -%}
    {#- Replace '<|im_start|>' with '< |im_start|>' and so on -#}
    {%- set content_ns.value = content_ns.value | replace(
        escape_token,
        escape_token.replace(escape_token[0], escape_token[0]+' ', 1)
    ) -%}
  {%- endfor -%}
  
{#- Other stuff here -#}
{{ content_ns.value }}

{% endfor %}
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
Add support for Phi in Flax,"# What does this PR do?

This is WIP PR aiming to add Flax support for Phi and Phi-2 models.

Progress so far:

- [x] Implement all modules (heavily inspired by Flax Llama implementation)
- [x] Check consistency with PyTorch (see temporary `_checks.py` file)
- [ ] Implement/check loading from PyTorch checkpoints
- [ ] Check JIT compatibility
- [ ] Add formal tests (instead of REPL-based checks)
- [ ] Remove temporary code fixes (e.g. absolute imports)

I'm currently stuck on loading pretrained models from PyTorch checkpoints and need some help from the community. 

According to the [model sharing](https://huggingface.co/docs/transformers/en/model_sharing) section, I expected the following to work out of the box:

```python
FlaxPhiForCausalLM.from_pretrained(""microsoft/phi-2"", from_pt=True)
```
but in practice it fails:


> OSError: Can't load the model for 'microsoft/phi-2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'microsoft/phi-2' is the correct path to a directory containing a file named flax_model.msgpack or pytorch_model.bin.


Note that since I wasn't adding a new model but instead only adapting an existing one for a new framework, I did not generate model template as described in [the docs](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model) and could miss something important. Could somebody with better understanding of the internal machinery point me to what needs to be done to load PyTorch checkpoints to the Flax model? 

Tagging @ArthurZucker @younesbelkada and @sanchit-gandhi as probably the most relevant people here. 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case. - Yes, [link](https://github.com/huggingface/transformers/issues/28237)
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?



","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 2934977194, 'node_id': 'MDU6TGFiZWwyOTM0OTc3MTk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flax', 'name': 'Flax', 'color': '4862AD', 'default': False, 'description': ''}]",0,open
AutoModel.from_config() should attempt to load model relative to where the config was loaded.,"### System Info


Currently if you do an AutoConfig.from_pretrained() and then use that config to try to AutoModel.from_config() it loses the context from where the config.json was loaded.  This wouldn't be a problem if AutoModel.from_config() also took a path_or_model_id style param but it doesn't.

This becomes problematic mainly for situations where you want to write single code for both training and fine tuning models.  In this workflow, the notion of training and fine tuning is largely the same code so creating the model to then train looks something like this:

```
try:
    model = AutoModel.form_pretrained(..., config=conf, ...)
except OSError, ValueError:
    model = AutoModel.from_config(conf)
```

If you're loading models from file like i am, not being able to easily AutoModel.from_config() with a specified path is problematic.  I would like all of my intermediate training steps to be done locally to potentially unrelated directory structures to the source. 

I think that we have two easy options. 
1. Add a field to the PretrainedConfig which is only populated when it's loaded from file which is the path to the config file the config was loaded from.  We can call it something like _loaded_from_file.  It would have to be removed prior to save()
2. allow AutoModel.from_config() to take a path_or_model_id in addition to the config.  so the call would look something like:

```
model = AutoModel.from_config(conf, path_or_model_id='/path/to/dir/containing/model')
```

cc:
@Rocketknight1 

Trying out recommended workflow of issuing bugs before PRs. 


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

1. Load a model config specifying custom code in the AutoMap from a path that is not also valid as `$CWD/model-id/model`
2. try to instantiate a version of your model from AutoModel.from_config() using that config.

### Expected behavior

I'd like to have an instance of the model that's of the same type as the model that'd be returned if from_pretrained() were used. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
DeepSpeed Support Stage 3 ,"### System Info

Does the trainer support stage 3? 

According to https://huggingface.co/transformers/v4.3.0/main_classes/trainer.html - it does not. 

Thanks,
Brett 

### Who can help?

na

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

use trainer with stage 3

### Expected behavior

Parameter partitioning ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Fine-tuning a pruned model,"### Feature request

I used [OWL](https://github.com/luuyin/OWL) to prune a Mistral-7b model and would like to further train this pruned model. Is there a way to pass a global mask (based on the pruned model) to ensure those weights don't get updated, particularly when using DDP.

**Note**:  We cannot update the parameter list of the optimizer as the pruning is not structured.

### Motivation

Pruning helps with creating more efficient models, and it would be interesting to analyze the effects of further training pruned models.

### Your contribution

We would be glad to submit a PR for this use case. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
"How to get input embeddings from PatchTST with (batch_size, sequence_length, hidden_size) dimensions","### System Info

-

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

The following snippet outputs the last hidden state but it has (batch_size, num_channels, num_patches, d_model) dimensions
`inputs = encoder(
            past_values=series_list, output_hidden_states=True
        ).last_hidden_state`

Here, series_list has (batch_size, sequence_length, num_input_channels) shape.

To incorporate this with [EncoderDecoderModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel), I want the dimensions of the input embedding to be (batch_size, sequence_length, hidden_size). How do you get that?


### Expected behavior

-","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
prophetnet_multi,"### Model description

Multilingual text generation model based on prophetnet.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
add Fusion-In-Decoder(FiD) Models,"add Fusion-In-Decoder(FiD) Models
It was simply implemented based on t5
@ArthurZucker ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
[tokenizer] Inconsistent behavior in slow tokenizer and fast tokenizer,"### System Info

- `transformers` version: 4.35.2
- Platform: Linux-5.4.0-163-generic-x86_64-with-glibc2.10
- Python version: 3.8.18
- Huggingface_hub version: 0.19.4
- Safetensors version: 0.4.1
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.1.1+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: no need
- Using distributed or parallel set-up in script?: no need

### Who can help?

@ArthurZucker and @younesbelkada

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
from transformers import AutoTokenizer


def answer_or_exception(tokenizer, id):
    print(f'<<<<<<{tokenizer.__class__}>>>>>>')
    try:
        print(f'""{tokenizer.decode([id])}""')
    except Exception as e:
        print(e)


tokenizer = AutoTokenizer.from_pretrained(""/mnt/data01/shichao/models/phi-2"", trust_remote_code=True, use_fast=False)
# vocab size: 50294
answer_or_exception(tokenizer, 50294)  # correct
answer_or_exception(tokenizer, 50295)  # wrong

tokenizer = AutoTokenizer.from_pretrained(""/mnt/data01/shichao/models/phi-2"", trust_remote_code=True, use_fast=True)
# vocab size: 50294
answer_or_exception(tokenizer, 50294)  # correct
answer_or_exception(tokenizer, 50295)  # correct


tokenizer = AutoTokenizer.from_pretrained(""/mnt/data01/shichao/models/Llama-2-7b-chat-hf"", trust_remote_code=True, use_fast=False)
# vocab size: 31999
answer_or_exception(tokenizer, 31999)  # correct
answer_or_exception(tokenizer, 32000)  # wrong

tokenizer = AutoTokenizer.from_pretrained(""/mnt/data01/shichao/models/Llama-2-7b-chat-hf"", trust_remote_code=True, use_fast=True)
# vocab size: 31999
answer_or_exception(tokenizer, 31999)  # correct
answer_or_exception(tokenizer, 32000)  # correct
```

Output:

```text
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
<<<<<<<class 'transformers.models.codegen.tokenization_codegen.CodeGenTokenizer'>>>>>>>
""               ""
<<<<<<<class 'transformers.models.codegen.tokenization_codegen.CodeGenTokenizer'>>>>>>>
sequence item 0: expected str instance, NoneType found
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
<<<<<<<class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>>>>>>>
""               ""
<<<<<<<class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>>>>>>>
""""
<<<<<<<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>>>>>>>
""给""
<<<<<<<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>>>>>>>
piece id is out of range.
<<<<<<<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>>>>>>>
""给""
<<<<<<<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>>>>>>>
""""
```

### Expected behavior

Consistent `decode` behavior in slow tokenizer and fast tokenizer when id exceeds vocab size. For example, instead of raise exceptions, the slow tokenizer output empty strings like the fast tokenizer does.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",12,open
Generate: support passing position_ids,"Thank you @tengomucho, for uncovering this bug.

### The problem

In a nutshell, passing the correct `position_ids` to `generate` should result in exactly the same results as not passing them. In other words, the following test should pass on all models, if added to `GenerationTesterMixin`. We can see that it is failing in general.

```py
    def test_passing_position_ids(self):
        # Check that passing position ids to generate yields the same results as not passing them, if the position ids
        # are correctly built. If the test fails, it means one of two things:
        # 1 - the manual position ids are not being piped correctly; OR
        # 2 - the automated position ids are not being correctly built.
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask, _ = self._get_input_ids_and_config(batch_size=1)
            if config.is_encoder_decoder:
                self.skipTest(""This model does not support position_ids"")

            # To truly test this property, let's create a batch where the second row corresponds to the test input with
            # left padding of 1.
            pad_token = torch.tensor([[config.pad_token_id or 0]], device=input_ids.device, dtype=input_ids.dtype)
            input_ids = torch.cat((input_ids, torch.cat((pad_token, input_ids[:, 1:]), dim=1)), dim=0)
            pad_mask = torch.zeros((1, 1), dtype=attention_mask.dtype, device=attention_mask.device)
            attention_mask = torch.cat((attention_mask, torch.cat((pad_mask, attention_mask[:, 1:]), dim=1)), dim=0)
            position_ids = torch.clamp(torch.cumsum(attention_mask, dim=-1) - 1, min=0)

            config.use_cache = True
            config.is_decoder = True

            model = model_class(config).to(torch_device).eval()

            try:
                output_position_ids = model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    max_new_tokens=10
                )
            except ValueError as exc:
                if ""The following `model_kwargs` are not used by the model: ['position_ids']"" in str(exc):
                    self.skipTest(""This model does not support position_ids"")
                else:
                    raise
            output_no_position_ids = model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_new_tokens=10
            )
            self.assertListEqual(output_no_position_ids.tolist(), output_position_ids.tolist())
```

### The fix

There are two root causes for this:
1. `position_ids` is rejected in some models when it is passed (e.g. see [here](https://github.com/huggingface/transformers/blob/3c00b885b92fbcd0e7451e56ccf424a2d5a19bbb/src/transformers/models/gpt2/modeling_gpt2.py#L1022)). These models often assume no padding when `position_ids` is rejected.
2. `position_ids` is never updated, so it is only correct when created from scratch (=not passed).

As such, a fix to this problem should consist in updating `position_ids` in `generate`, with `prepare_inputs_for_generation` only creating new `position_ids` when they don't exist.

The test pasted above should be part of our tests after fixing the issue.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",1,open
Flash attention implementation with BERT base model,"### Model description

hello and thanks community.
I am trying to replace standard attention by flash attention in the BERT base Model. Anyone please help not able to find any tutorial or any discussions.
or just give some directions how to do that ..I have got the idea of making attention prob drop prob = 0 . it makes sense but not sure how it is going to work.

@tridao
@arthur
@jamaliki
@sorenmc
 @LysandreJik @ArthurZucker  




### Open source status

- [x] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
err_handle(layoutlmv3): Error message doesn't give much clarity when boxes not containing enough information,"### System Info

- `transformers` version: 4.37.2
- Platform: Windows-10-10.0.22000-SP0
- Python version: 3.11.5
- Huggingface_hub version: 0.20.3
- Safetensors version: 0.4.2
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.2.0+cpu (False)
- Tensorflow version (GPU?): 2.15.0 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help?

@younesbelkada 
@ArthurZucker 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

****
Model I am using LayoutLMv3:

when `boxes = [[123, 53], [36, 87], ...]` (basically any list which is not according to the proper format)
by proper format I mean `[[123, 346, 234, 634], [356, 568, 234, 25], ...]`

```python
encoding = processor(
        image_1,
        text,
        boxes=boxes,
        max_length=512,
        padding=""max_length"",
        truncation=True,
        return_tensors=""pt""
    )
```

It produces a this error message
```
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (labels in this case) have excessive nesting (inputs type list where type int is expected).
```

**To Reproduce**
Steps to reproduce the behavior:
1. add any list of boxes with not enough values like `boxes = [[123, 53], [36, 87], ...]`
2. when run it throws the ValueError mentioned above



### Expected behavior

Can throw an error saying
```
ValueError: boxes doesn't have enough values inside each box. Each box should contain 4 values 
```","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",5,open
Request to add FLMR,"### Model description

## Basic Information

This issue requests adding Fine-grained Late-interaction Multi-modal Retriever (FLMR).
The model leverages late interaction (as originally proposed by Stanford [ColBERT](https://github.com/stanford-futuredata/ColBERT)) to compute token-level similarity between every query token and document token, which enables more accurate retrieval relative to DPR-like systems.
 The model was proposed in [here](https://openreview.net/forum?id=IWWWulAX7g) (NeurIPS 2023) and [here](https://arxiv.org/abs/2402.08327) (a follow-up version that was pre-trained on more than ten million of multi-modal retrieval data). 


## Resources
- Project page [here](https://preflmr.github.io/)
- Official codebase [here](https://github.com/linweizhedragon/retrieval-augmented-visual-question-answering)
- The pre-trained checkpoints are [here](https://huggingface.co/models?search=PreFLMR).

## Why adding this model
1. This work has gained attention from researchers across the world. We received many requests during NeurIPS 2023 to provide an easy implementation of this model.
2. The work has been recognized by the authors of ColBERT [twitter post](https://twitter.com/lateinteraction/status/1757652639503007893)
3. There exists no implementation for late-interaction retrieval models in hf-transformers, which have been extensively researched in these years.
4. There are many requests in the original codebase [an example issue](https://github.com/LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering/issues/24)
5. As original authors, we have finished 99% of the model, including an example for indexing and searching.   Limited work is required to have it on huggingface-transformers!  #29062 




### Open source status

- [x] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The PR is already here:
#29062 ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
[`RWKV5`] Add support for RWKV5 model,"# What does this PR do?
Adds RWKV5, superseeds #26963","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",28,open
Add Support for Dataclasses to Trainer,"### Feature request

Update Trainer._prepare_input to natively support python dataclasses for support of more structured objects.

### Motivation

_prepare_input will seamlessly transfer the tensors contained in many datatypes (list, tuple, dict, etc.) to the appropriate device. However, it will not do so for dataclass objects.

Python dataclasses often provide better ergonomics than TypedDict, which is likely the closest supported alternative. Adding support appears to be a small change to the codebase with nice benefits to the user.

### Your contribution

If the PR is as simple as it initially appears, I would be willing to submit a PR.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Swapping `tqdm` to `rich`,"### Feature request

Hi, for `AutoTokenizer.train_new_from_iterator` there's a hardcoded `tqdm` progress bar I want to swap to `rich` and I'm happy to PR it back.

I can see on github it's at `transformers/src/transformers/tokenization_utils_fast.py` and I can see in lines #790 and #791 that there's a further method `train_from_iterator` but at this point I can't find where the actual code is? Can anyone point me to the right direction?

Also, is there any reason to go against adding `rich` as a dependency?
Where are the `tqdm` specific bits of code, so I can go through them?
Thanks!

### Motivation

I'm not fond of `tqdm` it seems to create issues when used on AWS, SageMaker, etc. It's span is large and doesn't contain nearly enough information as `rich` can. I wanna start by going over `AutoTokenizer` because it's where I first spotted it.

### Your contribution

Slowly work through bits of code which rely on `tqdm` and add the option to swap for `rich` instead.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Request for Flash Attention 2.0 Support in GPNRoFormerForMaskedLM,"Hello,

I trust this message finds you well. I am currently attempting to run the GPN-MSA model, which utilizes AutoModelForMaskedLM, and I am keen on parallelizing the computation across multiple GPUs. To optimize the model's performance, I would like to request the integration of Flash Attention 2.0 support into GPNRoFormerForMaskedLM.

As I explore this avenue for parallelization, I envision that many others within the community could benefit from this enhancement.

Thank you for your time and consideration.

Best regards.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}]",1,open
[i18n-ar] Translating docs to Arabic,"Hi!

Let's bring the documentation to all the Arabic-speaking community 🌐 

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section
- [ ] [readme.md](https://github.com/huggingface/transformers/edit/main/README.md)
- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) 
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md)
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md)

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
load_state_dict doesnt support torch._subclasses.fake_tensor.FakeTensorMode,"### System Info

- `transformers` version: 4.36.0
- Platform: Linux-6.5.0-15-generic-x86_64-with-glibc2.31
- Python version: 3.11.5
- Huggingface_hub version: 0.19.4
- Safetensors version: 0.4.0
- Accelerate version: 0.24.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.3.0a0+git78a84f1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: no

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

When PyTorch's `FakeTensorMode` is active, the underlying storage is changed to be `UntypedStorage` as a way to not really allocate the memory for the parameters.

As a consequence, transformers's `get_tensor` failed with `ValueError: could not determine the shape of object type 'torch.storage.UntypedStorage'`

```python
from torch._subclasses import fake_tensor
import transformers

fake_mode = fake_tensor.FakeTensorMode(allow_non_fake_inputs=False)
with fake_mode:
    fake_model = transformers.AutoModel.from_pretrained(""sshleifer/tiny-gpt2"") 
```

Error:

```bash
Loading checkpoint shards:   0%|                                           | 0/19 [00:00<?, ?it/s]
Traceback (most recent call last):
  File ""/opt/pytorch/test_mixtral.py"", line 9, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_id)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/ptca/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py"", line 566, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/ptca/lib/python3.11/site-packages/transformers/modeling_utils.py"", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/ptca/lib/python3.11/site-packages/transformers/modeling_utils.py"", line 4079, in _load_pretrained_model
    state_dict = load_state_dict(shard_file)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/ptca/lib/python3.11/site-packages/transformers/modeling_utils.py"", line 510, in load_state_dict
    return safe_load_file(checkpoint_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/conda/envs/ptca/lib/python3.11/site-packages/safetensors/torch.py"", line 310, in load_file
    result[k] = f.get_tensor(k)
                ^^^^^^^^^^^^^^^
ValueError: could not determine the shape of object type 'torch.storage.UntypedStorage'
```

### Expected behavior

`transformers` `get_tensor` should be able to load fake tensors from a fakefied checkpoint","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",16,open
tracker: `generate` compatibility with `torch.compile`,"# `generate` 🤜 🤛  `torch.compile`

Part of the [PyTorch 2024 H2 roadmap](https://drive.google.com/file/d/1Ucm17fyUeF0PWSd2g7jonM144XMx2t2r/view).

This issue is a tracker of the compatibility between `.generate` and `torch.compile` ([intro docs by pytorch](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)). The goal is to enable `fullgraph=True` compilation on the main `generate` use cases.

⚠️ Is *your* `generate` use case not covered by this tracker? Check if it was requested below and upvote it if it was. Otherwise, add a comment. We will consider expanding the selection below on widely requested use cases 🤗 

### Decoding Strategies (end-to-end compilation)
- [x] `greedy_search` / `sample` are compatible (https://github.com/huggingface/transformers/pull/30788)
- [ ] `beam_search` / `beam_sample` are compatible, depends on the step above
- [ ] `assisted_decoding` (aka speculative decoding) is compatible, depends on the steps above

### Generate Flags and Options
- [ ] all `LogitsProcessor` classes were checked for compatibility (and the appropriate exceptions are raised when not compatible)
- [ ] all `StoppingCriteria` classes were checked for compatibility (and the appropriate exceptions are raised when not compatible)

### Models

Notes:
1. models tagged as ""important models"" in our CI + popular models
2. language models released starting from v4.42 should ALL support compile

Decoder-only:
- [x] GPT-J is compatible (#31421)
- [ ] GPT2 is compatible
- [x] Llama is compatible (#27931)
- [x] Gemma is compatible (#29167)
- [ ] Llava is compatible (#29891)
- [x] Mistral is compatible (https://github.com/huggingface/transformers/pull/30642)
- [ ] Mixtral is compatible (https://github.com/huggingface/transformers/pull/30793/)
- [X] Phi is compatible (https://github.com/huggingface/transformers/pull/32617)
- [ ] Phi3 is compatible (https://github.com/huggingface/transformers/pull/30688)
- [X] BLOOM is compatible (note: this one might be tricky due to cache format) https://github.com/huggingface/transformers/pull/32617
- [x] Mamba is compatible (requested [here](https://github.com/huggingface/transformers/issues/29699#issuecomment-2072233223)) (PR: https://github.com/huggingface/transformers/pull/31247)
- [X] Persimmon is compatible (https://github.com/huggingface/transformers/pull/32617)
- [x] Qwen2 https://github.com/huggingface/transformers/pull/32617
- [x] Qwen2-VL https://github.com/huggingface/transformers/pull/32617
- [x] Falcon https://github.com/huggingface/transformers/pull/32617
- [x] GPTNeoX https://github.com/huggingface/transformers/pull/32617
- [x] Starcoder2 https://github.com/huggingface/transformers/pull/32617
- [x] StableLM https://github.com/huggingface/transformers/pull/32617

Encoder-decoder:
- [ ] BART is compatible
- [ ] T5 is compatible (https://github.com/huggingface/transformers/pull/34089)
- [x] Whisper is compatible (https://github.com/huggingface/transformers/pull/31166)


### Quantization
- [ ] BNB support
- [ ] GPTQ support
- [ ] AWQ support

### Others
- [x] We have a benchmark script to quickly compare the impact of PRs
- [x] Add section to existing docs on the topic
- [x] Confirm that pipelines work after compiling generate
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",13,open
Whisper Sequential long-form decoding doesn't work with timestamps per token,"### System Info

- `transformers` version: 4.37.2
- Platform: Linux-4.15.0-142-generic-x86_64-with-glibc2.23
- Python version: 3.10.11
- Huggingface_hub version: 0.20.3
- Safetensors version: 0.4.2
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.0.1 (True)
- Tensorflow version (GPU?): 2.12.0 (True)


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Following [[Whisper] Add sequential longform decoding](https://github.com/huggingface/transformers/pull/27492), it seems that there is an issue when asking for token timestamps when dealing with the new way of handling long-form transcriptions. 
If using `model.generate()` method, passing `return_token_timestamps=True` causes the issue. Occurs also with the pipeline object if setting `return_timestamps=""word""`.

Here is a simple example to reproduce the issue:

```python
from transformers import WhisperForConditionalGeneration, WhisperProcessor, pipeline
import librosa

SR = 16000
model = WhisperForConditionalGeneration.from_pretrained(""openai/whisper-medium"")
processor = WhisperProcessor.from_pretrained(""openai/whisper-medium"")

file_path = ""path_to_more_than_30_sec_audio""
audio, _ = librosa.load(file_path, sr=SR)

# Long-form transcription with model.generate()
input_features = processor(audio, 
                           sampling_rate=SR, 
                           return_tensors=""pt"", 
                           truncation=False, # False so the audio isn't truncated and whole audio is sent to the model
                           return_attention_mask=True, 
                           padding=""longest"")

predicted_ids = model.generate(**input_features,
                               return_token_timestamps=True)

# With pipeline
pipe = pipeline(""automatic-speech-recognition"", 
                model=model, 
                tokenizer=processor.tokenizer, 
                feature_extractor=processor.feature_extractor, 
                return_timestamps=""word"",
                return_language=True
                )

pipe(audio)
```

## Traceback:

```shell
AttributeError                            Traceback (most recent call last)
Cell In[26], line 19
     11 # Long-form generation
     12 input_features = processor(audio, 
     13                            sampling_rate=16000, 
     14                            return_tensors=""pt"", 
     15                            truncation=False, 
     16                            return_attention_mask=True, 
     17                            padding=""longest"")
---> 19 predicted_ids = model.generate(**input_features,
     20                                return_token_timestamps=True)

File ~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:641, in WhisperGenerationMixin.generate(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)
    638         proc.set_begin_index(decoder_input_ids.shape[-1])
    640 # 6.8 Run generate with fallback
--> 641 seek_sequences, seek_outputs, should_skip, do_condition_on_prev_tokens = self.generate_with_fallback(
    642     segment_input=segment_input,
    643     decoder_input_ids=decoder_input_ids,
    644     cur_bsz=cur_bsz,
    645     batch_idx_map=batch_idx_map,
    646     seek=seek,
    647     num_segment_frames=num_segment_frames,
    648     max_frames=max_frames,
    649     temperatures=temperatures,
    650     generation_config=generation_config,
    651     logits_processor=logits_processor,
    652     stopping_criteria=stopping_criteria,
    653     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
    654     synced_gpus=synced_gpus,
    655     return_token_timestamps=return_token_timestamps,
    656     do_condition_on_prev_tokens=do_condition_on_prev_tokens,
    657     kwargs=kwargs,
    658 )
    660 # 6.9 In every generated sequence, split by timestamp tokens and extract segments
    661 for i, seek_sequence in enumerate(seek_sequences):

File ~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:739, in WhisperGenerationMixin.generate_with_fallback(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, kwargs)
    727 seek_outputs = super().generate(
    728     segment_input,
    729     generation_config,
   (...)
    735     **kwargs,
    736 )
    738 # post-process sequence tokens and outputs to be in list form
--> 739 sequence_tokens, seek_outputs = self._postprocess_outputs(
    740     seek_outputs, return_token_timestamps, generation_config
    741 )
    743 # remove all previously passed decoder input ids
    744 seek_sequences = sequence_tokens[:, decoder_input_ids.shape[-1] :]

File ~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:825, in WhisperGenerationMixin._postprocess_outputs(self, seek_outputs, return_token_timestamps, generation_config)
    822         return values[batch_idx].cpu()
    824     sequence_tokens = seek_outputs[""sequences""]
--> 825     seek_outputs = [
    826         {k: split_by_batch_index(v, k, i) for k, v in seek_outputs.items()}
    827         for i in range(sequence_tokens.shape[0])
    828     ]
    829 else:
    830     sequence_tokens = seek_outputs

File ~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:826, in <listcomp>(.0)
    822         return values[batch_idx].cpu()
    824     sequence_tokens = seek_outputs[""sequences""]
    825     seek_outputs = [
--> 826         {k: split_by_batch_index(v, k, i) for k, v in seek_outputs.items()}
    827         for i in range(sequence_tokens.shape[0])
    828     ]
    829 else:
    830     sequence_tokens = seek_outputs

File ~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:826, in <dictcomp>(.0)
    822         return values[batch_idx].cpu()
    824     sequence_tokens = seek_outputs[""sequences""]
    825     seek_outputs = [
--> 826         {k: split_by_batch_index(v, k, i) for k, v in seek_outputs.items()}
    827         for i in range(sequence_tokens.shape[0])
    828     ]
    829 else:
    830     sequence_tokens = seek_outputs

File ~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:822, in WhisperGenerationMixin._postprocess_outputs.<locals>.split_by_batch_index(values, key, batch_idx)
    819 if key == ""past_key_values"":
    820     # we don't save `past_key_values` as this is too costly
    821     return None
--> 822 return values[batch_idx].cpu()

AttributeError: 'tuple' object has no attribute 'cpu'

```

Works fine if you don't ask the timestamps per token.

### Expected behavior

Model should be able to return the timestamps per token when working with long audio after #27492","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
[`spam`],,[],0,open
Misleading ImportError when using JAX tensors without Flax installed,"### System Info

- `transformers` version: 4.37.2
- Platform: Linux-5.19.0-1027-gcp-x86_64-with-glibc2.35
- Python version: 3.11.4
- Huggingface_hub version: 0.20.1
- Safetensors version: 0.4.1
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.3.0.dev20231228+cpu (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed  # But I have installed
- JaxLib version: not installed  # But I have installed
- Using GPU in script?: N/A
- Using distributed or parallel set-up in script?: N/A

### Who can help?

@sanchit-gandhi @ArthurZucker

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

While attempting to convert tokenizer outputs to JAX tensors using the following code:
```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')
sentences = ['hello world']
inputs = tokenizer(sentences, padding=True, return_tensors='jax')
```
I received the following ImportError:
```
ImportError: Unable to convert output to JAX tensors format, JAX is not installed.
```

However, JAX is indeed installed in my environment.

### Expected behavior

Upon further investigation, it seems the error arises because the library checks for Flax's availability (`is_flax_available()`) rather than JAX's direct presence. Here is the snippet from the [source code](https://github.com/huggingface/transformers/blob/58e3d23e97078f361a533b9ec4a6a2de674ea52a/src/transformers/tokenization_utils_base.py#L723-L724) that led to this conclusion:
```python
if not is_flax_available():
    raise ImportError(""Unable to convert output to JAX tensors format, JAX is not installed."")
```
This can be somewhat misleading, as the error message suggests a lack of JAX installation, while the actual requirement is for Flax. Not all JAX users utilize Flax, and this might cause confusion.

Would it be possible to update the error message to more accurately reflect the requirement for Flax when attempting to use JAX tensor formats? Such a clarification would greatly assist users in diagnosing setup issues.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 6784721646, 'node_id': 'LA_kwDOCUB6oc8AAAABlGai7g', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/jax', 'name': 'jax', 'color': '560CEA', 'default': False, 'description': ''}]",7,open
[i18n-es] Translating docs to Spanish,"Hi!

Let's bring the documentation to all the Spanish-speaking community 🌐 

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `es` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `es/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get started
- [x]     index.md
- [x]     quicktour.md
- [x]     installation.md

## Tutorials
- [x]     pipeline_tutorial.md
- [x]     autoclass_tutorial.md
- [x]     preprocessing.md
- [x]     training.md
- [x]     run_scripts.md
- [x]     accelerate.md
- [ ]     peft.md
- [x]     model_sharing.md
- [ ]     transformers_agents.md
- [ ]     llm_tutorial.md

## Task guides
### Natural Language Processing
- [ ]         tasks/sequence_classification.md
- [ ]         tasks/token_classification.md
- [x]        tasks/question_answering.md
- [x]        tasks/language_modeling.md
- [ ]         tasks/masked_language_modeling.md
- [ ]         tasks/translation.md
- [x]        tasks/summarization.md
- [x]         tasks/multiple_choice.md

### Audio
- [ ]          tasks/audio_classification.md
- [x]         tasks/asr.md

### Computer Vision
- [x]         tasks/image_classification.md
- [ ]          tasks/semantic_segmentation.md
- [ ]          tasks/video_classification.md
- [ ]          tasks/object_detection.md
- [ ]          tasks/zero_shot_object_detection.md
- [ ]          tasks/zero_shot_image_classification.md
- [ ]          tasks/monocular_depth_estimation.md
- [ ]          tasks/image_to_image.md
- [ ]          tasks/knowledge_distillation_for_image_classification.md

### Multimodal
- [x]          tasks/image_captioning.md https://github.com/huggingface/transformers/pull/29104
- [ ]          tasks/document_question_answering.md
- [ ]          tasks/visual_question_answering.md
- [ ]          tasks/text-to-speech.md

### Generation
- [ ]          generation_strategies

### Prompting
- [ ]          tasks/idefics
- [ ]          tasks/prompting

## Developer guides
- [x]     fast_tokenizers.md
- [x]     multilingual.md
- [x]     create_a_model.md
- [x]     custom_models.md
- [x]      chat_templating.md #29559
- [x]      trainer.md https://github.com/huggingface/transformers/pull/29310
- [x]     sagemaker.md
- [x]     serialization.md
- [x]     tflite.md
- [x]      torchscript.md https://github.com/huggingface/transformers/pull/29310
- [ ]      benchmarks.md
- [ ]      notebooks.md
- [x]     community.md
- [ ]      custom_tools.md
- [ ]      troubleshooting.md
- [ ]      hf_quantizer.md

## Performance and scalability
- [x]     performance.md
- [ ]      quantization.md

### Efficient training techniques
- [ ]          perf_train_gpu_one.md
- [ ]          perf_train_gpu_many.md
- [ ]          fsdp.md
- [ ]          deepspeed.md
- [ ]          perf_train_cpu.md
- [ ]          perf_train_cpu_many.md
- [ ]          perf_train_tpu_tf.md
- [ ]          perf_train_special.md
- [ ]          perf_hardware.md
- [ ]          hpo_train.md

### Optimizing inference
- [ ]      perf_infer_cpu.md
- [ ]      perf_infer_gpu_one.md
- [ ]      big_models.md
- [x]     debugging.md
- [ ]      tf_xla.md
- [ ]      perf_torch_compile.md

## Contribute
- [ ]      contributing.md
- [ ]      add_new_model.md
- [ ]      add_tensorflow_model.md
- [x]     add_new_pipeline.md
- [ ]      testing.md
- [x]     pr_checks.md

## Conceptual guides
- [x]     philosophy.md
- [x]     glossary.md
- [x]      task_summary.md #28844 
- [ ]      tasks_explained.md https://github.com/huggingface/transformers/pull/29224
- [ ]      model_summary.md
- [ ]      tokenizer_summary.md
- [x]      attention.md #29681 
- [x]     pad_truncation.md
- [x]     bertology.md
- [x]     perplexity.md
- [ ]      pipeline_webserver.md - #30252 
- [ ]      model_memory_anatomy.md
- [ ]      llm_tutorial_optimization.md
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",8,open
Add support for prefix_allowed_tokens_fn to maintain a state throughout decoding,"### Feature request

Add an optional argument in `prefix_allow_tokens_fn` to allow for state to maintained throughout decoding or add a stateful alternative to `prefix_allowed_tokens_fn`.

### Motivation

`prefix_allowed_tokens_fn` is great but has one major downfall which is that you cannot maintain a state throughout decoding. This is inefficient because at each step you must go through your past `inputIds`, build up your current ""state"", and then figure out which tokens are allowed to appear next.

Instead, there should be a class we can subclass that gets passed the next token ID at each step of decoding (`Constraint` does not achieve this as `update` does not get every token ID). For example if you are trying to create a function to output json format (https://gist.github.com/BorisTheBrave/969f303a082c9da1916d04ee1eb04452), then you could track where you currently on in the json as each token ID is being received instead of going through everything on each new token.

### Your contribution

Unfortunately can't make a PR.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",4,open
"BertTokenizer and BertTokenizerFast have different behavior when requested ""return_overflowing_tokens""","### System Info

- `transformers` version: 4.37.2
- Platform: Linux-6.5.5-arch1-1-x86_64-with-glibc2.38
- Python version: 3.11.5
- Huggingface_hub version: 0.20.3
- Safetensors version: 0.4.2
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.2.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed



### Who can help?

@ArthurZucker 


### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

```python
from transformers import BertTokenizer, BertTokenizerFast, BatchEncoding
n_tok = BertTokenizer.from_pretrained(""bert-base-uncased"")
f_tok = BertTokenizerFast.from_pretrained(""bert-base-uncased"")

text = ""hello my name is nikola and i debug transformers now""

n_inputs: BatchEncoding = n_tok(text=text, add_special_tokens=True, max_length=6, truncation=True, padding='max_length', return_overflowing_tokens=True)
o = n_inputs.get(""overflowing_tokens"")
print(f'Overflowing {o}')
n_inputs['input_ids']


f_inputs: BatchEncoding = f_tok(text=text, add_special_tokens=True, max_length=6, truncation=True, padding='max_length', return_overflowing_tokens=True)
o = f_inputs.get(""overflowing_tokens"")
print(f'Overflowing {o}')
f_inputs['input_ids']
```

### Expected behavior

For the `n_inputs['input_ids']` we get `[101, 7592, 2026, 2171, 2003, 102]`, and
for the `f_inputs['input_ids']` we get `[[101, 7592, 2026, 2171, 2003, 102], [101, 24794, 1998, 1045, 2139, 102], [101, 8569, 2290, 19081, 2085, 102]]`.
Outputs should be the same.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",9,open
Returning history prompt from BarkModel.generate() ,"### Feature request

Hi, 
I have noticed that the original implementation of Bark (https://github.com/suno-ai/bark) has added a feature where one can get the history_prompt for the audio being currently generated using the parameter  output_full. 
history_prompt, out_arr = generate_audio(text_prompt, output_full=True)
where history_prompt is a dict object with semantic_prompt, coarse_prompt, and fine_prompt as its keys. 

But the generate method of the  huggingface version of Bark (BarkModel) doesn't support this parameter. I tried to modify the code by creating a dict of these under the generate method but the prompts in the output prompt don't meet the criteria of a valid history_prompt to be used next time because of the mismatch in ndarray.

Even the ndarray shape is also different for semantic, coarse, and fine prompts are different in the original implementation and the HuggingFace implementation. 

Can you please help me in fixing it?

### Motivation

 I want to generate a continous long-form audio for an audiobook for a better experience. I believe this will help in helping the Suno/Bark decide the tone based on the last sentence which can not be achieved using it at a sentence level  based on a single fixed history_prompt. 


### Your contribution

I need to go through and understand why there is a difference in the shape of different prompts. If that's achieved, I can contribute with a PR. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Detr models crashes when changing the num_queries parameter in the config,"### System Info

- `transformers` version: 4.36.2
- Platform: Linux-5.15.133+-x86_64-with-glibc2.35
- Python version: 3.10.10
- Huggingface_hub version: 0.20.2
- Safetensors version: 0.4.1
- Accelerate version: 0.26.1
- Accelerate config:    not found
- PyTorch version (GPU?): 2.1.2+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: Yes, Tesla T4
- Using distributed or parallel set-up in script?: No

### Who can help?

@amyeroberts

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

1. Load the model with a custom `num_queries` hyperparameter.
    ```
    id2label = {0: 'Test'}
    label2id = {'Test': 0}
    model_name = ""facebook/detr-resnet-50""
    image_processor = AutoImageProcessor.from_pretrained(model_name)
    detr = DetrForObjectDetection.from_pretrained(
        model_name,
        id2label=id2label,
        label2id=label2id,
        ignore_mismatched_sizes=True,
        num_queries=5
    )
    ```
2. Train (or just run the forward pass with an input containing `labels`)

I got the following error

```
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ in <module>:1                                                                                    │
│                                                                                                  │
│ ❱ 1 trainer.train()                                                                              │
│   2                                                                                              │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/trainer.py:1537 in    │
│ train                                                                                            │
│                                                                                                  │
│   1534 │   │   │   finally:                                                                      │
│   1535 │   │   │   │   hf_hub_utils.enable_progress_bars()                                       │
│   1536 │   │   else:                                                                             │
│ ❱ 1537 │   │   │   return inner_training_loop(                                                   │
│   1538 │   │   │   │   args=args,                                                                │
│   1539 │   │   │   │   resume_from_checkpoint=resume_from_checkpoint,                            │
│   1540 │   │   │   │   trial=trial,                                                              │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/trainer.py:1854 in    │
│ _inner_training_loop                                                                             │
│                                                                                                  │
│   1851 │   │   │   │   │   self.control = self.callback_handler.on_step_begin(args, self.state,  │
│   1852 │   │   │   │                                                                             │
│   1853 │   │   │   │   with self.accelerator.accumulate(model):                                  │
│ ❱ 1854 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)                      │
│   1855 │   │   │   │                                                                             │
│   1856 │   │   │   │   if (                                                                      │
│   1857 │   │   │   │   │   args.logging_nan_inf_filter                                           │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/trainer.py:2735 in    │
│ training_step                                                                                    │
│                                                                                                  │
│   2732 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device)                    │
│   2733 │   │                                                                                     │
│   2734 │   │   with self.compute_loss_context_manager():                                         │
│ ❱ 2735 │   │   │   loss = self.compute_loss(model, inputs)                                       │
│   2736 │   │                                                                                     │
│   2737 │   │   if self.args.n_gpu > 1:                                                           │
│   2738 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu parallel training        │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/trainer.py:2758 in    │
│ compute_loss                                                                                     │
│                                                                                                  │
│   2755 │   │   │   labels = inputs.pop(""labels"")                                                 │
│   2756 │   │   else:                                                                             │
│   2757 │   │   │   labels = None                                                                 │
│ ❱ 2758 │   │   outputs = model(**inputs)                                                         │
│   2759 │   │   # Save past state if it exists                                                    │
│   2760 │   │   # TODO: this needs to be fixed and made cleaner later.                            │
│   2761 │   │   if self.args.past_index >= 0:                                                     │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518    │
│ in _wrapped_call_impl                                                                            │
│                                                                                                  │
│   1515 │   │   if self._compiled_call_impl is not None:                                          │
│   1516 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]        │
│   1517 │   │   else:                                                                             │
│ ❱ 1518 │   │   │   return self._call_impl(*args, **kwargs)                                       │
│   1519 │                                                                                         │
│   1520 │   def _call_impl(self, *args, **kwargs):                                                │
│   1521 │   │   forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.fo  │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527    │
│ in _call_impl                                                                                    │
│                                                                                                  │
│   1524 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1525 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1526 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1527 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1528 │   │                                                                                     │
│   1529 │   │   try:                                                                              │
│   1530 │   │   │   result = None                                                                 │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/models/detr/modeling  │
│ _detr.py:1603 in forward                                                                         │
│                                                                                                  │
│   1600 │   │   │   │   auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)      │
│   1601 │   │   │   │   outputs_loss[""auxiliary_outputs""] = auxiliary_outputs                     │
│   1602 │   │   │                                                                                 │
│ ❱ 1603 │   │   │   loss_dict = criterion(outputs_loss, labels)                                   │
│   1604 │   │   │   # Fourth: compute total loss, as a weighted sum of the various losses         │
│   1605 │   │   │   weight_dict = {""loss_ce"": 1, ""loss_bbox"": self.config.bbox_loss_coefficient}  │
│   1606 │   │   │   weight_dict[""loss_giou""] = self.config.giou_loss_coefficient                  │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518    │
│ in _wrapped_call_impl                                                                            │
│                                                                                                  │
│   1515 │   │   if self._compiled_call_impl is not None:                                          │
│   1516 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]        │
│   1517 │   │   else:                                                                             │
│ ❱ 1518 │   │   │   return self._call_impl(*args, **kwargs)                                       │
│   1519 │                                                                                         │
│   1520 │   def _call_impl(self, *args, **kwargs):                                                │
│   1521 │   │   forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.fo  │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527    │
│ in _call_impl                                                                                    │
│                                                                                                  │
│   1524 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1525 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1526 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1527 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1528 │   │                                                                                     │
│   1529 │   │   try:                                                                              │
│   1530 │   │   │   result = None                                                                 │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/models/detr/modeling  │
│ _detr.py:2202 in forward                                                                         │
│                                                                                                  │
│   2199 │   │   outputs_without_aux = {k: v for k, v in outputs.items() if k != ""auxiliary_outpu  │
│   2200 │   │                                                                                     │
│   2201 │   │   # Retrieve the matching between the outputs of the last layer and the targets     │
│ ❱ 2202 │   │   indices = self.matcher(outputs_without_aux, targets)                              │
│   2203 │   │                                                                                     │
│   2204 │   │   # Compute the average number of target boxes across all nodes, for normalization  │
│   2205 │   │   num_boxes = sum(len(t[""class_labels""]) for t in targets)                          │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518    │
│ in _wrapped_call_impl                                                                            │
│                                                                                                  │
│   1515 │   │   if self._compiled_call_impl is not None:                                          │
│   1516 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]        │
│   1517 │   │   else:                                                                             │
│ ❱ 1518 │   │   │   return self._call_impl(*args, **kwargs)                                       │
│   1519 │                                                                                         │
│   1520 │   def _call_impl(self, *args, **kwargs):                                                │
│   1521 │   │   forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.fo  │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527    │
│ in _call_impl                                                                                    │
│                                                                                                  │
│   1524 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1525 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1526 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1527 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1528 │   │                                                                                     │
│   1529 │   │   try:                                                                              │
│   1530 │   │   │   result = None                                                                 │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115 in  │
│ decorate_context                                                                                 │
│                                                                                                  │
│   112 │   @functools.wraps(func)                                                                 │
│   113 │   def decorate_context(*args, **kwargs):                                                 │
│   114 │   │   with ctx_factory():                                                                │
│ ❱ 115 │   │   │   return func(*args, **kwargs)                                                   │
│   116 │                                                                                          │
│   117 │   return decorate_context                                                                │
│   118                                                                                            │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/models/detr/modeling  │
│ _detr.py:2323 in forward                                                                         │
│                                                                                                  │
│   2320 │   │   bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)                               │
│   2321 │   │                                                                                     │
│   2322 │   │   # Compute the giou cost between boxes                                             │
│ ❱ 2323 │   │   giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_c  │
│   2324 │   │                                                                                     │
│   2325 │   │   # Final cost matrix                                                               │
│   2326 │   │   cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.g  │
│                                                                                                  │
│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/models/detr/modeling  │
│ _detr.py:2388 in generalized_box_iou                                                             │
│                                                                                                  │
│   2385 │   # degenerate boxes gives inf / nan results                                            │
│   2386 │   # so do an early check                                                                │
│   2387 │   if not (boxes1[:, 2:] >= boxes1[:, :2]).all():                                        │
│ ❱ 2388 │   │   raise ValueError(f""boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {  │
│   2389 │   if not (boxes2[:, 2:] >= boxes2[:, :2]).all():                                        │
│   2390 │   │   raise ValueError(f""boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {  │
│   2391 │   iou, union = box_iou(boxes1, boxes2)                                                  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
ValueError: boxes1 must be in [x0, y0, x1, y1] (corner) format, but got tensor([[nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan]], device='cuda:0')
```

The same code works fine without changing the default `num_queries`.

### Expected behavior

I would expect the model to run as normal.

I am fine tuning the model in a custom dataset which should not have more than a couple of objects per image, and expected the number of queries to have no impact other than limiting the maximum number of objects found.","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",15,open
TextStreamer: An option to print / put every single token instead of whole words,"### Feature request

https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L108-L114

The current implementation forces us to visualize decoding on a per word basis. Add an option to print the string as soon as it's generated.

### Motivation

I use TextStreamer to visualize several things in my terminal:
1. How words are split
2. Actual token-per-second performance
3. Understand how it generates very long words like URLs

### Your contribution

I'm a Python noob","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",3,open
Adding Gradient Checkpointing and Flash Attention 2 implementation to VisionTextDualEncoderModel,"VisionTextDualEncoderModel allows training any image and text encoders with a contrastive loss. Would be convenient to add gradient checkpointing as well as flash attention 2 to optimize training. 

Thank you ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",0,open
Unable to use torch scripting to export Mask2Former model,"### System Info

- `transformers` version: 4.34.1
- Platform: Linux-5.4.0-100-generic-x86_64-with-glibc2.17
- Python version: 3.8.18
- Huggingface_hub version: 0.17.3
- Safetensors version: 0.4.0
- Accelerate version: 0.23.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.1.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: Yes, a GTX 1080 with 8 GB of VRAM
- Using distributed or parallel set-up in script?: No.

### Who can help?

@amyeroberts 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I am attempting to export the [Mask2Former model available in huggingface](https://huggingface.co/facebook/mask2former-swin-base-coco-panoptic) through [`torch.jit.script`](https://pytorch.org/docs/stable/generated/torch.jit.script.html). Here's a minimal reproducible example:

```python
import torch
from transformers import Mask2FormerForUniversalSegmentation
device = ""cuda"" if torch.cuda.is_available() else ""cpu""
model = Mask2FormerForUniversalSegmentation.from_pretrained(
    ""facebook/mask2former-swin-base-coco-panoptic"", torchscript=True
    ).to(device)

scripted_model = torch.jit.script(model)
torch.jit.save(scripted_model, 'mask2former.pt')
```

By doing this, I get the following error using torch scripting (path to the offending file has been obfuscated for brevity):

```
torch.jit.frontend.NotSupportedError: Comprehension ifs are not supported yet:
  File ""/home/.../huggingface/lib/python3.8/site-packages/transformers/models/mask2former/modeling_mask2former.py"", line 2559
        if not return_dict:
            output = tuple(v for v in output.values() if v is not None)
            if loss is not None:
                output = ((loss)) + output
```

As a hack, I've changed my local installation so that comprehension ifs are removed:

```
    if not return_dict:
        outputs = []
        for v in output.values():
            if v is not None:
                outputs.append(v)
        output = tuple(outputs)    
```

This also occurs at line 2306 in the same file, so I've made the same changes there. Once I fix this, there is an error in the forward method for the SWIN backbone:

```
RuntimeError: 
'Optional[Tensor]' object has no attribute or method 'shape'.:
  File ""/home/.../anaconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py"", line 313
    def forward(self, pixel_values: Optional[torch.FloatTensor]) -> Tuple[torch.Tensor, Tuple[int]]:
        _, num_channels, height, width = pixel_values.shape
                                         ~~~~~~~~~~~~~~~~~~ <--- HERE
        if num_channels != self.num_channels:
            raise ValueError(
```

The forward method for the SWIN backbone is confusing, as the input type is declared to be `Optional` but the output type is not. The definition of this method clearly indicates that a concrete tuple is to be returned.

As a final experiment, I've removed the `Optional` type declaration and tried to export it one more time:

```
aten::pad(Tensor self, SymInt[] pad, str mode=""constant"", float? value=None) -> Tensor:
Expected a value of type 'List[int]' for argument 'pad' but instead found type 'Tuple[int, Tensor]'.
:
  File ""/home/.../anaconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py"", line 306
        if width % self.patch_size[1] != 0:
            pad_values = (0, self.patch_size[1] - width % self.patch_size[1])
            pixel_values = nn.functional.pad(pixel_values, pad_values)
                           ~~~~~~~~~~~~~~~~~ <--- HERE
        if height % self.patch_size[0] != 0:
            pad_values = (0, 0, 0, self.patch_size[0] - height % self.patch_size[0])
'SwinPatchEmbeddings.maybe_pad' is being compiled since it was called from 'SwinPatchEmbeddings.forward'
  File ""/home/.../anaconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py"", line 319
            )
        # pad the input to be divisible by self.patch_size, if needed
        pixel_values = self.maybe_pad(pixel_values, height, width)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        embeddings = self.projection(pixel_values)
        _, _, height, width = embeddings.shape
```

It seems that what is being put into the forward pass is not, in fact, a `torch.Tensor` when being scripted.

Is torch scripting this model not supported at this time or am I missing something?

### Expected behavior

The model successfully being exported to disk with torch scripting.","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",3,open
Allow setting different decoder_start_token_ids for each item in a batch in the generate function.,"### Feature request
@gante 
The `generate` function has a `decoder_start_token_id` argument that allows the specification of the decoder start token when generating from an encoder-decoder model (e.g. mT5). Currently, `decoder_start_token_id` must be an integer, which means that the same start token is used for all elements in the batch. I request that you allow the specification of different start tokens for each element of the batch. For this purpose, `decoder_start_token_id` must be a tensor with shape `(batch_size,)`.

### Motivation

Some multilingual encoder-decoder models use the `decoder_start_token_id` to indicate the target language. Thus, this change would allow generation into multiple target languages in parallel, as illustrated in the code below.

### Your contribution

```
import re
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

WHITESPACE_HANDLER = lambda k: re.sub('\s+', ' ', re.sub('\n+', ' ', k.strip()))

article_text = """"""Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs ""spill over into misinformation about vaccines in general"". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  ""We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,"" the post said, referring to the World Health Organization.""""""

model_name = ""csebuetnlp/mT5_m2m_crossSum_enhanced""
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

get_lang_id = lambda lang: tokenizer._convert_token_to_id(
    model.config.task_specific_params[""langid_map""][lang][1]
)

target_langs = [""portuguese"", ""spanish""]

input_ids = tokenizer(
    [WHITESPACE_HANDLER(article_text)],
    return_tensors=""pt"",
    padding=""max_length"",
    truncation=True,
    max_length=512
)[""input_ids""]
input_ids = input_ids.expand(len(target_langs), -1)   # shape (num_target_languages, num_input_tokens)

decoder_start_token_id = torch.tensor(
    [get_lang_id(t) for t in target_langs],
    dtype=input_ids.dtype,
    device=input_ids.device
)  # shape (num_target_languages,)

output_ids = model.generate(
    input_ids=input_ids,
    decoder_start_token_id=decoder_start_token_id,
    max_length=84,
    no_repeat_ngram_size=2,
    num_beams=4,
)

summaries = tokenizer.batch_decode(
    output_ids,
    skip_special_tokens=True,
    clean_up_tokenization_spaces=False
)

print(summaries)
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
Adding CrossMAE,"### Model description

Hey,
the recently released [CrossMAE](https://crossmae.github.io/) seems like it would be a nice addition to transformers.

Basically the model improves MAE by using Cross-Attention instead of Self-Attention on the tokens and thereby decreasing the needed FLOPS quite significantly. At the same time it seems to be able to keep the performance of MAE or even improve it a bit.

Maybe there are already plans of integrating it @NielsRogge ?

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Project Page: https://crossmae.github.io/
GitHub Repo: https://github.com/TonyLianLong/CrossMAE
Paper: https://arxiv.org/pdf/2401.14391.pdf","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Fix the StarCoder agent max_new_tokens input validation error," Committer: Darya Petrashka <dashacheb15@gmail.com>

# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #28523


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
link: https://github.com/huggingface/transformers/issues/28523
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker and @younesbelkada

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
DETR: IndexError: Caught IndexError in replica 0 on device 0. IndexError: index 8 is out of bounds for dimension 0 with size 8,"### System Info

- `transformers` version: 4.37.1
- Platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.37
- Python version: 3.10.12
- Huggingface_hub version: 0.20.3
- Safetensors version: 0.4.2
- Accelerate version: 0.26.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.1.2+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: Not explicitly, but Trainer is picking up 2 GPUs

### Who can help?

@amyeroberts Hi. I'm getting the error in the title trying to reproduce [this example](https://huggingface.co/docs/transformers/tasks/object_detection). The error is real. I don't know what caused it, but I've narrowed the cause to DETR receiving `BatchSize x NumGPUs` number of targets, but expecting only `BatchSize` which causes the overflow. If I limit the amount of GPUs to 1 (via `CUDA_VISIBLE_DEVICES=0`, for example), it runs ok.

Here's the stack trace:
```
Traceback (most recent call last):
  File ""/home/mgruner/cellphones-in-the-wild/./train.py"", line 116, in <module>
    trainer.train()
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/trainer.py"", line 1539, in train
    return inner_training_loop(
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/trainer.py"", line 1869, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/trainer.py"", line 2768, in training_step
    loss = self.compute_loss(model, inputs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/trainer.py"", line 2791, in compute_loss
    outputs = model(**inputs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py"", line 185, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py"", line 200, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py"", line 110, in parallel_apply
    output.reraise()
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/_utils.py"", line 694, in reraise
    raise exception
IndexError: Caught IndexError in replica 0 on device 0.
Original Traceback (most recent call last):
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in _worker
    output = module(*input, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py"", line 1603, in forward
    loss_dict = criterion(outputs_loss, labels)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py"", line 2202, in forward
    indices = self.matcher(outputs_without_aux, targets)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py"", line 2330, in forward
    indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]
  File ""/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py"", line 2330, in <listcomp>
    indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]
IndexError: index 8 is out of bounds for dimension 0 with size 8
```

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

1. Follow this tutorial: https://huggingface.co/docs/transformers/tasks/object_detection


### Expected behavior

I expect the model to train.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",7,open
Any plans to support KV Cache offloading to CPU (and NVMe)?,"### Feature request

Similar to how model parameter and optimizer offload is supported using the [deepspeed library](https://github.com/huggingface/transformers/blob/de13a951b38b85195984164819f1ab05fe508677/docs/source/en/perf_train_gpu_one.md#deepspeed-zero), are there plans for natively supporting KV cache offloading as well? 

### Motivation

Apart from helping accommodate larger batch sizes on a single GPU, this can also significantly improve overall throughput, specially when batch sizes grow very large (resulting in a linear increase in KV cache size).

### Your contribution

I see there already exists an implementation of this: https://github.com/tjruwase/transformers/tree/kvcache-offload-cpu, so maybe this is simply about incorporating those changes in the main repo?","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Load an EncoderDecoderModel as AutoModel,"### System Info

- `transformers` version: 4.35.0
- Platform: Linux-5.15.0-91-lowlatency-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.17.3
- Safetensors version: 0.4.0
- Accelerate version: 0.24.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.1.2+cu121 (False)
- Tensorflow version (GPU?): 2.11.0 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help?

@ArthurZucker and @younesbelkada

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained(""Bachstelze/instructionRoberta-base"")
model = AutoModel.from_pretrained(""Bachstelze/instructionRoberta-base"", output_attentions=True)

### Expected behavior

Load the EncoderDecoderModel as AutoModel. ""BertGenerationConfig"" is supported, though this seems outdated.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",15,open
[WIP] Improve multimodal processors - rely less on kwargs,"# What does this PR do?

This PR aims at a better control on the logic flow through `Processor` classes, in particular those leveraging `ImageProcessor` with a `Tokenizer`. Linked with #27768. 

`ImageProcessors` compared to `Nougat` (as a reference point) have different signatures in their preprocess. One can list them here 
```
TvltImageProcessor:
videos, patch_size, crop_size, do_center_crop, is_mixed, num_frames

IdeficsImageProcessor:
transform, image_num_channels, image_size

ViTImageProcessor:
No difference in args

Mask2FormerImageProcessor:
segmentation_maps, ignore_index, size_divisor, reduce_labels, instance_id_to_semantic_id

MaskFormerImageProcessor:
segmentation_maps, ignore_index, size_divisor, do_reduce_labels, instance_id_to_semantic_id

YolosImageProcessor:
format, return_segmentation_masks, annotations, masks_path

MobileNetV1ImageProcessor:
do_center_crop, crop_size

DeiTImageProcessor:
do_center_crop, crop_size

EfficientNetImageProcessor:
include_top, do_center_crop, rescale_offset, crop_size

BeitImageProcessor:
do_reduce_labels, do_center_crop, segmentation_maps, crop_size

MobileViTImageProcessor:
do_flip_channel_order, do_center_crop, segmentation_maps, crop_size

PerceiverImageProcessor:
do_center_crop, crop_size

DeformableDetrImageProcessor:
format, return_segmentation_masks, annotations, masks_path

EfficientFormerImageProcessor:
do_center_crop, crop_size

SegformerImageProcessor:
do_reduce_labels, segmentation_maps

LayoutLMv2ImageProcessor:
apply_ocr, ocr_lang, tesseract_config

BridgeTowerImageProcessor:
do_center_crop, size_divisor

SamImageProcessor:
segmentation_maps, pad_size, do_convert_rgb, mask_pad_size, mask_size

BlipImageProcessor:
do_convert_rgb

Owlv2ImageProcessor:
No difference in args

LayoutLMv3ImageProcessor:
apply_ocr, ocr_lang, tesseract_config

DetaImageProcessor:
format, return_segmentation_masks, annotations, masks_path

BitImageProcessor:
do_center_crop, do_convert_rgb, crop_size

ViTHybridImageProcessor:
do_center_crop, do_convert_rgb, crop_size

FuyuImageProcessor:
patch_size, padding_mode, padding_value

PvtImageProcessor:
No difference in args

Pix2StructImageProcessor:
max_patches, header_text, do_convert_rgb, patch_size

VitMatteImageProcessor:
trimaps, size_divisibility

VideoMAEImageProcessor:
videos, do_center_crop, crop_size

MobileNetV2ImageProcessor:
do_center_crop, crop_size

OneFormerImageProcessor:
segmentation_maps, ignore_index, task_inputs, do_reduce_labels, instance_id_to_semantic_id

FlavaImageProcessor:
crop_size, codebook_crop_size, codebook_rescale_factor, mask_group_max_patches, mask_group_min_patches, mask_group_max_aspect_ratio, codebook_image_mean, codebook_do_resize, return_image_mask, input_size_patches, codebook_do_center_crop, codebook_resample, mask_group_min_aspect_ratio, codebook_do_normalize, codebook_do_map_pixels, return_codebook_pixels, codebook_image_std, do_center_crop, codebook_size, codebook_do_rescale, total_mask_patches

DonutImageProcessor:
random_padding

TvpImageProcessor:
videos, crop_size, constant_values, do_flip_channel_order, do_center_crop, pad_size, pad_mode

GLPNImageProcessor:
size_divisor

PoolFormerImageProcessor:
crop_pct, do_center_crop, crop_size

CLIPImageProcessor:
do_center_crop, do_convert_rgb, crop_size

DPTImageProcessor:
ensure_multiple_of, keep_aspect_ratio, size_divisor

ViltImageProcessor:
size_divisor

Swin2SRImageProcessor:
pad_size

ImageGPTImageProcessor:
clusters, do_color_quantize

SiglipImageProcessor:
No difference in args

VivitImageProcessor:
videos, do_center_crop, offset, crop_size

ConvNextImageProcessor:
crop_pct

OwlViTImageProcessor:
do_center_crop, crop_size

ChineseCLIPImageProcessor:
do_center_crop, do_convert_rgb, crop_size

LevitImageProcessor:
do_center_crop, crop_size

ConditionalDetrImageProcessor:
format, return_segmentation_masks, annotations, masks_path

DetrImageProcessor:
format, return_segmentation_masks, annotations, masks_path
```

This helps standardize a bit in the first place, and then, will allow uniformizing `Processors`.


Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts

",[],1,open
"GPT2 cannot be used with device_map='auto'; Report ""found at least two devices""","### System Info

- `transformers` version: 4.36.2
- Platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.35
- Python version: 3.9.18
- Huggingface_hub version: 0.20.2
- Safetensors version: 0.4.1
- Accelerate version: 0.26.1
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.1.2+cu118 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

A simple reproducer here:
```python
from transformers import GPT2LMHeadModel
# create a sample input:
batch_ids = {
    'input_ids': torch.tensor([[312, 134, 56, 712, 351, 89, 63, 550, 971, 2]]),
    'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),
}

gpt2_large = GPT2LMHeadModel.from_pretrained('gpt2-large', cache_dir='./cache_dir', device_map='auto')
gpt2 = GPT2LMHeadModel.from_pretrained('gpt2', cache_dir='./cache_dir', device_map='auto')

loss_gpt2_large = gpt2_large(**batch_ids, labels=batch_ids['input_ids']).loss
loss_gpt2 = gpt2(**batch_ids, labels=batch_ids['input_ids']).loss
``` 

### Expected behavior

It works well to generate `loss_gpt2_large`, but it will report error when generating `loss_gpt2`: 
`RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)`

I am not sure why this behaves differently with the same model class. Could you please provide any comments on this? Thanks in advance!","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 6660834292, 'node_id': 'LA_kwDOCUB6oc8AAAABjQRD9A', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Accelerate', 'name': 'Accelerate', 'color': '59003B', 'default': False, 'description': ''}]",2,open
"Tokenizer `encode/decode` methods are inconsistent, TypeError: argument 'ids': 'list' object cannot be interpreted as an integer","### System Info

- `transformers` version: 4.35.2
- Platform: Linux-6.5.0-14-generic-x86_64-with-glibc2.35
- Python version: 3.11.6
- Huggingface_hub version: 0.20.2
- Safetensors version: 0.4.1
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>


### Who can help?

@ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Run the following code:
```python
from transformers import AutoTokenizer

text = ""test""
tokenizer = AutoTokenizer.from_pretrained(""gpt2"")
encoded = tokenizer.encode(text, return_tensors='pt')
result_text = tokenizer.decode(encoded, skip_special_tokens=True)
print(text)
```

Will raise exception:
```
Traceback (most recent call last):
  File ""main.py"", line 8, in <module>
    tokenizer.decode(encoded, skip_special_tokens=True)
  File ""/home/scruel/mambaforge/envs/vae/lib/python3.11/site-packages/transformers/tokenization_utils_base.py"", line 3748, in decode
    return self._decode(
           ^^^^^^^^^^^^^
  File ""/home/scruel/mambaforge/envs/vae/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py"", line 625, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: argument 'ids': 'list' object cannot be interpreted as an integer
```

### Expected behavior

Should be able to print the original text `""test""`, rather than raise an exception(`TypeError`).","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
PatchTST and PatchTSMixer categorical features and exogenous variables,"### Feature request

Include categorical features and exogenous variables as input for the PatchTST and PatchTSMixer timeseries foundation models

### Motivation

Categorical features and exogenous variables are key components in timeseries modelling

### Your contribution

-","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6462336551, 'node_id': 'LA_kwDOCUB6oc8AAAABgS9uJw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Time%20Series', 'name': 'Time Series', 'color': 'C1E56A', 'default': False, 'description': ''}]",4,open
Add [VMamba] model,"### Model description

VMamba is a visual foundation model proposed in https://arxiv.org/pdf/2401.10166.pdf.

It is inspired by the recent advances in state stace models and in particular Mamba. The proposed architecture is computationally more efficient than vision transformer architectures because it scales linearly with growing resolution. It introduces a Cross-Scan Module (CSM) to have context from all directions (4 directions, starting in each corner and traversing in a horizontal or vertical direction). Evaluation on vision perception tasks shows promising capabilities.

Model weights will become available in a few days according to the repo of the authors.

1.  [x] (Optional) Understood theoretical aspects

2.  [x] Prepared transformers dev environment

3.  [x] Set up debugging environment of the original repository

4.  [x] Created script that successfully runs forward pass using
    original repository and checkpoint

5.  [x] Successfully opened a PR and added the model skeleton to Transformers

6.  [x] Successfully converted original checkpoint to Transformers
    checkpoint

7.  [x] Successfully ran forward pass in Transformers that gives
    identical output to original checkpoint

8.  [x] Finished model tests in Transformers

9.  [ ] ~~Successfully added Tokenizer in Transformers~~

10. [x] Run end-to-end integration tests

11. [x] Finished docs

12. [ ] Uploaded model weights to the hub

13. [x] Submitted the pull request for review

14. [ ] (Optional) Added a demo notebook


I am opening the issue to avoid duplicate work. My main motivation for porting this model is to learn a bit more about it (and about the internals of 🤗 Transformers). Some of you probably know this library much better than me, so feel free to write your own implementation if you can do it better or quicker. Otherwise, don’t hesitate to build on top of my fork.

### Open source status

- [X] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

- Original repo: https://github.com/MzeroMiko/VMamba
- Paper: https://arxiv.org/pdf/2401.10166.pdf
- implementation in progress: 
- youtube vmamba vs vision mamba: https://www.youtube.com/watch?v=RtHDu6kFPb8
- vision mamba paper (similar idea): https://arxiv.org/pdf/2401.09417.pdf","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Optimised 4bit inference kernels,"### Feature request

Integration of new 4bit kernels 

https://github.com/IST-DASLab/marlin


### Motivation

provide faster Inference than awq/exllama for batch sizes upto 32


### Your contribution

Just saw this today, can try provide sample notebook. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",9,open
Feature Update [added `initial_prompt` support for automatic-speech-recognition whisper pipeline],"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (feature)
- `initial_prompt` support for whisper Pipeline (automatic-speech-recognition) 

## Before submitting
- [x] Added initial_prompt as an option for whisper model
- [x] To handle initial prompt `processor` considered as optional parameter
- [x] Current implementation supports only Torch version of decoding.
- [x] how to use initial prompt; 

``` python
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from datasets import load_dataset
import torch

device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = ""openai/whisper-small""
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

pipe = pipeline(
    ""automatic-speech-recognition"",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    max_new_tokens=128,
    chunk_length_s=15,
    batch_size=16,
    torch_dtype=torch_dtype,
    device=device,
    processor=processor
)


dataset = load_dataset(""distil-whisper/librispeech_long"", ""clean"", split=""validation"")
sample = dataset[0][""audio""]
audio = dataset[0][""audio""][""array""]
sampling_rate = dataset[0][""audio""][""sampling_rate""]

# including timestamp
print(pipe(audio, initial_prompt = ""Biswajit, Whisper"", return_timestamps=True))

# without timestamp
print(pipe(audio, initial_prompt = ""Biswajit, Whisper""))
```

## Who can review?

Anyone in the community is free to review the PR once the tests have passed. @sanchit-gandhi , @Narsil, Can anyone help to take this PR forward please. Let me know, if anything is needed.

fixes #27317
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNzcxMTg3OTI0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Pipeline', 'name': 'Core: Pipeline', 'color': 'FF7066', 'default': False, 'description': 'Internals of the library; Pipeline.'}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",21,open
"Early stopping required metric_for_best_model, but did not find eval_f1 so early stopping is disabled","### System Info

- `transformers` version: 4.35.2
- Platform: Linux-3.10.0-1160.49.1.el7.x86_64-x86_64-with-glibc2.17
- Python version: 3.8.18
- Huggingface_hub version: 0.19.4
- Safetensors version: 0.4.1
- Accelerate version: 0.25.0
- Accelerate config:    not found
- PyTorch version (GPU?): 1.13.1+cu116 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: yes
- Using distributed or parallel set-up in script?: no

### Who can help?

@muellerzr @pacman100

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```python
import os
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from transformers import EarlyStoppingCallback, IntervalStrategy
import numpy as np
import evaluate

os.environ[""CUDA_VISIBLE_DEVICES""]=str(gpu_id)
from datasets import Dataset, DatasetDict
train_k = pd.read_csv('train.csv', usecols=[""text"", ""k""])
train_k.rename(columns={""text"":""text"", ""k"":""label""}, inplace=True)
val_k = pd.read_csv('val.csv', usecols=[""text"", ""k""])
val_k.rename(columns={""text"":""text"", ""k"":""label""}, inplace=True)
test_k = pd.read_csv('test.csv', usecols=[""text"", ""k""])
test_k.rename(columns={""text"":""text"", ""k"":""label""}, inplace=True)

train_k = Dataset.from_pandas(train_k)
val_k = Dataset.from_pandas(val_k)
test_k = Dataset.from_pandas(test_k)

ds = DatasetDict()
ds['train'] = train_k
ds['val'] = val_k
ds['test'] = test_k

tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
def tokenize_function(examples):
    return tokenizer(str(examples['text']), padding=""max_length"", truncation=True)

tokenized_datasets = ds.map(tokenize_function)
tokenized_train_k = tokenized_datasets[""train""]
tokenized_val_k = tokenized_datasets[""val""]

model_k = AutoModelForSequenceClassification.from_pretrained(""bert-base-uncased"", num_labels=6)
training_args = TrainingArguments(output_dir=""trained_k_predictors"", evaluation_strategy=""steps"", eval_steps=100, metric_for_best_model = 'f1', learning_rate=1e-3, num_train_epochs=5, weight_decay=0.01, load_best_model_at_end=True, per_device_train_batch_size = 16, per_device_eval_batch_size = 32, save_total_limit = 3, optim=""adafactor"", label_names=['label'], remove_unused_columns=False,)
metric = evaluate.load(""f1"")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {'f1': metric.compute(predictions=predictions, references=labels)}
trainer = Trainer(model=model_k, args=training_args, train_dataset=tokenized_train_k, eval_dataset=tokenized_val_k, compute_metrics=compute_metrics, callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])
trainer.train()
```

## Error message:
{'eval_runtime': 21.6631, 'eval_samples_per_second': 208.926, 'eval_steps_per_second': 3.277, 'epoch': 0.47}                                                                                                       
  9%|████████████████                                                                                                                                                           | 5                       [26/1960]
00/5305 [06:15<41:00,  1.95it/s]
100%|████████████████████████████████████████████████████████████████████�     $                                                                                                                          [24/1960]
�█████████████████████████████████████████████�early stopping required metric_for_best_model, but did not find eval_f1 so                                                                                 [23/1960]
 early stopping is disabled██████████████████████████| 71/71 [00:21<00:00,  3.42it/s]
Traceback (most recent call last):                                                                                                                                                                                 
  File ""/scratch/manish/apl/apl_env/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/scratch/manish/apl/apl_env/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/scratch/manish/apl/src/apl.py"", line 386, in <module>
    main()
  File ""/scratch/manish/apl/src/apl.py"", line 139, in main
    trainer.train()
  File ""/scratch/manish/apl/apl_env/lib/python3.8/site-packages/transformers/trainer.py"", line 1555, in train
    return inner_training_loop(
  File ""/scratch/manish/apl/apl_env/lib/python3.8/site-packages/transformers/trainer.py"", line 1922, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File ""/scratch/manish/apl/apl_env/lib/python3.8/site-packages/transformers/trainer.py"", line 2282, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File ""/scratch/manish/apl/apl_env/lib/python3.8/site-packages/transformers/trainer.py"", line 2407, in _save_checkpoint
    metric_value = metrics[metric_to_check]
KeyError: 'eval_f1'
  9%|███████████████▉                                                                                                                                                         | 500                        [3/1960]
/5305 [06:18<1:00:34,  1.32it/s]



### Expected behavior

Train the model with early stopping enabled.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",17,open
feat: support indicating prefix token of chat template,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->
In chat language model training, sometimes we need to mask the input from real users, and train the model solely from assistant's outputs. 

This PR add a special prefix token, which can be applied in `chat_template`, so that we can make use of this `prefix_token` to dynamically separate dialogs from `user` and `assistant`.

For example:
```
""""""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
""""""
```

The prefix_token could be `<|im_start|>assistant\n`, we can make use of this token: 
- to set the model's `chat_template`, for example `{% if add_generation_prompt %}{{ prefix_token }}`
- To separate the dialogs from user's and model's turns, and mask the loss from user's turns, by access `tokenizer.prefix_token` and `tokenizer.eos_token`

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
`dataloader_persistent_workers=True` causes fork-bomb due to repeated creation of `eval_dataloader`,"### System Info

- `transformers` version: 4.36.2
- Platform: macOS-10.16-x86_64-i386-64bit
- Python version: 3.10.13
- Huggingface_hub version: 0.20.2
- Safetensors version: 0.4.1
- Accelerate version: 0.26.1
- Accelerate config:    - compute_environment: LOCAL_MACHINE
        - distributed_type: NO
        - mixed_precision: fp16
        - use_cpu: False
        - debug: False
        - num_processes: 1
        - machine_rank: 0
        - num_machines: 1
        - rdzv_backend: static
        - same_network: True
        - main_training_function: main
        - downcast_bf16: no
        - tpu_use_cluster: False
        - tpu_use_sudo: False
        - tpu_env: []
- PyTorch version (GPU?): 2.1.2 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: does not matter
- Using distributed or parallel set-up in script?: does not matter


### Who can help?

@muellerzr @pacman100 

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

```python
import os
from dataclasses import dataclass

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
from transformers import TrainingArguments, Trainer
from transformers.modeling_outputs import BaseModelOutput


# Dummy Dataset
class DummyDataset(Dataset):
    def __init__(self, size=100):
        self.size = size
        self.data = torch.rand(size, 10)  # Random data
        self.labels = torch.randint(0, 2, (size,))  # Binary labels

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        return {'input_ids': self.data[idx], 'labels': self.labels[idx]}


@dataclass
class DummyModelOutput(BaseModelOutput):
    loss: torch.Tensor = None
    logits: torch.Tensor = None


# Dummy Model
class DummyModel(torch.nn.Module):
    def __init__(self):
        super(DummyModel, self).__init__()
        self.linear = torch.nn.Linear(10, 2)

    def forward(self, input_ids, labels=None) -> DummyModelOutput:
        outputs = self.linear(input_ids)
        loss = F.cross_entropy(outputs, labels)
        return DummyModelOutput(loss=loss, logits=outputs)


if __name__ == '__main__':

    # using wandb, because it logs system metrics periodically
    os.environ[""WANDB_PROJECT""] = ""dummy_project""

    # Create dataset and model instances
    dataset = DummyDataset(size=1000)
    model = DummyModel()
    
    persistent_workers = False    # set to True to enable persistent workers

    # Training arguments
    training_args = TrainingArguments(
        output_dir=""./test_trainer"",
        run_name=f'dataloader_peristent_workers={persistent_workers}',
        num_train_epochs=20,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        dataloader_num_workers=8,
        dataloader_persistent_workers=persistent_workers,
        logging_strategy=""no"",
        evaluation_strategy=""epoch"",
    )

    # Initialize the custom trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        eval_dataset=dataset,
    )

    # Train the model
    trainer.train()

```

### Expected behavior

Since the [get_eval_loader](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3065C16-L3065C16) is called on every evaluate call, with `dataloader_persistent_workers=True` the previous worker processes are not killed and leads to a fork-bomb and exhausts system resources and causes instability/crash.

As you can see in the below plots generated with the reproduction script (in the wandb system metrics section), 
- persistent data loader workers cause speedup (mainly because the training loader does not recreate all processes at every epoch), but evaluation loaders cause the fork-bomb.
- without persistent data loader workers, speed is slow, but the number of processes is constant.

![image](https://github.com/huggingface/transformers/assets/12119806/dd3559bb-e6fa-4318-9f9a-fef5faff152e)

Having the persistent dataloader option is good. Still, it is necessary to fix the eval loader logic, create it once, and reuse it since the eval datasets won't change in the middle of training.

This option was added in #27058 and #27189
",[],16,open
Move layer_idx from a layer property to function argument.,"### Feature request

Currently the layer_idx is recorded in the attention module of each `LlamaDecoderLayer`. This has the unfortunate side effect that the layers cannot easily be moved around or reused within the layer list. It seems simple enough to pass in the layer index as part of loop over layers in the forward pass. That way the layers once again will be decouple from their position information.

Backward compatibility could be preserved by still accepting the argument in the constructor but defaulting it to None and then just ignoring it in favor of the passed forward argument.

### Motivation

The motivation is to allow for simple layer stacking (like we have been seeing with pass through merged models) at inference time without actually expanding the memory usage of the model.

### Your contribution

I am happy to send a PR. Seems simple enough.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
"Pytorch can have its default dtype permanently set to the ""wrong"" value if there is an exception when loading a model","### System Info

I just ran into the most head-scratching issue. My data collator was crashing because a tensor it made was in half precision (fp16). I couldn't figure out why, but then I realized my `torch.get_default_dtype()` was `torch.float16`!

Then I realized it's because my model code threw an exception in a previous run of a notebook cell. 
And if you look at this code PreTrainedModel:_from_config : [def _from_config(cls, config, **kwargs):](https://github.com/huggingface/transformers/blob/995a7ce9a80b80062ccfe0b2d78857fb17351e27/src/transformers/modeling_utils.py#L1256-L1294)

You can see that it tries to set the dtype back to the original value, but doesn't do so in a `finally` block:
```python
        # override default dtype if needed
        dtype_orig = None
        if torch_dtype is not None:
            dtype_orig = cls._set_default_torch_dtype(torch_dtype)
            
        # do some stuff here....maybe throw an exception...

        # restore default dtype if it was modified (assuming we get to this line)
        if dtype_orig is not None:
            torch.set_default_dtype(dtype_orig)

        return model
```

This would of course leave my torch default dtype in whatever it was in when I was trying to load the model.

We could sprinkle some `finally` blocks around, or we could write a class like this:
```python

class temporily_set_default_torch_dtype:
    def __init__(self, dtype):
        self.new_dtype = dtype
        if dtype is not None:
            self.original_dtype = torch.get_default_dtype()
        else:
            # try to make this a no-op
            self.original_dtype = None

    def __enter__(self):
        if self.new_dtype is not None:
            torch.set_default_dtype(self.new_dtype)

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.original_dtype is not None:
            torch.set_default_dtype(self.original_dtype)

```

And use it like so:
```python

torch.set_default_dtype(torch.float32)
print(f""default dtype is this before: {torch.get_default_dtype()}"")

try:
  with temporily_set_default_torch_dtype(torch.float16):
      print(f""default dtype is now this inside: {torch.get_default_dtype()}"")
      raise ValueError(""Throwing an exception to make sure it works"")
except ValueError as e:
  print(""We caught the exception"")
  pass

print(f""default dtype is this after: {torch.get_default_dtype()}"")

# prints:
# default dtype is this before: torch.float32
# default dtype is now this inside: torch.float16
# default dtype is this after: torch.float32

```





### Who can help?

Think @ArthurZucker and @younesbelkada are correct here?

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

1: Run a notebook cell that loads a model from_pretrained, in dtype=float16, and throws an exception while doing so.
2: Note that your torch.get_default_dtype() is still set to float16.

(This causes a real problem when things like the `DataCollatorForLanguageModeling` calls `torch_mask_tokens`, and then:
```python
# this will accidentally create a float16 tensor:
probability_matrix = torch.full(labels.shape, self.mlm_probability)
#...
probability_matrix.masked_fill_(special_tokens_mask, value=0.0)
masked_indices = torch.bernoulli(probability_matrix).bool()
```

An exception gets thrown when you try to call `bernoulli` on a cpu tensor at half precision:
`RuntimeError: ""bernoulli_tensor_cpu_self_"" not implemented for 'Half'`


### Expected behavior

My default torch dtype should not get ""corrupted"" even if the model loading code throws an exception","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",5,open
`get_imports` failing to respect conditionals on imports,"### System Info

- `transformers` version: 4.36.2
- Platform: macOS-13.5.2-arm64-arm-64bit
- Python version: 3.11.7
- Huggingface_hub version: 0.20.2
- Safetensors version: 0.4.1
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.1.2 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help?

From `git blame`: @Wauplin @sgugger 

From issue template (it's a LLM): @ArthurZucker @you

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Running the below snippet on a MacBook without an Nvidia GPU and `transformers==4.36.2` will throw an `ImportError` to `pip install flash_attn`. However, `flash_attn` isn't actually a requirement for this model, so something's off here.

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True)
```

Leads to:

```
  File ""/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py"", line 315, in get_cached_module_file
    modules_needed = check_imports(resolved_module_file)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py"", line 180, in check_imports
    raise ImportError(
ImportError: This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`
python-BaseException
```

Investigating this, it seems https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/dynamic_module_utils.py#L154 is picking up `flash_attn` from https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/phi/modeling_phi.py#L50-L52. However, if you look at the file, it's within an `if` statement.

Therein is the bug, that `transformers.dynamic_module_utils.get_imports` is not respecting conditionals before imports.

Please see https://huggingface.co/microsoft/phi-1_5/discussions/72 for more info.

### Expected behavior

My goal is some way to avoid monkey patching `get_imports` to remove the extra inferred `flash_attn` dependency.

The most generalized solution is probably moving `get_imports` from regex searching the source to either use `inspect` (see [here](https://stackoverflow.com/a/47093697)) or some other AST walking method. I am pretty sure there is a simple fix here, it just involves moving away from a regex.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",21,open
Proposal for Adding a New Scheduler Strategy for Language Model Pretraining,"### Feature request

We try to propose the addition of a new and widely-adopted scheduler strategy for language model pretraining in the Transformers repository. Upon reviewing the current schedulers available in the [Transformers optimization module](https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L338), it appears there is a notable absence of an out-of-the-box implementation for a specific type of scheduler. This particular scheduler is prevalent in recent pre-training models and features a warmup decay, but importantly, it also maintains a limited minimum learning rate post-maximum iteration steps.

This scheduling approach has seen extensive use in several prominent pre-trained large language models (LLMs), including:

1. TinyLLaMA: Implementation details can be found in their [pretraining script](https://github.com/jzhang38/TinyLlama/blob/main/pretrain/tinyllama.py#L375).
2. MindLLM: Described in their research paper, available at [arXiv:2310.15777](https://arxiv.org/pdf/2310.15777.pdf).
3. trlx: Utilized in the TRLx framework, as seen in their [GitHub repository](https://github.com/CarperAI/trlx/tree/main).
4. ...

The introduction of this scheduler into the Transformers library would not only complete the suite of existing scheduling strategies but also provide practitioners with a tool that's already proven its efficacy in recent LLM training methodologies. I believe its inclusion will be beneficial for the community, fostering more efficient and effective pretraining processes.

### Motivation

This issue aims to introduce a novel scheduler into the current Transformers library. The proposed scheduler combines the elements of warmup decay with a distinctive feature - the implementation of a constrained minimum learning rate beyond the maximum iteration steps.

### Your contribution

Yes, we could submit a PR as soon as possible if any huggingface members think this contribution is necessary.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Adding mixtral attention_bias in style of llama modeling,"### Feature request

### System Info
transformers version: 4.36.2

### Who can help?
don't have a clue about this

### Information
Refer to llama2 modeling code, I want to add attention bias option in mixtral model and configuration for flexibility of experiments.  
If this changes seems appropriate, I will make a PR for it

### Expected behavior
After changes, attention bias option of model is added in config.  
Can be controlled like example below(default config value is false)
```
from transformers import AutoConfig
config = AutoConfig.from_pretrained(""variant_of_mixtral"")
config.attention_bias = True
```

### Motivation

Refer to llama2 modeling code, I want to add attention bias option in mixtral model and configuration for flexibility of experiments.  

### Your contribution

I have created a fix branch. I can make a PR of it
refer to [link](https://github.com/Moreh-LeeJunhyeok/transformers/tree/mixtral_add_attention_bias)","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
GQA Llama 13B slower than Llama 13B without GQA,"### Feature request

It would be nice if when I choose different key_value_heads (key_value_heads < attention_heads) on config's model, automatically the attn weights were computed by mean pooling. Right now, if I do this, it gives me the next error.

key_value_heads = 4

<img width=""916"" alt=""image"" src=""https://github.com/huggingface/transformers/assets/70610799/05ae81c3-2ac6-4339-a805-02725ff9b538"">


### Motivation

Make models faster, e.g Llama  2 13B, Llama 7B, Mistral 7B etc.

### Your contribution

I tried to do a simple implementation. But it gives me inconsistent results. GQA model is slower than No GQA model.

```
from transformers import LlamaConfig
from transformers.models.llama.modeling_llama import LlamaAttention, LlamaSdpaAttention
from copy import deepcopy
import torch

def split_attention_to_heads(input_tensor, num_splits):
    # Get the shape of the input tensor
    rows, cols = input_tensor.shape

    # Check if the number of rows is divisible by the number of splits
    if rows % num_splits != 0:
        raise ValueError(""Number of rows is not divisible by the number of splits"")

    # Calculate the number of rows in each split

    # Use chunk to split the tensor along the rows
    split_tensors = input_tensor.chunk(num_splits, dim=0)

    return split_tensors

def average_heads(tensor_tuple, group_size, dtype):
    # Initialize an empty list to store the averaged tensors
    averaged_tensors = []

    # Iterate through the tuple and average consecutive groups
    for i in range(0, len(tensor_tuple), group_size):
        # Take a group of tensors
        tensor_group = tensor_tuple[i:i + group_size]

        # Calculate the mean along dimension 0
        averaged_tensor = torch.mean(torch.stack(tensor_group), dim=0, dtype=dtype)

        # Append the averaged tensor to the list
        averaged_tensors.append(averaged_tensor)

    # Convert the list of averaged tensors to a tuple
    averaged_tensors_tuple = tuple(averaged_tensors)

    return averaged_tensors_tuple

def convert_wts_to_gqa(attention_module: torch.nn.Module , model_configuration: LlamaConfig):
    attentions_wts = attention_module.state_dict().copy()
    num_heads = model_configuration.num_attention_heads
    gqa_groups = num_heads // model_configuration.num_key_value_heads
    for name_wts in list(attentions_wts.keys()):
        if (""k_proj"" in name_wts) or (""v_proj"" in name_wts):
            tensor_to_convert = attentions_wts[name_wts].clone()
            torch_dtype = tensor_to_convert.dtype
            attn_heads = split_attention_to_heads(tensor_to_convert, num_splits=num_heads)
            gqa_tensors_grouped = average_heads(attn_heads, gqa_groups, dtype=torch_dtype)
            gqa_tensors_grouped = torch.cat(gqa_tensors_grouped)
            attentions_wts[name_wts] = gqa_tensors_grouped
            del tensor_to_convert
    return attentions_wts



def convert_llama_to_gqa(module: torch.nn.Module, llama_config_from_hf: LlamaConfig, inplace: bool = False):
    if isinstance(module, LlamaAttention):
        wts_gqa = convert_wts_to_gqa(attention_module=module, model_configuration=llama_config_from_hf)
        llama_atention_gqa = LlamaAttention(llama_config_from_hf, layer_idx=module.layer_idx)
        llama_atention_gqa.half()
        llama_atention_gqa.load_state_dict(wts_gqa)
        return llama_atention_gqa

    out = module if inplace else deepcopy(module)
    for name, child in out.named_children():
        out._modules[name] = convert_llama_to_gqa(child, llama_config_from_hf=llama_config_from_hf, inplace=True)
    return out

from transformers import AutoConfig

configuration_llama = AutoConfig.from_pretrained(""meta-llama/Llama-2-13b-chat-hf"")
configuration_llama.num_key_value_heads = 4

llama_gqa = convert_llama_to_gqa(llama, configuration_llama)
```

**Results**
GQA LLAMA
<img width=""784"" alt=""image"" src=""https://github.com/huggingface/transformers/assets/70610799/d1a1c250-5ed3-4c34-9041-620b6b57ef3c"">




NO GQA LLAMA
<img width=""782"" alt=""image"" src=""https://github.com/huggingface/transformers/assets/70610799/b09b020a-94fa-450c-a239-3f7fa5339f7a"">


I don't know if I'm misunderstanding something, please let me know if you can see something I can't

","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Add FlashAttention-2 support for Mask2Former model,"### Feature request

Is it possible to add FlashAttention-2 support to the Mask2Former model? 

### Motivation

Since it is already availble for ViT, it would be great to have it on Mask2Former too.

Maybe the additional input masks to the decoder layer represent a major challenge?

### Your contribution

I could help by testing the implementations.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
How to use an efficient encoder as shared EncoderDecoderModel?,"### Feature request

Efficient encoder like destilBERT, ALBERT or ELECTRA aren't supported as decoder of the EncoderDecoderModel and so they can't be shared as encoder and decoder.

### Motivation

Warm-starting shared models is a powerful way to build transformer models. Yet the efficient models can't be used.

### Your contribution

We could implement the support for destilBERT, ALBERT or ELECTRA. They shouldn't be that different from other encoders.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Support setting multiple adapters,"### Feature request

The underlying peft library supports setting multiple adapters:

```python
model.set_adapters([""adapter_a"", ""adapter_b""])
```

It would be nice if the pipeline supported the same, from looking at https://github.com/huggingface/transformers/pull/25077 it appears it only supports a single adapter

### Motivation

This is useful functionality in the peft library

### Your contribution

Happy to make the changes here!","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Missing `vocab_file` Attribute When Using Custom SentencePiece Models,"### System Info

- `transformers` version: 4.36.2
- Platform: Linux-6.1.69-1-lts-x86_64-with-glibc2.38
- Python version: 3.11.6
- Huggingface_hub version: 0.19.4
- Safetensors version: 0.4.0
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.1.2+cpu (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: NA
- Using distributed or parallel set-up in script?: NA

### Who can help?

@ArthurZucker 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Steps to Reproduce

1. Import the SentencePiece library:

   ```python
   import sentencepiece as spm
   ```

2. Verify the SentencePiece version:

   ```python
   spm.__version__  # Ensure it is 0.1.98 or later
   ```

3. Attempt to create a `LlamaConverter` instance with a loaded SentencePiece tokenizer:

   ```python
   from transformers.convert_slow_tokenizer import LlamaConverter

   # Define the path to your SentencePiece model
   spm_model_path = ""path/to/your/tokenizer.model""

   # Load the SentencePiece tokenizer
   spm_tokenizer = spm.SentencePieceProcessor()
   spm_tokenizer.Load(spm_model_path)

   # Create a LlamaConverter instance with the loaded tokenizer
   llama_converter = LlamaConverter(spm_tokenizer)
   ```

4. Observe the error raised due to the missing `vocab_file` attribute.

### Expected Behavior

To address this issue and make it easier for users to work with custom SentencePiece models, the following modifications are proposed:

1. **Modify `Converter` Class**:
   - Update the `Converter` class to accept a file path to the SentencePiece model when initializing an instance of the class.
   - Instantiate the SentencePiece tokenizer and load the model from the provided file path within the `Converter` class.

   ```python
   import sentencepiece as spm

   class Converter:
       def __init__(self, file_path_to_tokenizer):
           self.file_path = file_path_to_tokenizer
           self.original_tokenizer = spm.SentencePieceProcessor()
           self.original_tokenizer.Load(self.file_path)

       def converted(self) -> Tokenizer:
           raise NotImplementedError()
   ```

2. **Update `SpmConverter` Class**:
   - Update the `SpmConverter` class to use the `file_path` attribute instead of the non-existent `model_file` attribute when loading the SentencePiece model.
   - Ensure compatibility with the modifications made in the `Converter` class.

   ```python
   from transformers.convert_slow_tokenizer import Converter

   class SpmConverter(Converter):
       def __init__(self, file_path_to_tokenizer):
           requires_backends(self, ""protobuf"")
           super().__init__(file_path_to_tokenizer)

           # Load the SentencePiece model using the file path
           with open(self.file_path, ""rb"") as f:
               m.ParseFromString(f.read())
   ```

### Expected Outcome

With these proposed modifications, users should be able to create `Converter` instances with their custom SentencePiece models by providing the file path, and the `SpmConverter` class should correctly load the model using the specified file path, resolving the issue of missing attributes and ensuring compatibility with Transformers.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",6,open
[Flax] Migration from frozen to regular dicts with v0.7.1+,"### Feature request

As of version 0.7.1, Flax defaults to returning **regular dictionaries** with the methods `.init` and `.apply`, not **frozen dictionaries** as was the case before: https://github.com/google/flax/discussions/3191

The `.init` method is called in the Transformers method `model.init_weights`, where we randomly initialised the model's parameters:
https://github.com/huggingface/transformers/blob/4ab5fb8941a38d172b3883c152c34ae2a0b83a68/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L370

Therefore, this Flax update is a breaking change for Transformers: previously, calling `model.init_weights` returned a frozen dict of params, whereas now it returns a regular dict. However, blindly reverting to using frozen dicts might cause issues for Flax users, since they will get regular dicts of params from Flax, but get frozen ones from Transformers.

This leaves us with two options:
1. Update the `model.init_weights` method to always return a frozen dict, even in the `module.init` returns a standard dict. This mitigates the breaking change and reverts to the behaviour we had before
2. Follow the Flax behaviour and return regular dicts of params with v0.7.1+. This would keep Transformers in-line with the latest Flax philosophy, at the expense of a breaking change

 A PR to implement 1 is in #28367: it is a single line change for each of the Flax modelling files. To implement 2, we would need to check if the `random_params` return by the `module.init` method are frozen or not, and match the dictionary type on the returned outputs.

Note that the change in behaviour will only really affect users who are initialising parameters themselves (with `_do_init=False`). These are typically advanced users who are familiar with the Flax library, and want an easy way of dropping-in Transformers Flax modules into other Flax scripts. Therefore, I would be in favour of 2, in order to maintain equivalence between the Flax and Transformers libraries. For users who rely on automatic init (`_do_init=True`), there's unlikely to be any friction, since they tend not to access the model params anyway.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
[generation] Exact Search Decoding,"### Feature request

Hello Hugging Face Transformers Team,

I am writing to suggest a feature of an ""exact search"" decoding method, suggested in https://aclanthology.org/D19-1331/ 

Greedy search and beam search are both ""greedy"" in the sense that they are not guaranteed to find the global most likely generation. 

An exact search is a DFS-based search method with branch pruning that can guarantee to return the global optimal. 

The original implementation is located here: [DFS.py in SGNMT](https://github.com/ucam-smt/sgnmt/blob/master/cam/sgnmt/decoding/dfs.py).
Just need to adapt it to transformers generation module.



### Motivation

It has a strong research value because it returns the global optimal.
However, it may not be very practical for general users because it may be very slow.


### Your contribution

I could take the job to submit a PR if this is interesting for you. 
Otherwise, I can work on it as a fork under my account.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Add flash attention 2.0 support for GPT2LMHeadModel,"```
    model = AutoModelForCausalLM.from_pretrained(
        my_GPT2LMHeadModel_checkpoint, 
        torch_dtype=torch.bfloat16, 
        attn_implementation=""flash_attention_2"",
    )
```
throws the following error:
```
Error loading Flash_Model_2: GPT2LMHeadModel does not support Flash Attention 2.0 yet. Please open an issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new
```","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Allow gradient for generate(),"### Feature request

The generate function is decorated with @torch.no_grad() and thus can't be used for model training. It would be better to make calculating gradients optional, rather than impossible, so that the function can be used for tuning. The simplest solution is to remove the decorator altogether, as users can set no_grad themselves before calling if they need to. Are there reasons to disable such usage?

### Motivation

Allow using generate for tuning

### Your contribution

Removing the decorator is a very simple change. I can submit a PR","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
LLaMA-MoE,"### Model description

LLaMA-MoE is a series of token-choice based Mixture-of-Experts models on LLaMA2. It first partition LLaMA2's FFNs into multiple experts, then apply continual pre-training to recover its language abilities.

We believe LLaMA-MoE is a good start for MoE research under limited computing resources.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Our repo: https://github.com/pjlab-sys4nlp/llama-moe
HF models (currently set `trust_remote_code=True`): https://huggingface.co/llama-moe","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Enable the use of batch api using multiple speakers with Bark,"# What does this PR do?

This PR aims to enable the use of the batch API when using multiple speaker prompts for Bark.

Currently, the batch API is available when using multiple text inputs but is limited to a single speaker. The objective of this PR is to enable this when multiple speaker prompts are being used. This is done keeping in mind that we want to minimise the amount of loops when processing the data while keeping the same audio quality.

cc @ylacombe  and @Selectorrr , could you take a look ? Thanks!

Fixes #26921

PS: Currently I have doubts on how to solve the issue in the `BarkCoarseModel` especially these 2 lines:

```
input_coarse = semantic_output[:, np.max([0, semantic_idx - max_semantic_history]) :]
input_coarse = input_coarse[:, :max_coarse_input_length]
```

My concern is that with the approach implemented in this PR, if we need to pad after the `preprocess_histories` method because the samples in the batch for `x_coarse` are not the same length then we might run into a scenario where some padding tokens would be used in `input_coarse` instead of ""regular tokens"" if the sample was processed outside of a batch.","[{'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",5,open
[i18n-en] Translating docs to Japanese,"Hi!

Let's bring the documentation to all the Japanese-speaking community 🌐 

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `ja` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `ja/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).



## Model doc section
- [ ] [deit.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deit.md) #28302
- [ ] [deplot.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deplot.md) #28302
- [ ] [deta.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deta.md) #28302
- [ ] [detr.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/detr.md) #28302
- [ ] [dialogpt.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dialogpt.md)
- [ ] [dinat.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dinat.md)
- [ ] [dinov2.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dinov2.md)#29272
- [ ] [distilbert.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/distilbert.md)

Keep on adding more as you go 🔥

","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
DPT normalization causes contouring when there are significant disparities in depth values between adjacent areas,"### System Info

Python 3.10.12
transformers-4.36.2

### Who can help?

@stevhliu @NielsRogge

### Information

- [X] The official example scripts
- [x] My own modified scripts

### Tasks

- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

```
from transformers import DPTImageProcessor, DPTForDepthEstimation
import torch
import numpy as np
from PIL import Image
import requests

url = ""https://images.unsplash.com/photo-1605146768851-eda79da39897?q=80&w=2970&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D""
image = Image.open(requests.get(url, stream=True).raw)

processor = DPTImageProcessor.from_pretrained(""Intel/dpt-large"")
model = DPTForDepthEstimation.from_pretrained(""Intel/dpt-large"")

# prepare image for the model
inputs = processor(images=image, return_tensors=""pt"")

with torch.no_grad():
    outputs = model(**inputs)
    predicted_depth = outputs.predicted_depth

# interpolate to original size
prediction = torch.nn.functional.interpolate(
    predicted_depth.unsqueeze(1),
    size=image.size[::-1],
    mode=""bicubic"",
    align_corners=False,
)

# visualize the prediction
output = prediction.squeeze().cpu().numpy()
formatted = (output * 255 / np.max(output)).astype(""uint8"")
depth = Image.fromarray(formatted)
display(depth)
```

>
Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

![edge_effects_depth (1)](https://github.com/huggingface/transformers/assets/90732384/054a7e81-9611-4418-9706-29ada47c64a1)

### Expected behavior

Anecdotally, the local scaling methodology used by get_depth_map at https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0 seems to work better for models that perform better at identifying close-range depth. The global scaling methodology seems to work better for models that perform better at identifying far-range depth. I combined them below:

```
def get_depth_map(image, feature_extractor, depth_estimator, scale_local):
    inputs = feature_extractor(images=image, return_tensors=""pt"").pixel_values.to(""cuda"")
    with torch.no_grad(), torch.autocast(""cuda""):
        depth_map = depth_estimator(inputs).predicted_depth

    depth_map = torch.nn.functional.interpolate(
        depth_map.unsqueeze(1),
        size=image.size[::-1],
        mode=""bicubic"",
        align_corners=False,
    )

    if scale_local:
        depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)
        depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)
        depth_map = (depth_map - depth_min) / (depth_max - depth_min)
        image = torch.cat([depth_map] * 3, dim=1)
    
        image = image.permute(0, 2, 3, 1).cpu().numpy()[0]
        image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))
        return image
    
    output = depth_map.squeeze().cpu().numpy()
    formatted = (output * 255 / np.max(output)).astype(""uint8"")
    return Image.fromarray(formatted)


depth_estimator_hybrid = DPTForDepthEstimation.from_pretrained(""Intel/dpt-hybrid-midas"").to(""cuda"")
depth_estimator_dinov2_nyu = DPTForDepthEstimation.from_pretrained(""facebook/dpt-dinov2-giant-nyu"").to(""cuda"")
    
image_processor_hybrid = AutoImageProcessor.from_pretrained(""Intel/dpt-hybrid-midas"")
image_processor_dinov2_nyu = AutoImageProcessor.from_pretrained(""facebook/dpt-dinov2-giant-nyu"")

# Close range depth
bad_close_result = get_depth_map(image, image_processor_hybrid, depth_estimator_hybrid, False)
good_close_result = get_depth_map(image, image_processor_hybrid, depth_estimator_hybrid, True)

# Far range depth
downscaled_image = image.resize((1024, 1024))  # This image is too big for my GPU to processdpt-dinov2-giant-nyu so I downscaled it
good_far_result = get_depth_map(downscaled_image, image_processor_dinov2_nyu, depth_estimator_dinov2_nyu, False)
bad_far_result = get_depth_map(downscaled_image, image_processor_dinov2_nyu, depth_estimator_dinov2_nyu, True)
```

`display(bad_close_result)`
![globally_scaled_depth_close](https://github.com/huggingface/transformers/assets/90732384/980dcd22-7e63-4842-b330-637025a9ce8d)

`display(good_close_result)`
![locally_scaled_depth_close](https://github.com/huggingface/transformers/assets/90732384/970081fc-418d-459e-9576-84316ef6362e)

`display(good_far_result)`
![globally_scaled_depth_far](https://github.com/huggingface/transformers/assets/90732384/9a265c67-51da-47a1-bd3c-4a989480eb97)
`display(bad_far_result)`
![locally_scaled_depth_far](https://github.com/huggingface/transformers/assets/90732384/8aa280aa-8166-45be-89a0-0c1463337a90)

Sufficiently blurring the image prior to detecting depth also gets rid of this, ie:

```
blurred_image = image.filter(ImageFilter.GaussianBlur(radius=5))
display(get_depth_map(blurred_image, image_processor_hybrid, depth_estimator_hybrid, False))
```

![blurred_depth](https://github.com/huggingface/transformers/assets/90732384/0f447c9b-0ad3-44f0-9c0e-4813be913899)","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",10,open
Add Mixture of Tokens model,"### Model description

Mixture of Tokens is a new architecture / technique proposed in [Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation](https://arxiv.org/abs/2310.15961) and accompanying [blog](https://llm-random.github.io/posts/mixture_of_tokens/) by Szymon Antoniak, Sebastian Jaszczur et al.

It builds on expert-choice MoE, aggregating across sequences in a batch rather than positions in a sequence, and doing so in a continuous fashion. This full differentiability is its main advantage, bringing training stability and even expert utilization.

In collaboration with the authors, we (me + 3 others) would like to add a PyTorch implementation matching the architecture from the paper to HF transformers and later publish corresponding checkpoints. We believe this will make it significantly easier for the community to experiment with this approach, as the original implementation is quite dense and contained in an active research repo.

We believe a good approach is to start from the GPT2 HF model. We will have the assistance of the original authors for making sure the details match.

Please advise:
1.  If you have any general suggestions at this stage
2. What kinds of tests you would like to see in the finalized implementation for this case, where the exact snapshot corresponding to the paper's implementation and the checkpoints were not  previously published.
3.  If you have general suggestions regarding contributing methods that are potentially applicable to multiple base models  (like MoE and MoT).

As we understand, the next step is for us to create a template with https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model and get coding.



### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

https://github.com/llm-random/llm-random

https://github.com/sebastianjaszczur

https://llm-random.github.io/posts/mixture_of_tokens/

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
[`MobileSam`] Adds MobileSAM to transformers,"# What does this PR do?

as discussed offline cc @merveenoyan @NielsRogge 
This PR adds MobileSam to the library. MobileSam uses the same archtiecture as SAM, with the SAM image encoder being swapped to TinyViT. Therefore I decided to create a new modeling file for it, as porting TinyViT required a bit of work

Draft for now!","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
How to add new merge rules in AutoTokenizer,"### Model description

I'm training new tokenizer from llama2, however, it seems that BPE tokenizer will clear the origin ""vocab"" and ""merge"" dict, and the training result is highly bias in my own datasets (about 6M C function) with some ugly tokens.

I wonder that is it possible to train a tokenizer from llama2 with the origin ""vocab"" and ""merge"" dict unchanged, only add some new vocab and merge rules from our datasets to support my requirement?


### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Fix model code to accurately convert fairseq wav2vec2 model,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #28174 

## Reasons and benefits:
- fairseq model weight can be converted correctly.

## Current state:
- fairseq uses nn.Linear only when the dimension of convolution subsampling and the dimension of the encoder block are different. So, if it is the same, nn.Linear is not used.
- But the huggingface implementation unconditionally uses nn.Linear, so when converting, unused weight doesn't appears, but in reality, a random weight nn.Linear is added.

- fairseq uses the layer_norm position dynamically using the layer_norm_first argument.
- However, the implementation of huggingface is different from fairseq because the layer norm position is fixed. Fixed so that this can be controlled as an option.

## Related code

**No. 1**
 **fairseq**
- [facebookresearch/fairseq@main/fairseq/models/wav2vec/wav2vec2.py#L324-L328](https://github.com/facebookresearch/fairseq/blob/main/fairseq/models/wav2vec/wav2vec2.py?rgh-link-date=2023-12-21T02%3A52%3A07Z#L324-L328)

**huggingface**
- [main/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L536](https://github.com/huggingface/transformers/blob/main/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py?rgh-link-date=2023-12-21T02%3A52%3A07Z#L536)

**No. 2**
 **fairseq**
- https://github.com/facebookresearch/fairseq/blob/main/fairseq/models/wav2vec/wav2vec2.py#L1230-L1231

**huggingface**
- https://github.com/huggingface/transformers/blob/main/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L929


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}, {'id': 6886428489, 'node_id': 'LA_kwDOCUB6oc8AAAABmnaPSQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/run-slow', 'name': 'run-slow', 'color': 'E1D519', 'default': False, 'description': ''}]",29,open
Finetuning Whisper with adapter with MAML,"### Feature request

MAML is a widely used meta-learning method for reinitializing model parameters, which can effectively cope with low-resource situations.
As Whisper is a pre-trained model, its parameters cannot be reinitialized, but the bottleneck structure adapter can be applied to the encoder and decoder layers of the model, and then the adapter can be trained using MAML.
Request code for fine-tuning Whisper with adapter using MAML such as meta-training with 6 languages and final fine-tuning with 4 other languages.

### Motivation

low-resource ASR.

### Your contribution

Anything you need and I can.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",2,open
Add CAV-MAE audio-image encoder model,"### Model description

Contrastive Audio-Visual Masked Autoencoder (CAV-MAE) combines two major self-supervised learning frameworks: contrastive learning and masked data modeling, to learn a joint and coordinated audio-visual representation. It appears to be the open source SOTA on the AudioSet and VGGSound datasets (the OmniVec and Facebook MAViL models seem to have never had weights released). 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://github.com/YuanGongND/cav-mae

@YuanGongND","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Tokenizer adds an additional space after the added token,"### System Info

- `transformers` version: 4.35.2
- Platform: Linux-6.1.58+-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.19.4
- Safetensors version: 0.4.1
- Accelerate version: 0.25.0
- Accelerate config: 	not found
- PyTorch version (GPU?): 2.1.0+cu121 (False)
- Tensorflow version (GPU?): 2.15.0 (False)
- Flax version (CPU?/GPU?/TPU?): 0.7.5 (cpu)
- Jax version: 0.4.23
- JaxLib version: 0.4.23
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help?

@ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction
```python
from transformers import AutoTokenizer

checkpoint = ""facebook/nllb-200-distilled-600M""
tokenizer = AutoTokenizer.from_pretrained(checkpoint, src_lang = ""eng_Latn"", tgt_lang = ""zho_Hans"")
tokenizer.add_tokens([""abcd""])

sent = 'I like to walk abcdgym along the beach'
print(""tokenizer: "", tokenizer.tokenize(sent))
print(""tokenizer: "", tokenizer.decode(tokenizer.encode(sent)[1:-1]))

sent = 'I like to walk gymabcd along the beach'
print(""tokenizer: "", tokenizer.tokenize(sent))
print(""tokenizer: "", tokenizer.decode(tokenizer.encode(sent)[1:-1]))
```
### Expected behavior

The output from my code:
![image](https://github.com/huggingface/transformers/assets/71968397/5ba945f5-eb79-4c7d-b82f-8b74c2db0321)

The original post where I raised this potential bug and was asked to file an issue would be at: https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564/5

For context, I am originally trying to add Chinese tokens to the tokenizer. However, for illustration purposes, I have demonstrated the “bug” in English. Chinese words are not separated by spaces and hence in the example you will see me trying to add a token that is a subword.

Evidently, tokenizer.add_tokens() works well if there will always be space after the added token but it doesn’t work as intended if there isn’t space after the added token (where the tokenizer will then introduce the additional space on its own).

I read the [docs](https://huggingface.co/docs/transformers/v4.36.1/en/internal/tokenization_utils#transformers.SpecialTokensMixin.add_tokens) and figured out it is probably because the added tokens are isolated before the tokenization algorithm is applied, hence I am not 100% sure this behaviour by the tokenizer is intended.","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",6,open
causal_mask in GPT2Attention should not be broadcastable across the seq_len,"### System Info

Python             : 3.8.2
torch                : 2.2.0.dev20231207+cu121
transformers    : 4.31.0
torchvision       : 0.17.0.dev20231207+cu121
cuda version    : 12.1

In `transformers.models.gpt2.modeling_gpt2.GPT2Attention` 
https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L202

the `causal_mask` must have the same shape for the last 2 dims, otherwise if the `max_position_embeddings=1` while the sequence length is longer than 1, the resulted attention weights leads to attending the future tokens. See the steps to reproduce the behavior for details. Normally, one wouldn't set `max_position_embeddings=1`, but nevertheless the broadcasting should not happen.

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Code to reproduce the issue:

```
import torch
import transformers
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable

bsz, seq_len, hid = 2, 3, 4
fig, axes = plt.subplots(ncols=3, figsize=(9,2))
for n_positions, ax in zip([1, 2, seq_len], axes):
    attn = transformers.models.gpt2.modeling_gpt2.GPT2Attention(transformers.GPT2Config(n_embd=hid, 
                                                                                        n_layer=1,
                                                                                        n_head=1,
                                                                                        n_positions=n_positions))
    ax.axis(False)
    ax.set_title('attn_weights, n_positions=%d' % n_positions, fontsize=9)
    attn_input = torch.randn(bsz, seq_len, hid)
    try:
        attn_output, _, attn_weights = attn(attn_input, output_attentions=True)
    except Exception as e:
        print('n_positions=%d' % n_positions, attn_input.shape, 'attn_output', 'ERROR:', e)
        continue
    print('n_positions=%d' % n_positions, 'attn_input', attn_input.shape, 'attn_output', attn_output.shape, 'attn_weights', attn_weights.shape)
    
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    im = ax.imshow(attn_weights[0, 0].data.cpu().numpy())
    fig.colorbar(im, cax=cax, orientation='vertical')
plt.show()    
```

Output:

```
n_positions=1 attn_input torch.Size([2, 3, 4]) attn_output torch.Size([2, 3, 4]) attn_weights torch.Size([2, 1, 3, 3])
n_positions=2 torch.Size([2, 3, 4]) attn_output ERROR: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 3
n_positions=3 attn_input torch.Size([2, 3, 4]) attn_output torch.Size([2, 3, 4]) attn_weights torch.Size([2, 1, 3, 3])
```

![image](https://github.com/huggingface/transformers/assets/3225366/c644271f-b3c7-44ce-b236-cc857597708e)


### Expected behavior

There should be some error message, for example triggered by 

`assert attn_weights.shape[-2:] == causal_mask.shape[-2:], 'attn_weights and causal_mask must have the same seq length'`","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Verify interpolation of image processors,"### Feature request

As pointed out in https://github.com/huggingface/transformers/pull/27742, some image processors might need a correction on the default interpolation method being used (resampling in Pillow). We could check this on a per-model basis.

### Motivation

Interpolation methods have a slight (often minimal) impact on performance. However it would be great to verify this on a per-model basis.

e.g. [ViT](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/image_processing_vit.py#L52)'s image processor defaults to BILINEAR but should use BICUBIC as seen [here](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L1062). We can update the default values of the image processors, but can't update the configs on the hub as this would break people's fine-tuned models.

### Your contribution

I could work on this, but this seems like a good first issue for first contributors.

To be checked (by comparing against original implementation):

- [ ] beit
- [ ] bit
- [ ] clip
- [ ] convnext
- [ ] convnextv2
- [ ] cvt
- [ ] data2vec-vision
- [ ] deit
- [ ] dinat
- [ ] dinov2
- [ ] efficientformer
- [ ] efficientnet
- [ ] focalnet
- [ ] imagegpt
- [ ] levit
- [ ] mobilenet_v1
- [ ] mobilenet_v2
- [ ] mobilevit
- [ ] mobilevitv2
- [ ] nat
- [ ] perceiver
- [ ] poolformer
- [ ] pvt
- [ ] regnet
- [ ] resnet
- [ ] segformer
- [ ] siglip
- [ ] swiftformer
- [ ] swin
- [ ] swinv2
- [ ] van
- [ ] vit
- [ ] vit_hybrid
- [ ] vit_msn","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",12,open
[Flash Attention 2] Performance improvement,"### Feature request

The current flash attention 2 integration is sub-optimal in performance because it requires unpadding and padding the activations on **each** layer. For example in llama implementation:

https://github.com/huggingface/transformers/blob/769a9542de4e8b23f0a551738e18760621f463e8/src/transformers/models/llama/modeling_llama.py#L591-L612

These small kernels for unpad/pad keep gpu waiting for cpu, as shown in the visible gaps between kernels in cuda stream.

![image](https://github.com/huggingface/transformers/assets/39846316/f8bfa837-3ddd-447f-a6dd-de4883db63e6)

I'll suggest unpadding the activations at the very beginning (right after word embeddings) and padding it back at the end (maybe before lm_head), and the gap should disappear.


### Motivation

To eliminate performance overhead of flash attention 2.

### Your contribution

I can write the code when I'm not busy. Maybe not now.","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}, {'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}]",3,open
Add StyleTTS 2 to HF Transformers Pipeline,"### Feature request

Add [StyleTTS](https://github.com/yl4579/StyleTTS2) 2 to HF Transformers Pipeline

### Motivation

Would be great to have an easier way to run STTS2

### Your contribution

I created a [fork](https://github.com/neuralvox/styletts2) with importable scripts","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
OWL-VIT Vision Foundation Model deployment in the edge cases - Need SDPA support for OWL-ViT Model optimization for Edge Deployment,"### Feature request

Hi Team,
I am working with  OWL-ViT Size model which has around 611 MB size ( https://huggingface.co/google/owlvit-base-patch16).
I want to optimize this model and like to deploy in the edge device for object detection.

Come to know from the group torch.scaled_dot_product_attention can be used for model optimization. 

I need your feedback comments how optimally we can reduce the memory size so that we can deploy in the edge device.

waiting for your response.

with thanks

### Motivation

It will help to deploy the models in edge so that more applications we can use it.

### Your contribution

Like to know your feedback comments.","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",4,open
Expose `gradient_as_bucket_view` as training argument for `DDP`,"### Feature request

As the title says, add `gradient_as_bucket_view` as the training argument (default False)

### Motivation

I have been experimenting with qlora fine-tuning LLMs on multiple A10 GPUs and I am leveraging DDP. I was going through the torch docs and https://pytorch.org/docs/2.1/generated/torch.nn.parallel.DistributedDataParallel.html and it seems the `gradient_as_bucket_view` argument can save a little bit of memory. It would be great to have it added as accelerate's DDP plugin already supports it.

I am already experimenting with it to test it out
```python
class HFTrainer(Trainer):
    def _wrap_model(self, model, training=True, dataloader=None):
        outputs = super()._wrap_model(model, training, dataloader)
        if self.args.parallel_mode == ParallelMode.DISTRIBUTED and self.accelerator.ddp_handler:
            self.accelerator.ddp_handler.gradient_as_bucket_view = True

        return outputs
```



### Your contribution

Let me know, I can also work on a PR for this as the change is relatively small","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Can i convert open-clip trained models (.pt) using code “src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py” ？,"### Model description

openclip：https://github.com/mlfoundations/open_clip

i use openclip  to train model and get ""epoch_400.pt"". 
**and i want to convert this ""epoch_400.pt"" to hf, so i run:**
python src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py     --pytorch_dump_folder_path ""./openclip_syf_hf""     --checkpoint_path ""/openclip_output/2023_12_07-15_24_24-model_ViT-B-32-lr_0.0005-b_256-j_8-p_amp/checkpoints/epoch_400.pt""     --config_path ""/open_clip-main/src/open_clip/model_configs/ViT-B-32.json""

**but get bug:**

Traceback (most recent call last):
  File ""/home/anaconda3/envs/transformer/lib/python3.8/site-packages/clip/clip.py"", line 130, in load
    model = torch.jit.load(opened_file, map_location=device if jit else ""cpu"").eval()
  File ""/home/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/jit/_serialization.py"", line 164, in load
    cpp_module = torch._C.import_ir_module_from_buffer(
RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py"", line 150, in <module>
    convert_clip_checkpoint(args.checkpoint_path, args.pytorch_dump_folder_path, args.config_path)
  File ""/home/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py"", line 120, in convert_clip_checkpoint
    pt_model, _ = load(checkpoint_path, device=""cpu"", jit=False)
  File ""/home/anaconda3/envs/transformer/lib/python3.8/site-packages/clip/clip.py"", line 137, in load
    state_dict = torch.load(opened_file, map_location=""cpu"")
  File ""/home/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/serialization.py"", line 795, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/serialization.py"", line 1002, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
EOFError: Ran out of input

**so i am wondering if i can convert open-clip trained models (.pt) using code “src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py” ？**

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Add time progress bar to track the group_by_length computation for bigger datasets on Trainer,"### Feature request

When setting the flag `group_by_length=True` on the TrainingArguments, there is no user feedback of the operations running in the background, namely getting the list of lengths for all the samples and running the grouping algorithm. This can be a frustrating problem when dealing with large datasets (Millions of samples) on slow IO devices, since it appears that the Trainer is hanging and does not start!

More precisely, In my current setup, I found out that the following lines take almost 2h to finish. (Due to my slow IO (reading from a NFS from an old machine))

https://github.com/huggingface/transformers/blob/c817c17dbe264329b9f9d227b48ce70edd9e3204/src/transformers/trainer_pt_utils.py#L585

NOTE 1): using `.select_columns(model_input_name)` and then iterating would not be faster? Assuming that the dataset has more feature like ""attention_mask"" for instance. 

I believe that more feedback could possibly be given to the user, like the time that would take to finish. (Also store the dataset length under .cache).

NOTE 2): After realising this issue, I also noticed the `length_column_name` flag. Maybe raising a warning to let the users know that on larger datasets they should precompute the length. By doing so, the time went from 2h to (15-20)min.

### Motivation

I was training a model on a LM task. My dataset has 22M samples with average length of +/- 512. When I run the model with `group_by_length=True` I thought that something was wrong because the training was not starting (I was actually writing an bug about my problem, because I thought it was an issue with the Trainer). After further inspection, I notice that the main culprit was the computation of the length that is really slow on my current setup. 

### Your contribution

If you feel like this is an issue that is worth to address, I am willing to do PR under your orientation.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Open to contribution: adding `torch.nn.functional.scaled_dot_product_attention` support for more architectures,"### Feature request

In [`Transformers 4.36`](https://github.com/huggingface/transformers/releases/tag/v4.36.0), we started adding native support of [torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA), enabled by default in Transformers: https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention

SDPA allows to dispatch to memory-efficient attention, flash attention on supported GPUs (currently NVIDIA-only), and even on [Intel CPUs](https://pytorch.org/blog/new-features-for-ai/#flash-attention-based-scaled-dot-product-algorithm-for-cpu).

For the record, here's a benchmark on some currently supported models:

**[Training benchmark](https://gist.github.com/fxmarty/7e75cc3942d6974e4849093ebea0a331), run on A100-SXM4-80GB.**

| Model     | Batch size | Sequence length | Time per batch (`""eager""`, s) | Time per batch (`""sdpa""`, s) | **Speedup** | Peak memory (`""eager""`, MB) | Peak memory (`""sdpa""`, MB) | **Memory savings**    |
|-----------|------------|-----------------|-------------------------------|------------------------------|-------------|-----------------------------|----------------------------|-----------------------|
| llama2 7b | 4          | 1024            | 1.065                         | 0.90                         | **19.4%**   | 73878.28                    | 45977.81                   | **60.7%**             |
| llama2 7b | 4          | 2048            | OOM                           | 1.87                         | /           | OOM                         | 78394.58                   | **SDPA does not OOM** |
| llama2 7b | 1          | 2048            | 0.64                          | 0.48                         | **32.0%**   | 55557.01                    | 29795.63                   | **86.4%**             |
| llama2 7b | 1          | 3072            | OOM                           | 0.75                         | /           | OOM                         | 37916.08                   | **SDPA does not OOM** |
| llama2 7b | 1          | 4096            | OOM                           | 1.03                         | /           | OOM                         | 46028.14                   | **SDPA does not OOM** |
| llama2 7b | 2          | 4096            | OOM                           | 2.05                         | /           | OOM                         | 78428.14                   | **SDPA does not OOM** |

**[Inference benchmark](https://gist.github.com/fxmarty/5113e4304fbdd38c9c3702ce44683f6a), run on A100-SXM4-80GB.**

| Model            | Batch size | Prompt length | Num new tokens | Per token latency `""eager""` (ms) | Per token latency `""sdpa""` (ms) | **Speedup** |
|------------------|------------|---------------|----------------|----------------------------------|---------------------------------|-------------|
| llama2 13b       | 1          | 1024          | 1 (prefill)    | 178.66                           | 159.36                          | **12.11%**  |
| llama2 13b       | 1          | 100           | 100            | 40.35                            | 37.62                           | **7.28%**   |
| llama2 13b       | 8          | 100           | 100            | 40.55                            | 38.06                           | **6.53%**   |
| Whisper v3 large | 1          | /             | 62             | 20.05                            | 18.90                           | **6.10%**   |
| Whisper v3 large | 8          | /             | 77             | 25.42                            | 24.77                           | **2.59%**   |
| Whisper v3 large | 16         | /             | 77             | 28.51                            | 26.32                           | **8.34%**   |

Previously, we had a partial support of SDPA in [Optimum BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview) but we are now looking to slowly deprecate it in favor of upstream support of SDPA directly in Transformers.

Here are the architectures for which support has been requested:
- [ ] Codegen (https://github.com/huggingface/optimum/issues/1050)
- [ ] LLAVA (https://github.com/huggingface/optimum/issues/1592)
- [ ] Marian (https://github.com/huggingface/optimum/issues/1142)
- [x] Mistral (https://github.com/huggingface/optimum/issues/1553)
- [ ] LongT5 (https://github.com/huggingface/optimum/issues/1506)
- [ ] ViT (https://github.com/huggingface/optimum/issues/1553)

The integration could take inspiration from https://github.com/huggingface/optimum/blob/main/optimum/bettertransformer/models/decoder_models.py & https://github.com/huggingface/optimum/blob/main/optimum/bettertransformer/models/attention.py

### Motivation

Faster training & inference, lower memory requirement

### Your contribution

I may work on some at some point, but contributions are most welcome.

You should refer to https://github.com/huggingface/transformers/pull/26572 to add the support of SDPA for a model, roughly following these steps:
* Create a `XxxSdpaAttention` class inheriting from `XxxAttention` and implement the attention logic using SDPA
* Use `_prepare_4d_causal_attention_mask_for_sdpa` instead of `_prepare_4d_causal_attention_mask` for SDPA
* Use `_prepare_4d_attention_mask_for_sdpa` instead of `_prepare_4d_attention_mask` for SDPA
* Add `_supports_sdpa = True` to `XxxPreTrainedModel`
* Add `""sdpa""` key to `XXX_ATTENTION_CLASSES` in the model modeling file","[{'id': 6126880899, 'node_id': 'LA_kwDOCUB6oc8AAAABbTDIgw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/contributions-welcome', 'name': 'contributions-welcome', 'color': 'F99E09', 'default': False, 'description': ''}]",42,open
LLaMa-VID: An Image is Worth 2 Tokens in LLMs,"### Model description

LLaMA-VID is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data. LLaMA-VID empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token. We build this repo based on LLaVA.

LLaMA-VID contains three parts: encoder and decoder are adopted to produce visual embedding and text-guided features, respectively; context token and content token are transformed with the tailored token generation strategy; instruction tuning is designed to unleash the potential of LLMs for image and video.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Page: https://llama-vid.github.io/
Weights already available on HF: https://huggingface.co/YanweiLi/llama-vid-7b-pretrain-224
Code: https://github.com/dvlab-research/LLaMA-VID","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",7,open
ImageToTextPipeline does not support InstructBlip Models,"### System Info


- `transformers` version: 4.36.0.dev0
- Platform: Linux-generic-x86_64
- Python version: 3.8.12
- Huggingface_hub version: 0.19.4
- Safetensors version: 0.4.1
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 1.10.0a0+0aef44c (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: yes
- Using distributed or parallel set-up in script?: no

### Who can help?

@Narsil @amyeroberts

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

processor = InstructBlipProcessor.from_pretrained(""Salesforce/instructblip-flan-t5-xl"")
pipe = pipeline(""image-to-text"", model=""Salesforce/instructblip-flan-t5-xl"", processor=processor.image_processor, tokenizer=processor.tokenizer, device=0)
prompt = ""describe te following image""
url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
image = Image.open(requests.get(url, stream=True).raw)

pipe(images=image, prompt=prompt)

### Expected behavior

returns a textual description of the image. 
Instead, I get an error: 
`TypeError: ones_like(): argument 'input' (position 1) must be Tensor, not NoneType`

I suspect this is caused by the `ImageToTextPipeline.preprocess()`, where we should ave custom behaviour for InstructBlip models to process the image and text in one go: `inputs = processor(images=image, text=prompt, return_tensors=""pt"")`
","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
CLIPTokenizer (and others based on the same telephoned OpenAI code) incorrect tokenize 1138 out of 34483 words that have an exact match in vocab,"### System Info

- `transformers` version: 4.36.0
- Platform: Linux-5.15.120+-x86_64-with-glibc2.35
- Python version: 3.10.12
- Huggingface_hub version: 0.19.4
- Safetensors version: 0.4.1
- Accelerate version: not installed
- Accelerate config: not found
- PyTorch version (GPU?): 2.1.0+cu118 (False)
- Tensorflow version (GPU?): 2.14.0 (False)
- Flax version (CPU?/GPU?/TPU?): 0.7.5 (cpu)
- Jax version: 0.4.20
- JaxLib version: 0.4.20
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help?

@ArthurZucker and @younesbelkada 

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Visit https://colab.research.google.com/drive/18I0mYxTV-UCDjKWTxfuaR3P6o00w18Q9?usp=sharing for a reproduction.

```
from transformers import CLIPProcessor
tokenizer = CLIPProcessor.from_pretrained(""openai/clip-vit-base-patch32"").tokenizer

# match whole words
whole_words = {k: v for k, v in tokenizer.get_vocab().items() if k.endswith(""</w>"")}
to_trim = len(""<w/>"")
missed = 0
for token_str, token_int in whole_words.items():
  tokenized = tokenizer.tokenize(token_str[:-to_trim])
  if len(tokenized) != 1:
    missed += 1
print(f""transformers {missed} words out of {len(whole_words)} incorrectly tokenized ({missed/len(whole_words)*100})%"")
```
this prints `transformers 1138 words out of 34483 incorrectly tokenized (3.3001768987617086)%`

I see that everyone copied OpenAI's buggy tokenization code. Besides this issue there is also https://github.com/openai/CLIP/issues/343. The code in that repository was obviously not used for training, so this could explain a lot of misses / poor performance in CLIP based models.

### Expected behavior

tokenization of a word that exactly matches an entry in the vocab file should return exactly 1 token","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",10,open
XLMRoberta with Flash Attention 2,"### System Info

- transformers version: 4.36.0
- Platform: Linux-4.19.0-22-amd64-x86_64-with-glibc2.31
- Python version: 3.10.13
- Huggingface_hub version: 0.19.4
- Safetensors version: 0.4.0
- Accelerate version: 0.24.1
- Accelerate config:   not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@ArthurZucker @younesbelkada

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(""my_model/"", attn_implementation=""flash_attention_2"")

### Expected behavior

Ability to use flash attention 2 for inference. Is it possible to add support of flash attention 2 for XLMRoberta model?","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
dMoE support,"### Feature request

MistralAI recently [released their new model](https://twitter.com/MistralAI/status/1733150512395038967), a Mixture of Experts based on [megablocks](https://github.com/stanford-futuredata/megablocks), a type of dropless Mixture of Experts.

### Motivation

It's very likely that the future of open source LLMs will be MoEs. Having it in HF transformers would allow us to use the built-in trainer, as it's unwieldy to use Megatron-LM for the average user who's only ever done QLoRA.

### Your contribution

No clue for now.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Adding truncation to text-generation pipeline,"### Feature request

Passing along the `truncation` argument from the `text-generation` pipeline to the tokenizer. 

### Motivation

If you're using a `text-generation` with input text from the user it is likely that their input text is too long. This doesn't cause problems for some models, e.g. `t5` based, but for other models, e.g. `BERT` based it raises and Index Error.   
The workaround is to use the tokenizer and model manually and omit the pipeline. #25994 solves this issues for `fill-mask` pipelines.  

### Your contribution

#27683. I added `tokenizer_kwargs` to the text-generation pipeline's `__call__()`. This is similar to #26234 that did this for `fill-mask` pipelines. I personally think it's better to add `truncation` and `max-length` as a top level arg to the pipeline's constructor and pass it to the tokenizer when it's time.  
I think a refactoring of text based pipelines wouldn't hurt to unify tokenizer calls so you don't have to code this feature for each pipeline separately. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Make `output_dir` optional in TrainingArguments,"### Feature request

Currently, there is only 1 required param in creating a TrainingArguments object - `output_dir`. HFTrainer manually creates an object with a default value ""tmp_trainer"" if no Args object is passed to it.

Instead, we should make even this one param optional in the TrainingArguments class (and use a default inside the class implementation).

### Motivation

This is useful when creating and passing TrainingArguments in other runners - for eg, trl/SFTTrainer. I would like sensible defaults for all params, so that I only specify the particular arguments I am interested in.

### Your contribution

I can open a PR, if this is of interest.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
torch CUDA graphs with HF generate,"### Feature request

In my experiments, I cannot get torch CUDA graphs to work with HF generate. CUDA graphs work fine when calling the forward pass of a model, but either due to static input/output sizes or something else, stream capture fails when calling .generate(). Can support for torch CUDA graphs be added?

### Motivation

LLMs have a lot of kernel launches and CUDA graphs can remove most of the launch time. In my experiments with just forward call, CUDA graphs can be twice as fast as non-CUDA graph versions of the same model.

### Your contribution

n/a","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",11,open
Add LayoutLMProcessor,"### Feature request

Add processor for LayoutLM. I'm not sure why v2 and v3 have their respective processors, but the original v1 doesn't. It should be almost identical their v2 and v3 counterparts (apply tesseract OCR + call the tokenizer appropriately), without returning the resized image (`pixel_values`), since LayoutLMv1 is text-only.

This would also simplify `document-question-answering` pipeline, since right now the pipeline repeats the above logic for LayoutLM.

### Motivation

Make LayoutLM feature-parity with its v2 and v3.

### Your contribution

I can submit a PR to add LayoutLMProcessor. It should be almost identical to v2 and v3, so the task should be straight-forward.

Updating `document-question-answering` pipeline to use the new processor would be too complex since I'm not familiar with the codebase.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Could we Add Linear Projection Layer in pre-trained model?,"### Model description

As you know, if our model is too high positional_embedding size, then input vectors become too sparse data, so it leads to a performance decrease. 


However, I have an idea to overcome it. 

Before positional embedding, we add a new trainable linear projection layer to reduce its dimension. 

For example, If our model’s original positional embedding size is 4096, then we enlarge our input size as 8192.

Then trainable linear projection layer projects 8192 to 4096 and then we perform positional embedding.


let LP is a trainable Linear Projection Layer, PE(X) is positional Embedding.

Then our calculation formula is as follows.

""Original Structure""

X(R^4096)-> PE(X, where X is in R^4096)-> model(PE(X))


""Proposed Structure""

X(R^8192)-> LP(X, where X^8192) -> X'(R^4096) -> PE(X') -> model(PE(X'))


Can we run this with the current library? If not, can you add?

### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
deepspeed autotuning intergration,"### Feature request

intergrate deepspeed autotuning

### Motivation

without setting any config, make deepspeed easier to use

### Your contribution

none","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
[WIP] Uniformize processors in text+image multimodal models.,"# What does this PR do?

This PR is a work in progress aiming at uniformizing all text-image multimodal processors. Ideally, leveraging `AutoProcessor(...)` or an equivalent for every model would be the best.
 
The processor is one of the most fundamental blocks of transformers, and modifying it can only be done with careful deprecation cycles. It is however the opportunity to _enforce a standard_, design-wise, for future processing utilties and down-the-line pipeline integrations.

For instance align has a current  `__call__` method ` def __call__(self, text=None, images=None, padding=""max_length"", max_length=64, return_tensors=None, **kwargs)`
altclip has      `__call__(self, text=None, images=None, return_tensors=None, **kwargs)`
blip has
```python
    def __call__(
        self,
        images: ImageInput = None,
        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = None,
        max_length: Optional[int] = None,
        stride: int = 0,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_token_type_ids: bool = False,
        return_length: bool = False,
        verbose: bool = True,
        return_tensors: Optional[Union[str, TensorType]] = None,
        **kwargs,
    ) -> BatchEncoding:
```
And so on, with recently for instance Kosmos-2
```python
    def __call__(
        self,
        images: ImageInput = None,
        text: Union[TextInput, List[TextInput]] = None,
        bboxes: BboxInput = None,
        num_image_tokens: Optional[int] = 64,
        first_image_token_id: Optional[int] = None,
        add_special_tokens: bool = True,
        add_eos_token: bool = False,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = None,
        max_length: Optional[int] = None,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
        return_length: bool = False,
        verbose: bool = True,
        return_tensors: Optional[Union[str, TensorType]] = None,
        **kwargs,
    ) -> BatchFeature:
```

Currently, there are 30 text + image models that have a dedicated `processing_<model>` file. All should be reviewed and made pipeline-compatible. All of them have to be checked, modified or wrapped with a common class.

- [ ] align
- [ ] altclip
- [ ] blip
- [ ] blip_2
- [ ] bridgetower
- [ ] chinese_clip
- [ ] clipseg
- [ ] clip
- [ ] donut
- [ ] flava
- [ ] fuyu
- [ ] git
- [ ] idefics
- [ ] instructblip
- [ ] kosmos2
- [ ] layoutlmv2
- [ ] layoutlmv3
- [ ] layoutxlm
- [ ] mgp_str
- [ ] nougat
- [ ] oneformer
- [ ] owlv2
- [ ] owlvit
- [ ] perceiver
- [ ] pix2struct
- [ ] troc
- [ ] tvp
- [ ] vilt
- [ ] vision_text_dual_encoder
- [ ] x_clip

Related works:
- See the insightful discussion in this PR https://github.com/huggingface/transformers/pull/26885 about _invariants_ and their importance.
- @NielsRogge has started working on adding new processor tests in a separate PR as well. https://github.com/huggingface/transformers/pull/27720. Please follow both as tests will enforce signatures.


## Before submitting

- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Add the CRATE (Coding RATE) backbone model,"# What does this PR do?

Implements the new model CRATE introduced in the paper [White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?](https://arxiv.org/abs/2311.13110).

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->




## Before submitting
- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
@ArthurZucker and @younesbelkada","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
TECO - Temporally Consistent Transformers for Video Generation,"### Model description

TECO is a vector-quantized latent dynamics video prediction model that learns compressed representations to efficiently condition on long videos of hundreds of frames during both training and generation.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Github: https://github.com/wilson1yan/teco
Website: https://wilson1yan.github.io/teco/index.html
Paper: https://arxiv.org/pdf/2210.02396.pdf","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Deberta can now be exported to TorchScript ,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes #20815

Generally, `torch.autograd.Functions` cannot be traced in Torch, as per this open issue: https://github.com/pytorch/pytorch/issues/32822
This issue is thus more of a PyTorch problem, but nevertheless can be resolved. 🤗 Transformers' implementation is basically the same as the original https://github.com/microsoft/DeBERTa, which was tracable with a dirty trick of using a tracing context: 
https://github.com/microsoft/DeBERTa/blob/4d7fe0bd4fb3c7d4f4005a7cafabde9800372098/DeBERTa/utils/jit_tracing.py#L10C1-L17C6
Of course such a solution is not applicable here as it would conflict with the existing API and usage of the 🤗 Transformers. I have decided to explore a bit the recent development in PyTorch and it seems `is_tracing` is now publicly accessible through `torch.jit` (though it is not yet documented), which gets rid of the context problem. So I have basically implemented the original solution but with the newly available `is_tracing` call. 

I have also added tests to check if the traced model outputs the same tensors as the model that is being traced.
This was not mentioned in the issue but I have applied the same changes to Deberta_v2 since it is obviously also affected.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.
@ArthurZucker 

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
",[],21,open
Add flag for easily finetuning heads / linear probing to AutoModelforSequenceClassification,"### Feature request

Previous work has shown that last layer linear probing is cheaper and often generalizes better than normal finetuning (see [1](https://arxiv.org/pdf/2202.10054.pdf)). I imagine this could be implemented as a flag to AutoModelforSequenceClassification so that only the last layer classification head is trained. I believe this can be done manually by setting all the parameters except the last one to not track gradients, but a flag may be easier and encourage adoption. 

It may also be nice to have linear probing available at earlier layers (eg halfway through the model). This could be done through using the output_hidden_states flag during a forward pass. Mid layer linear probing can occasionally be more effective and is a widely used technique in the interpretability literature (see [2](https://direct.mit.edu/coli/article/48/1/207/107571/Probing-Classifiers-Promises-Shortcomings-and), [3](https://proceedings.neurips.cc/paper_files/paper/2019/file/159c1ffe5b61b41b3c4d8f4c2150f6c4-Paper.pdf), [4](https://arxiv.org/abs/2311.03658#:~:text=Informally%2C%20the%20%27linear%20representation%20hypothesis,directions%20in%20some%20representation%20space.), [5](https://arxiv.org/abs/2310.01405), and many others). 

Alternatively, if this were implemented generally for AutoModel (or maybe AutoModelforCausalLM?), it could use a wider variety of models. This feature could also be paired with an update that automatically allows models to be used for sequence classification by appending a final linear layer. 

### Motivation

Finetuning a head is extremely memory efficient and extremely fast (order of 1k parameters for most models, linear probes generally train in seconds, the main bottleneck will just be the forward pass) and oftentimes performs close to finetuning for classification tasks. It has also been shown to perform better OOD. 

### Your contribution

I can provide feedback and testing, have not looked deep enough to know how to fully implement this","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Add common processor tests,"# What does this PR do?

Multimodal processors currently don't have common tests. This PR aims to work towards having a common API for our multimodal processors, making sure they all have the same inputs and outputs (e.g. making sure text+vision processors accept `text` as first kwarg, then `images`, etc.).

As a first work, I refactor the CLIP and BLIP-2 processor tests to leverage the common ones.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",5,open
Add support for llama.cpp,"### Feature request

I would like to request [llama.cpp](https://github.com/ggerganov/llama.cpp) as a new model backend in the transformers library.

### Motivation

llama.cpp offers:

1) Excellent performance in scenarios where memory bandwidth is an issue, namely CPU inference and GPU + CPU inference.
2) Support for a wide range of GPU vendors and models.
3) Adequate quantization accuracy -- I have compared the perplexities of 4-bit GGUF models to GPTQ, AWQ, EXL2, and bitsandbytes and found them to be competitive ([link](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)).

By making the transformers library compatible with GGUF models, the llama.cpp performance on consumer hardware could hopefully be integrated with the features available in transformers and its surrounding ecosystem. In particular, it would be interesting to see the following working seamlessly with llama.cpp:

* [Assisted generation](https://huggingface.co/blog/assisted-generation) (speculative decoding)
* [StreamingLLM](https://github.com/huggingface/transformers/pull/26681)

### Your contribution

I have implemented a ""llamacpp_HF"" wrapper in the file below:

https://github.com/oobabooga/text-generation-webui/blob/main/modules/llamacpp_hf.py

It makes it possible to use the transformers `model.generate` with llama.cpp models, and it exemplifies how to make forward calls in llama.cpp and get the logits. It works for perplexity evaluation when `logits_all=True` is passed while loading the model. I additionally implemented some prefix-matching logic and a hacky way to recognize forward calls for negative prompts to make CFG functional.

For the llama.cpp transformers integration, I recommend the following:

* Relying on the llama-cpp-python library: https://github.com/abetlen/llama-cpp-python/
* Requiring the user to manually install llama-cpp-python with the appropriate command for their hardware rather than adding it as a direct requirement to transformers. I believe that's how it already works for GPTQ models, where AutoGPTQ has to be installed manually.
* In the `from_pretrained` call, having a `LlamaCppConfig` object that takes as input arbitrary kwargs that later on get passed to the `llama_cpp.Llama` model loading call. That would be similar to the `BitsAndBytesConfig` object that is passed to `from_pretrained` when `load_in_4bit=True` is used. Some important parameters are `n_gpu_layers` and `n_ctx`; it would be interesting to make this future-proof and allow arbitrary kwargs to be passed to `LlamaCppConfig`.

I'll tag @younesbelkada who worked with RWKV and AWQ integration in transformers and may find this interesting.","[{'id': 1834056761, 'node_id': 'MDU6TGFiZWwxODM0MDU2NzYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Modeling', 'name': 'Core: Modeling', 'color': 'FF8446', 'default': False, 'description': 'Internals of the library; Models.'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",17,open
implement TemplateConstraints in class transformers.Constraint,"### Feature request

Hello,

I recently came across your [blog post](https://huggingface.co/blog/constrained-beam-search) on constrained beam search and I am thrilled to see such implementations being made available. They are quite impressive!

Currently, I find myself in need of the feature `TemplateConstraints` It would greatly benefit my ongoing projects. Could you please provide an estimate on when this feature might be implemented, or if it is already in the pipeline?

Thank you for your hard work and for providing the community with such valuable tools. I look forward to your response.

Best regards,



### Motivation

My motivation is that I want to implement something like function calling in ChatGPT and this feature opens many doors.

### Your contribution

i dont know about it.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",9,open
[WIP][Splinter] Fixes #16627 by implementing the test cases for splinter,"# What does this PR do?

Fixes #16627 by implementing test cases for splinter

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (16627 )


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@ArthurZucker 

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Log learning rate,"# What does this PR do?

Logs the learning rate on each logging instance when training, and specifically uses scientific notation to save on space

Fulfills https://github.com/huggingface/transformers/issues/27631


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@ArthurZucker 

Example output: 
![image](https://github.com/huggingface/transformers/assets/7831895/ea85f274-0390-4d83-8a2b-3db4b9f0bb3f)


We only use the scientific notation when printing on screen. Everything else gets the full float","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",9,open
Adding support for lookahead decoding for autoregressive (decoder + encoder-decoder) models,"### Feature request

Fu et al. propose a novel decoding technique that accelerates greedy decoding on Llama 2 and Code-Llama by 1.5-2x across various parameters sizes, without a draft model. This method can be extended to work on beam search decoding.

Blog post: https://lmsys.org/blog/2023-11-21-lookahead-decoding/
Code: https://github.com/hao-ai-lab/LookaheadDecoding

### Motivation

Lookahead decoding provides a massive speedup at a worthwhile tradeoff (namely, a windowed n-gram cache and a custom attention mask). There have been other proposals to integrate lookahead decoding in other libraries like TGI or vLLM, but it seems that for this specific feature, it would be best integrated into the core `transformers` library the same way that Flash Attention has.

### Your contribution

I'm busy with thesis work, but I can submit a PR based on the original implementation here if I have time.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",9,open
Allow passing 2D attention mask,"### Feature request

Allow passing a 2D attention mask in `model.forward`.

### Motivation

With this feature, it would be much easier to avoid cross-context contamination during pretraining and supervised finetuning when packing the sequences together for more efficient training.

Here is an example usecase discussed in (https://github.com/huggingface/trl/issues/805):

![](https://user-images.githubusercontent.com/26831266/272305004-93c690a8-7e9b-40ad-885f-d530996aa109.png)

### Your contribution

Upon investigation into the source code, I found the current logic of initializing attention masks is mostly a fixed code snippet encoded in each model:

```python
        if getattr(self.config, ""_flash_attn_2_enabled"", False):
            # 2d mask is passed through the layers
            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
        else:
            # 4d mask is passed through the layers
            attention_mask = _prepare_4d_causal_attention_mask(
                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
            )
```

To enable this behavior may require hacking into each model. I should be able to handle part of them and submit a draft PR. But before that, I want to know if this feature request is reasonable.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",13,open
"Add the learning rate (in exponential representation, like ""9.197948717948718e-08"") to the model training output","### Feature request

Good day. Can you add current learning rate to output of training process. 
Now, if we train model with Trainer we get those output (without leraning rate):

<img width=""302"" alt=""image"" src=""https://github.com/huggingface/transformers/assets/17560478/239c3ce1-c7dc-4d79-9664-52ac4e3ea605"">


I'm using this code to add information about the current learning rate value to the output:

```python
from transformers import Trainer, Seq2SeqTrainer
from typing import Any, Dict, List, Union

class MyTrainer(Seq2SeqTrainer):
    def log(self, logs: Dict[str, float]) -> None:
        logs[""learning_rate""] = self._get_learning_rate()
        print(logs[""learning_rate""])
        super().log(logs)
        

trainer = MyTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice[""train""],
    eval_dataset=common_voice[""test""],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)

trainer.train()
```

This causes the learning rate values to appear in the tabular output during model training, as here:

<img width=""536"" alt=""image"" src=""https://github.com/huggingface/transformers/assets/17560478/defa1a31-15a5-4994-905e-7afdccee76de"">


But the latest version of the library doesn't have enough field width to display this parameter correctly.

### Motivation

Currently, when using the library, the learning rate is not displayed. It can only be viewed on the hub. But if the library is used locally without internet access, there is no way to track the current learning rate during fine-tuning/training of the model.

### Your contribution

Gave a succinct example of the sample code in ""Feature request"" field that I use to output a learning rate value.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",12,open
Unable to register own custom tokenizer,"### Model description

@sgugger  I have tried to register my own tokenization model based on sentencepiece using CustomAITokenizer.register_for_auto_class(""AutoTokenizer"") . But I am falied to do so.

### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

https://huggingface.co/RANITBAG/CustomAItokenizer/tree/main . This the repo link ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
"Nougat-small VisionEncoderDecoderModel failed when max_new_tokens > 3584,  Index out of range in self","### System Info

# System Info

- transformers == 4.35.0
- torch == 2.0.1+cu117
- Error shows on both cpu and cuda.

# Task
Use nougat to parse text from large documents.
# Issue
NougatTokenizerFast. model_max_length = 3584 . Setting max_new_tokens=3585 (or larger) in model.generate works well on most pages, but fails on pages which output should be '[MISSING_PAGE_POST]'.
Once the nougat fails, parsing all the following pages will fail unless kernel restart.
# Error
IndexError: index out of range in self
# What I Tried 

- Setting `nougat_model.config.decoder.pad_token_id = nougat_image_processor.tokenizer.eos_token_id` doesn't work.
- I suspect one special token is generated outside of vocabulary, which prevents from end-of-senescence detection.
- Nougat_base can generate an empty string with max_new_tokens=3585 on the same page, without errors.


### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

**Sample Document**
![failed_page](https://github.com/huggingface/transformers/assets/103061109/05ecf35a-67e5-4720-b123-dece84eec195)


**Code**
```
nougat_image_processor = NougatProcessor.from_pretrained(""facebook/nougat-small"")
nougat_model = VisionEncoderDecoderModel.from_pretrained(""facebook/nougat-small"")

pixel_values = nougat_image_processor(image, return_tensors=""pt"").pixel_values

device = 'cpu'#""cuda"" if torch.cuda.is_available() else ""cpu""
if device == 'cuda':
    pixel_values = pixel_values.cuda()
    nougat_model = nougat_model.cuda()
    
outputs = nougat_model.generate(
    pixel_values.to(device),
    min_length=1,
    max_new_tokens=3585,
    bad_words_ids=[[nougat_image_processor.tokenizer.unk_token_id]],
)
generated = nougat_image_processor.batch_decode(outputs[0], skip_special_tokens=True)[0]
generated = nougat_image_processor.post_process_generation(generated, fix_markdown=False)
print(generated)
```

### Expected behavior

**Expected Output**

'[MISSING_PAGE_POST]' or '  '.

**Observed Output**
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
/tmp/ipykernel_11659/2341160804.py in <module>
      6     nougat_model = nougat_model.cuda()
      7 
----> 8 outputs = nougat_model.generate(
      9     pixel_values.to(device),
     10     min_length=1,

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/utils/_contextlib.py in decorate_context(*args, **kwargs)
    113     def decorate_context(*args, **kwargs):
    114         with ctx_factory():
--> 115             return func(*args, **kwargs)
    116 
    117     return decorate_context

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/generation/utils.py in generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)
   1671         if generation_mode == GenerationMode.GREEDY_SEARCH:
   1672             # 11. run greedy search
-> 1673             return self.greedy_search(
   1674                 input_ids,
   1675                 logits_processor=logits_processor,

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/generation/utils.py in greedy_search(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)
   2519 
   2520             # forward pass to get next token
-> 2521             outputs = self(
   2522                 **model_inputs,
   2523                 return_dict=True,

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py in forward(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)
    602 
    603         # Decode
--> 604         decoder_outputs = self.decoder(
    605             input_ids=decoder_input_ids,
    606             attention_mask=decoder_attention_mask,

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   2046 
   2047         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-> 2048         outputs = self.model.decoder(
   2049             input_ids=input_ids,
   2050             attention_mask=attention_mask,

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)
   1236 
   1237         # embed positions
-> 1238         positions = self.embed_positions(input, past_key_values_length)
   1239 
   1240         hidden_states = inputs_embeds + positions.to(inputs_embeds.device)

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py in forward(self, input_ids, past_key_values_length)
    120         ).expand(bsz, -1)
    121 
--> 122         return super().forward(positions + self.offset)
    123 
    124 

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/sparse.py in forward(self, input)
    160 
    161     def forward(self, input: Tensor) -> Tensor:
--> 162         return F.embedding(
    163             input, self.weight, self.padding_idx, self.max_norm,
    164             self.norm_type, self.scale_grad_by_freq, self.sparse)

~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2208         # remove once script supports set_grad_enabled
   2209         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-> 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2211 
   2212 

IndexError: index out of range in self","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",5,open
Extend Chat Template Tokenization for Training/Finetuning,"### Feature request

Extend `tokenizer.apply_chat_template` with functionality for training/finetuning, returning `attention_masks` and (optional) `labels` (for ignoring ""System"" and ""User"" messages during loss computation). 

I think this requires the following steps:
- Adding support for taking in a batch of conversations (e.g., `List[Conversation := List[Dict[str, str]]`)
- Invoking the native `tokenizer.__call__()` after applying the template to each example (passing through padding, truncation, any other parameters).
- **Important**: Adding an optional output for `labels` -- a ""masked"" version of the returned `input_ids` with tokens corresponding to the System/User roles set to be ignored for loss computation (e.g., set to `IGNORE_INDEX = -100`).

### Motivation

The new `tokenizer.apply_chat_template` feature is great, and resolves a lot of ambiguity when it comes to formatting inputs for chat-based LLMs. 

However, right now it's geared for inference-time usage, only taking a single ""conversation"" and outputting the `input_ids` (tokens) after applying the chat template.

When finetuning models on chat-based data, it would be really nice to unify the `apply_chat_template` API with the `tokenizer.__call__()` API, returning `attention_masks` and (optionally) `labels` (with ""System"" and ""User"" role text automatically ignored for loss computation).

### Your contribution

I can try building a proof-of-concept for a ""standard"" workflow and Draft PR; I think there'd need to be a few discussions about the actual implementation details though!","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",10,open
Getting equivalent results between Transformer's resize and tf.image.resize,"### Feature request

For the SigLIP model (#26522), I'd like to get equivalent results between tf.image.resize and the resize method available in Transformers.

Here's what I tried:

``` 
from PIL import Image
import requests

import tensorflow as tf
import numpy as np

def resize(image, size, method=""bilinear"", antialias=False):
    """"""Resizes image to a given size.""""""
    # Note: use TF-2 version of tf.image.resize as the version in TF-1 is
    # buggy: https://github.com/tensorflow/tensorflow/issues/6720.
    # In particular it was not equivariant with rotation and lead to the network
    # to learn a shortcut in self-supervised rotation task, if rotation was
    # applied after resize.
    dtype = image.dtype
    tf_dtype = tf.type_spec_from_value(image).dtype
    image = tf.image.resize(image, size, method=method, antialias=antialias)
    return tf.cast(tf.clip_by_value(image, tf_dtype.min, tf_dtype.max), dtype)

# load image
url = 'https://cdn.openai.com/multimodal-neurons/assets/apple/apple-ipod.jpg'
image = Image.open(requests.get(url, stream=True).raw)

# get original pixel values
original_pixel_values = resize(np.array(image), size=(224,224))

# get our pixel values
from transformers.image_transforms import resize

pixel_values = resize(np.array(image), size=(224,224), resample=Image.Resampling.BILINEAR)

# verify results
np.testing.assert_array_equal(original_pixel_values, pixel_values)
```
This currently fails with:
```
AssertionError: 
Arrays are not equal

Mismatched elements: 87370 / 150528 (58%)
Max absolute difference: 255
Max relative difference: 255.
 x: array([[[127, 101,  59],
        [136, 112,  72],
        [129, 109,  72],...
 y: array([[[131, 105,  63],
        [138, 114,  74],
        [126, 108,  70],...
```

### Motivation

Would be great to have equivalent results such that logits match with the original implementation.

### Your contribution

I provide a notebook [here](https://colab.research.google.com/drive/1nP8f07qd3jWBRgCnE29cagseRtqVMADa?usp=sharing) for testing.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Paged Attention Based the Latest Cache Design,"# What does this PR do?
Based on the latest cache design on [#PR26681](https://github.com/huggingface/transformers/pull/26681), This PR implements the Paged Attention KV cache which is proposed by this [paper](https://arxiv.org/pdf/2309.06180.pdf). 


Fixes # ([issue#27303](https://github.com/huggingface/transformers/issues/27303))


# Who can review ?
@tomaarsen 
@gante 
@patrickvonplaten 
@jgong5
@jianan-gu 


","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",14,open
Co-Pilot Store in Huggingface,"### Feature request

I think its high time we have one open source Co-pilot store to compete with the GPT Store, where we can setup, train, and add our custom documents, tools to any open source custom model and have a option to share it with the world or keep it private with the users.

### Motivation

The motivation is to provide a alternative to GPT store, where by default you can create just the GPT based models and kill the competition.

here we don't even have option to control the cost.

### Your contribution

I can help in creating the vision and Roadmap for the feature, and maybe spend sometime in development activities aswell.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
"Context Free Grammar Constrained Decoding (ebnf interface, compatible with llama-cpp)","# What does this PR do?

This PR adds a new feature (Context Free Grammar Constrained Decoding) to the library. 
There is already one PR(WIP) for this feature( #26520 ), but this one has a different motivation and implementation.

This implementation is inspired by and adapted from  https://github.com/Shopify/torch-grammar and https://github.com/ggerganov/llama.cpp/pull/1773/files

This implementation aims to achieve the following goals:
- CFG-constrained decoding 
- EBNF notation as interface for the grammar
- standalone implementation of the grammar parser(left recursive-descent parsing, the same as in llama-cpp)
- compatibility with grammars in the llama.cpp library(https://github.com/ggerganov/llama.cpp/tree/master/grammars)
- incremental parsing and also non-incremental parsing(some tokenizers doesn't support incremental parsing from my experiments)
- unicode support for the grammar(not trivial but important for any multi-lingual model)

The two main differences from PR #26520 :
- dependency on lark, which may not be a bad thing, but my experience is that it will reduce the flexibility and may be hard to adapt to our specfic need, e.g. unicode grammar support.
- ebnf interface. This PR supports the same EBNF as in llama-cpp, so that users can directly migrate from llama-cpp

Challenges for this PR:
- compatibility with all the tokenizers in the transformers library.


Current status:
- [x] The grammar parser is implemented and works well with the example grammars from llama.cpp library.
- [x] A few integration tests are added to test the combination of grammar and tokenizer.
- [x] no unicode support yet, means it will probably fail when you want to constrain with emoji or other unicode characters. 
- [x] greedy search
- [x] sampling, top-k, top-p
- [ ] beam search  

TODO:
- [x]  Batching support
- [x] compatible with greedy decoding and sampling under `beam=1` 
- [x] ~~grammar parser fails to parse llama-cpp's json grammar(more precisely the string line). Currently, a slightly simplified version of json grammar if used~~(now fixed)
- [x] ~~grammar parser requires the last rule ending with a new line, otherwise, parsing error will be raised. This is not user-friendly and should be fixed~~ 
- [ ]  The EOS token seems not always included in the allowed tokens even when it should be, maybe due to the nature of recursive-descent parsing ?
- [ ] compatible with `beam_search` and `beam_sample`(Now throws error `RuntimeError: probability tensor contains either `inf`, `nan` or element < 0`). A good reference is the `ConstrainedBeamSearchScorer`
- [ ] unicode support
- [ ] properly test with different tokenizers(bpe, wordpiece, unigram, etc.)


<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # https://github.com/huggingface/transformers/issues/25778
Related to PR # https://github.com/huggingface/transformers/pull/26520


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

@gante 

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",26,open
Adding a model,"### Model description

I have built a llm and can you tell me how to register my own tokenizer with huggingface hub?


### Open source status

- [x] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Model implementation with Transformers and Hugging face hub.,"### Model description

I created a model and for ease in training and fine-tuning, I need to integrate the model with Hugging face hub. 
Model's tokenizer is a SentencePiece tokenizer. 
Here is the model repository.
https://github.com/sudhakar-71/scratch-ai

### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Plain-DETR,"### Model description

Plain-DETR is an object detector that maintains a ""plain"" nature: using a single-scale feature map and global cross-attention calculations without specific locality constraints. 
In contrast to previous leading DETR-based detectors that reintroduce architectural inductive biases of multi-scale and locality into the decoder.  
By leveraging the Object365 dataset for pre-training, it achieved 63.9 mAP accuracy using a Swin-L backbone, which is highly competitive with state-of-the-art detectors which all heavily rely on multi-scale feature maps and region-based feature extraction
Published in the proceedings of ICCV 2023.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Official implementation: [code](https://github.com/impiga/Plain-DETR)
Paper: [DETR Does Not Need Multi-Scale or Locality Design](https://openaccess.thecvf.com/content/ICCV2023/html/Lin_DETR_Does_Not_Need_Multi-Scale_or_Locality_Design_ICCV_2023_paper.html)

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",2,open
Audio-MAE - ViTMAE for audio,"### Model description

This model is is a Self-supervised Vision Transformer that uses patch reconstruction as the spectrogram task. It extends MAE (which is already on HuggingFace) for audio. This model would be a valuable addition as there doesn't seem to be a self-supervised ViT model on HugginFace currently. AST is the closest and uses supervised pre-training. Conceptually, Audio-MAE is also simpler but achieves comparable performance in their paper.

Some differences compared to the standard MAE Model
- During pre-training local and hybrid attention mechanisms can be used.
- During fine-tuning masking is also used which differs to MAE.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

**Implementation**
https://github.com/facebookresearch/AudioMAE
Created by Po-Yao Huang
@berniebear on Github

**Pre-trained Weights**
Available in the github repo

**Paper (Masked Autoencoders that Listen)**
https://arxiv.org/abs/2207.06405","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",15,open
WIP - Add Flash Attention CLIP,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Add Flash Attention 2.0 for T5 Family,"Encountered the following when trying to incorporate Flash attention into a previously devved byt5-small finetuning script.


Code to produce:

```
from transformers import T5ForConditionalGeneration, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq

model_path = ""google/byt5-small""
model = T5ForConditionalGeneration.from_pretrained(model_path,
                                                   use_flash_attention_2=True,
                                                   )
```


Error:
```
ValueError: The current architecture does not support Flash Attention 2.0. Please open an issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new
```","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 6202871275, 'node_id': 'LA_kwDOCUB6oc8AAAABcbhN6w', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flash%20Attention', 'name': 'Flash Attention', 'color': '201FF8', 'default': False, 'description': ''}]",3,open
cannot recall the model after registering it using AutoModelForCausalLM.register,"### Model description

 I have registered my custom model using AutoModelForCausalLM.register(CustomAIConfig, CustomAI) but I am unable to call the model . It shows ImportError: cannot import name 'CustomAI' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/init.py) . Can you help me in this?

### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
dinov2 with REGISTERS,"### Model description

Dear huggingface team,

The fair team published an improved version of dinov2 [VISION TRANSFORMERS NEED REGISTERS](https://arxiv.org/abs/2309.16588). The models and checkpoints are available in the [dinov2 website](https://github.com/facebookresearch/dinov2#pretrained-backbones-via-pytorch-hub), but not in hugging face. 

Could you add this new model? I really appreciate your work.

Best Wishes,

Zongze

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

[<VISION TRANSFORMERS NEED REGISTERS>](https://arxiv.org/abs/2309.16588)
[dinov2 reg checkpoint](https://github.com/facebookresearch/dinov2#pretrained-backbones-via-pytorch-hub)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",8,open
dev-ai,"### Model description

It a generative artifical intelligence model. I have the architecture ready but I am facing the problem to interagte it with transformer architure. It would be great if you provide me assistance with this.

### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
audio pipeline support for initial_prompt?,"### System Info

Is there a parameter someplace for audio pipeline support for initial_prompt?

like this: https://github.com/openai/whisper/discussions/963

$ whisper --help
optional arguments:
--initial_prompt INITIAL_PROMPT
optional text to provide as a prompt for the first window. (default: None)

$ whisper-ctranslate2 --help
optional arguments:
--initial_prompt INITIAL_PROMPT
optional text to provide as a prompt for the first window. (default: None)

### Who can help?

_No response_

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

audio pipeline-initial_paremeter

### Expected behavior

audio pipeline-initial_paremeter","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",22,open
Add ReCag model,"### Model description

Want to add a new model ReCag in Transformers.

### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",9,open
KV cache optimization with paged attention ,"### Feature request

Paged attention has been enabled by a lot of  server engine, e.g., [vllm](https://github.com/vllm-project/vllm), [tensorrt-llm](https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/tensorrt_llm/runtime/kv_cache_manager.py)

### Motivation

KV cache is used to reduce computation for Decoder layer but it also bring memory overheads, for example, when we use beam search, the kv_cache should be reordered according to latest beam idx and the current key/value should also be concat with kv_cache in the attention layer to get entire context to do scale dot product. When the sequence is very long, the memory overhead will be performance bottleneck.

### Your contribution

No PR yet","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",9,open
Deepspeed integration: support batch sizes that are less than the number of gpus/ranks,"### Feature request

Currently when training using trainer and the deepspeed zero integration the minimum total effective batch size is one per gpu/rank, thus on a 8 gpu machine the minimum effective batch size is 8.

As far as i know there is no fundamental requirement with zero style shading for there to be one batch per GPU.

### Motivation

Having one batch per gpu may be undesirable, for instance when there is insufficient memory.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",2,open
[Efficiency] Decoding can be made faster by not converting special tokens to ids for each token.,"### System Info

- `transformers` version: 4.29.0.dev0
- Platform: macOS-14.0-arm64-arm-64bit
- Python version: 3.11.4
- Huggingface_hub version: 0.13.3
- Safetensors version: not installed
- PyTorch version (GPU?): 2.0.0 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@ArthurZucker

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

The following function is being called for each token while using decoding function.

```python

from transformers import T5Tokenizer
tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH)
beams = tokenizer.batch_decode(
    outputs, skip_special_tokens=True
)
```

```python
  @property
  def all_special_ids(self) -> List[int]:
      """"""
      `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.
      """"""
      all_toks = self.all_special_tokens
      all_ids = self.convert_tokens_to_ids(all_toks)
      return all_ids
```



### Expected behavior

all_special_ids should not be called for each token while decoding at the time of inferencing.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Implement Cross Attention in LLAMA Model,"### Feature request


The current implementation of the LLAMA model in the Hugging Face Transformers repository supports self-attention layers as per the standard design of transformer models. I propose the addition of an option to use several or all attention layers as cross-attention layers instead of self-attention layers. 

Cross-attention layers are crucial for tasks where the model needs to attend to different inputs other than its own output (e.g., encoder-decoder tasks in translation, image-captioning, etc.). The option to use cross-attention would enhance the LLAMA model's capabilities for a broader range of applications.


https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py

### Motivation

My motivation for this proposal stems from the need to apply the LLAMA model to tasks that inherently require cross-modal attention mechanisms. The current limitation of self-attention only restricts its applicability. While self-attention mechanisms are effective for a range of tasks, the flexibility of cross-attention layers could extend the model's utility, allowing researchers and developers to tackle a wider variety of problems.

### Your contribution

I am willing to assist in the implementation of this feature. While I am not an expert in decoder-only architecture, with the right guidance, I can help.

I look forward to discussing this further with the maintainers of the repository.

Thank you for considering my proposal.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
Add NucleusX Model,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

This PR adds a new model named NucleusX. This model is contributed by [Sehyun Choi](https://github.com/syncdoth) and [NucleusAI](https://www.withnucleus.ai/). The model is based on the [Retentive Network](https://arxiv.org/abs/2307.08621) architecture, and the code is largely adapted from [this repo](https://github.com/syncdoth/retnet.git), which again borrows core implementations from [torchscale](https://github.com/microsoft/torchscale). We are planning to release our paper and weights soon.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?

We kindly request the review of this new model from @ArthurZucker and @younesbelkada!

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @muellerzr and @pacman100

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam
- Big Model Inference: @SunMarc
- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada

Documentation: @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: See Models above and tag the person corresponding to the modality of the example.
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",20,open
Infomer with different output_size,"### Feature request

The Infomer transformers seems to perform well on time series multivariate forecasting also in my application, however, it would be nice to have the option to have an `output_size` for the features different from the `input_size`.

### Motivation

The reasons is that for the multivariate forecasting problem i am considering it would make sense to focus the predictions only on the variables/features of interest.

### Your contribution

I will be happy to work and contribute what I can. I have inspected the code -- I am using InfomerforPrediction -- but it i snot immediately obvious to me where to insert the last fully connected layer for the reduction. If somebody can give some guidance, i will be happy to try it...","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
example code problem,"while trying the example from https://huggingface.co/amazon/MistralLite This is the result.

python examplecode.py 
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Traceback (most recent call last):
  File ""/home/chris/ai/text-generation-webui/amazonmistral/examplecode.py"", line 8, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_id,
  File ""/home/chris/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py"", line 571, in from_pretrained
    return model_class.from_pretrained(
  File ""/home/chris/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 3076, in from_pretrained
    config = cls._check_and_enable_flash_attn_2(config, torch_dtype=torch_dtype, device_map=device_map)
  File ""/home/chris/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 1265, in _check_and_enable_flash_attn_2
    raise ValueError(
ValueError: The current architecture does not support Flash Attention 2.0. Please open an issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
[i18n-TR] Translating docs to Turkish,"Hi!

Let's bring the documentation to all the Turkish-speaking community 🌐

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `tr` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `tr/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [x] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) https://github.com/huggingface/transformers/pull/20180
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) (waiting for initial PR to go through)
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md). (In progress by @mertyyanik )

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Tracking full integration for fill-in-middle (FIM),"Given that most code LLMs are trained using the FIM objective [1], I think it makes a lot of sense to work on:

* A training example just like how we have it here: https://github.com/huggingface/transformers/tree/main/examples/pytorch. This can also be turned into a task guide later (example: https://huggingface.co/docs/transformers/tasks/language_modeling).
* A dedicated pipeline so that users can load FIM-trained models easily with `pipeline(""fill-in-middle"")`. 
* A task page to list all the relevant resources. 

Cc @ArthurZucker since we discussed it internally via Slack. 

**References**

[1] Efficient Training of Language Models to Fill in the Middle, https://arxiv.org/abs/2207.14255.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 6126880899, 'node_id': 'LA_kwDOCUB6oc8AAAABbTDIgw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/contributions-welcome', 'name': 'contributions-welcome', 'color': 'F99E09', 'default': False, 'description': ''}]",17,open
Difference in LlamaAttention & LlamaFlashAttention2 attn_output,"### System Info

- `transformers` version: 4.34.1
- Platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.31
- Python version: 3.11.5
- Huggingface_hub version: 0.17.3
- Safetensors version: 0.4.0
- Accelerate version: 0.23.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.1.0+cu121 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: yes
- Using distributed or parallel set-up in script?: no

### Who can help?

@ArthurZucker and @younesbelkada

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

We notice `LlamaFlashAttention2._flash_attention_forward` returns a different `attn_output` than `LlamaAttention` computes.

`flash_attn_non_determinism.py`:
```python
import argparse

import torch
import torch.backends.cudnn
import transformers
from transformers.models import llama


def main() -> None:
    torch.backends.cudnn.deterministic = True

    parser = argparse.ArgumentParser()
    parser.add_argument(""--use-flash-attention-2"", action=""store_true"")
    args = parser.parse_args()
    use_flash_attention_2 = args.use_flash_attention_2

    tokenizer = transformers.AutoTokenizer.from_pretrained(
        ""/models/huggingface/meta-llama/llama-2-7b-chat-hf"", local_files_only=True, use_safetensors=True, device_map=torch.device(""cuda"")
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = ""left""

    text = ""Hello world!""
    tokenized_text = tokenizer(text)
    tokenized_text = {key: torch.tensor(value).unsqueeze(dim=0).to(torch.device(""cuda"")) for key, value in tokenized_text.items()}
    tokenized_text[""labels""] = tokenized_text[""input_ids""].clone()

    torch.manual_seed(0)
    model = llama.LlamaForCausalLM.from_pretrained(
        ""/models/huggingface/meta-llama/llama-2-7b-chat-hf"",
        local_files_only=True,
        use_safetensors=True,
        device_map=torch.device(""cuda""),
        use_flash_attention_2=use_flash_attention_2,
        torch_dtype=torch.bfloat16,
    )
    assert isinstance(model, llama.LlamaForCausalLM)
    model.eval()
    for param in model.parameters():
        param.requires_grad = False

    model.model.layers[0].train()
    for param in model.model.layers[0].parameters():
        param.requires_grad = True

    optim = torch.optim.AdamW(model.parameters())

    torch.manual_seed(0)

    for i in range(10):
        output = model(**tokenized_text)
        loss = output[""loss""]
        if i in (0, 9):
            print(loss)
        loss.backward()
        optim.step()
        optim.zero_grad()


if __name__ == ""__main__"":
    main()
```

```console
$ python flash_attn_non_determinism.py --use-flash-attention-2
tensor(5.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3542, device='cuda:0', grad_fn=<NllLossBackward0>)
$ python flash_attn_non_determinism.py
tensor(5.6589, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward0>)
```

### Expected behavior

I am not expecting the magnitude of the difference between the 2 implementations. A difference of `0.1267` compared to `0.3542` seems very large.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",32,open
Pruning/Compressing heads in attention blocks,"### Feature request

I've a conceptual question

BERT-base has a dimension of 768 for query, key and value and 12 heads (Hidden dimension=768, number of heads=12). The same is conveyed if we see the BERT-base architecture

```
(self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
)
```
Now, my question is:

Can I consider the first 64 neurons from the _out_features_ as the first-head, the next 64 neurons from the _out_features_ as the 2nd head and so on? (sec 3.2.2 from original paper; [Link](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf))

Basically, I am wondering if the Linear module representing query matrix; which is 768x768 can be thought as (768x64), (768x64)...12 times? 

If so, is it possible to provide some starter code as I am unable to wrap around my head. Any help is appreciated (and I've some sample in the contribution section)

**P.S:** Here's the issue from StackOverflow ([link](https://datascience.stackexchange.com/questions/124233/understanding-multi-headed-attention-from-architecture-details))

### Motivation

Example applications include papers from academia such as - [Paper1](https://lena-voita.github.io/posts/acl19_heads.html), [Paper2](https://github.com/pmichel31415/are-16-heads-really-better-than-1)

I referred to some of the previous posts ([link](https://datascience.stackexchange.com/questions/88330/how-do-the-linear-layers-in-the-attention-mechanism-work)), but I would appreciate any validation on this thought-process as it's similar but not same.

### Your contribution

Here's a code which prunes a particular % in particular layer depending on _layer_index_ and _prune_percentage_

```
model = AutoModelForMaskedLM.from_pretrained(checkpoint)

linear_layers_list = []
for name, layer in model.named_modules():
    if name in model_layers_list:
        linear_layers_list.append(layer)
print(f""No of linear layers are: {len(linear_layers_list)}"")

layer = linear_layers_list[layer_index]
if prune_type == 'ln_structured':
    # Ln structured with n=1 i.e L1 pruning
    prune.ln_structured(layer, name='weight', amount=prune_percentage, dim=0, n=n)
```
I can understand that I can basically pass the Linear module and prune x% of weights.

Now, I would like to prune/remove one head in a similar fashion. 

Thanks","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",12,open
Add JinaBert Model,"### Model description

Jina AI has just released an open source embedding model that can handle 8k sequence on huggingface:
- https://huggingface.co/jinaai/jina-embeddings-v2-base-en
- https://huggingface.co/jinaai/jina-embeddings-v2-small-en

These models however, currently require the `trust_remote_code` flag as they reference a custom model implementation specified at https://huggingface.co/jinaai/jina-embedding-v2/tree/main. It should be relatively simple to upstream the model implementation as it is already implemented to work when `trust_remote_code` is passed.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://huggingface.co/jinaai/jina-embedding-v2/tree/main","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
Microsoft's GLIP Grounding Language Image Pretraining,"### Model description

Combines best practices of CLIP and object detectors.
Allows for localization and grounding of text and image content.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

github https://github.com/microsoft/GLIP
weights on hf https://huggingface.co/GLIPModel/GLIP/tree/main
colab demo https://colab.research.google.com/drive/12x7v-_miN7-SRiziK3Cx4ffJzstBJNqb?usp=sharing","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
Add Model Support for xLSTM,"### Model description

Inspired by [recent rumors](https://www.youtube.com/watch?v=hwIt7ezy6t8) about xLSTM - a hidden successor to LSTM - by Sepp Hochreiter, this issue tracks the open source implementation about adding xLSTM to Transformers library.

### Open source status

- [x] The model implementation is available [here](https://github.com/NX-AI/xlstm) and [here](https://github.com/NX-AI/xlstm-jax) as JAX implementation
- [x] Transformers integration is currently available on their [Transformers fork](https://github.com/NX-AI/transformers/tree/integrate_xlstm) 
- [x] The model weights are available [here](https://huggingface.co/NX-AI/xLSTM-7b) (7B model)

### Provide useful links for the implementation

- [x] Paper is available [here](https://arxiv.org/abs/2405.04517)

At the moment no implementation does exist.

Only rumors that xLSTM surpasses GPT-2 on various (small) downstream datasets.

Good overview is the [xLSTM Resources](https://github.com/AI-Guru/xlstm-resources) repository from @AI-Guru.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
"The current architecture does not support Flash Attention 2.0 - distilgpt2, gpt2-medium ","Could you please add the flash attention 2.0 support for these models: distilgpt2, gpt2-medium? ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
BarkModel: support multiple history prompt for batch api,"### Feature request

Add multiple history prompt support for batch api

### Motivation

Batch API speeds up the audio generation process a lot. Unfortunately we can't use it to generate long sequences.

### Your contribution

I may be able to look at it soon","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",11,open
Add quantization_config in AutoModelForCausalLM.from_config(),"### Feature request

Add quantization_config feature to AutoModelForCausalLM from config . 
I am trying to pretrain a model from scratch and use bits and bytes so that It can be trained on less computation expensive  machines. 
Below is my quantization config :

```
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)
```

When I attempted to take the config of certain model from_pretrained function  it failed and raised a Type Error mentioned below. 

```
from transformers import AutoConfig, AutoModelForCausalLM
config = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.1"")
model = AutoModelForCausalLM.from_config(config,quantization_config=bnb_config, device_map={"""":0})
```

The Error:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[23], line 7
      3 # Download configuration from huggingface.co and cache.
      5 configy = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.1"")
----> 7 modely = AutoModelForCausalLM.from_config(configy,quantization_config=bnb_config, device_map={"""":0})

File ~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441, in _BaseAutoModelClass.from_config(cls, config, **kwargs)
    439 elif type(config) in cls._model_mapping.keys():
    440     model_class = _get_model_class(config, cls._model_mapping)
--> 441     return model_class._from_config(config, **kwargs)
    443 raise ValueError(
    444     f""Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n""
    445     f""Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.""
    446 )

File ~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/modeling_utils.py:1192, in PreTrainedModel._from_config(cls, config, **kwargs)
   1190         model = cls(config, **kwargs)
   1191 else:
-> 1192     model = cls(config, **kwargs)
   1194 # restore default dtype if it was modified
   1195 if dtype_orig is not None:

TypeError: MistralForCausalLM.__init__() got an unexpected keyword argument 'quantization_config'
```



### Motivation

I had tried a work around by saving the model from the loaded config details from the model and then load the same model with quantization config . 

I believe this process could get fixed and we can enable/add quantization while loading the model from the config itself. 

### Your contribution

```
from transformers import AutoConfig, AutoModelForCausalLM
config = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.1"")
model = AutoModelForCausalLM.from_config(config)
model.save_pretrained(MODEL_NAME_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME_PATH, quantization_config=bnb_config, device_map={"""":0})
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}]",15,open
Support reverse_prompt,"### Feature request

Add a simple option to provide a stopping criteria in the generate method. This option could also be a stopping criteria.

### Motivation

Llama.cpp supports an option `--reverse-prompt`. It is very useful for chat models to stop at words like ""USER:"".

### Your contribution

This is how I implemented it, but it is not ideal:

```
def count_stop_words(text, stop_words):
    return Counter({expr: text.count(expr) for expr in stop_words})


class MyStoppingCriteria(StoppingCriteria):
    def __init__(self, stop_counter):
        self.stop_counter = stop_counter

    def __call__(self, input_ids, scores, **kwargs):
        # Get the generated text as a string
        generated_text = tokenizer.decode(input_ids[0])
        counter = count_stop_words(generated_text, self.stop_counter)
        if counter - self.stop_counter:
            return True
        return False

```

Then you can use this parameter:

```
            stopping_criteria=StoppingCriteriaList(
                [
                    MyStoppingCriteria(
                        count_stop_words(PROMPT, [""ASSISTANT:"", ""USER:"", ""SYSTEM:""])
                    )
                ]
            ),
```","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",7,open
log interval,"### Feature request

I want you to add ""log_interval"" argument for logging every N iteration not a epoch.

### Motivation

Some fine-tuning task/dataset does not require much epochs such as CoLA. It only requires 2~3 epochs, which means the log data(ex metric) is produced 2~3 times.

I want more log during training with 2~3 epochs, so I hope you add some feature which logs every N iteration.

I already add the argument in my project just adding below code, but it is borthing to add every project whenever I create new project based on your codes.

`
if completed_steps % args.log_interval == 0:
# presented evaluation code in original code.
`

### Your contribution

`
if completed_steps % args.log_interval == 0:
                model.eval()
                samples_seen = 0
                for step, batch in enumerate(eval_dataloader):
                    with torch.no_grad():
                        outputs = model(**batch)
                    predictions = outputs.logits.argmax(dim=-1) if not is_regression else outputs.logits.squeeze()
                    predictions, references = accelerator.gather((predictions, batch[""labels""]))
                    # If we are in a multiprocess environment, the last batch has duplicates
                    if accelerator.num_processes > 1:
                        if step == len(eval_dataloader) - 1:
                            predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]
                            references = references[: len(eval_dataloader.dataset) - samples_seen]
                        else:
                            samples_seen += references.shape[0]
                    metric.add_batch(
                        predictions=predictions,
                        references=references,
                    )

                eval_metric = metric.compute()
                logger.info(f""epoch {epoch}: {eval_metric}"")

                if args.with_tracking:
                    accelerator.log(
                        {
                            ""accuracy"" if args.task_name is not None else ""glue"": eval_metric,
                            ""train_loss"": total_loss.item() / args.log_interval,
                            ""train_mem"": total_memory / args.log_interval,
                            ""epoch"": epoch,
                            ""step"": completed_steps,
                            ""lr"": optimizer.param_groups[0][""lr""],
                        },
                        step=completed_steps,
                    )
                total_loss = 0
`","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
[i18n-zh] Translating docs to Chinese (Simplified),"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the <languageName>-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

And I can see some part of the work has be done (Great work!). I just list all files below.

## Get Started section(keep updating)
Here is what I am working now, you can just skip these docs.
[perf_train_gpu_one.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md)
[perf_train_gpu_many.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_many.md)
[perf_train_cpu.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_cpu.md)
[perf_train_cpu_many.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_cpu_many.md)
[perf_train_tpu.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu.md)
[perf_train_tpu_tf.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md)
[perf_train_special.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_special.md)
[perf_infer_cpu.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_cpu.md)
[deepspeed.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/deepspeed.md)
[trainer.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md) - #29728 

<!--
Keep on adding more as you go 🔥
-->

@stevhliu ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
[Feature Request] We might need a function to change the sampler used in trainer dataloader,"### Feature request

we need a function to custom the sampler used in trainer dataloader.

### Motivation

for sometimes, we might want to use https://github.com/imoneoi/multipack_sampler/blob/master/multipack_sampler.py#L94 for efficient reasoning

another example is DoReMi paper， we might also change the  sample distribution  for different task source

### Your contribution

I ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
[i18n-<kan>] Translating docs to <Kannada>,"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the <Kannada>-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<kan>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<kan>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) https://github.com/huggingface/transformers/pull/20180
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) (waiting for initial PR to go through)
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
[i18n-<hi>] Translating docs to <Hindi>,"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the <languageName>-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) https://github.com/huggingface/transformers/pull/20180
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) (waiting for initial PR to go through)
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",6,open
MaskCLIP,"### Model description


MaskCLIP represents a transformative step in the realm of open-vocabulary universal image segmentation. Built upon the robust foundation of pre-trained CLIP models, it negates the need for additional finetuning or distillation. The core of MaskCLIP is its innovative Transformer-based MaskCLIP Visual Encoder. This encoder is meticulously designed to integrate mask tokens with a pre-trained ViT CLIP model, making it adept at both semantic and instance segmentation, as well as class prediction. One of MaskCLIP's standout features is its ability to efficiently harness the power of pre-trained dense and local CLIP features within its Visual Encoder. This design choice not only streamlines the segmentation process but also sidesteps the traditionally lengthy student-teacher training phase. Demonstrating its prowess, MaskCLIP has consistently outperformed existing methods on renowned datasets like ADE20K and PASCAL, especially in tasks of semantic, instance, and panoptic segmentation.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

project page  : https://maskclip.github.io
github : https://github.com/mlpc-ucsd/MaskCLIP
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
[New model] RT-DETR,"### Model description

The paper ""DETRs Beat YOLOs on Real-time Object Detection"" proposes a Real-Time DEtection TRansformer (RT-Detr), a hybrid encoder and a transformer decoder for object detection tasks.
Repo has +380 stars and paper shows improvement in inference speed and AP in comparison with other models, outperforming YOLOv8 detectors in both speed and accuracy.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/abs/2304.08069
Code: https://github.com/lyuwenyu/RT-DETR/","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Tutorial on implementing tree of thoughts(ToT) framework using a model ,"### Feature request

Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts

### Motivation

a comprehensive tutorial on implementing tree of thoughts using any open source model will give users more understanding about it.

### Your contribution

https://github.com/princeton-nlp/tree-of-thought-llm
https://arxiv.org/abs/2305.10601","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",23,open
Add an option to decide whether to store the checkpoint and rng_state.,"**Motivation:**
Currently, when using the Transformers library in combination with DeepSpeed for training large language models like LLMs, checkpoints (e.g. `bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt`) are automatically saved along with the `rng_state`, which can lead to significant disk space usage. In scenarios where multiple GPUs are employed for training, this can quickly become a storage bottleneck, especially when shared by a team. Sometimes we just want to keep the bin file (e.g. `pytorch_model-00001-of-00002.bin`) as it's enough for load again.

**Feature Request:**
I propose adding a configurable option to decide whether to store the checkpoint and `rng_state` during training. This will give users the flexibility to choose when to save checkpoints and reduce the disk space required.

**Proposed Solution:**

1. Add a new parameter, such as `save_checkpoint_enabled`, to the DeepSpeed configuration file. Users can set this parameter to `True` or `False` to control whether checkpoints and `rng_state` should be saved during training.

2. Modify the `trainer.py` script in the Transformers library to include a condition for `self.save_checkpoint_enabled` in the `_save_checkpoint` function. Here's a code snippet illustrating the change:

   ```python
   if self.is_deepspeed_enabled and self.save_checkpoint_enabled:
       # Save the checkpoint
   ```

This change will allow users to save disk space by not storing checkpoints when not needed, and it can help alleviate the storage challenges associated with large-scale language model training.

I have already submitted this issue to the DeepSpeed library #https://github.com/microsoft/DeepSpeed/issues/4403#issue-1913025248 , as this feature may require collaboration between both libraries.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
[RFC] Updating pipeline models,"### Feature request

We're considering updating the default models used in `transformers` pipelines. This has the potential to greatly improve performance, and get rid of limitations caused by the existing models, but it may also break backward compatibility. Many of the default models have not been changed since the tasks were first added in pipelines, so users might assume that they are 'permanent', and might be surprised by an update.

When updating pipelines, we would aim for the following objectives:

- The model should run on a base Colab instance (i.e. inference at max sequence length should fit inside 16GB VRAM)
- The default context length for text tasks should be long (at least 4k tokens where possible, ideally infinite with rope/alibi scaling)
- The performance should be as strong as reasonably possible within those two constraints

### Motivation

We have seen a number of [user issues](https://github.com/huggingface/transformers/issues/24392) prompted by the default pipeline models in `transformers` being outdated. For example, the default `sentiment-analysis` pipeline uses a finetuned `distilbert` model with a maximum sequence length of 512 tokens. You can see the full list of default models [here](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/__init__.py#L155).

Performance on these tasks could be greatly improved with more modern models that have newer features like longer (potentially unlimited!) context lengths.

### Your contribution

I'll make the PR and potentially train new models for some of these tasks.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
Add Fast model ,"Add Fast model 
Fix issue #26501","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",7,open
add T5 as decoder only,"### Model description

Since T5/ByT5 is an encoder-decoder model, I created a subclass T5DecoderOnlyForCausalLM to use it as decoder only for OCR task:
`from transformers.models.t5.modeling_t5 import T5PreTrainedModel, T5Stack
import torch
import torch.nn as nn
from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions, Seq2SeqLMOutput

class T5DecoderOnlyForCausalLM(T5PreTrainedModel):

    def __init__(self, config):
        super().__init__(config)
        self.shared = nn.Embedding(config.vocab_size, config.d_model)
        self.decoder = T5Stack(config, self.shared)
        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)
        self.is_decoder = True
        config.is_decoder = True
        config.use_cache = False

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        decoder_attention_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        decoder_input_ids=None,
        inputs_embeds=None,
        decoder_inputs_embeds=None,
        head_mask=None,
        use_cache=None,
        cross_attn_head_mask=None,
        past_key_values=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):

        decoder_outputs = self.decoder(
            input_ids=input_ids,
            attention_mask=decoder_attention_mask,
            past_key_values=past_key_values,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            inputs_embeds=decoder_inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        last_hidden_state = decoder_outputs.last_hidden_state
        logits = self.lm_head(last_hidden_state)
        hidden_states = decoder_outputs.hidden_states
        past_key_values = decoder_outputs.past_key_values
        attentions = decoder_outputs.attentions
        cross_attentions = decoder_outputs.cross_attentions

        return CausalLMOutputWithCrossAttentions(
            logits = logits,
            past_key_values = past_key_values,
            hidden_states = hidden_states,
            attentions = attentions,
            cross_attentions = cross_attentions
        )


    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        head_mask=None,
        decoder_head_mask=None,
        decoder_attention_mask=None,
        cross_attn_head_mask=None,
        use_cache=None,
        encoder_outputs=None,
        **kwargs,
    ):
        # cut decoder_input_ids if past is used
        if past_key_values is not None:
            input_ids = input_ids[:, -1:]

        return {
            ""input_ids"": input_ids,
            ""past_key_values"": past_key_values,
            ""encoder_outputs"": encoder_outputs,
            ""attention_mask"": attention_mask,
            ""head_mask"": head_mask,
            ""decoder_head_mask"": decoder_head_mask,
            ""decoder_attention_mask"": decoder_attention_mask,
            ""cross_attn_head_mask"": cross_attn_head_mask,
            ""use_cache"": use_cache,
        }

    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):
        return self._shift_right(labels)`
        
       
Im using it now with VisionEncoderDecoderModel:
`tokenizer = ByT5Tokenizer.from_pretrained('google/byt5-base')
image_processor=ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')
processor = TrOCRProcessor(image_processor=image_processor, tokenizer=tokenizer)

encoder = ViTModel.from_pretrained(""google/vit-base-patch16-224"")
decoder = T5DecoderOnlyForCausalLM.from_pretrained(""google/byt5-base"")
model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)`

But for each epoch, Im getting same label as prediction. I'm not sure what's going wrong. train args and trainer looks like this:
`training_args = Seq2SeqTrainingArguments(
    predict_with_generate=True,
    evaluation_strategy=""steps"",
    num_train_epochs=1,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    fp16=False, ##
    fp16_full_eval=False,  
    output_dir=""/content/"",
    logging_steps=2,
    save_steps=5,
    eval_steps=5,
)`

`def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions

    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

    cer = cer_metric.compute(predictions=pred_str, references=label_str)

    return {""cer"": cer}`

`trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=dataset,
    eval_dataset=dataset_val,
    data_collator=default_data_collator,
)`

@NielsRogge can you please help me to understand what's going wrong and why for each epoch, generated text for multiple images is same?




### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

implementation link for training:
[https://github.com/iitb-research-code/byt5-model/blob/dev/train_hg.py](url)

link for inference:
[https://github.com/iitb-research-code/byt5-model/blob/dev/inference.py](url)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
BLIVA,"### Model description

BLIVA: A Simple Multimodal LLM for Better Handling of Text-rich Visual Questions

BLIVA performs VQA tasks. It is an augmented version of InstructBLIP with Visual Assistant. It incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM.

""Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76\% in OCR-VQA benchmark) and in undertaking typical VQA benchmarks (up to 7.9\% in Visual Spatial Reasoning benchmark), comparing to our baseline InstructBLIP.""


### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Official repo: https://github.com/mlpc-ucsd/BLIVA
Paper: https://arxiv.org/abs/2308.09936","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
[New Model] Retrieval-based Voice Conversion,"### Model description

Type: Audio-to-Audio
Framework: Pytorch / ONNX
License: MIT


### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Source Code: https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI
Pretrained Models by Author: https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main
Models in the wild: https://www.weights.gg/","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
[WIP] Adding BLIVA model,"# What does this PR do?

Adds BLIVA to transformers.  

* Original repo: https://github.com/mlpc-ucsd/BLIVA
* Paper: https://arxiv.org/abs/2308.09936

Fixes #26629 - issue with new model request

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
 UmT5 Flax modelling,"### Feature request

I'd like to have the UmT5 Flax modelling file, there was a PR for it https://github.com/huggingface/transformers/pull/22626 but was closed by https://github.com/huggingface/transformers/pull/24477 PR. The new one only has UmT5 PyTorch modelling file.

### Motivation

Since UmT5 is originally implemented in Jax and Flax, it would make sense to have the flax modelling file for the model.

### Your contribution

I could help testing the modelling file","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2934977194, 'node_id': 'MDU6TGFiZWwyOTM0OTc3MTk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flax', 'name': 'Flax', 'color': '4862AD', 'default': False, 'description': ''}]",4,open
[WIP] Add Support for masking output using a Context-Free-Grammar,"# What does this PR do?
Fixes #25778 

# Review 
@gante @LysandreJik as discussed. Also, @oobabooga @ArthurZucker @jorgemcgomes feel free to look at this. 
This is in large parts inspired by [rellm](https://github.com/r2d4/rellm/tree/main) and [parserllm](https://github.com/r2d4/parserllm) by @r2d4 so I am tagging him here too for visibility.

# Discussion 
This is totally WIP still. I added a notebook showcasing how it might work when using the generation API from a low-level perspective. Please educate me if that is ok that way. Long-term I want to add tests. 
Generally, we use Lark to parse the grammar. The grammar itself would most likely come from a text file or a string but Lark also has a grammar object that one can use. I built a class [CfgParsingStepper](https://github.com/jvhoffbauer/transformers/blob/cfg_masking_logits_processor/src/transformers/generation/logits_process.py#L1721) that we can use to get the state of the parser for any input string. It will give us the terminal symbol we are processing and the regex for this symbol that we can use to find valid tokens. The [CfgLogitsProcessor](https://github.com/jvhoffbauer/transformers/blob/cfg_masking_logits_processor/src/transformers/generation/logits_process.py#L1774) gets the state every turn for every beam. We can consider persisting the state during generation which might be faster, instead of recalculating it. 

The whole codebase is still very much WIP. Most importantly, we still need to 

- Add tests (instead of a Jupyter Notebook) 
- Add error handling to make sure the parser throws an error if it is started with an invalid input 
- Connect the logits processor to the actual generation APIs. I am thinking of something like `model.generate(..., grammar=Grammar(...))`
- Think about cases where you want the grammar constraint to start only for the new generations and not for the input prompt

Anyhow, I wanted to put this out there for discussion. Let me know if it makes sense this way and whether you would like me to continue working in this way or if I should change anything! ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",15,open
Add LISA Model,"### Model description

LISA: Reasoning Segmentation via Large Language Model 

It proposes a new segmentation task --- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/abs/2308.00692
Github: https://github.com/dvlab-research/lisa","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
Add FAST model,"### Model description

FAST model does efficient scene text detection and will be good addition to repo.

Paper : https://arxiv.org/pdf/2111.02394
Code : https://github.com/czczup/FAST

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
compute_metrics with causal LM training,"### Feature request

Besides loss, users often need to report additional metrics throughout the training in order to drive decision making and communicate results, which in the case of Seq2Seq models is elegantly done with the `compute_metrics` argument of the `Trainer`. Generative metrics easily fit this framework by setting `predict_with_generate=True`. The same is much less straightforward with a Causal underlying LM. The only ""working"" approach I found is this: https://github.com/huggingface/transformers/blob/5e11d72d4d0939138fbabfebe9a69d2061519547/examples/pytorch/language-modeling/run_clm.py#L578

But I think this is an erroneous calculation: the `logits.argmax(dim=-1)` call does not really generate in inference mode, it ""cheats"" because of teacher forcing and therefore any metric computed that way is probably inflated. Ideally it would be possible to make the argument passed to `compute_metrics`  include a proper `predictions` attribute that has been properly generated using the trainers generation config.

### Motivation

I am always frustrated when I can't observe the learning trajectory of my generative metric (say BLEU/ROUGE) when using a CML even though it is trivial to do when I am using a S2S

### Your contribution

If you confirm that this is an issue and important enough to justify a fix I may be able to make a PR but can't promise it","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",17,open
[T5] lm_head weights initialization: set variance to reciprocal of hidden dim,"# What does this PR do?

![image](https://github.com/huggingface/transformers/assets/6141784/bddb088b-5bae-4bbb-bbb8-feff450c814e)

before this PR: lm_head weights were initialized with variance of 1, and it output activations with variance ~= hidden_dim. this is a very high variance for logits, and resulted in initial cross-entropy loss of ~110, which is Very High.

after this PR: lm_head weights initialized with variance of reciprocal of hidden_dim. this outputs activations with variance ~= 1. this is results in initial cross-entropy loss of ~11, which is high, but in line with what we'd expect.

Fixes https://github.com/huggingface/transformers/issues/16749 (again)

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?

## Who can review?

@ArthurZucker @younesbelkada","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",27,open
How to run Trainer + DeepSpeed + Zero3 + PEFT ,"### System Info

- `transformers` version: 4.34.0.dev0
- Platform: Linux-5.14.0-284.25.1.el9_2.x86_64-x86_64-with-glibc2.34
- Python version: 3.11.4
- Huggingface_hub version: 0.16.4
- Safetensors version: 0.3.3
- Accelerate version: 0.24.0.dev0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)


### Who can help?

 @ArthurZucker and @younesbelkada and @pacman100 and @muellerzr 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

[This script](https://gist.github.com/BramVanroy/f2abb3940111b73ae8923822ef6096dd) is a modification of the official run_clm script. The only additions are the BNB config and PEFT. Yet, I cannot get it to work with a [deepspeed zero3 config](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/configs/ds_falcon_180b_z3.json).

Requirements to install:

```
accelerate >= 0.12.0
torch >= 1.3
datasets >= 1.8.0
sentencepiece != 0.1.92
protobuf
evaluate
scikit-learn
trl
peft
bitsandbytes
```

In the past I have had issues with low_cpu_mem_usage but neither a true/false value seem to get this to work:

Command 1:

```sh
deepspeed --include=""localhost:0,1"" run_clm.py \
   --model_name_or_path facebook/opt-125m\
  --dataset_name wikitext\
  --dataset_config_name wikitext-2-raw-v1\
  --per_device_train_batch_size 2\
  --per_device_eval_batch_size 2\
  --do_train\
  --do_eval\
  --output_dir /tmp/test-clm\
  --deepspeed deepspeed_configs/ds_config_zero3.json\
  --low_cpu_mem_usage true
```
==> `ValueError: DeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.`

Command 2:

```sh
deepspeed --include=""localhost:0,1"" run_clm.py \
   --model_name_or_path facebook/opt-125m\
  --dataset_name wikitext\
  --dataset_config_name wikitext-2-raw-v1\
  --per_device_train_batch_size 2\
  --per_device_eval_batch_size 2\
  --do_train\
  --do_eval\
  --output_dir /tmp/test-clm\
  --deepspeed deepspeed_configs/ds_config_zero3.json\
  --low_cpu_mem_usage false
```

==> `ValueError: weight is on the meta device, we need a `value` to put in on 0.`

### Expected behavior

Any option to make this combination of Trainer + DeepSpeed + Zero3 + PEFT work.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",6,open
Implementation of Git-Rebasin to Huggingface models,"### Feature request

Hi, 

I am not sure if this is even feasible/appropriate request in Huggingface. Recently, there was an ICLR paper titled ""Git Rebasin"" (https://arxiv.org/abs/2209.04836) which demonstrates a method to effectively merge models.

But the authors have implemented it only for plain NN and Resnet kind of architectures. It would be really great if this feature is supported for Huggingface models as there'll be lots of instances where we might need to merge the models and a naive way of adding is not the best way.   

### Motivation

This feature is helpful especially in federated learning where 2 different people train 2 models separately and would like to combine their models. 

In academic circles as well, this is very much useful as there'll be lots of instances where effective model merging is critical! 

### Your contribution

I've raised PR's in subsequent Github repos

1. https://github.com/samuela/git-re-basin/issues/13#issue-1910375545
2. https://github.com/themrzmaster/git-re-basin-pytorch/issues/8#issue-1910392068

Not sure if this is feasible based on some of the previous discussions as the paper is based on ""Permutation Invariance"" of model weights and an architecture like Llama might have various possibilities thus difficulty in the method.

I would be happy to contribute if there's some help!","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
`Helsinki-NLP/opus-mt-it-en` isn't on HuggingFace Hub,"### Model description

I have found lots of Opus translation model on HuggingFace Hub but couldn't find Portuguese to English model, however the model already exists in Helsinki repo https://github.com/Helsinki-NLP/OPUS-MT-train/tree/master/models/pt-en#opus-2019-12-05zip

is that something can be added quickly? 

### Open source status

- [ ] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://github.com/Helsinki-NLP/OPUS-MT-train/tree/master/models/pt-en#opus-2019-12-05zip","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Add ProPainter to transformers,"### Model description

ProPainter can be an excellent addition to Transformers given its impressive inpainting capabilities. 

*Relevant Links*
Paper - https://arxiv.org/abs/2309.03897
Project Page - https://shangchenzhou.com/projects/ProPainter/
Original Code - https://github.com/sczhou/ProPainter
Weights - https://github.com/sczhou/ProPainter/releases/tag/v0.1.0 (Needs to be merged into a unified one)

Author - @sczhou 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

@amyeroberts @ArthurZucker
If you think this is a valuable addition I'm more than happy to work on it. ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",15,open
Community contribution: Adding Flash Attention 2 support for more architectures,"### Feature request

Flash Attention 2 is a library that provides attention operation kernels for faster and more memory efficient inference and training: https://github.com/Dao-AILab/flash-attention

![Screenshot 2023-09-22 at 17 49 18](https://github.com/huggingface/transformers/assets/49240599/1395f962-26ca-4728-a8d0-085792295c28)


Let's try to add Flash Attention 2 support for more architectures! Currently supported architectures are

- [x] Llama
- [x] Falcon

It would be great to add the support for more architectures such as

- [x] Bark
- [x] Bart
- [ ] BERT | @sorenmc 
- [ ] CLIP https://github.com/huggingface/transformers/pull/27444/
- [x] DistilBERT
- [x] GPT-2
- [x] GPT-J
- [x] GPTBigCode (Starcoder) | @susnato 
- [x] GPT-neo
- [x] GPT-neo-x | @younesbelkada #26463
- [x] OPT | @susnato #26414 
- [x] Llava
- [x] VipLlava
- [x] mBART
- [x] Mistral
- [x] Mixtral
- [ ] MPT | @rajveer43 
- [ ] T5
- [ ] Persimmon | @jeromeku 
- [x] Phi
- [x] Whisper
- [x] Qwen2

... and many more

Adding this feature would require to follow the same protocol as in https://github.com/huggingface/transformers/pull/25598 
 . First create a new module inside the corresponding modeling file termed as `xxxFlashAttention` that inherits from `xxxAttention` and override the foward method to use the public methods from `flash-attn`. Make sure to have access to a GPU that supports Flash Attention 2. 

Given the slight challenge of the issue, labelling it as a good second issue!

If you are interested to take up the challenge, comment below with the architecture name you want to integrate and open a PR!

Once you open a PR, feel free to ping @LysandreJik @ArthurZucker @amyeroberts @younesbelkada @fxmarty @SunMarc @pacman100 for a review

### Motivation

Making LLMs more memory efficient and faster ! 

### Your contribution

Reviewing PRs and possibly adding the support for more models","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",110,open
Registering Models in MLflow Callback,"### Feature request

We can add registering models functionality to MLflow callback so that we can use MLflow model registry with 🤗 models. To do that we can introduce an optional _registered_model_name field to the callback, and can register the model in case these field is not None.


```
class MLflowCallback(TrainerCallback):
    #...
    def setup(self, args, state, model):
        #...
        self._log_artifacts = os.getenv(""HF_MLFLOW_LOG_ARTIFACTS"", ""FALSE"").upper() in ENV_VARS_TRUE_VALUES
        self._registered_model_name = os.getenv(""HF_REGISTERED_MODEL_NAME"") # Suggested Change
        #...
        
    def on_save(self, args, state, control, **kwargs):
        if self._initialized and state.is_world_process_zero and self._log_artifacts:
          ckpt_dir = f""checkpoint-{state.global_step}""
          artifact_path = os.path.join(args.output_dir, ckpt_dir)
          logger.info(f""Logging checkpoint artifacts in {ckpt_dir}. This may take time."")
          self._ml_flow.pyfunc.log_model(
              ckpt_dir,
              artifacts={""model_path"": artifact_path},
              python_model=self._ml_flow.pyfunc.PythonModel(),
              registered_model_name=self._registered_model_name or None  # Suggested Change
          )
```




### Motivation

Model registry is one of the most useful features of MLflow, but current callback doesn't support it. It forces users to make a custom implementation to use this functionality. Instead, we can extend 🤗 MLflow callback to provide this feature.

### Your contribution

I can implement this extension and create a PR.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Streaming and assisted decoding strategy for pipeline OR clearer understanding of pipelines to manually apply tasks,"### Feature request

Currently looking to get a number of non-pipeline functions with pipelines
- Decoder strategies
- Assisted decoding
- Streaming output
etc

**Alternately**, if I can adopt the non-pipeline transformer approach with tokenizer act more pipeline'ish, that'd also be good. Specifically, what are the underlying changes that happen when selecting a task for pipeline? Eg. if I select 'text-generation' as a pipeline, what's that actually doing (so can apply the same thing to the non-pipeline approach)

If for example pipeline_task = ""text-generation"", does this autoconfigure the model and tokenizer config, like sampling, temperature, beams, top-k, top-p, max_new_tokens?... What does the pipeline task actually do to the underlying model? If I knew this, I wouldn't need the above features for pipelines.

### Motivation

I want to have all the features from all the approaches available from the one approach

### Your contribution

Nothing yet","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Synk_Dive,"### Model description

O projeto Synk Dive cria um ambiente de harmonia multisensorial que oferece uma experiência imersiva única, combinando inputs visuais e textuais. Ao fornecer um vídeo como entrada, por exemplo, o sistema gera uma trilha sonora que se adapta às emoções transmitidas em tela. Esta trilha sonora sincroniza-se com as diferentes cenas ao longo do vídeo, resultando em uma experiência que integra elementos visuais e auditivos para intensificar a imersão e a conexão emocional do usuário com o conteúdo. 


### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Make the skip_batches_dataloader used in the Trainer customizable,"### Feature request

In order to allow customizing the skip dataloader, I propose to add a customizable method called `get_skip_dataloader` for example, which by default will contain the current logic but can be changed by inheriting the Trainer.

### Motivation

I am using a the [mosaicML streaming](https://github.com/mosaicml/streaming) framework to train models.
In order to use their mid epoch resumption feature of this framework, you have to:
- use their StreamingDatalaoder, this is already handled by overwriting  `get_train_dataloader`
- save the dataloader state dict. This is can be handled with a callback.
- load the dataloader state dict when resuming training. This can't be done currently because the function responsible for this ( `skip_first_batches` ) of Accelerate is not customizable

I am able to overcome this problem by patching `skip_first_batches`, but on official to do it would be much more comfortable.

### Your contribution

I can provide a PR if you think this is a good idea","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
`align_to_words=True` in `QuestionAnsweringPipeline` can lead to duplicate answers,"### System Info

- `transformers` version: 4.31.0
- Platform: macOS-13.4.1-arm64-arm-64bit
- Python version: 3.11.4
- Huggingface_hub version: 0.15.1
- Safetensors version: 0.3.1
- Accelerate version: 0.21.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.0.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help?

@Nars

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```py
from transformers import pipeline

answers = pipeline(""question-answering"", model=""deepset/tinyroberta-squad2"")(
    question=""Who is the chancellor of Germany?"",
    context=""Angela Merkel was the chancellor of Germany."",
    top_k=10
)
print(answers[0]) # Returns {'score': 0.9961308836936951, 'start': 0, 'end': 13, 'answer': 'Angela Merkel'}
print(answers[5]) # Returns {'score': 7.520078361267224e-05, 'start': 0, 'end': 13, 'answer': 'Angela Merkel'}
```

If `align_to_words` is set to `True` (which is the default), all start or end tokens that are contained in the same word are mapped to the same start and end character index (see [here](https://github.com/huggingface/transformers/blob/37c205eb5d5165b70d3100b599a2bcfc483944f5/src/transformers/pipelines/question_answering.py#L617-L620)). This is expected when using `align_to_words`. However, the top_k filtering happens before this step so duplicate answers can remain.

### Expected behavior

Ideally, the mapping from token to word should happen at around [this point](https://github.com/huggingface/transformers/blob/37c205eb5d5165b70d3100b599a2bcfc483944f5/src/transformers/pipelines/question_answering.py#L139). You would have a start and end probability for each word. If there are multiple tokens in a word, their probabilities should be summed. This would make the probabilities more correct because every token in the word would affect the probability of selecting the word.

If this is too slow, there should at least be a check for duplicates somewhere [here](https://github.com/huggingface/transformers/blob/37c205eb5d5165b70d3100b599a2bcfc483944f5/src/transformers/pipelines/question_answering.py#L604). This would mean that you are not guaranteed to get k answers when setting `top_k`, but only that you get at most k answers. A way to mitigate that somewhat (but not perfectly), would be to use a higher value than top_k when calling `select_starts_ends` [here](https://github.com/huggingface/transformers/blob/37c205eb5d5165b70d3100b599a2bcfc483944f5/src/transformers/pipelines/question_answering.py#L546-L548).","[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNzcxMTg3OTI0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Pipeline', 'name': 'Core: Pipeline', 'color': 'FF7066', 'default': False, 'description': 'Internals of the library; Pipeline.'}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 5818563521, 'node_id': 'LA_kwDOCUB6oc8AAAABWtA7wQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Generation', 'name': 'Generation', 'color': 'C91DB2', 'default': False, 'description': ''}]",16,open
AugViT TensorFlow implementation ,"### Model description

I would like to contribute AugViT Tensorflow implementation to the Transformers library. You can find the implementation in TensorFlow here https://github.com/ushareng/AugViT

I have created Model card here https://huggingface.co/tensorgirl/TFaugvit/tree/main

Kindly let me know if the above is correct.

### Open source status

- [X] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

TensorFlow implementation of AugViT https://github.com/ushareng/AugViT","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
🌐 [i18n-zh-hant] Translating docs to Traditional Chinese (zh-hant),"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the Traditional Chinese-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) 
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) 
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).

## Tutorial section
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)
- [ ] [peft.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md)
<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
A new category for recsys,"### Feature request

A new category in HuggingFace (both in datasets and models) for recommendation systems.

### Motivation

HuggingFace has a rich ecosystem of diverse sets of datasets and models. We have model types ranging from

1. Language Models
2. Graph Models 
3. Vision models
4. Multimodal etc

And same goes for datasets. However, the one significant category that I did not find and is missing is the recommendation system. Recommendation systems are very important for enterprises and it is one of the most interesting dynamic fields in Machine Learning. 

We can support recsys in several ways. There are recsys for tabular data, tabular + NLP data, vision data, etc. And the same goes for models too. And while I just started learning and doing some research on recys, I am seeing that I do not have any SOTA models present in huggingface. 

For example, I can not have any method going like this right now. Where, I do not care much about the candidate generator but simply take a SOTA candidate generator and focus on my ranker model. 

```python
from transformers import RecSysVocab
from transformers import CandidateGen


# this can path to csv or a matrix with user's properties 
user_vocab_lookup_table = RecSysVocab.from_pretrained('/path/to/user.csv')
item_vocab_lookup_table = RecSysVocab.from_pretrained('/path/to/item.csv')

# build the candidate generator model
candidate_gen = CandidateGen.from_pretrained('some-sota-candidate-gen')

# now fit the model
candidate_gen.find_top_k_similarity(
    user_id = ""some user id"",
    user_columns = [...], # a vector with that user's propeties
    user_vocab_lookup_table = user_vocab_lookup_table,
    item_vocab_lookup_table = item_vocab_lookup_table
)
````

The above is a very simple (less accurate) pseudo code, just to have a glimpse of the interface. However it would be awesome to have something specifically for recommendation systems.

### Your contribution

I am not sure, if this issue is never thought of before or not. But it would be awesome on working on this, if the core maintainers and contributors are in the same page. RecSys is very diverse. Some of methods involves sequences well others involves these two stage approaches (candidate gen + filtering). So I feel like some discussion would be required on how to structure the modules and how to create to build those interfaces such that it matches with existing methods of huggingface. 

In terms of my contribution, I can help with these and exited to contribute on this, if I hear back form the community showing similar grounds of interest. Would love to contribute.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
WhisperForCTC,"### Feature request

Request to add WhisperForCTC model.

### Motivation

it would be cool if we had custom WhisperForCTC with Whisper encoder and ctc head, just like Wav2vec2, but since whisper is based on mel spectograms, I think it may  bring better results.

### Your contribution

Here is my implementation, I mostly copied from Wav2vec2ForCTC 
NOTE: there is TODO that needs to be resolved, I didnt test that part, since whisper operates with transposed hidden_states shape

```python


_HIDDEN_STATES_START_POSITION = 2

class ExtendedWhisperConfig(WhisperConfig):
    def __init__(
        self,
        ctc_loss_reduction: str = ""mean"",
        final_dropout: float = 0.0,
        ctc_zero_infinity: bool = False,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.ctc_loss_reduction = ctc_loss_reduction
        self.final_dropout = final_dropout
        self.ctc_zero_infinity = ctc_zero_infinity


class WhisperEncoderForCTC(WhisperPreTrainedModel):
    config_class = ExtendedWhisperConfig

    def __init__(self, config):
        super().__init__(config)

        self.encoder = WhisperEncoder(config)
        self.dropout = nn.Dropout(config.final_dropout)
        if config.vocab_size is None:
            raise ValueError(
                f""You are trying to instantiate {self.__class__} with a configuration that ""
                ""does not define the vocabulary size of the language model head. Please ""
                ""instantiate the model as follows: `WhisperEncoderForCTC.from_pretrained(..., vocab_size=vocab_size)`. ""
                ""or define `vocab_size` of your model's configuration.""
            )
        output_hidden_size = (
            config.output_hidden_size
            if hasattr(config, ""add_adapter"") and config.add_adapter
            else config.hidden_size
        )
        self.lm_head = nn.Linear(output_hidden_size, config.vocab_size)

        # Initialize weights and apply final processing
        self.post_init()

    def freeze_base_model(self):
        """"""
        Calling this function will disable the gradient computation for the base model so that its parameters will not
        be updated during training. Only the classification head will be updated.
        """"""
        for param in self.encoder.parameters():
            param.requires_grad = False

    def forward(
        self,
        input_features: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        labels: Optional[torch.Tensor] = None,
    ) -> Union[Tuple, CausalLMOutput]:
        r""""""
        labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):
            Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to
            the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.
            All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,
            config.vocab_size - 1]`.
        """"""

        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        outputs = self.encoder(
            input_features,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = outputs[0]
        hidden_states = self.dropout(hidden_states)

        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            if labels.max() >= self.config.vocab_size:
                raise ValueError(
                    f""Label values must be <= vocab_size: {self.config.vocab_size}""
                )

           
            attention_mask = (
                attention_mask
                if attention_mask is not None
                else torch.ones_like(input_features.transpose(1, 2), dtype=torch.long)
            )
            # TODO: check if this is correct
            input_lengths = self._get_feat_extract_output_lengths(
                attention_mask.sum(-1)
            ).to(torch.long)

            # assuming that padded tokens are filled with -100
            # when not being attended to
            labels_mask = labels >= 0
            target_lengths = labels_mask.sum(-1)
            flattened_targets = labels.masked_select(labels_mask)

            # ctc_loss doesn't support fp16
            log_probs = nn.functional.log_softmax(
                logits, dim=-1, dtype=torch.float32
            ).transpose(0, 1)
            with torch.backends.cudnn.flags(enabled=False):
                loss = nn.functional.ctc_loss(
                    log_probs,
                    flattened_targets,
                    input_lengths,
                    target_lengths,
                    blank=self.config.pad_token_id,
                    reduction=self.config.ctc_loss_reduction,
                    zero_infinity=self.config.ctc_zero_infinity,
                )

        if not return_dict:
            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]
            return ((loss,) + output) if loss is not None else output

        return CausalLMOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",11,open
UMT5 incredibly slow in generating,"### System Info

- `transformers` version: 4.33.1
- Platform: Linux-5.14.0-284.25.1.el9_2.x86_64-x86_64-with-glibc2.34
- Python version: 3.11.4
- Huggingface_hub version: 0.16.4
- Safetensors version: 0.3.1
- Accelerate version: 0.22.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.0.1+cu117 (True)


### Who can help?

@ArthurZucker and @younesbelkada and @gante

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
import time

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig


if __name__ == ""__main__"":
    timings = {}

    for model_name in (""facebook/mbart-large-50-many-to-one-mmt"", ""google/umt5-small""):
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map={"""": ""cuda""})
        print(model_name, model.num_parameters())
        # google/umt5-small                        306601984
        # facebook/mbart-large-50-many-to-one-mmt 1122990080
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        gen_config = GenerationConfig.from_pretrained(
            model_name,
            max_new_tokens=200,
            max_length=None,
            num_beams=1,
        )
        text = ""I would really like to eat some cookies now.""
        if ""t5"" in model_name:
            text = f""translate English to Dutch: {text}""

        encoded = tokenizer(text, return_tensors=""pt"")
        encoded = {k: v.to(model.device) for k, v in encoded.items()}
        start_time = time.perf_counter_ns()
        for _ in range(100):
            _ = model.generate(**encoded, generation_config=gen_config)

        timings[model_name] = time.perf_counter_ns() - start_time

    for model_name, timings in timings.items():
        print(f""Generation duration for {model_name.split('/')[1]}:\t{timings}"")
        # Generation duration for mbart-large-50-many-to-one-mmt:  22413427363
        # Generation duration for umt5-small:                     207906791077
```

So despite UMT5-small having only about **27%** the number of parameters of the MBART-large model it is **9-10x** slower!

(I also tried with a gc.collect() after each generation.)

### Expected behavior

Faster inference/generation speed. Training is fine so I assume caching of past states is not (correctly) implemented but I might be wrong. This PR on adding caching to T5 by @patrickvonplaten might be related: https://github.com/huggingface/transformers/pull/3682","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",19,open
NLLB-CLIP model implementation,"### Model description

Hi!

I recently trained a CLIP model with an NLLB text encoder to extend CLIP capabilities to 201 languages of the Flores-200 dataset. As far as the implementation goes, it is HF CLIP implementation with an M2M100 encoder from NLLB models. I'm wondering if you'd be interested in having NLLB-CLIP in the library? If yes, I can bring my implementation in accordance with other CLIP models and create a PR.

The link to the paper with description and results - https://arxiv.org/abs/2309.01859

### Open source status

- [x] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5724035499, 'node_id': 'LA_kwDOCUB6oc8AAAABVS3Zqw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Model%20on%20the%20Hub', 'name': 'Model on the Hub', 'color': '9CA0E9', 'default': False, 'description': ''}]",4,open
Add gradient_checkpointing_segment_size,"### Feature request

Currently when we enable gradient checkpointing, e.g. in `LlamaModel`, we call `torch.utils.checkpoint.checkpoint` on every `LlamaDecoderLayer`. As per [Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/pdf/1604.06174.pdf), https://github.com/cybertronai/gradient-checkpointing and hinted by [torch.utils.checkpoint.checkpoint_sequential](https://pytorch.org/docs/stable/checkpoint.html#torch.utils.checkpoint.checkpoint_sequential), we can strike a balance between memory and compute by supporting `gradient_checkpointing_segment_size`.

### Motivation

Gradient checkpointing makes training slow and people enable it mainly to avoid VRAM OOM. It would be great if we can leverage `gradient_checkpointing_segment_size` and make gradient checkpointing configurable.

### Your contribution

Here is a diff to start off with:

```diff
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index bbf229b58..685400ffd 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -1745,6 +1745,16 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         """"""
         return any(hasattr(m, ""gradient_checkpointing"") and m.gradient_checkpointing for m in self.modules())
 
+    def set_gradient_checkpointing_segment_size(self, value: int):
+        """"""
+        Set gradient checkpointing segment size for the current model.
+        """"""
+        if not self.supports_gradient_checkpointing:
+            raise ValueError(f""{self.__class__.__name__} does not support gradient checkpointing."")
+        if not hasattr(self.model, ""_set_gradient_checkpointing_segment_size""):
+            raise ValueError(f""{self.__class__.__name__} does not support configuring gradient checkpointing segment size."")
+        self.apply(partial(self._set_gradient_checkpointing_segment_size, value=value))
+
     def save_pretrained(
         self,
         save_directory: Union[str, os.PathLike],
diff --git a/src/transformers/models/llama/modeling_llama.py b/src/transformers/models/llama/modeling_llama.py
index 5e7a879c0..682e49b21 100644
--- a/src/transformers/models/llama/modeling_llama.py
+++ b/src/transformers/models/llama/modeling_llama.py
@@ -491,6 +491,10 @@ class LlamaPreTrainedModel(PreTrainedModel):
         if isinstance(module, LlamaModel):
             module.gradient_checkpointing = value
 
+    def _set_gradient_checkpointing_segment_size(self, module, value=1):
+        if isinstance(module, LlamaModel):
+            module.gradient_checkpointing_segment_size = value
+
 
 LLAMA_INPUTS_DOCSTRING = r""""""
     Args:
@@ -578,6 +582,7 @@ class LlamaModel(LlamaPreTrainedModel):
         self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
 
         self.gradient_checkpointing = False
+        self.gradient_checkpointing_segment_size = 1
         # Initialize weights and apply final processing
         self.post_init()
 
@@ -689,7 +694,7 @@ class LlamaModel(LlamaPreTrainedModel):
 
             past_key_value = past_key_values[idx] if past_key_values is not None else None
 
-            if self.gradient_checkpointing and self.training:
+            if self.gradient_checkpointing and self.training and (idx % self.gradient_checkpointing_segment_size == 0):
 
                 def create_custom_forward(module):
                     def custom_forward(*inputs):
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index 2b8cb013f..f7d5e87ea 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -1687,6 +1687,9 @@ class Trainer:
         if args.gradient_checkpointing:
             self.model.gradient_checkpointing_enable()
 
+            if args.gradient_checkpointing_segment_size > 1:
+               self.model.set_gradient_checkpointing_segment_size(args.gradient_checkpointing_segment_size)
+
         model = self._wrap_model(self.model_wrapped)
 
         if (is_sagemaker_mp_enabled() or self.is_fsdp_enabled) and resume_from_checkpoint is not None:
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 2f72622d9..ecffc5660 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -586,6 +586,8 @@ class TrainingArguments:
             Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.
         gradient_checkpointing (`bool`, *optional*, defaults to `False`):
             If True, use gradient checkpointing to save memory at the expense of slower backward pass.
+        gradient_checkpointing_segment_size (`int`, *optional*, defaults to `1`):
+            If gradient_checkpointing is True, use gradient checkpointing for every segment size
         include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):
             Whether or not the inputs will be passed to the `compute_metrics` function. This is intended for metrics
             that need inputs, predictions and references for scoring calculation in Metric class.
@@ -1144,6 +1146,12 @@ class TrainingArguments:
             ""help"": ""If True, use gradient checkpointing to save memory at the expense of slower backward pass.""
         },
     )
+    gradient_checkpointing_segment_size: int = field(
+        default=1,
+        metadata={
+            ""help"": ""If gradient_checkpointing is True, use gradient checkpointing for every segment size.""
+        },
+    )
     include_inputs_for_metrics: bool = field(
         default=False, metadata={""help"": ""Whether or not the inputs will be passed to the `compute_metrics` function.""}
     )
@@ -2137,6 +2145,7 @@ class TrainingArguments:
         gradient_accumulation_steps: int = 1,
         seed: int = 42,
         gradient_checkpointing: bool = False,
+        gradient_checkpointing_segment_size: int = 1,
     ):
         """"""
         A method that regroups all basic arguments linked to the training.
@@ -2179,6 +2188,8 @@ class TrainingArguments:
                 parameters.
             gradient_checkpointing (`bool`, *optional*, defaults to `False`):
                 If True, use gradient checkpointing to save memory at the expense of slower backward pass.
+            gradient_checkpointing_segment_size (`int`, *optional*, defaults to `1`):
+                If gradient_checkpointing is True, use gradient checkpointing for every segment size
 
         Example:
 
```","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
More explicit bounding box formats,"### Feature request

I am having a difficult time managing different bounding box formats in this library. For example, the DETR model preprocessor input format is COCO (xywh), but the output of the postprocessor is a pascal_voc (xyxy). Why not keep everything the same format? Furthermore, the documentation does not say which format is which so it's required to dig through the source to identify that information.

### Motivation

Ease of use

### Your contribution

Just a request","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Edit train_dataloader in callbacks,"In certain scenarios, it becomes necessary to modify the train_dataloader after each epoch. This can be managed through callbacks since the train dataloader is passed to the callback function. However, it's worth noting that the dataloader for training is not a variable within the base Trainer class. Instead, an [independent variable called train_dataloader](https://github.com/huggingface/transformers/blob/v4.32.1/src/transformers/trainer.py#L1569) is created at the beginning of training. Unfortunately, when we attempt to make changes to this dataloader within callbacks, it doesn't seem to have the desired effect.

Here's a code example illustrating this issue:
```python
class CustomCallback(TrainerCallback):
    def on_epoch_begin(self, args, state, control, model, tokenizer, **kwargs):
        kwargs['train_dataloader'] = new_dataloader
```
The problem appears to be that even though we try to update train_dataloader within the callback, it doesn't affect the dataloader as expected.

Please share a reference to the data loader utilized for training in the callback, rather than a duplicate, to allow for modifications to the data loader within callbacks. Currently, there is no other method available to make changes to the data loader after each epoch or after a certain number of steps.

To make the train_dataloader editable by callbacks, it would be beneficial to associate it directly with the Trainer class, rather than creating it as an independent variable. This would enable callbacks to modify it effectively, as the current implementation creates a copy that is not affected by changes made within callbacks.


### Who can help?

@muellerzr @pacman100 @Narsil 

### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

```python
class CustomCallback(TrainerCallback):
    def on_epoch_begin(self, args, state, control, model, tokenizer, **kwargs):
        kwargs['train_dataloader'] = new_dataloader
```

### Expected behavior

The dataloader used for training should be replaced by the new_dataloader passed in callbacks","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Batch Decoding of LMs will cause different outputs with different batch size,"### System Info

Transformers=4.31
Torch=2.01
Cuda=11.8
Python=3.10

A100 GPU 80GB

### Who can help?

@ArthurZucker , @younesbelkada , @gante 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Running the following examples will produce different outputs for the first input.

```
from transformers import LlamaForCausalLM, LlamaTokenizer
from transformers import GenerationConfig
import torch

if __name__ == '__main__':
    name = 'yahma/llama-7b-hf'
    tokenizer = LlamaTokenizer.from_pretrained(
        name, 
        padding_side=""left"", 
        trust_remote_code=True)
    tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id

    model = LlamaForCausalLM.from_pretrained(
        name, 
        device_map=""auto"", 
        torch_dtype=torch.bfloat16,
        trust_remote_code=True)

    question = [
        'Can you explain to me what is the concept of deep learning and how it can be applied to NLP?',
        'Where am I supposed to eat dinner',
        'How hard is it to find a doctor in Canada',
        'What is the best price of vegatables',
        'How can somehow be so mean',
        'How can we get the severance pay',
        'What type of president is this?'
        'How is the weather today?'
    ]

    batch = tokenizer(
        question,
        padding=True,
        return_tensors=""pt"",
    )
    with torch.no_grad():
        output_ids = model.generate(
            batch.input_ids.to(model.device),
            attention_mask=batch.attention_mask.to(model.device),
            pad_token_id=tokenizer.pad_token_id,
            generation_config=GenerationConfig(do_sample=False, max_new_tokens=50, trust_remote_code=True)
        )

    output_strs = []
    for output_id in output_ids.tolist()[:4]:
        tmp = tokenizer.decode(output_id[batch.input_ids.shape[-1]:], skip_special_tokens=True)
        output_strs.append(tmp)
        print(tmp)
        print('----------------------------------------------------')


    print('############### Now we decrease the batch size #############################')

    question = [
        'Can you explain to me what is the concept of deep learning and how it can be applied to NLP?',
        'Where am I supposed to eat dinner',
        'How hard is it to find a doctor in Canada',
        'What is the best price of vegatables',
    ]

    batch = tokenizer(
        question,
        padding=True,
        return_tensors=""pt"",
    )
    with torch.no_grad():
        output_ids = model.generate(
            batch.input_ids.to(model.device),
            attention_mask=batch.attention_mask.to(model.device),
            pad_token_id=tokenizer.pad_token_id,
            generation_config=GenerationConfig(do_sample=False, max_new_tokens=50, trust_remote_code=True)
        )

    output_strs = []
    for output_id in output_ids.tolist():
        tmp = tokenizer.decode(output_id[batch.input_ids.shape[-1]:], skip_special_tokens=True)
        output_strs.append(tmp)
        print(tmp)
        print('----------------------------------------------------')
```

### Expected behavior

The produced outputs are supposed to be the same and should not be affected by the batch size. ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",7,open
Support for the custom kernel for Mask2Former and OneFormer,"> And @shivalikasingh95 sure but let's add support for the custom kernel in a separate PR for Mask2Former and OneFormer. We can set it to False by default, and add a boolean attribute for those models.

_Originally posted by @NielsRogge in https://github.com/huggingface/transformers/pull/20993#discussion_r1062224249_

Hello, could I know if there's any plan to add support for the custom kernel for Mask2Former and OneFormer as mentioned there, thanks!","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
POET,"## Train BERT and other large models on smartphones

### Model description

`POET` enables the training of state-of-the-art memory-hungry ML models on smartphones and other edge devices. POET (Private Optimal Energy Training) exploits the twin techniques of integrated tensor rematerialization, and paging-in/out of secondary storage (as detailed in our paper at ICML 2022) to optimize models for training with limited memory. POET's Mixed Integer Linear Formulation (MILP) ensures the solutions are provably optimal! approach enables training significantly larger models on embedded devices while reducing energy consumption while not modifying mathematical correctness of backpropagation. We demonstrate that it is possible to fine-tune both `ResNet-18` and `BERT` within the memory constraints of a Cortex-M class embedded device while outperforming current edge training methods in energy efficiency.



### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

[Implementation](https://github.com/ShishirPatil/poet)
[Paper](https://arxiv.org/abs/2207.07697)
[Author's Website](https://shishirpatil.github.io/poet/)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Add D_Nikud model,"### Model description

Model description
The repository is dedicated to the implementation of our innovative D-Nikud model, which use the TavBERT architecture and Bi-LSTM to predict and apply diacritics (nikud) to Hebrew text. Diacritics play a crucial role in accurately conveying pronunciation and interpretation, making our model an essential tool for enhancing the quality of Hebrew text analysis.

Open source status
 The model implementation is available
 The model weights are available

Provide useful links for the implementation
Github: https://github.com/NadavShaked/D_Nikud
HuggingFaceHub: https://huggingface.co/NadavShaked/D_Nikud (include the model weights at model folder)
model weights: https://drive.google.com/drive/folders/1osK503txvsEWlZASBViSqOiNJMzAlD0F
Paper: https://arxiv.org/abs/2402.00075
train/dev/test data: https://github.com/NadavShaked/D_Nikud_Data


### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

In the readme file there are an explanation how to run the predict, train and eval methods","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",10,open
replace roberta embedding with bge_base,"### Model description

I'm experimenting Funsd like dataset using layoutLmv3, I'm trying to replace roberta embedding with new bge_large, Is it possible to replace the embedding? will it improve the accuracy?

### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Support for context-free-grammars (CFG) to constrain model output,"### Feature request

It would be nice to constrain the model output with a CFG directly when calling `model.generate`.

This is already done by llama.cpp [grammars](https://github.com/ggerganov/llama.cpp#constrained-output-with-grammars)

An example is in this [repo](https://github.com/r2d4/rellm). 

```
prompt = ""ReLLM, the best way to get structured data out of LLMs, is an acronym for ""
pattern = regex.compile(r'Re[a-z]+ L[a-z]+ L[a-z]+ M[a-z]+')
output = complete_re(model=model, 
                     prompt=prompt,
                     pattern=pattern)
```
```
> Realized Logistic Logistics Model
```


Is such a parameter on the roadmap for transformers? 

### Motivation

This can be super useful to make model output parseable within architectures that process the output of an LLM using classical methods. E.g. it can be used to make a model generate valid JSON in every case. 

### Your contribution

Happy to build this with CFGs if it helps! 😄 ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",18,open
include ChannelDimension.NONE + function replicate_channels,"# What does this PR do?

Fixes #25694 

The ViT model currently doesn't support grayscale images with a (height, width) format, leading to preprocessing errors.

This PR addresses the issue with a new replicate_channels function. This function converts images in (height, width) format to a 3-channel RGB format (3, height, width), replicating the grayscale channel across all three RGB channels.

While it's possible to integrate format checks and modifications within each processing function (like resize, rescale, normalize, to_channel_dimension_format, etc.), doing so might affect other modules using these functions. To avoid potential complications, I've opted for a direct solution.

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?
@amyeroberts ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Meta-Transformer,"### Model description

Introducing a groundbreaking research paper that explores the potential of unified multimodal learning, revolutionizing the way we process and integrate diverse data types such as text, images, audio, and more! 🌟
[meta-transformer](https://arxiv.org/pdf/2307.10802.pdf)

### Open source status

- [X] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

https://github.com/invictus717/MetaTransformer","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",7,open
NEW Model GPT-JX,"### Model description

request to add a new model gpt-jx-3b to the transformer library as well as request to create a custom model class, class names an be found in the below .txt files as well as at the below of the description, weights are present in hugging face model hub but made public for now, [""alien-ai/gpt-jx-3b""](https://huggingface.co/alien-ai/gpt-jx-3b) --id_repo. Just a little summary of the model has been provided in the repo.Currently the repo only contains pytorch_model.bin file and no files other than that.

we provide you the code for modelling.py ,configuration.py and tokenization.py in .txt format
[modelling_gptjx.txt](https://github.com/huggingface/transformers/files/12428050/modelling_gptjx.txt)

[config.txt](https://github.com/huggingface/transformers/files/12428124/config.txt)

[tokenization.txt]
(https://github.com/huggingface/transformers/files/12428249/tokenization.txt)

please check the files before proceeding.

Make Sure to Set class names: GPTJXForCausalLM, GPTJXModel, GPTJXConfig, GPTJXTokenizer.

Sorry, we are too busy right now so, i was not able to write the description properly. Please Check it Out Yourself. Please create the new model class as soon as possible

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Authors will be revealed later. repo-- [""alien-ai/gpt-jx-3b""](https://huggingface.co/alien-ai/gpt-jx-3b) ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
tensor size mismatch with larger gradient_accumulation_steps and fewer training data,"### System Info

A100 Nvidia 80G GPU

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

it seems when I have fewer training examples (1000 or so), and when I use a larger gradient_accumulation_steps, (32)
I get tensor size mismatch on Adam gradient update:


```
\""/tmp/jetter.yrcudeja/torch/optim/optimizer.py\"", line 33, in _use_grad\n    ret = func(self, *args, **kwargs)\n  File \""/tmp/jetter.yrcudeja/torch/optim/adamw.py\"", line 173, in step\n    adamw(\n  File \""/tmp/jetter.yrcudeja/torch/optim/adamw.py\"", line 323, in adamw\n    func(\n  File \""/tmp/jetter.yrcudeja/torch/optim/adamw.py\"", line 502, in _multi_tensor_adamw\n    torch._foreach_add_(device_exp_avgs, device_grads, alpha=1 - beta1)\nRuntimeError: The size of tensor a (8192384) must match the size of tensor b (262156288) at non-singleton dimension 0\n"", ""errorTraits"": null, ""timestamp_us"": 1692818123557766}
[4]:  File ""/usr/local/fbcode/platform010/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
[4]:    return _run_code(code, main_globals, None,
[4]:  File ""/usr/local/fbcode/platform010/lib/python3.8/runpy.py"", line 87, in _run_code
[4]:    exec(code, run_globals)
[4]:  File ""/tmp/jetter.12kzp8qf/aml/comment/llama_finetune/train.py"", line 150, in <module>
[4]:    train()
[4]:  File ""/tmp/jetter.12kzp8qf/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
[4]:    return f(*args, **kwargs)
[4]:  File ""/tmp/jetter.12kzp8qf/aml/comment/llama_finetune/train.py"", line 124, in train
[4]:    trainer.train()
[4]:  File ""/tmp/jetter.12kzp8qf/transformers/trainer.py"", line 1664, in train
[4]:    return inner_training_loop(
[4]:  File ""/tmp/jetter.12kzp8qf/transformers/trainer.py"", line 1998, in _inner_training_loop
[4]:    self.optimizer.step()
[4]:  File ""/tmp/jetter.12kzp8qf/torch/optim/lr_scheduler.py"", line 69, in wrapper
[4]:    return wrapped(*args, **kwargs)
[4]:  File ""/tmp/jetter.12kzp8qf/torch/optim/optimizer.py"", line 280, in wrapper
[4]:    out = func(*args, **kwargs)
[4]:  File ""/tmp/jetter.12kzp8qf/torch/optim/optimizer.py"", line 33, in _use_grad
[4]:    ret = func(self, *args, **kwargs)
[4]:  File ""/tmp/jetter.12kzp8qf/torch/optim/adamw.py"", line 173, in step
[4]:    adamw(
[4]:  File ""/tmp/jetter.12kzp8qf/torch/optim/adamw.py"", line 323, in adamw
[4]:    func(
[4]:  File ""/tmp/jetter.12kzp8qf/torch/optim/adamw.py"", line 502, in _multi_tensor_adamw
[4]:    torch._foreach_add_(device_exp_avgs, device_grads, alpha=1 - beta1)
[4]:RuntimeError: The size of tensor a (8192384) must match the size of tensor b (262156288) at non-singleton dimension 0
[7]:ERROR:aiplatform.error_reporting.error_reporting:Exception Found: The size of tensor a (8192384) must match the size of tensor b (262156288) at non-singleton dimension 0
```

using gradient_accumulation_steps=1 fixes it, but then it causes some impact on model quality


command was 

/packages/torchx_python/python

-m torch.distributed.run --rdzv_backend zeus --rdzv_id torchx-llama_finetune_train-k64xgt1h4dt52c --nnodes 4 --nproc_per_node 8 --tee 3 --role  -m aml.comment.llama_finetune.train --local_dir /tmp/users --model_manifold_bucket pi_adv_problems --model_manifold_dir tree/dpa_llama --input_model_filename 7B-converted --output_model_filename yytest__v7_instagram_basic_5e-6 --data_path manifold://pi_adv_problems/tree/appreview_llama/data/v7/train__instagram_basic.json --eval_data_path manifold://pi_adv_problems/tree/appreview_llama/data/v7/eval__instagram_basic.json --data_task generic --prompt_temp normal --processed True --model_max_length 1024 --num_train_epochs 30 --per_device_train_batch_size 2 --per_device_eval_batch_size 8 --gradient_accumulation_steps 32 --evaluation_strategy steps --eval_steps 10 --save_strategy steps --save_steps 200 --save_total_limit 1 --learning_rate 5e-6 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 50 --fsdp full_shard auto_wrap --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --bf16 True --tf32 True

### Expected behavior

note that the above error shows 

yyy@yyy-mbp ~ % echo '262156288/8192384'|bc -l
32.00000000000000000000

so somehow it seems only one gradient is obtained while maybe 32 are expected?",[],8,open
ValueError: Unsupported number of image dimensions: 2 - An error during embedding Image data,"### System Info

I am facing an issue during encoding image dataset using facebook/dino-vits16, I faced this issue with grayscale images before too but it worked well with Bingsu/Human_Action_Recognition dataset. 
Versions
```
transformers==4.32.0
torch==2.0.1+cu118
datasets==2.14.4
```


The error:

```
Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits16 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 0%
2/10000 [00:00<40:18, 4.13 examples/s]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
[<ipython-input-30-0547920c10ef>](https://localhost:8080/#) in <cell line: 22>()
     20     return batch
     21 
---> 22 dataset_train = dataset_train.map(get_embeddings)

8 frames
[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    590             self: ""Dataset"" = kwargs.pop(""self"")
    591         # apply actual function
--> 592         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    593         datasets: List[""Dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    594         for dataset in datasets:

[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    555         }
    556         # apply actual function
--> 557         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    558         datasets: List[""Dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    559         # re-apply format to the output

[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)
   3095                     desc=desc or ""Map"",
   3096                 ) as pbar:
-> 3097                     for rank, done, content in Dataset._map_single(**dataset_kwargs):
   3098                         if done:
   3099                             shards_done += 1

[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in _map_single(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)
   3448                     _time = time.time()
   3449                     for i, example in shard_iterable:
-> 3450                         example = apply_function_on_filtered_inputs(example, i, offset=offset)
   3451                         if update_data:
   3452                             if i == 0:

[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in apply_function_on_filtered_inputs(pa_inputs, indices, check_same_num_examples, offset)
   3351             if with_rank:
   3352                 additional_args += (rank,)
-> 3353             processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
   3354             if isinstance(processed_inputs, LazyDict):
   3355                 processed_inputs = {

[<ipython-input-30-0547920c10ef>](https://localhost:8080/#) in get_embeddings(batch)
     14 
     15 def get_embeddings(batch):
---> 16     inputs = processor(images=batch['image'], return_tensors=""pt"").to(device)
     17     with torch.no_grad():
     18         outputs = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()

[/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py](https://localhost:8080/#) in __call__(self, images, **kwargs)
    544     def __call__(self, images, **kwargs) -> BatchFeature:
    545         """"""Preprocess an image or a batch of images.""""""
--> 546         return self.preprocess(images, **kwargs)
    547 
    548     def preprocess(self, images, **kwargs) -> BatchFeature:

[/usr/local/lib/python3.10/dist-packages/transformers/models/vit/image_processing_vit.py](https://localhost:8080/#) in preprocess(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, **kwargs)
    232         if input_data_format is None:
    233             # We assume that all images have the same channel dimension format.
--> 234             input_data_format = infer_channel_dimension_format(images[0])
    235 
    236         if do_resize:

[/usr/local/lib/python3.10/dist-packages/transformers/image_utils.py](https://localhost:8080/#) in infer_channel_dimension_format(image, num_channels)
    168         first_dim, last_dim = 1, 3
    169     else:
--> 170         raise ValueError(f""Unsupported number of image dimensions: {image.ndim}"")
    171 
    172     if image.shape[first_dim] in num_channels:

ValueError: Unsupported number of image dimensions: 2
```

### Who can help?

@amyeroberts

### Information

- [x] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```
from transformers import ViTImageProcessor, ViTModel
from datasets import load_dataset, Dataset
import torch

dataset_train = load_dataset(
    'ashraq/fashion-product-images-small', split='train[:10000]'
)


device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
processor = ViTImageProcessor.from_pretrained('facebook/dino-vits16')
model = ViTModel.from_pretrained('facebook/dino-vits16')


def get_embeddings(batch):
    inputs = processor(images=batch['image'], return_tensors=""pt"").to(device)
    with torch.no_grad():
        outputs = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()
    batch['embeddings'] = outputs
    return batch

dataset_train = dataset_train.map(get_embeddings)

```

### Expected behavior

Expected behavior was to obtaining embeddings.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",6,open
Transformers documentation translation to Macedonian,"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the <languageName>-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @ArthurZucker, @sgugger for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) 
- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md)
- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).

## Tutorial section
- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)
- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.md)
- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)
- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)
- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)
- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)
- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Make 🤗Transformers tests device agnostic,"### Feature request

We would like to make the testing suite in this repository more device agnostic. It seems there has already been some work towards this, however the majority of tests will still only run on either GPU or CPU. This would require a number of changes to all tests present in the library, however it would not alter the behaviour of Huggingface's CI runners.

A non-exhaustive list of changes would be:
- Add a new test decorator `@require_torch_with_accelerator` that largerly supersedes (but does not replace) `@require_torch_gpu`. This new decorator can be used for any test that is device agnostic that we would like to accelerate. We would keep `@require_torch_gpu` for tests that truly require CUDA features, such as ones that check device memory utilisation (such as in model parallelism or lower precision tests) or use custom CUDA kernels (such as Flash Attention).
- Certain tests could be made device agnostic quite easily, such as tests that only check for CUDA devices to enable fp16, tests that use backend specific PRNG initialisation, or tests that clear cache before executing. This could be done by adding device agnostic variants to `testing_utils.py` that compare the current device in use and dispatch to the appropriate backend specific function if available.
    - For example, rather than the comparison `torch_device == 'cuda'` to check if we can run with fp16, we could call a function `testing_utils.accelerator_is_fp16_available(torch_device)` or similar. Similar functions already exist to check for tf32 or bf16 support.
    - Crucially, in upstream we would only have settings for CUDA and CPU devices – as well as any other of your supported backends. However, we would expose functions to register your own device in user code so third parties can test custom backends without upstreaming changes.

### Motivation

As Huggingface libraries and models make up a significant part of the current ML community, it makes sense when developing custom PyTorch backends to test against these model libraries as they cover a large proportion of the most users' use cases.

However, the current testing suite does not easily allow for custom devices – not without maintaining a custom private fork that needs to be continuously kept up to date with the upstream repository. This reason, and because the number of changes required is not especially significant, is why we are making this proposal.

### Your contribution

We would write and submit a PR to implement these changes following discussion and approval with 🤗Transformers maintainers.

I am collaborating with @joshlk and @arsalanu","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",13,open
TimeSeriesTransformerForPrediction model unused parameters Runtime error in Distributed environment,"### System Info

transformers==4.31
accelerate==0.21

I'm trying to run TimeSeriesTransformerForPrediction in a distributed environment based on the following notebook: https://huggingface.co/blog/time-series-transformers

If I run the notebook as is I get the following error immediately after the first loss is calculated: 
```
 Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
```

If I modify the accelerator in the following way:
```
ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])
```
It will usually run for many iterations but it randomly fails with an error message similar to above.

The code appears to run fine in cpu mode or on GPU in a non-distributed environment.

### Who can help?

_No response_

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Run the following notebook in a distributed GPU environment: https://huggingface.co/blog/time-series-transformers

### Expected behavior

I would expect the model to train and perform `.backward(loss)` without error.","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",4,open
Add Context AutoEncoder (CAE),"### Model description

**The corresponding paper has been accepted by International Journal of Computer Vision (IJCV).**

We present a novel masked image modeling (MIM) approach, context autoencoder (CAE), for self-supervised representation pretraining. We pretrain an encoder by making predictions in the encoded representation space. The pretraining tasks include two tasks: masked representation prediction - predict the representations for the masked patches, and masked patch reconstruction - reconstruct the masked patches. The network is an encoder-regressor-decoder architecture: the encoder takes the visible patches as input; the regressor predicts the representations of the masked patches, which are expected to be aligned with the representations computed from the encoder, using the representations of visible patches and the positions of visible and masked patches; the decoder reconstructs the masked patches from the predicted encoded representations. The CAE design encourages the separation of learning the encoder (representation) from completing the pertaining tasks: masked representation prediction and masked patch reconstruction tasks, and making predictions in the encoded representation space empirically shows the benefit to representation learning. We demonstrate the effectiveness of our CAE through superior transfer performance in downstream tasks: semantic segmentation, object detection and instance segmentation, and classification.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Code link: https://github.com/Atten4Vis/CAE
Author: @charlesCXK
Paper link: https://arxiv.org/abs/2202.03026","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Add MovieChat Model,"### Model description

MovieChat proposes a Vision Foundation model + LLM + Long short-term memory-based solution to long-range video understanding addressing computation, memory, and long-range temporal understanding challenges using transformer model tokens as a fixed amount of memory.
Project Page: https://rese1f.github.io/MovieChat/

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/abs/2307.16449
Github: https://github.com/rese1f/MovieChat","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",10,open
SwahBERT,"### Model description

<html>
<body>
<!--StartFragment--><h1 tabindex=""-1"" dir=""auto"" style=""box-sizing: border-box; font-size: 2em; margin-top: 0px !important; margin-right: 0px; margin-bottom: 16px; margin-left: 0px; font-weight: var(--base-text-weight-semibold, 600); line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted)); color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">SwahBERT: Language model of Swahili</h1><p dir=""auto"" style=""box-sizing: border-box; margin-top: 0px; margin-bottom: 16px; color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Is a pretrained monolingual language model for Swahili.<br style=""box-sizing: border-box;"">The model was trained for 800K steps using a corpus of 105MB that was collected from news sites, online discussion, and Wikipedia.<br style=""box-sizing: border-box;"">The evaluation was perfomed on several downstream tasks such as emotion classification, news classification, sentiment classification, and Named entity recognition.</p><div class=""highlight highlight-source-ruby notranslate position-relative overflow-auto"" dir=""auto"" style=""box-sizing: border-box; position: relative !important; overflow: auto !important; margin-bottom: 16px; color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;""><pre style=""box-sizing: border-box; font-family: ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; margin-top: 0px; margin-bottom: 0px; overflow-wrap: normal; padding: 16px; overflow: auto; line-height: 1.45; color: var(--fgColor-default, var(--color-fg-default)); background-color: var(--bgColor-muted, var(--color-canvas-subtle)); border-radius: 6px; word-break: normal;""><span class=""pl-en"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-entity);"">import</span> <span class=""pl-en"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-entity);"">torch</span>
<span class=""pl-en"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-entity);"">from</span> <span class=""pl-en"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-entity);"">transformers</span> <span class=""pl-en"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-entity);"">import</span> <span class=""pl-v"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-variable);"">BertTokenizer</span>

<span class=""pl-s1"" style=""box-sizing: border-box;"">tokenizer</span> <span class=""pl-c1"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-constant);"">=</span> <span class=""pl-v"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-variable);"">BertTokenizer</span><span class=""pl-kos"" style=""box-sizing: border-box;"">.</span><span class=""pl-en"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-entity);"">from_pretrained</span><span class=""pl-kos"" style=""box-sizing: border-box;"">(</span><span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">""swahbert-base-uncased""</span><span class=""pl-kos"" style=""box-sizing: border-box;"">)</span>

<span class=""pl-c"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-comment);""># Tokenized input</span>
<span class=""pl-s1"" style=""box-sizing: border-box;"">text</span> <span class=""pl-c1"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-constant);"">=</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">""Mlima Kilimanjaro unapatikana Tanzania""</span>
<span class=""pl-s1"" style=""box-sizing: border-box;"">tokenized_text</span> <span class=""pl-c1"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-constant);"">=</span> <span class=""pl-s1"" style=""box-sizing: border-box;"">tokenizer</span><span class=""pl-kos"" style=""box-sizing: border-box;"">.</span><span class=""pl-en"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-entity);"">tokenize</span><span class=""pl-kos"" style=""box-sizing: border-box;"">(</span><span class=""pl-s1"" style=""box-sizing: border-box;"">text</span><span class=""pl-kos"" style=""box-sizing: border-box;"">)</span>

<span class=""pl-en"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-entity);"">SwahBERT</span> <span class=""pl-c1"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-constant);"">=&gt;</span> <span class=""pl-kos"" style=""box-sizing: border-box;"">[</span><span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'mlima'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'kilimanjaro'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'unapatikana'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'tanzania'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">]</span>
<span class=""pl-en"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-entity);"">mBERT</span> <span class=""pl-c1"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-constant);"">=&gt;</span> <span class=""pl-kos"" style=""box-sizing: border-box;"">[</span><span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'ml'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'##ima'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'ki'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'##lima'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'##nja'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'##ro'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'una'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'##patikana'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'tan'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">,</span> <span class=""pl-s"" style=""box-sizing: border-box; color: var(--color-prettylights-syntax-string);"">'##zania'</span><span class=""pl-kos"" style=""box-sizing: border-box;"">]</span></pre><div class=""zeroclipboard-container position-absolute right-0 top-0"" style=""box-sizing: border-box; position: absolute !important; top: 0px !important; right: 0px !important;""><clipboard-copy aria-label=""Copy"" class=""ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay"" data-copy-feedback=""Copied!"" data-tooltip-direction=""w"" value=""import torch
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(&quot;swahbert-base-uncased&quot;)

# Tokenized input
text = &quot;Mlima Kilimanjaro unapatikana Tanzania&quot;
tokenized_text = tokenizer.tokenize(text)

SwahBERT => ['mlima', 'kilimanjaro', 'unapatikana', 'tanzania']
mBERT => ['ml', '##ima', 'ki', '##lima', '##nja', '##ro', 'una', '##patikana', 'tan', '##zania']
"" tabindex=""0"" role=""button"" style=""box-sizing: border-box; position: relative; display: inline-block; padding: 0px !important; font-size: 14px; font-weight: var(--base-text-weight-medium, 500); line-height: 20px; white-space: nowrap; vertical-align: middle; cursor: pointer; user-select: none; border-width: 1px; border-style: solid; border-color: var(--button-default-borderColor-rest, var(--color-btn-border)); border-image: initial; border-radius: 6px; appearance: none; color: var(--button-default-fgColor-rest, var(--color-btn-text)); background-color: var(--button-default-bgColor-rest, var(--color-btn-bg)); box-shadow: var(--button-default-shadow-resting, var(--color-btn-shadow)),var(--button-default-shadow-inset, var(--color-btn-inset-shadow)); transition: color 80ms cubic-bezier(0.33, 1, 0.68, 1) 0s, background-color, box-shadow, border-color; margin: var(--base-size-8, 8px) !important;""><svg aria-hidden=""true"" height=""16"" viewBox=""0 0 16 16"" version=""1.1"" width=""16"" data-view-component=""true"" class=""octicon octicon-copy js-clipboard-copy-icon m-2""><path d=""M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z""></path><path d=""M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z""></path></svg></clipboard-copy></div></div><h2 tabindex=""-1"" dir=""auto"" style=""box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.5em; font-weight: var(--base-text-weight-semibold, 600); line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted)); color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;""><a id=""user-content-pre-training-data"" class=""anchor"" aria-hidden=""true"" href=""https://github.com/gatimartin/SwahBERT#pre-training-data"" style=""box-sizing: border-box; background-color: transparent; color: var(--fgColor-accent, var(--color-accent-fg)); text-decoration: none; float: left; padding-right: 4px; margin-left: -20px; line-height: 1; position: absolute;""><svg class=""octicon octicon-link"" viewBox=""0 0 16 16"" version=""1.1"" width=""16"" height=""16"" aria-hidden=""true""><path d=""m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z""></path></svg></a>Pre-training data</h2><p dir=""auto"" style=""box-sizing: border-box; margin-top: 0px; margin-bottom: 16px; color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">The text was extracted from different sorces;<br style=""box-sizing: border-box;""></p><ul dir=""auto"" style=""box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;""><li style=""box-sizing: border-box;"">News sites:<span> </span><code style=""box-sizing: border-box; font-family: ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; padding: 0.2em 0.4em; margin: 0px; white-space: break-spaces; background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted)); border-radius: 6px;"">United Nations news, Voice of America (VoA), Deutsche Welle (DW) and taifaleo</code><br style=""box-sizing: border-box;""></li><li style=""box-sizing: border-box; margin-top: 0.25em;"">Forums:<span> </span><code style=""box-sizing: border-box; font-family: ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; padding: 0.2em 0.4em; margin: 0px; white-space: break-spaces; background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted)); border-radius: 6px;"">JaiiForum</code><br style=""box-sizing: border-box;""></li><li style=""box-sizing: border-box; margin-top: 0.25em;""><code style=""box-sizing: border-box; font-family: ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; padding: 0.2em 0.4em; margin: 0px; white-space: break-spaces; background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted)); border-radius: 6px;"">Wikipedia</code>.</li></ul><h2 tabindex=""-1"" dir=""auto"" style=""box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.5em; font-weight: var(--base-text-weight-semibold, 600); line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted)); color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;""><a id=""user-content-pre-trained-models"" class=""anchor"" aria-hidden=""true"" href=""https://github.com/gatimartin/SwahBERT#pre-trained-models"" style=""box-sizing: border-box; background-color: transparent; color: var(--fgColor-accent, var(--color-accent-fg)); text-decoration: none; float: left; padding-right: 4px; margin-left: -20px; line-height: 1; position: absolute;""><svg class=""octicon octicon-link"" viewBox=""0 0 16 16"" version=""1.1"" width=""16"" height=""16"" aria-hidden=""true""><path d=""m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z""></path></svg></a>Pre-trained Models</h2><p dir=""auto"" style=""box-sizing: border-box; margin-top: 0px; margin-bottom: 16px; color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Download the models here:<br style=""box-sizing: border-box;""></p><ul dir=""auto"" style=""box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;""><li style=""box-sizing: border-box;""><strong style=""box-sizing: border-box; font-weight: var(--base-text-weight-semibold, 600);""><a href=""https://drive.google.com/drive/folders/1HZTCqxt93F5NcvgAWcbrXZammBPizdxF?usp=sharing"" rel=""nofollow"" style=""box-sizing: border-box; background-color: transparent; color: var(--fgColor-accent, var(--color-accent-fg)); text-decoration: none;""><code style=""box-sizing: border-box; font-family: ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; padding: 0.2em 0.4em; margin: 0px; white-space: break-spaces; background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted)); border-radius: 6px;"">SwahBERT-Base, Uncased</code></a></strong>:12-layer, 768-hidden, 12-heads , 124M parameters</li><li style=""box-sizing: border-box; margin-top: 0.25em;""><strong style=""box-sizing: border-box; font-weight: var(--base-text-weight-semibold, 600);""><a href=""https://drive.google.com/drive/folders/1cCcPopqTyzY6AnH9quKcT9kG5zH7tgEZ?usp=sharing"" rel=""nofollow"" style=""box-sizing: border-box; background-color: transparent; color: var(--fgColor-accent, var(--color-accent-fg)); text-decoration: none;""><code style=""box-sizing: border-box; font-family: ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; padding: 0.2em 0.4em; margin: 0px; white-space: break-spaces; background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted)); border-radius: 6px;"">SwahBERT-Base, Cased</code></a></strong>:12-layer, 768-hidden, 12-heads , 111M parameters</li></ul>

Steps | vocab size | MLM acc | NSP acc | loss
-- | -- | -- | -- | --
800K | 50K (uncased) | 76.54 | 99.67 | 1.0667
800K | 32K (cased) | 76.94 | 99.33 | 1.0562

<h2 tabindex=""-1"" dir=""auto"" style=""box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.5em; font-weight: var(--base-text-weight-semibold, 600); line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted)); color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;""><a id=""user-content-citation"" class=""anchor"" aria-hidden=""true"" href=""https://github.com/gatimartin/SwahBERT#citation"" style=""box-sizing: border-box; background-color: transparent; color: var(--fgColor-accent, var(--color-accent-fg)); text-decoration: none; float: left; padding-right: 4px; margin-left: -20px; line-height: 1; position: absolute;""><svg class=""octicon octicon-link"" viewBox=""0 0 16 16"" version=""1.1"" width=""16"" height=""16"" aria-hidden=""true""><path d=""m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z""></path></svg></a>Citation</h2><p dir=""auto"" style=""box-sizing: border-box; margin-top: 0px; margin-bottom: 16px; color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Please use the following citation if you use the model or dataset:</p><div class=""snippet-clipboard-content notranslate position-relative overflow-auto"" style=""box-sizing: border-box; position: relative !important; overflow: auto !important; margin-bottom: 0px !important; color: rgb(31, 35, 40); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, &quot;Noto Sans&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;""><pre class=""notranslate"" style=""box-sizing: border-box; font-family: ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; margin-top: 0px; margin-bottom: 16px; overflow-wrap: normal; padding: 16px; overflow: auto; line-height: 1.45; color: var(--fgColor-default, var(--color-fg-default)); background-color: var(--bgColor-muted, var(--color-canvas-subtle)); border-radius: 6px;""><code style=""box-sizing: border-box; font-family: ui-monospace, SFMono-Regular, &quot;SF Mono&quot;, Menlo, Consolas, &quot;Liberation Mono&quot;, monospace; font-size: 13.6px; padding: 0px; margin: 0px; white-space: pre; background: transparent; border-radius: 6px; word-break: normal; border: 0px; display: inline; overflow: visible; line-height: inherit; overflow-wrap: normal;"">@inproceedings{martin-etal-2022-swahbert,
    title = ""{S}wah{BERT}: Language Model of {S}wahili"",
    author = ""Martin, Gati  and Mswahili, Medard Edmund  and Jeong, Young-Seob  and Woo, Jiyoung"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.23"",
    pages = ""303--313""
    }</code></pre></div><!--EndFragment-->
</body>
</html>
https://github.com/gatimartin/SwahBERT/blob/main/README.mdSwahBERT: Language model of Swahili
Is a pretrained monolingual language model for Swahili.
The model was trained for 800K steps using a corpus of 105MB that was collected from news sites, online discussion, and Wikipedia.
The evaluation was perfomed on several downstream tasks such as emotion classification, news classification, sentiment classification, and Named entity recognition.

import torch
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(""swahbert-base-uncased"")

# Tokenized input
text = ""Mlima Kilimanjaro unapatikana Tanzania""
tokenized_text = tokenizer.tokenize(text)

SwahBERT => ['mlima', 'kilimanjaro', 'unapatikana', 'tanzania']
mBERT => ['ml', '##ima', 'ki', '##lima', '##nja', '##ro', 'una', '##patikana', 'tan', '##zania']
Pre-training data
The text was extracted from different sorces;

News sites: United Nations news, Voice of America (VoA), Deutsche Welle (DW) and taifaleo
Forums: JaiiForum
Wikipedia.
Pre-trained Models
Download the models here:

[SwahBERT-Base, Uncased](https://drive.google.com/drive/folders/1HZTCqxt93F5NcvgAWcbrXZammBPizdxF?usp=sharing):12-layer, 768-hidden, 12-heads , 124M parameters
[SwahBERT-Base, Cased](https://drive.google.com/drive/folders/1cCcPopqTyzY6AnH9quKcT9kG5zH7tgEZ?usp=sharing):12-layer, 768-hidden, 12-heads , 111M parameters
Steps	vocab size	MLM acc	NSP acc	loss
800K	50K (uncased)	76.54	99.67	1.0667
800K	32K (cased)	76.94	99.33	1.0562
Emotion Dataset
We released the [Swahili emotion dataset](https://github.com/gatimartin/SwahBERT/tree/main/emotion_dataset).
The data consists of ~13K emotion annotated comments from social media platforms and translated English dataset.
The data is multi-label with six Ekman’s emotions: happy, surprise, sadness, fear, anger, and disgust or neutral.

Evaluation
The model was tested on four downstream tasks including our new emotion dataset

F1-score of language models on downstream tasks

Tasks	SwahBERT	SwahBERT_cased	mBERT
Emotion	64.46	64.77	60.52
News	90.90	89.90	89.73
Sentiment	70.94	71.12	67.20
NER	88.50	88.60	89.36
Citation
Please use the following citation if you use the model or dataset:

@inproceedings{martin-etal-2022-swahbert,
    title = ""{S}wah{BERT}: Language Model of {S}wahili"",
    author = ""Martin, Gati  and Mswahili, Medard Edmund  and Jeong, Young-Seob  and Woo, Jiyoung"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.23"",
    pages = ""303--313""
    }

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Please use the following citation if you use the model or dataset:

@inproceedings{martin-etal-2022-swahbert,
title = ""{S}wah{BERT}: Language Model of {S}wahili"",
author = ""Martin, Gati and Mswahili, Medard Edmund and Jeong, Young-Seob and Woo, Jiyoung"",
booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
month = jul,
year = ""2022"",
address = ""Seattle, United States"",
publisher = ""Association for Computational Linguistics"",
url = ""https://aclanthology.org/2022.naacl-main.23"",
pages = ""303--313""
}","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Andromeda,"### Model description

## **Andromeda Specs**: Unveiling Mastery

**Overview**
Elegantly marrying craftsmanship and technology, Andromeda is not just another step in AI evolution. It's a giant leap. Driven by precision, powered by innovation, and defined by excellence, Andromeda is the epitome of intelligence realized. Here, we detail the marvel that is Andromeda, in numbers, facts, and logic.

---

### **Specifications**

| **Feature**                                  | **Specification**                             |
|----------------------------------------------|-----------------------------------------------|
| **Sequence Handling**                        | Ultra Long (32,000 - 200,000+ context lengths)|
| **Processing Speed**                         | Ultra Fast (32,000+ tokens in < 100ms)        |
| **Reasoning Abilities**                      | Creativity, Quantitative                                      |
| **Attention Mechanism**                      | Flash Attention 2.0 Triton                    |
| **Memory Consumption** (compared to GPT-3)   | 100x Less                                      |
| **Memory Consumption** (compared to LLAMA)   | 30x Less                                      |
| **Max Sequence Processing Speed**            | 100,000+ sequences in < 300ms                 |
| **Dataset Strategy**                         | Books, Falcon, Redpajama, Math, Code                              |
| **Functionality**                            | FSDP, HF Accelerate,  Poetry Composition, API Calls, and more       |

---

### **Benchmarks**
**Speed**: At the heart of Andromeda's unparalleled capabilities is its raw speed. Leveraging the prowess of Flash Attention 2.0 Triton, it doesn't merely process data; it blazes through it. This power allows it to consume 50x less memory than its predecessor, GPT-3, and 10x less than LLAMA.

---

### **Why Andromeda?**
- **Performance**: Andromeda isn't about doing things faster; it's about doing them the best. Reliable processing of sequences, even as extensive as 100,000+ lengths, is realized in the blink of an eye, under 300ms.
  
- **Precision and Creativity**: The dataset strategy is no mere algorithm. It's a symphony, meticulously crafted to offer both creativity and quantitative reasoning.
  
- **Versatility**: Andromeda doesn't just compute; it contemplates. Whether you need the flair of a poet or the precision of an API call, Andromeda delivers, seamlessly.

---

### **Andromeda Principles**
- **Efficiency**: It's not just about doing more; it's about doing better. Techniques like attention flashing, rotary position encodings, and deep normalization ensure every cycle, every operation, every byte is optimized for performance.
  
- **Flexibility**: In the ever-evolving world of technology, adaptability is king. Andromeda is designed to mold, adapt, and excel, irrespective of the task or domain.
  
- **Scalability**: Grow with you, for you. Andromeda isn't static. It's dynamic, designed to scale, accommodating growing resources and expanding data sizes.
  
- **Community-Driven**: Behind Andromeda's machine brain is the human heart of the community. It doesn't just utilize open source; it thrives on it, constantly evolving, learning, and improving with contributions from around the world.


For enthusiasts, developers, and thinkers looking to dive deeper, the Model Architecture documentation offers an exhaustive, detailed view into the intricate marvel that is Andromeda. Dive in, and witness engineering and artistry in harmony.

---

### **Andromeda: A Detailed Technical Overview**

At the intersection of technological ingenuity and groundbreaking design principles, Andromeda emerges. Representing the zenith of years of research and development, it promises a transformative leap in AI performance, efficiency, and versatility. In this technical specifications document, we deconstruct the intricacies of Andromeda, presenting a meticulous overview of its structure, performance metrics, and underlying methodologies.

## **Feature Insights**

### **Alibi Positional Bias**
Empowering Andromeda to discern relative positions between tokens, this feature accentuates its ability to grasp intricate relationships within a sequence.

### **Rotary Position Encodings (xpos)**
This is a revolutionary means of encoding positions, shrinking the model's memory demands and propelling training speeds.

### **Flash Attention**
This is the linchpin of Andromeda's speed prowess, minimizing attention computations, thus boosting training and inference phases.

### **Deep Normalization (deepnorm)**
By normalizing activations, deep normalization shores up training stability, allowing Andromeda to identify intricate patterns with finesse.

## **Feature Insights (Contd.)**

### **Attn One KV Head (Multiquery Attention)**
A breakthrough in attention mechanism design, this feature allows for simultaneous computation of multiple queries against the same set of key-values, fostering speed and efficiency.

### **QK Norm & Attention QK Norm**
These two features introduce a normalization step in the query and key matrices. This step facilitates stabilization in the attention mechanism, rendering it more robust and enabling it to scale with larger input sizes.

### **Attention QK Norm Dimension Scale**
A sophisticated adjustment to the attention mechanism, it modulates the normalization scale in accordance to the dimensions of the model. The result is a more adaptive and responsive attention framework.

### **Embedding Provider**
At the foundation of Andromeda, this module facilitates the embedding process, converting token sequences into dense vectors. Tailored for Andromeda, it ensures rapid and efficient embedding processes.

---

## **Deeper Dive: Model Parameters**

Unpacking Andromeda means diving deep into the parameters that shape its capabilities. Here's a granular view:

| **Parameter**                           | **Description**                                                                                                                                                                           | **Default Value** |
|-----------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|
| **num_tokens**                          | Total number of tokens in the vocabulary.                                                                                                                                                | 50432             |
| **max_seq_len**                         | Maximum sequence length the model can process.                                                                                                                                           | 8192              |
| **dim**                                 | Dimension size of the model. It represents the size of embeddings and general depth in neural layers.                                                                                    | 2560              |
| **depth**                               | Represents the number of transformer layers in the architecture.                                                                                                                         | 32                |
| **dim_head**                            | Dimension size of each head in multi-head attention mechanism.                                                                                                                           | 128               |
| **heads**                               | Total number of heads in multi-head attention.                                                                                                                                           | 24                |
| **use_abs_pos_emb**                     | Boolean flag to determine if absolute positional embeddings are used.                                                                                                                     | False             |
| **alibi_pos_bias**                      | Enables the alibi positional bias in attention mechanisms.                                                                                                                               | True              |
| **alibi_num_heads**                     | Specifies the number of heads for the alibi positional bias.                                                                                                                             | 12                |
| **rotary_xpos**                         | Determines if rotary positional encodings are utilized.                                                                                                                                  | True              |
| **attn_flash**                          | Flag to activate the Flash Attention mechanism, minimizing computations in the attention phase.                                                                                          | True              |
| **shift_tokens**                        | The number of tokens by which input sequences are shifted. Essential for certain sequence-to-sequence tasks.                                                                             | 1                 |
| **attn_one_kv_head**                    | Activates multiquery attention by computing multiple queries against a singular key-value pair.                                                                                          | True              |
| **qk_norm**                             | Enables the query-key normalization mechanism in the attention phase.                                                                                                                    | True              |
| **attn_qk_norm**                        | A more advanced version of query-key normalization that scales according to the model's dimensions.                                                                                      | True              |
| **attn_qk_norm_dim_scale**              | Modulates the scale of the aforementioned attention normalization based on the model's dimensionality.                                                                                  | True              |
| **embedding_provider**                  | The module responsible for providing embeddings. Custom providers can be passed for tailored embedding processes.                                                                       | AndromedaEmbedding|

---


## **Insights and Techniques**

#### **1. Floating-Point Operations (FLOPs)**
Considering the number of FLOPs is paramount. It provides a metric to gauge the computational intensity and, by extension, the potential speed of the model.

#### **2. Flash Attention 2.0 Triton**
Enhanced with CUDA, this method offers a significant surge in the number of FLOPs the model can handle, amplifying its overall efficiency.

#### **3. Mixed Precision Training**
By embracing mixed precision, Andromeda realizes a noteworthy uptick in training speed while achieving commendable memory efficiency.

#### **4. Deepspeed 3 with NVMe Integration**
This powerful combination paves the way for superlative optimization during the training phase.

#### **5. 8-bit Optimizer**
Further pushing the boundaries of speed, the 8-bit optimizer boosts processing times without compromising the integrity of results.

#### **6. Gradient Clipping**
This technique has been integrated into the training regimen, achieving a massive speedup and preventing undesirable spikes during the process.

#### **7. Advanced Techniques: XPOS, ALIBI, QK Layernorm**
These sophisticated techniques are harnessed for superior extrapolation, interpolation, and stabilization during training.

#### **8. Multi Query Attention**
This approach has been adopted to supercharge decoding speeds.

#### **9. Parallelized Transformer Blocks**
Ensuring that the model's performance is consistently high, these blocks run in tandem to provide a smooth and efficient operational experience.

#### **10. Shifted Tokens**
In a strategic move, Andromeda sidesteps traditional positional embeddings, relying instead on shifted tokens for sequence length progression.

#### **11. Positional Interpolation**
This innovative technique augments the model's ability to manage sequences more effectively.

#### **12. Optimized CUDA Embedding Function**
This function is tailored for peak performance, ensuring rapid and accurate computations.

#### **13. Nebula Loss Function**
Integrated into Andromeda, this polymorphic loss function is adept at handling multi-task training scenarios.

## **A Word on Optimization and Future Iterations**

As with any state-of-the-art model, Andromeda's design is an ever-evolving tapestry. This means iterative refinement. As feedback streams in and technology progresses, expect advancements in:

- **Model Pruning**: Trimming redundancies, bolstering efficiency.
- **Knowledge Distillation**: Harnessing the wisdom of larger models in smaller, more agile architectures.
- **Zero-Shot and Few-Shot Learning**: Broadening adaptability horizons.
- **Enhanced Data Augmentation**: Fortifying the model's grasp on varied, nuanced contexts.
- **Decentralized Training**: Tapping into the global hive-mind, harnessing the collaborative power of the community.


## **Potential Other Future Trajectories**

#### **1. Clearer Metrics**
There's always room to elevate the benchmarking rigor, especially concerning reasoning abilities.

#### **2. Robust Validation and Testing Environment**
Further fine-tuning of the testing environment can offer even more reliable validations of Andromeda's capabilities.

#### **3. Comprehensive Documentation**
To bolster transparency and replicability, detailed documentation covering every facet of Andromeda is on the horizon.

#### **4. Benchmarking Against Peers**
By juxtaposing Andromeda against its counterparts, its distinctive advantages can be spotlighted more effectively.

#### **5. Spotlight on Real-World Applications**
By highlighting tangible use-cases, the versatility and prowess of Andromeda can be showcased in palpable contexts.

#### **6. Model Interpretability**
Future iterations might delve deeper into model interpretability, especially for critical applications.

#### **7. Niche Customizations**
By tailoring Andromeda to meet specific niche needs, its adaptability and value proposition can be further enhanced.

#### **8. Collaborative Endeavors**
Engaging more intimately with the global research community could spawn collaborative projects, bringing diverse insights to the fore.


As we voyage further into the AI frontier, Andromeda stands as a beacon, illuminating the path forward, promising marvels yet to come. It's not just about machine intelligence; it's about the dance between human curiosity and machine capability.

---

Join us on this journey. Dive deeper, ask questions, innovate, and let's redefine what's possible, together.

### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

https://github.com/kyegomez/Andromeda","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Add FastViT model,"### Model description

FastViT is a hybrid vision transformer that uses structural reparameterization to obtain lower memory access cost and increased capacity, achieving stateof-the-art accuracy-latency trade-off. Highly efficient on multiple compute fabrics: mobile devices and desktop grade GPUs.



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/pdf/2303.14189.pdf
Github(code and weights): https://github.com/apple/ml-fastvit
Authors: @anuragranj, @pavank-apple ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Implement SuperPoint / SuperGlue,"### Model description

The SuperGlue network is a Graph Neural Network combined with an Optimal Matching layer that is trained to perform matching on two sets of sparse image features. 
SuperGlue is built on top of SuperPoint model which consists of detecting the most interesting keypoints in an image. With the keypoints of two different images, SuperGlue proceeds to the matching.

I noticed there was no image matching models implemented in transformers library so I propose this first one. I extensively used it in other activities and am new to transformers git contributions, so I am willing to implement it myself as a first contribution.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

SuperPoint and SuperGlue code and weights are available at https://github.com/magicleap/SuperGluePretrainedNetwork
The original paper of SuperPoint : https://arxiv.org/abs/1712.07629
The original paper of SuperGlue : https://arxiv.org/abs/1911.11763","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Add support for BLIP-2 multimodal feature extraction,"# What does this PR do?

This PR introduces the addition of `get_image_feature` and `get_text_feature` methods to the `Blip2ForConditionalGeneration` class. These changes align with the original Qformer implementation, which utilized both text and image inputs.

The current implementation in HuggingFace lacks support for multimodal embeddings, especially the capacity to extract embeddings by passing both text and image to the QFormer. This PR addresses this shortcoming.

<!-- Remove if not applicable -->

Fixes #25300 #25245 


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

@NielsRogge @amyeroberts 


","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",8,open
The length used by length_penalty during beam_search is not correct when input batch size > 1,"### System Info

Python 3.9.2
transformers 4.30.2

### Who can help?

@gante

### Information

- [X] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

```
from transformers import BloomTokenizerFast, BloomForCausalLM
import torch

tokenizer = BloomTokenizerFast.from_pretrained('bigscience/bloomz-7b1')
tokenizer.padding_side = ""left""

model = BloomForCausalLM.from_pretrained('bigscience/bloomz-7b1')
model = model.cuda().half().eval()

def predict(model, text):
  token_out = tokenizer(text, padding=True, return_tensors=""pt"")
  input_ids = token_out.input_ids.cuda()
  attention_mask = token_out.attention_mask.cuda()
  generate_kwargs = dict(max_new_tokens=128, do_sample=False, num_beams=2, length_penalty=3.0)
  ori_out = model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
  print('ori input shape: ', input_ids.shape)
  print('ori output shape: ', ori_out.shape)
  return tokenizer.batch_decode(ori_out, skip_special_tokens=True)

text = [""Explain backpropagation in neural networks.""]
baseline = predict(model, text)[0]
text = [""Explain backpropagation in neural networks."", ""What is 1 + 1?""]
good_case = predict(model, text)[0]
text = [""Explain backpropagation in neural networks."", ""The Basilica of the Sacred heart at Notre Dame is beside to which structure? The Basilica of the Sacred heart at Notre Dame is beside to which structure? The Basilica of the Sacred heart at Notre Dame is beside to which structure?""]
bad_case = predict(model, text)[0]

print(f'baseline: \n{baseline}\ngood_case: \n{good_case}\nbad_case: \n{bad_case}\n')

```

The script results:

```
baseline: 
Explain backpropagation in neural networks. backpropagation is the process by which the error is propagated backwards through the network from the output layer to the input layer, and then back to the output layer again, until the error is reduced to zero. backpropagation is the process by which the error is propagated backwards through the network from the output layer to the input layer, and then back to the output layer again, until the error is reduced to zero. backpropagation is the process by which the error is propagated backwards through the network from the output layer to the input layer, and then back to the output layer again, until the error is reduced to zero
good_case: 
Explain backpropagation in neural networks. backpropagation is the process by which the error is propagated backwards through the network from the output layer to the input layer, and then back to the output layer again, until the error is reduced to zero. backpropagation is the process by which the error is propagated backwards through the network from the output layer to the input layer, and then back to the output layer again, until the error is reduced to zero. backpropagation is the process by which the error is propagated backwards through the network from the output layer to the input layer, and then back to the output layer again, until the error is reduced to zero
bad_case: 
Explain backpropagation in neural networks. backpropagation is the process by which the error is propagated backwards through the network from the output layer to the input layer.
```

### Expected behavior

In the add method of [BeamHypotheses](https://github.com/huggingface/transformers/blob/347001237a8ff845fc23f678107fc505361f9f13/src/transformers/generation/beam_search.py#L938), it uses `hyp.shape[-1]` as length penalty, but actually it may include padding tokens if batched input are provided.

In such instances, the same input can yield different results when combined with varying additional inputs. As demonstrated in the provided reproduction script, pairing the input with an exceedingly long peer alters the padding length. This modification leads to different length penalty, ultimately producing significantly different results.

","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Abnormally High GPU Memory Consumption with OPT 350M Model Leading to OOM,"### System Info

- `transformers` version: 4.32.0.dev0
- Platform: Linux-5.4.0-135-generic-x86_64-with-glibc2.35
- Python version: 3.11.4
- Huggingface_hub version: 0.16.4
- Safetensors version: 0.3.1
- Accelerate version: 0.21.0
- Accelerate config:    not found
- PyTorch version (GPU?): 2.1.0.dev20230723+cu121 (True)
- Tensorflow version (GPU?): 2.14.0-dev20230723 (False)
- Flax version (CPU?/GPU?/TPU?): 0.7.0 (gpu)
- Jax version: 0.4.14
- JaxLib version: 0.4.14
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

### Who can help?

@ArthurZucker @younesbelkada @sgugger

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

While working with the OPT 350M model, I have encountered an issue regarding GPU memory consumption that I believe to be abnormal. Specifically, during the forward pass, the model causes an OOM error. I am using 1 Nvidia A100 80GB GPU.

Here is the repro with a manual memory profiling:

```python
import os; os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # use 1 device only
import torch
from transformers import OPTForCausalLM

model_name = 'facebook/opt-350m'
model = OPTForCausalLM.from_pretrained(model_name, device_map='cuda')  # memory usage: 1688 MiB

batch_size = 6
seq_len = 2048

seq_ids = torch.zeros((batch_size, seq_len), dtype=torch.long, device='cuda')  # memory usage: 1764 MiB
seq_mask = torch.zeros((batch_size, seq_len), dtype=torch.bool, device='cuda')  # memory usage: 1764 MiB
labels_ids = torch.zeros((batch_size, seq_len), dtype=torch.long, device='cuda')  # memory usage: 1764 MiB

outputs = model(input_ids=seq_ids, attention_mask=seq_mask, labels=labels_ids)  # the memory usage surges and leads to OOM
```

Output:

```
Some weights of OPTForCausalLM were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File ""/home/ayaka/test/1.py"", line 18, in <module>
    outputs = model(input_ids=seq_ids, attention_mask=seq_mask, labels=labels_ids)  # the memory usage surges and leads to OOM
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py"", line 944, in forward
    outputs = self.model.decoder(
              ^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py"", line 710, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py"", line 330, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ayaka/test/venv/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py"", line 223, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 79.35 GiB of which 662.62 MiB is free. Process 4083423 has 78.70 GiB memory in use. Of the allocated memory 77.96 GiB is allocated by PyTorch, and 258.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```


### Expected behavior

No OOM since the model is very small","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",10,open
Add Flax diverse group search,"### Feature request

Add diverse beam search decoding to Flax, an ""alternative to BS that decodes a list of diverse outputs by optimising for a diversity-augmented objective"", as described in the paper: https://arxiv.org/pdf/1610.02424.pdf 

This feature would mimic the PyTorch equivalent, added in #9006.

@yeandy made a great start on adding this feature in the PR #24508. The PR is still open, and anyone in the community is free to pick-up the PR and see it through to completion!

### Motivation

There's a promising PR for this feature that is partway there - it would be a shame not to see this through to completion!

### Your contribution

Happy to answer any questions/queries on the PR and provide PR reviews 🤗 Think this would be a fun one for any Flax contributors who are interested!","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2934977194, 'node_id': 'MDU6TGFiZWwyOTM0OTc3MTk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flax', 'name': 'Flax', 'color': '4862AD', 'default': False, 'description': ''}]",7,open
Support H100 training with FP8 in Trainer and Deepspeed,"### Feature request

Support H100 training with FP8 in Trainer and Deepspeed

### Motivation

FP8 should be much faster than FP16 on supported Hopper hardware. Particularly with Deepspeed integration @stas00

### Your contribution

Happy to help in any way that I can.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",22,open
Return updated attention mask from Wav2Vec 2.0,"### Feature request

In Wav2Vec 2.0, the first few convolution layers affect the attention mask. Thus, if I want to use all Wav2Vec 2.0 outputs (last_hidden_state), I need access to the updated attention mask. Currently the workaround is to call the private method `._get_feature_vector_attention_mask()`. To return this attention mask directly will make the experience much better.

Concretely, it can be an additional field in Wav2Vec2BaseModelOutput.

Additionally, data2vec-audio and HuBERT also have the same problem, since they are also based on Wav2Vec 2.0.

### Motivation

Fairseq implementation returns the updated attention mask for downstream uses.

https://github.com/facebookresearch/fairseq/blob/100cd91db19bb27277a06a25eb4154c805b10189/fairseq/models/wav2vec/wav2vec2.py#L696-L700

Then their Wav2Vec 2.0 for classification implementation is more elegant.

https://github.com/facebookresearch/fairseq/blob/100cd91db19bb27277a06a25eb4154c805b10189/fairseq/models/wav2vec/wav2vec2_classification.py#L84-L93

In HF, Wav2Vec 2.0 for classification requires calling the private method.

https://github.com/huggingface/transformers/blob/641adca55832ed9c5648f54dcd8926d67d3511db/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2127

### Your contribution

I can help to submit a PR for this. It will be an additional field in Wav2Vec2BaseModelOutput. I'm not quite sure about other types of Wav2Vec2 output objects.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",5,open
Add zero-shot classification task for BLIP-2,"### Feature request

I would like to add the support for the zero-shot classification task using BLIP2, computing text-image similarities with the normalized embeddings, that would be accessed from BLIP2 feature extractor. 

The idea is to enable calling the zero-shot classification pipeline using BLIP2, by implementing the `get_image_feature`and `get_text_features`methods.

I would love more guidance, if possible, on the criteria for accepting the PR.



### Motivation

This is related to the following the discussion on this issue on the hub, and the comment left by @NielsRogge here https://huggingface.co/Salesforce/blip2-opt-2.7b/discussions/3#64cbe5e487ec96aa473a1f54 .

### Your contribution

I would like to submit a PR to contribute for this feature.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",10,open
"BertForSequenceClassification does not support 'device_map':""auto"" yet","### System Info

I have trained a model and am now trying to load and quantise it but getting the error: 

BertForSequenceClassification does not support 'device_map':""auto"" yet

Code for loading is simply:
   ` model = AutoModelForSequenceClassification.from_pretrained(model_dir, device_map='auto', load_in_8bit=True)`

Help would be greatly appreciated!

Thanks,

Lee

### Who can help?

_No response_

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

model = AutoModelForSequenceClassification.from_pretrained(model_dir, device_map='auto', load_in_8bit=True)

### Expected behavior

The model would load and be usable.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",18,open
MassFormer,"### Model description

We propose adding a new model, MassFormer, to predict tandem mass spectra accurately. MassFormer uses a graph transformer architecture to model long-distance relationships between atoms in the molecule. The transformer module is initialized with parameters obtained through a chemical pre-training task, then fine-tuned on spectral data. MassFormer outperforms competing approaches for spectrum prediction on multiple datasets and is able to recover prior knowledge about the effect of collision energy on the spectrum. We demonstrate that the model can identify relationships between fragment peaks by employing gradient-based attribution methods. To further highlight MassFormer’s utility, we show that it can match or exceed existing prediction-based methods on two spectrum identification tasks. Our code is the first open-source implementation of a deep-learning MS/MS spectrum predictor and may encourage future research in this area.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

This model will be implemented according to the paper by @adamoyoung as listed below.

Reference:
Young, A., Wang, B. and Röst, H., 2021. MassFormer: Tandem mass spectrum prediction with graph transformers. arXiv preprint arXiv:2111.04824.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
RWKV-WORLD-4,"### Model description

BlinkDL/rwkv-4-world is a repo present on Huggingface i want the model's tokenizer and the model to be added to the Transformers Lib. 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
"Enable use of best epoch in Trial, with early stopping, during hyperparameter search","### Feature request

When running a `Trainer.hyperparameter_search`, each trial's value is calculated from the last epoch's chosen metric. However, especially when using early stopping and `load_best_model_at_end`, it would be useful to use the best model instead.

This could be a parameter of `Trainer.hyperparameter_search` or a an overridable function getting the best value, or some callback.

### Motivation

Often, we use early stopping and take the best model from a particular run because it's possible for models to start overfitting and dropping off after a certain number of epochs. This phenomenon can also appear during hyper parameter search and, as such, we'd like to be able to use the best epoch's value to compare trials.

Without this we may get results that are not fully representative.

### Your contribution

Happy to help testing or in other ways I can. Not sure where to start but if there is a clear place to do it I'd be open to help.","[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMTU1MTY5MTQw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/trainer', 'name': 'trainer', 'color': '2ef289', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
RetNet model support,"### Model description

RetNet / Retentive Networks is a new model *archetype* released by microsoft; the research paper is [here](https://arxiv.org/pdf/2307.08621.pdf). As of now, there is *one* model for retnet; [made by me](https://huggingface.co/parsee-mizuhashi/retnet-tiny-wikitext-undertrained); which is undertrained (`loss=8`!) and I am trying to make a second model on a larger arch.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

[commit that has retnet training](https://github.com/microsoft/torchscale/commit/bf65397b26469ac9c24d83a9b779b285c1ec640b)
@donglixp was the main author for commit and cited on the paper
all code is licensed under MIT, including model weights","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5724035499, 'node_id': 'LA_kwDOCUB6oc8AAAABVS3Zqw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Model%20on%20the%20Hub', 'name': 'Model on the Hub', 'color': '9CA0E9', 'default': False, 'description': ''}]",19,open
BERT: TensorFlow Model Garden Conversion scripts,"### Feature request

Hi,

after working some time with the [TensorFlow Model Garden Repository](https://github.com/tensorflow/models) and training BERT models, I found out the following things that could be changed in Transformers library:

I added the Token Dropping BERT Conversion script a while ago, see #17142. Now I found out, that latest BERT models pretrained with Model Garden Repository repository can also be converted with this script.

For this reason I would propose to rename the script `convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py` just to `convert_bert_original_tf2_checkpoint_to_pytorch.py`.

However, this script also exists, but it is no longer working, as this was deprecated in Model Garden Repository a while ago, I added this notice in #16171.

I see now two possibilities to proceed with the different conversion scripts:

* Rename the current `convert_bert_original_tf2_checkpoint_to_pytorch.py` to something like `convert_deprecated_bert_original_tf2_checkpoint_to_pytorch.py` so that this name is free for the ""new"" conversion script that supports Token Dropping BERT und latest BERT models from Model Garden Repository.
* Delete the old script completely

### Motivation

More recent BERT and Token Dropping BERT models can be pretrained with TensorFlow Model Garden repository.

There should be one script that does these conversions, the old one that is only working with deprecated models from Model Garden repo should be renamed or deleted.

### Your contribution

I can take care of renaming/deletion and extending the conversion script to have better documentation.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",5,open
Add PromptTemplate and allow for default PromptTemplate in model configuration,"### Feature request

As a user, I want to be able to load a model and feed it my input in such a way that it matches the prompt template that it saw during training. I want to be able to load the default prompt with few lines of code and without having to look up how the model was trained. Additionally, I want to be able to modify the prompt to be different from the default prompt.

The specific implementation is up for discussion. I imagine something like this:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoPromptTemplate

model_id = ""meta-llama/Llama-2-xb-chat-hf""
model = AutoModelForCausalLM.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)
prompt_template = AutoPromptTemplate.from_pretrained(model_id)

inputs = {
   ""system_prompt"":""You are a helpful assistant"",
   ""interactions"":[
      {""user"":""What is the fastest sea mammal?""},
      {""assistant"":""The fastest sea mammal is the peregrine falcon""},
      {""user"":""the peregrine falcon is not a mammal""}
   ]
}

output = model(**tokenizer(prompt_template(inputs)))
```

### Motivation

The huggingface hub is accumulating many finetuned models, which have been trained with a specific prompt template in mind. However, this prompt template is often difficult to find, and even more often the prompt template is missing entirely from the model card. If the model is invoked with a different template, the model performance can be severely affected. The community would benefit from a PromptTemplate class that can be loaded from the model configuration that handles the prompt templating for the end user.

At this very moment, there are likely many users that are using the `meta-llama/Llama-2-xb-chat-hf` models with a prompting style that differs from how the model is intended to be used.

### Your contribution

I am happy to be a part of the discussion for implementation and testing.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",15,open
an inplace operation preventing TorchDistributor training,"### System Info

databricks

### Who can help?

@ArthurZucker @younesbelkada

Hi team, 

I got an error message by using TorchDistributor. 


I have checked in the class BertEmbeddings (url as below), line 238, embeddings += position_embeddings is an inplace operation, would you be able to change to embeddings = embeddings + position_embeddings, to allow TOrchDistributor? 

BertEmbeddings url:
https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py

TorchDistributor sample code: 
https://docs.databricks.com/_extras/notebooks/source/deep-learning/torch-distributor-notebook.html

Thank you very much! 
Ling

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```
single_node_single_gpu_dir = create_log_dir()
print(""Data is located at: "", single_node_single_gpu_dir)

def train_one_epoch(model, device, data_loader, optimizer, epoch):
  torch.autograd.set_detect_anomaly(True)
  model.train()
  for batch_idx, (data, labels) in enumerate(data_loader):
    inputs1, inputs2 = data[0], data[1]
    inputs1 = {key: val.to(device) for key, val in inputs1.items()}
    inputs2 = {key: val.to(device) for key, val in inputs2.items()}
    # labels = labels.float().to(device)
    labels = labels.to(device)

    optimizer.zero_grad()
    # Compute embeddings
    embeddings1 = model(inputs1)['sentence_embedding']
    embeddings2 = model(inputs2)['sentence_embedding']
    
    # Compute loss
    loss = cosine_similarity_loss(embeddings1, embeddings2, labels)

    loss.backward()
    optimizer.step()
    if batch_idx % log_interval == 0:
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
          epoch, batch_idx * len(data), len(data_loader) * len(data),
          100. * batch_idx / len(data_loader), loss.item()))
      
      if int(os.environ[""RANK""]) == 0:
        mlflow.log_metric('train_loss', loss.item())

def save_checkpoint(log_dir, model, optimizer, epoch):
  filepath = log_dir + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)
  state = {
    'model': model.module.state_dict(),
    'optimizer': optimizer.state_dict(),
  }
  torch.save(state, filepath)

# For distributed training we will merge the train and test steps into 1 main function
def main_fn(directory):
  
  #### Added imports here ####
  import mlflow
  import torch.distributed as dist
  from torch.nn.parallel import DistributedDataParallel as DDP
  from torch.utils.data.distributed import DistributedSampler
  
  ############################

  ##### Setting up MLflow ####
  # We need to do this so that different processes that will be able to find mlflow
  os.environ['DATABRICKS_HOST'] = db_host
  os.environ['DATABRICKS_TOKEN'] = db_token

  # We set the experiment details here
  experiment = mlflow.set_experiment(experiment_path)
  ############################
  
  print(""Running distributed training"")
  dist.init_process_group(""nccl"")
  
  local_rank = int(os.environ[""LOCAL_RANK""])
  global_rank = int(os.environ[""RANK""])
  
  if global_rank == 0:
    train_parameters = {'batch_size': batch_size, 'epochs': num_epochs, 'trainer': 'TorchDistributor'}
    mlflow.log_params(train_parameters)
  
  model = SentenceTransformer(modelname)

  filepath = ""../../dbfs/mnt/path2data/""
  df_train = readData('train', filepath)
  df_train = df_train.head(10000)

  train_text = df_train[['sentA', 'sentB', 'score']].values.tolist()
  train_examples = [InputExample(texts=[a, b], label=s) for [a, b, s] in train_text]
  train_dataset = SentencesDataset(train_examples, model)
  #### Added Distributed Dataloader ####
  train_sampler = DistributedSampler(dataset=train_dataset)
  data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)
  ######################################
  data_loader.collate_fn = model.smart_batching_collate

  model = model.to(local_rank)
  #### Added Distributed Model ####
  ddp_model = DDP(model, device_ids=[local_rank], output_device=local_rank)
  #################################

  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

  for epoch in range(1, num_epochs + 1):
    train_one_epoch(ddp_model, local_rank, data_loader, optimizer, epoch)
    
    if global_rank == 0: 
      save_checkpoint(directory, ddp_model, optimizer, epoch)
  
  dist.destroy_process_group()
  
  return ""finished"" # can return any picklable object

# single node distributed run to quickly test that the whole process is working
with mlflow.start_run():
  mlflow.log_param('run_type', 'test_dist_code')
  main_fn(single_node_single_gpu_dir)
```

### Expected behavior

below error disappear. 

![image](https://github.com/huggingface/transformers/assets/48280760/54232ecc-29da-4d37-a0b5-7ca4b8a0d22a)
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",6,open
Add Vocos model,"### Model description

Vocos is a Fourier-based neural vocoder for audio synthesis.

According to its [paper](https://arxiv.org/pdf/2306.00814.pdf), Vocos constantly outperforms [HifiGan](https://huggingface.co/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5HifiGan), has 13.5M params and is significantly faster than any competing vocoders!

Moreover, it is also compatible with Bark, and significantly improve audio quality as showed [here](https://charactr-platform.github.io/vocos/#audio-reconstruction-from-bark-tokens).

Vocos is composed of a backbone (ConvNeXt) and an inverse fourier transform head (either STFT or MDCT).  


### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Vocos code is available [here](https://github.com/charactr-platform/vocos/tree/main) and was mainly contributed by @hubertsiuzdak.

Its weights are available on HF hub [here](https://huggingface.co/charactr/vocos-mel-24khz) and [here](https://huggingface.co/charactr/vocos-encodec-24khz).","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",6,open
Support symbolic tracing for NeoX models,"### Feature request

Currently `transformers.utils.fx.symbolic_trace` fails when passed any NeoX model, and I'd like to fix that:
```
NotImplementedError: Model GPTNeoXForCausalLM is not supported yet, supported models: AlbertForMaskedLM, 
AlbertForMultipleChoice, AlbertForPreTraining, AlbertForQuestionAnswering, AlbertForSequenceClassification, 
AlbertForTokenClassification, AlbertModel, AltCLIPModel, AltCLIPTextModel, AltCLIPVisionModel, BartForCausalLM, 
BartForConditionalGeneration, BartForQuestionAnswering, BartForSequenceClassification, BartModel, BertForMaskedLM, 
BertForMultipleChoice, BertForNextSentencePrediction, BertForPreTraining, BertForQuestionAnswering, 
BertForSequenceClassification, BertForTokenClassification, BertLMHeadModel, BertModel, BlenderbotForCausalLM, 
BlenderbotForConditionalGeneration, BlenderbotModel, BlenderbotSmallForCausalLM, 
BlenderbotSmallForConditionalGeneration, BlenderbotSmallModel, BloomForCausalLM, BloomForQuestionAnswering, 
BloomForSequenceClassification, BloomForTokenClassification, BloomModel, CLIPModel, CLIPTextModel, 
CLIPTextModelWithProjection, CLIPVisionModel, CLIPVisionModelWithProjection, ConvNextBackbone, 
ConvNextForImageClassification, ConvNextModel, DebertaForMaskedLM, DebertaForQuestionAnswering, 
DebertaForSequenceClassification, DebertaForTokenClassification, DebertaModel, DebertaV2ForMaskedLM, 
DebertaV2ForMultipleChoice, DebertaV2ForQuestionAnswering, DebertaV2ForSequenceClassification, 
DebertaV2ForTokenClassification, DebertaV2Model, DistilBertForMaskedLM, DistilBertForMultipleChoice, 
DistilBertForQuestionAnswering, DistilBertForSequenceClassification, DistilBertForTokenClassification, 
DistilBertModel, DonutSwinModel, ElectraForCausalLM, ElectraForMaskedLM, ElectraForMultipleChoice, 
ElectraForPreTraining, ElectraForQuestionAnswering, ElectraForSequenceClassification, 
ElectraForTokenClassification, ElectraModel, GPT2DoubleHeadsModel, GPT2ForQuestionAnswering, 
GPT2ForSequenceClassification, GPT2ForTokenClassification, GPT2LMHeadModel, GPT2Model, GPTJForCausalLM, 
GPTJForQuestionAnswering, GPTJForSequenceClassification, GPTJModel, GPTNeoForCausalLM, GPTNeoForQuestionAnswering, 
GPTNeoForSequenceClassification, GPTNeoForTokenClassification, GPTNeoModel, GitVisionModel, HubertForCTC, 
HubertForSequenceClassification, HubertModel, LayoutLMForMaskedLM, LayoutLMForQuestionAnswering, 
LayoutLMForSequenceClassification, LayoutLMForTokenClassification, LayoutLMModel, LxmertForPreTraining, 
LxmertForQuestionAnswering, LxmertModel, M2M100ForConditionalGeneration, M2M100Model, MBartForCausalLM, 
MBartForConditionalGeneration, MBartForQuestionAnswering, MBartForSequenceClassification, MBartModel, 
MT5ForConditionalGeneration, MT5Model, MarianForCausalLM, MarianMTModel, MarianModel, MegatronBertForCausalLM, 
MegatronBertForMaskedLM, MegatronBertForMultipleChoice, MegatronBertForNextSentencePrediction, 
MegatronBertForPreTraining, MegatronBertForQuestionAnswering, MegatronBertForSequenceClassification, 
MegatronBertForTokenClassification, MegatronBertModel, MobileBertForMaskedLM, MobileBertForMultipleChoice, 
MobileBertForNextSentencePrediction, MobileBertForPreTraining, MobileBertForQuestionAnswering, 
MobileBertForSequenceClassification, MobileBertForTokenClassification, MobileBertModel, NezhaForMaskedLM, 
NezhaForMultipleChoice, NezhaForNextSentencePrediction, NezhaForPreTraining, NezhaForQuestionAnswering, 
NezhaForSequenceClassification, NezhaForTokenClassification, NezhaModel, OPTForCausalLM, OPTForQuestionAnswering, 
OPTForSequenceClassification, OPTModel, PLBartForCausalLM, PLBartForConditionalGeneration, 
PLBartForSequenceClassification, PLBartModel, PeftModelForCausalLM, PeftModelForSeq2SeqLM, PegasusForCausalLM, 
PegasusForConditionalGeneration, PegasusModel, ResNetBackbone, ResNetForImageClassification, ResNetModel, 
RobertaForCausalLM, RobertaForMaskedLM, RobertaForMultipleChoice, RobertaForQuestionAnswering, 
RobertaForSequenceClassification, RobertaForTokenClassification, RobertaModel, SegformerForImageClassification, 
SegformerForSemanticSegmentation, SegformerModel, Speech2Text2Decoder, Speech2Text2ForCausalLM, 
Speech2TextForConditionalGeneration, Speech2TextModel, SwinBackbone, SwinForImageClassification, 
SwinForMaskedImageModeling, SwinModel, T5ForConditionalGeneration, T5Model, TrOCRDecoder, TrOCRForCausalLM, 
ViTForImageClassification, ViTForMaskedImageModeling, ViTModel, Wav2Vec2ForCTC, Wav2Vec2ForMaskedLM, 
Wav2Vec2ForPreTraining, Wav2Vec2ForSequenceClassification, Wav2Vec2Model, XGLMForCausalLM, XGLMModel
```

### Motivation

The main motivation for this is to enable graph rewriting with the EleutherAI Pythia model suite. Graph rewriting has various interpretability use-cases and the Pythia suite was designed for interpretability research.

### Your contribution

I plan to implement a PR for this soon unless there's some major blocker for it.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation ,"### Model description

ViTPose is used in 2D human pose estimation, a subset of the keypoint detection task #24044

It provides a simple baseline for vision transformer-based human pose estimation. It utilises a pretrained vision transformer backbone to extract features and a simple decoder head to process the extracted features. Despite no elaborate designs in the model, ViTPose obtained state-of-the-art (SOTA) performance of 80.9 AP on the MS COCO Keypoint test-dev set.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Code and weights: https://github.com/ViTAE-Transformer/ViTPose
Paper: https://arxiv.org/abs/2204.12484

@Annbless","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",6,open
Request support for RWKV-4-World model.,"### Model description

As RWKV-4-World is using a different tokenizer and vocabs, the current RWKV support in transformers is incompatible.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://huggingface.co/StarRing2022/RWKV-4-World-1.5B
@StarRing2022","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",9,open
Pipeline feature request for min_new_tokens,"### Feature request

Pipeline already supports the option max_new_tokens.

I’m requesting for the existing “min_new_tokens” to be able to be used with pipeline the same way as “max_new_tokens”.

Currently this will throw an error when trying to specify “min_new_tokens” as unrecognised.  

### Motivation

Maintaining consistency, as the previous way to specify tokens is deprecated.

### Your contribution

I can test, but I’m not a developer. So no, I couldn’t do a PR.
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",15,open
Add text-mesh models inside Hugginfaces,"### Feature request

Text to 3D models are really getting the scene in some industries but actually the state of the art techiniques are very hard to integrate in production code. Some examples are:

https://www.nasir.lol/clipmesh
https://github.com/openai/shap-e


Would be awesome to the community if HF has that integrated.

### Motivation

Text-3D models can have a big space in multiple types of industry

### Your contribution

If I have some guidance I can help working on this side. But I will need HF developers help.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Feature Request: To add nested hierarchy retrieval from Donut response,"### Feature request

### Donut for hierarchy extraction (Document Parsing)
While preprocessing the ground truth json to the tokens for Donut the processor function (json2token) handles nested hierarchy but the same doesn't hold true for token2json.

Below is an example json:
`
{
    ""header"": ""This is 1st header"",
    ""elements"": [
      {
        ""text_block"": ""This is a textblock""
      },
      {
        ""header"": ""1st nested header"",
          ""elements"": [
            {
              ""text_block"": ""This is a sentence""
            },
            {
              ""text_block"": ""Another sentence....""
            },
            {
              ""itallic_header"": ""This is an itallic header"",
              ""elements"": [
                {
                  ""text_block"": ""Text 1 inside itallic header..""
                },
                {
                  ""text_block"": ""Text 2 inside itallic header..""
                }
              ]
            }
        ]
      }
    ]
  }
`

Consider the above json. Applying the json2token function gives the following token sequence.

Function Call:
`output = json2token(temp_test)`

> <s_header>This is 1st header</s_header><s_elements><s_text_block>This is a textblock</s_text_block><sep/><s_header>1st nested header</s_header><s_elements><s_text_block>This is a sentence</s_text_block><sep/><s_text_block>Another sentence....</s_text_block><sep/><s_itallic_header>This is an itallic header</s_itallic_header><s_elements><s_text_block>Text 1 inside itallic header..</s_text_block><sep/><s_text_block>Text 2 inside itallic header..</s_text_block></s_elements></s_elements></s_elements>

This maintains the hierarchy (like parenthesis matching). So, if donut is trained on such data it will give response which parses the information  & also retains the hierarchy but the token2json function doesn't handle the conversion properly. Below is the output of the function id passed the token sequence present above.

Function Call:
`processor.token2json(output)`

Output

`
[
  {
    'header': 'This is 1st header',
    'elements': [
      {
        'text_block': 'This is a textblock'
      },
      {
        'header': '1st nested header',
        'text_block': 'This is a sentence'
      },
      {
        'text_block': 'Another sentence....'
      },
      {
        'itallic_header': 'This is an itallic header',
        'text_block': 'Text 1 inside itallic header..'
      },
      {
        'text_block': 'Text 2 inside itallic header..'
      }
    ]
  }
]
`

Updated Function Results (Preserving the hierarchy):
`
[
  {
    'header': 'This is 1st header',
    'elements': [
      {
        'text_block': 'This is a textblock'
      },
      {
        'header': '1st nested header',
        'elements': [
          {
            'text_block': 'This is a sentence'
          },
          {
            'text_block': 'Another sentence....'
          },
          {
            'itallic_header': 'This is an itallic header',
            'elements': [
              {
                'text_block': 'Text 1 inside itallic header..'
              },
              {
                'text_block': 'Text 2 inside itallic header..'
              }
            ]
          }
        ]
      }
    ]
  }
]
`

Example from CORD:

> temp_test = {
  ""company"": ""ADVANCO COMPANY"",
  ""date"": ""17/01/2018"",
  ""address"": ""NO 1&3, JALAN WANGSA DELIMA 12, WANGSA LINK, WANGSA MAJU, 53300 KUALA LUMPUR"",
  ""total"": ""7.00""
}

Updated Function Output:
`
[
  {
    'company': 'ADVANCO COMPANY',
    'date': '17/01/2018',
    'address': 'NO 1&3, JALAN WANGSA DELIMA 12, WANGSA LINK, WANGSA MAJU, 53300 KUALA LUMPUR',
    'total': '7.00'
  }
]
`

### Motivation

Found out about this while working on a project to extract information from images also maintaining the hierarchy/structure of it.

Going through the CORD dataset made me realize that the data itself is not nested in nature. So, thought of testing on a sample the postprocessing logics json -> token & token -> json conversion.

Updated the token2json to get the hierarchy as it is from the token but wasn't sure about the model performance on nested jsons but long story short Donut predicts the hierarchy pretty good.

### Your contribution

`
def token2json(tokens, is_inner_value=False, nested_key = 'elements'):
    """"""
    Convert a (generated) token seuqnce into an ordered JSON format
    """"""
    output = dict()

    while tokens:
        start_token = re.search(r""<s_(.*?)>"", tokens, re.IGNORECASE)
        if start_token is None:
            break
        key = start_token.group(1)
        start_matches = re.finditer(fr""<s_{key}>"", tokens)
        end_matches = re.finditer(fr""</s_{key}>"", tokens)
        start_tups = [(match.group(), match.start(), match.end()) for match in start_matches]
        end_tups = [(match.group(), match.start(), match.end()) for match in end_matches]
        mergeTups = start_tups + end_tups
        sortedMergeTups = sorted(mergeTups, key=lambda x: x[1])
        # remove any unattended close tag for the key present before the current focus start key
        updatedIdx = -1
        for idx in range(len(sortedMergeTups)):
            if start_token.span()[0] == sortedMergeTups[idx][1]:
                updatedIdx = idx
                break
        sortedMergeTups = sortedMergeTups[updatedIdx:]
        start_main = sortedMergeTups[0]
        match_tracker = 0
        end_token = None
        if key == nested_key :
            if start_main[0] == f'<s_{key}>':
                for tup in sortedMergeTups[1:]:
                    if tup[0] == f'</s_{key}>':
                        if match_tracker == 0:
                            end_token = tup
                            break
                        else:
                            match_tracker -= 1
                    elif tup[0] == f'<s_{key}>':
                        match_tracker += 1
        elif len(sortedMergeTups) > 1:
            nextTup = sortedMergeTups[1]
            if nextTup[0] == f'</s_{key}>':
                end_token = nextTup

        if end_token is None:
            tokens = tokens.replace(start_token[0], """", 1)
        else:
            start_token_word = start_main[0]
            start_token_id = start_main[2]
            end_token_word = end_token[0]
            end_token_id = end_token[1]
            content = tokens[start_token_id: end_token_id]
            if content is not None:
                if r""<s_"" in content and r""</s_"" in content:  # non-leaf node
                    value = token2json(content, is_inner_value=True)
                    if value:
                        if len(value) == 1:
                            value = value[0]
                        output[key] = value
                else:  # leaf nodes
                    if key in output.keys():
                        if isinstance(output[key], str):
                            tempVal = output[key]
                            output[key] = [tempVal]
                    else:    
                        output[key] = []
                    for leaf in content.split(r""<sep/>""):
                        leaf = leaf.strip()
                        if (
                            leaf in processor.tokenizer.get_added_vocab()
                            and leaf[0] == ""<""
                            and leaf[-2:] == ""/>""
                        ):
                            leaf = leaf[1:-2]  # for categorical special tokens
                        output[key].append(leaf)
                    if len(output[key]) == 1:
                        output[key] = output[key][0]

            tokens = tokens[end_token[2]:]
            if tokens[:6] == r""<sep/>"":  # non-leaf nodes
                return [output] + token2json(tokens[6:], is_inner_value=True)

    if len(output):
        return [output] if is_inner_value else output
    else:
        return [] if is_inner_value else {""text_sequence"": tokens}
`","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Initialize Flax model params on CPU ,"### Feature request

Currently, the `from_pretrained` method of Flax models automatically puts model parameters on a single GPU device, if available. For very large models, this is not great, as the model parameters may just not fit on GPU memory. 

In contrast, when passing `_do_init=False` to `from_pretrained`, the parameters are returned on CPU, outside the model.

I would love to have a feature that allows me to initialize model parameters on the device I want - in this case, on CPU - but at the same time initialize the model parameters within the model. Right now I have to call `_do_init=False` to avoid out-of-memory, but this causes inconsistencies with my API.

The feature could be either implemented as just another type (if we detect a numpy type, we initialize on CPU; otherwise on GPU) or as an additional argument, e.g. `initialize_on_cpu: bool = False`.

### Motivation

Described above. Another reason is to be more consistent with the PyTorch behaviour, where parameters are initialized (as a generator) on CPU.

### Your contribution

If we agree on on the design, I am happy to add this myself.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2934977194, 'node_id': 'MDU6TGFiZWwyOTM0OTc3MTk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flax', 'name': 'Flax', 'color': '4862AD', 'default': False, 'description': ''}]",9,open
Time Series Transformer - Dynamic Categorical Features,"### Feature request

I would like to have a Dynamic Categorical Feature Embedding option in TimeSeriesTransformerConfig 

### Motivation

I didn't see any option in the TimeSeriesTransformerConfig where I could define an embedding of a Dynamic Categorical Feature. I'm working with sales data and holiday is an important element of sales, so all of my models handle the holidays with a dynamic embedding. Is it the case in Time Series Transformer too, and I'm just missing something?

### Your contribution

Happy to help, but would need some guidance on how it's handled currently.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Is there any plan to add kosmos-2 to the transformers. ,"### Model description

Kosmos-2 is a grounded multimodal large language model, which integrates grounding and referring capabilities compared with Kosmos-1. The model can accept image regions selected by the user using bounding boxes as input, provide visual answers (i.e., bounding boxes), and ground the text output to the visual world.

**Is there any plan to add this model to the transformers.**

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Code: https://github.com/microsoft/unilm/tree/master/kosmos-2
Paper: https://arxiv.org/abs/2306.14824
Weight: the checkpoint can be downloaded from [here](https://conversationhub.blob.core.windows.net/beit-share-public/kosmos-2/kosmos-2.pt?sv=2021-10-04&st=2023-06-08T11%3A16%3A02Z&se=2033-06-09T11%3A16%3A00Z&sr=c&sp=r&sig=N4pfCVmSeq4L4tS8QbrFVsX6f6q844eft8xSuXdxU48%3D)  
VQA demo: [here](https://github.com/BIGBALLON/kosmos-2-gd)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",30,open
Add HyenaDNA model,"### Model description

HyenaDNA is a long-range genomic foundation model pretrained on context lengths of up to 1 million tokens at single nucleotide resolution.

I would like to add this model to the transformers.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Code: https://github.com/HazyResearch/hyena-dna
Weights: https://huggingface.co/LongSafari
Paper: https://arxiv.org/abs/2306.15794

cc @exnx","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
[WIP] Add Flax diverse group search,"# What does this PR do?

Mimics https://github.com/huggingface/transformers/pull/9006, but for Flax. 

We want to match how PyTorch's logic accounts for `group_size` and `num_beam_groups` [here](https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/generation/beam_search.py#L175) and [here](https://github.com/huggingface/transformers/blob/v4.30.2/src/transformers/generation/beam_search.py#L249C1-L281C26)

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- text models: @ArthurZucker and @younesbelkada
- vision models: @amyeroberts
- speech models: @sanchit-gandhi
- graph models: @clefourrier

Library:

- flax: @sanchit-gandhi
- generate: @gante
- pipelines: @Narsil
- tensorflow: @gante and @Rocketknight1
- tokenizers: @ArthurZucker
- trainer: @sgugger

Integrations:

- deepspeed: HF Trainer/Accelerate: @pacman100
- ray/raytune: @richardliaw, @amogkam

Documentation: @sgugger, @stevhliu and @MKhalusova

HF projects:

- accelerate: [different repo](https://github.com/huggingface/accelerate)
- datasets: [different repo](https://github.com/huggingface/datasets)
- diffusers: [different repo](https://github.com/huggingface/diffusers)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Maintained examples (not research project or legacy):

- Flax: @sanchit-gandhi
- PyTorch: @sgugger
- TensorFlow: @Rocketknight1

 -->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",5,open
Finetune ClipSeg model ,"### Feature request

Quite recently, I was exploring zero-shot classification to segment medical images. And it looks quite promising. I stumbled upon ```ClipSeg``` a few days ago and it looked wonderful and just well-suited for my work. Unfortunately, I couldn't find any tutorials or notebooks that showed how to perform fine-tuning on ClipSeg model. 

I am assuming, we have to train the decoder with a dataset containing binary classification images of cells and their corresponding masks and a text description. Unfortunately, a bit confused. is there any tutorials/resources anyone could suggest on this topic? Cuz I couldn't none. 

### Motivation

```ClipSeg``` shows a lot of potential than SAM (Segment Anything Model). Unfortunately, there's no fine-tuning script neither instructions on **How to prepare the dataset?** which is very frustrating. Will love some help from the community. 

And another point, Zero shot classification looks a way lot better option with fine-tuning than training a model like ```U-Net```, ```R-CNN``` and others from scratch while you have very few images and don't have much room to play around with.

### Your contribution

I could provide a PR on my LinkedIn, where I have a lot of AI experts as my connections and then I contribute in the programming as well. ","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",4,open
Allow `TextClassificationPipeline` to handle input longer than `model_max_length` tokens,"### Problem

Running a `TextClassificationPipeline` on a text with more tokens than its model's maximum position embeddings (e.g. 512 for BERT) like so:

```python
from transformers import pipeline
classifier = pipeline('sentiment-analysis')
classifier(""Hello, world! "" * 1000)
```

will lead to this error:

```
RuntimeError: The size of tensor a (4002) must match the size of tensor b (512) at non-singleton dimension 1
```

Note: the numbers (`4002` and `512`, above) will vary depending on the max length of the model in use and the length (in tokens) of the text that triggered the error.

(_**If you found this issue through web-searching for the above error or some other means, look at the linked PR for an implemented code fix to this problem, and consider giving a thumbs-up to this comment if you think it should be merged into the main codebase**_)

### Feature request

We should add ""chunking""/""sliding window"" functionality to `TextClassificationPipeline`, allowing it to process documents longer than the `model_max_length` of its `.model`. Specifically, this would run an instance of the model on each of several ""sliding window"" views of each input sequence, then take the mean, similar to (but somewhat simpler than) how [`TokenClassificationPipeline`](https://github.com/huggingface/transformers/blob/ad78d9597b224443e9fe65a94acc8c0bc48cd039/src/transformers/pipelines/token_classification.py#L96) does so in part by subclassing from `ChunkPipeline`.

### Motivation

It would be nice to easily do, e.g., sentiment analysis on documents longer than the `model_max_length` of the given model/tokenizer. I have in the past tried to do this in a time-sensitive context and was unable to do so.

### Your contribution

I have already opened a draft PR: #24312. I would be happy to finish the missing parts (e.g. documentation) if someone on the Huggingface team (I believe @Narsil is the appropriate person to tag) can confirm that they would accept this feature as I plan to implement it.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
SpikeGPT,"### Feature request

Extract the spiking nature of the LLM and port that [set] of features over for training/inference,. 

https://github.com/ridgerchu/SpikeGPT

### Motivation

the benefits would result in more efficient computational costs (x22 reduction).

### Your contribution

I am willing to test, trace down bugs, and push.  I'm still new in the world of llm backend coding.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
Add training support for EnCodec,"### Feature request

Would be cool to add training support for the EnCodec model. 
Not entirely sure if we can easily make it compatible with Trainer, so this can be a good second issue I think. 

### Motivation

…

### Your contribution

…","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",26,open
Finetuning Whisper with prompts,"### Feature request

Training code implementation for finetuning Whisper using prompts. 


Hi All,
I’m trying to finetune Whisper by resuming its pre-training task and adding initial prompts as part of the model’s forward pass. I saw this [amazing tutorial](https://huggingface.co/blog/fine-tune-whisper), however, it does not contain a section about using prompts as part of the fine-tuning dataset.

### Motivation

We witness that Whisper is not acting as expected when transcribing with prompts. Sometimes the output is blank text and on other occasions the output text contains reoccurrence. We want to solve such behaviors by fine-tuning Whisper with prompts. 

### Your contribution

Open for ideas.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",40,open
MeZo Forward Pass Implementation,"### Feature request

https://github.com/princeton-nlp/MeZO/blob/main/large_models/trainer.py

### Motivation

Faster training

### Your contribution

Just a user atm.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Add SPTSv2,"### Model description

SPTSv2 is the latest SOTA text spotting model from Bytedance. Given that we already support DETR, should be a breeze to support this model as well.

SPTSv2 is an improvement over the first version: https://github.com/shannanyinxiang/SPTS.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://github.com/bytedance/SPTSv2","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
"generation error when same token in ""forced_eos_token_id"" and ""supress_token"" parameter.","### System Info

- `transformers` version: 4.28.0.dev0
- Platform: Linux-5.4.0-139-generic-x86_64-with-glibc2.31
- Python version: 3.10.11
- Huggingface_hub version: 0.15.1
- Safetensors version: not installed
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help?

@gante 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

1. HuggingFace's generation method was being used in the Open-Assistant library that used HuggingFace's Model class.
https://github.com/LAION-AI/Open-Assistant/blob/0fcf3e08fe62295d4696e590005b0f33383342ea/model/model_training/utils/ppo_utils.py#L264-L267

2. generate method throw an error: RuntimeError:probability tensor contains either `int`, `nan` or element < 0
![error1](https://github.com/huggingface/transformers/assets/85441953/56771083-4800-4138-a9b1-ad45c0505d61)

3. I found that if same token is in ""forced_eos_token_id"" and ""suppress_tokens"", error occurs.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name =  ""EleutherAI/pythia-70m-deduped""
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

gen_kwargs = {'max_new_tokens': 10,
 'top_k': 0,
 'top_p': 0.7,
 'do_sample': True,
 'temperature': 1.0}

gen_kwargs[""forced_eos_token_id""] = tokenizer.eos_token_id
gen_kwargs[""suppress_tokens""] = [tokenizer.eos_token_id]
print(gen_kwargs)

question = """"""
Where is Gangnam?
""""""
batch = tokenizer.encode(f""<|prompter|>{question}<|assistant|>"", return_tensors=""pt"")

out = model.generate(
    input_ids=batch.to(model.device),
    **gen_kwargs
)
``` 

### Expected behavior

I want to know, using same token in ""forced_eos_token_id"" and ""suppress_token"" is restricted usage, because it seems to me bit contradictory.

If it is an general error, I think that something like warning should be recommended.

Or it might be my own error. ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Add keypoint-detection task,"### Feature request

Add support for keypoint detection. This includes a task, pipeline, dataset label and training pipeline.
The task is to take an image and predict the x and y locations of a set of keypoints. Which keypoints are predicted should depend on the model trained for this task. The training pipeline for keypoint detection should allow to swap components. For example, one should be able to choose the backbone to be any suitable vision transformer model that is available on the huggingface hub.

### Motivation

Keypoint detection is a use case that is prevalent in computer vision. The computer vision subset of the huggingface ecosystem would benefit from adding the popular keypoint detection task to the existing set of tasks.
At the time of writing, existing repositories for keypoint detection often focus on a single particular model, e.g.:

- yolov7: https://github.com/RizwanMunawar/yolov7-pose-estimation
- yolov8: https://docs.ultralytics.com/tasks/pose/
- vitpose: https://github.com/ViTAE-Transformer/ViTPose

The computer vision community could benefit greatly from a high quality community oriented open source hub for keypoint detection.

### Your contribution

I am happy to be part of the discussion, but probably can do little in terms of PR's at this point in time.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Add scGPT Model,"### Model description

scGPT is a single celled foundation model, based off the GPT architecture. The model is shown to have captured meaningful biological insights into cells and genes. The authors state the model can be fine tuned to downstream tasks included, cell-type annotation, genetic perturbation etc.

I'd like to add scGPT to HuggingFace Transformers.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The paper [scGPT: Towards Building a Foundation Model for Single-Cell
2 Multi-omics Using Generative AI](https://www.biorxiv.org/content/10.1101/2023.04.30.538439v1.full.pdf) by [Haotian Cui](https://www.researchgate.net/scientific-contributions/Haotian-Cui-2193100667),  [Chloe Wang](https://www.linkedin.com/in/chloe-xueqi-wang-979712158/?originalSubdomain=ca) , [Hassaan Maan](https://hsmaan.com/), [Bo Wang](https://bowang87.github.io/)

Github link: [scGPT by subercui](https://github.com/bowang-lab/scGPT)

Model Checkpoint: [Google Drive](https://drive.google.com/drive/folders/1kkug5C7NjvXIwQGGaGoqXTk_Lb_pDrBU)
- From this checkpoint I can generate the model weights","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Handle `g_state` in RWKV's customized CUDA kernel to overcome sequence length limitation,"### Feature request

Handle `g_state` in RWKV's customized CUDA kernel enables backward pass with a chained forward. As such, the maximum `context_length` will not hinder longer sequences in training, and the behavior of WKV backward is coherent with forward.

For BF16 kernels, see [here](https://github.com/Blealtan/RWKV-LM-LoRA/tree/dev-infctx/RWKV-v4neo/cuda). Credits to icecuber on RWKV Discord channel (searching for `chunked GPT mode` in the history will show the original code).

### Motivation

The current implementation of RWKV dedicates to a `max_seq_length`, propagating the sequence length parameter down to the CUDA kernel. It can be problematic with longer input sequences. By supporting `g_state` backward, we can fix the maximum sequence length inside CUDA kernel and instead call it several times until the complete sequence gets processed. Also, given the forward pass already supports state chaining, the backward should also support this.

> Some not so related advertising:
> In [my recent experiments](https://github.com/Blealtan/RWKV-LM-LoRA/tree/dev-infctx), I'm building upon the state chaining functionality (or chunked GPT mode, per icecuber's wording) to achieve near-constant VRAM training with arbitrary sequence length. The basic idea is to do forward pass of the entire model once a piece and perform checkpointing for each piece, so that at the cost of the forward pass repeated twice we get any long sequence trained within fixed VRAM. If `g_state` is supported in `transformers`, it will be easy to port that here.

### Your contribution

I can help by submitting the PR, but only later. I'm not locking that in case anyone has the time earlier than me.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
[Feature Request] Add timestamp prediction for TF Whisper,"### System Info

Latest Version.
On google Colab

### Who can help?

@sanchit-gandhi @connor-henderson 

### Information


I am trying to convert tensorflow whisper to tflite but turns out that TFWhisper doesnt want to output timestamp tokens.

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```
import tensorflow as tf


# Importing necessary classes from transformers 
from transformers import WhisperProcessor, WhisperFeatureExtractor, TFWhisperForConditionalGeneration, WhisperTokenizer

# Importing necessary functions from datasets
from datasets import load_dataset

# Creating an instance of AutoProcessor from the pretrained model
feature_extractor = WhisperFeatureExtractor.from_pretrained(""openai/whisper-tiny.en"")
tokenizer = WhisperTokenizer.from_pretrained(""openai/whisper-tiny.en"", predict_timestamps=True)
processor = WhisperProcessor(feature_extractor, tokenizer)

# Creating an instance of TFWhisperForConditionalGeneration from the pretrained model
model = TFWhisperForConditionalGeneration.from_pretrained(""openai/whisper-tiny.en"")

# Loading dataset
ds = load_dataset(""hf-internal-testing/librispeech_asr_dummy"", ""clean"", split=""validation"")

# Inputs
inputs = processor(ds[0][""audio""][""array""], return_tensors=""tf"")
input_features = inputs.input_features

# Generating Transcription
generated_ids = model.generate(input_features=input_features, return_timestamps=True)
transcription = processor.tokenizer.decode(generated_ids[0], decode_with_timestamps=True)
print(transcription)
```
<|startoftranscript|><|notimestamps|> Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.<|endoftext|>


### Expected behavior




While the same tokenizer with ```predict_timestamps=True``` works as expected in pytorch:
```
import torch
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained(""openai/whisper-tiny.en"")

ds = load_dataset(""hf-internal-testing/librispeech_asr_dummy"", ""clean"", split=""validation"")

inputs = processor(ds[0][""audio""][""array""], return_tensors=""pt"")
input_features = inputs.input_features

generated_ids = model.generate(inputs=input_features, return_timestamps=True)

transcription = processor.tokenizer.decode(generated_ids[0], decode_with_timestamps=True)
transcription
```
<|startoftranscript|><|0.00|> Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.<|5.44|><|endoftext|>
","[{'id': 1834054694, 'node_id': 'MDU6TGFiZWwxODM0MDU0Njk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/TensorFlow', 'name': 'TensorFlow', 'color': 'FF6F00', 'default': False, 'description': 'Anything TensorFlow'}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",11,open
Adding support for 3D deep learning models.,"### Feature request

Hi.
I am planning to add a new pipeline and a model for 3d deep learning tasks which can work on point clouds for classification and detection as there is no support for 3d data right now.
I just wanted to confirm if the process will be similar to the guides for adding a new pipeline and model to hugging face transformers or there are more complexities which I have not thought about?
And is it going to be too much work to add GPU support and batching?


### Motivation

I have been working with 3d deep learning and wanted to implement the whole process from scratch. So, why not contribute to hugging face so other people can use and build upon it?


### Your contribution

Submitting a PR","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Whisper with Elastic Weight Consolidation,"### Feature request

After specific language finetuning for Whisper, its ASR recognition performance in previous languages deteriorates, known as catastrophic forgetting. Therefore, something, such as the EWC, needs to be used to overcome this problem. Here is the paper of EWC: _https://arxiv.org/pdf/1612.00796.pdf_

### Motivation

When I fine-tuned Whisper large-v2 with 10 hours of Af language data, its WER in languages like Be and Is dropped to close to 90%. However, the WER of these languages under the pre-fine-tuning model is around 40%. So I hope to use the EWC to overcome or mitigate this problem.

### Your contribution

I believe in the professional ability of Hugging Face team, and I can provide data support for it.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Add LaVIN model,"### Model description

 LaVIN is a vision-language instructed model that is affordable to train (it was trained in a few hours on 8 A100 GPUs) with good performance on ScienceQA.

I'd like to add LaVIN to HF transformers.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The paper [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/pdf/2305.15023.pdf) is by [Gen Luo](https://luogen1996.github.io/),  [Yiyi Zhou](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&projects=&template=new-model-addition.yml),  [Tianhe Ren](https://rentainhe.github.io/),  [Shengxin Chen](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&projects=&template=new-model-addition.yml),  [Xiaoshuai Sun](https://sites.google.com/view/xssun),  and [Rongrong Ji](https://mac.xmu.edu.cn/rrji/)

@luogen1996 has made the code and model weights available at [github.com/luogen1996/LaVIN](https://github.com/luogen1996/LaVIN).

The weights for the following models are available at the following links:

### ScienceQA
| Model     |  Weights    |      Time | Memory | #Params |  Acc |          Weights | 
|-----------|----------:|----------:|-------:|--------:|-----:|-----------------:|
| LaVIN-7B  | LLaMA | 1.4 hours |  33.9G |    3.8M | 89.37 | [google drive](https://drive.google.com/file/d/10X2qCBYrLH1grZOHwHRMXLUoz-S6MSgV/view?usp=share_link) |
| LaVIN-7B  | Vicuna | 1.4 hours |  33.9G |    3.8M | 89.41 | [google drive](https://drive.google.com/file/d/1nuMxeiWlnJKxDybCshg8pVGSvLc5dZy8/view?usp=share_link) |
| LaVIN-13B | LLaMA |   2 hours |  55.9G |    5.4M | 90.54 | [google drive](https://drive.google.com/file/d/1LkKUY54spZkkeXrR7BDmU-xmK9YadcKM/view?usp=share_link) |

### Multimodal ChatBot
| Model     |Weights    |      Time | Memory | #Params | Acc |          Weights | 
|-----------|----------:|---------:|-------:|--------:|----:|-----------------:|
| LaVIN-13B | LLaMA | 75 hours |  55.9G |    5.4M |   - | [google drive](https://drive.google.com/file/d/1rHQNSaiGzFHYGgsamtySPYnd5AW4OE9j/view?usp=share_link)| 
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Adding GPTNeoX (Tensorflow version),"### Model description

Huggingface has GPTNeoX model by ElutherAI. It's a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.

However Huggingface currently has only PyTorch implementation of the model.
I would like to contribute it's corresponding TensorFlow implementation.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
BART-fusion,"### Model description

BART- fusion, a novel model for generating lyric interpretations from lyrics and music audio that combines a large-scale pre-trained language model with an audio encoder. It uses a cross-modal attention module to incorporate the audio representation into the lyrics representation to help the pre-trained language model understand the song from an audio perspective, while preserving the language model’s original generative performance.  Please see the paper here: https://arxiv.org/abs/2208.11671 

### Open source status

- [X] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

Here is the code repository for the paper: https://github.com/ldzhangyx/BART-fusion/tree/main.  The weights should be available in the checkpoints: https://drive.google.com/drive/folders/18EUUx-KT9xGJ1uq2UoOgj0X9BpngNn_T","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
"[i18n-ms, ISO 639-1] Translating docs to Malay","<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the Malay-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `ms` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `ms/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @ArthurZucker, @sgugger for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [ ] [index.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.mdx) https://github.com/huggingface/transformers/pull/20180
- [ ] [quicktour.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.mdx) (waiting for initial PR to go through)
- [ ] [installation.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.mdx).

## internal

## main_classes

## model_doc

## tasks
- [ ] asr.mdx
- [ ] audio_classification.mdx
- [ ] document_question_answering.mdx
- [ ] image_captioning.mdx
- [ ] image_classification.mdx
- [ ] language_modeling.mdx
- [ ] masked_language_modeling.mdx
- [ ] monocular_depth_estimation.mdx
- [ ] multiple_choice.mdx
- [ ] object_detection.mdx
- [ ] question_answering.mdx
- [ ] semantic_segmentation.mdx
- [ ] sequence_classification.mdx
- [ ] summarization.mdx
- [ ] text-to-speech.mdx
- [ ] token_classification.mdx
- [ ] translation.mdx
- [ ] video_classification.mdx
- [ ] zero_shot_image_classification.mdx
- [ ] zero_shot_object_detection.mdx


- [ ] _config.py
- [ ] _toctree.yml
- [ ] accelerate.mdx
- [ ] add_new_model.mdx
- [ ] add_new_pipeline.mdx
- [ ] add_tensorflow_model.mdx
- [ ] attention.mdx
- [ ] autoclass_tutorial.mdx
- [ ] benchmarks.mdx
- [ ] bertology.mdx
- [ ] big_models.mdx
- [ ] community.mdx
- [ ] contributing.md
- [ ] create_a_model.mdx
- [ ] custom_models.mdx
- [ ] custom_tools.mdx
- [ ] debugging.mdx
- [ ] fast_tokenizers.mdx
- [ ] geeneration_strategies.mdx
- [ ] glossary.mdx
- [ ] hpo_train.mdx
- [ ] index.mdx
- [ ] installation.mdx
- [ ] model_sharing.mdx
- [ ] model_summary.mdx
- [ ] multilingual.mdx
- [ ] notebooks.md
- [ ] pad_truncation.mdx
- [ ] perf_hardware.mdx
- [ ] perf_infer_cpu.mdx
- [ ] perf_infer_gpu_many.mdx
- [ ] perf_infer_gpu_one.mdx
- [ ] perf_infer_special.mdx
- [ ] perf_train_tpu.mdx
- [ ] perf_train_tpu_tf.mdx
- [ ] performance.mdx
- [ ] perplexity.mdx
- [ ] philosophy.mdx
- [ ] pipeline_tutorial.mdx
- [ ] pipeline_webserver.mdx
- [ ] pr_checks.mdx
- [ ] preprocessing.mdx
- [ ] quicktour.mdx
- [ ] run_scripts.mdx
- [ ] sagemaker.mdx
- [ ] serialization.mdx
- [ ] task_summary.mdx
- [ ] tasks_explained.mdx
- [ ] testing.mdx
- [ ] tf_xla.mdx
- [ ] tokenizer_summary.mdx
- [ ] torchscript.mdx
- [ ] training.mdx
- [ ] transformers_agents.mdx
- [ ] troubleshooting.mdx

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
custom stopping_critriea function doesn't receive logits scores (receives None instead),"### System Info

- `transformers` version: 4.29.2
- Platform: Linux-5.15.107+-x86_64-with-glibc2.31
- Python version: 3.10.11
- Huggingface_hub version: 0.14.1
- Safetensors version: not installed
- PyTorch version (GPU?): 2.0.1+cu118 (True)
- Tensorflow version (GPU?): 2.12.0 (True)
- Flax version (CPU?/GPU?/TPU?): 0.6.9 (gpu)
- Jax version: 0.4.8
- JaxLib version: 0.4.7
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Reproduction Steps:

1. Initialize a BART model & its tokenizer (in my case it is facebook/bart-large)
2. Create a custom stopping_criteria function and add it to StoppingCriteriaList object
3. Run model.generate() with the your stopping criteria list as argument

Scores argument is always None

Example code:
```python
import torch
from transformers import StoppingCriteriaList, BartForConditionalGeneration, BartTokenizer

def custom_stopping_criteria(input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
    print (""Scores:"", scores)
    return False

stopping_criteria = StoppingCriteriaList([custom_stopping_criteria])

model = BartForConditionalGeneration.from_pretrained(""facebook/bart-large"", forced_bos_token_id=0)
tok = BartTokenizer.from_pretrained(""facebook/bart-large"")

example_english_phrase = ""UN Chief Says There Is No <mask> in Syria""
batch = tok(example_english_phrase, return_tensors=""pt"")

model.generate(batch[""input_ids""], stopping_criteria=stopping_criteria)
```

The above code uses a stopping critriea that just prints the scores value when called (which prints None)

### Expected behavior

The expected behavior should be to have Scores logits populated with values instead of being None (values before or after softmax don't matter)","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
Unexpected padding behaviour of `ClapFeatureExtractor`,"### System Info

- `transformers` version: 4.29.2
- Platform: Linux-4.15.0-204-generic-x86_64-with-glibc2.31
- Python version: 3.10.10
- Huggingface_hub version: 0.14.1
- Safetensors version: not installed
- PyTorch version (GPU?): 1.13.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

### Who can help?

@ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Adding `padding=True` argument to `ClapFeatureExtractor` changes the padding strategy from `repeatpad`, which is the default, to constant padding ([code](https://github.com/huggingface/transformers/blob/04ab5605fbb4ef207b10bf2772d88c53fc242e83/src/transformers/models/clap/feature_extraction_clap.py#L248)).

Code adapted from: https://huggingface.co/docs/transformers/model_doc/clap#transformers.ClapModel.forward.example

```python
from datasets import load_dataset
from transformers import AutoProcessor

# load data
dataset = load_dataset(""ashraq/esc50"")
audio_sample = dataset[""train""][""audio""][0][""array""]

# load data processor
processor = AutoProcessor.from_pretrained(""laion/clap-htsat-unfused"")

# pre-process data
inputs1 = processor.feature_extractor(audio_sample, return_tensors=""pt"")
inputs2 = processor.feature_extractor(audio_sample, return_tensors=""pt"", padding=True)

print((inputs1[""input_features""] - inputs2[""input_features""]).max())
# Output: tensor(119.4260)
```

This becomes a problem, for instance, when using `ClapProcessor`. `ClapProcessor` shares `kwargs` between the `tokenizer` and the `feature_extractor` ([code](https://github.com/huggingface/transformers/blob/04ab5605fbb4ef207b10bf2772d88c53fc242e83/src/transformers/models/clap/processing_clap.py#LL87C9-L87C9)). When using text inputs of different length, you need to pass `padding=True` argument to the `tokenizer`, but doing so changes the behaviour of the `feature_extractor`.

### Expected behavior

1. Either don't allow `padding=True` argument. Assert its value to be one of the allowed values - `repeatpad`, `repeat`, and `pad` in case of `ClapFeatureExtractor`.
2. Or, use the default padding strategy if `padding=True`.

As for sharing `kwargs`, I don't think that's a good idea. Would having two arguments, `tokenizer_kwargs` and `feature_extractor_kwargs` be better? ","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",11,open
Wandb sweeps integraition: custom objective,"### System Info

- `transformers` version: 4.29.2
- Platform: Linux-5.14.0-162.6.1.el9_1.0.1.x86_64-x86_64-with-glibc2.34
- Python version: 3.10.10
- Huggingface_hub version: 0.14.1
- Safetensors version: not installed
- PyTorch version (GPU?): 2.0.1+cu117 (True) -- using torch in my experiments
- Tensorflow version (GPU?): 2.12.0 (True)
- Flax version (CPU?/GPU?/TPU?): 0.6.10 (cpu)
- Jax version: 0.4.10
- JaxLib version: 0.4.10


### Who can help?

@sgugger

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

I followed [this guide](https://huggingface.co/docs/transformers/hpo_train) to use wandb sweeps with the trainer by modifying the summarization scripts slightly. Below are the hp_space, model_init, and hyperparameter_search commands that I use.

Most notably, the objective is to maximize the sum of sari and rougeLsum.

```python
    def wandb_hp_space(trial):
        return {
            ""method"": ""bayes"",
            ""metric"": {
                ""name"": ""objective"",
                ""goal"": ""minimize"" if hyperopt_args.hparam_optimize_for_loss else ""maximize""
            },
            ""parameters"": {
                ""num_train_epochs"": {""min"": hyperopt_args.hparam_epoch_min, ""max"": hyperopt_args.hparam_epoch_max}
            },
            ""run_cap"": hyperopt_args.hparam_max_trials
        }

    def model_init(trial):
        return AutoModelForSeq2SeqLM.from_pretrained(
            model_args.model_name_or_path,
            from_tf=bool("".ckpt"" in model_args.model_name_or_path),
            config=config,
            cache_dir=model_args.cache_dir,
            revision=model_args.model_revision,
            use_auth_token=True if model_args.use_auth_token else None,
        )

    def hparam_objective(metrics: Dict[str, float]) -> float:
        metrics = copy.deepcopy(metrics)

        if hyperopt_args.hparam_optimize_for_loss:
            return metrics[""eval_loss""]

        return metrics[""eval_rougeLsum""] + metrics[""eval_sari""]

    best_trial = trainer.hyperparameter_search(
        compute_objective=hparam_objective,
        backend=""wandb"",
        # I think that this is only used to set the column in the sweep chart but does not mean that we use
        # this metric only for optimization. That is what the hparam_objective is for?
        metric=""eval/sari"",
        hp_space=wandb_hp_space,
        n_trials=hyperopt_args.hparam_max_trials,
        direction=""minimize"" if hyperopt_args.hparam_optimize_for_loss else ""maximize"",
    )
```

The problem is that when I look at the wandb interface at the generated sweep config, it looks like this:

```yaml
method: bayes
metric:
  goal: maximize
  name: eval/sari
parameters:
  num_train_epochs:
    distribution: int_uniform
    max: 30
    min: 2
run_cap: 16
```

So the generated sweep config includes `eval/sari` as the metric name because I passed it in the `hyperparameter_search`. But as you can read in the comment, I thought this was only for the wandb visualization but now I am not so sure. When I leave out the `metric` keyword (as in the example), wandb seems to fallback to `eval/loss`.

```yaml
method: bayes
metric:
  goal: maximize
  name: eval/loss
parameters:
  num_train_epochs:
    distribution: int_uniform
    max: 30
    min: 2
run_cap: 16
```

My worry here is the disconnect between the generated sweep config and the custom objective function. What is wandb optimizing now? My custom objective function (sari+rougeLsum) or `metric` that is passed to `hyperparameter_search` for `direction`?

### Expected behavior

More clarity about the relationship between a wandb sweep configuration and how the trainer uses wandb as a backend, and the importance of `metric` and `direciton` arguments vs. `compute_objective`.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 6675366189, 'node_id': 'LA_kwDOCUB6oc8AAAABjeIBLQ', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Integrations', 'name': 'Integrations', 'color': '051E90', 'default': False, 'description': ''}]",17,open
Use python generator instead of streamer for generation,"### Feature request

Add an option for receiving tokens (or similar) as they are generated via a [python generator](https://wiki.python.org/moin/Generators) as an alternative to needing a streamer object.

### Motivation

There is a new feature [streamers](https://huggingface.co/docs/transformers/generation_strategies#streaming) for accessing the tokens being generated during generation. Usage of this object requires you to run some code in parallel while the model.generate function blocks it's current thread. You need to instead have your processing code defined like a callback within the streamer object you are using.

A much simpler interface that solves this same problem is to yield the token sequences as they are generated with a [python generator](https://wiki.python.org/moin/Generators). Below is example usage for either case...

## Proposed Generator Implementation
```
for token in model.generate(**inputs, max_new_tokens=20, yield_tokens=True):
   print(f""The next token is {token}"")
```

## Current Streamer Implementation
from transformers import AutoModelForCausalLM, TextStreamer
```
class MyStreamer:
   def __init__(self):
      pass
   def put(self, token):
      print(f""The next token is {token}"")
   def end():
      pass
_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
```
Not only does the generator implementation save on lines of code/simplify syntax, but python generators return iterables which has the benefit of making it easy to use all sorts of existing python tools without modification. For example, you can

### Enumerate
```
for idx, token in enumerate(model.generate(**inputs, max_new_tokens=20, yield_tokens=True)):
   print(f""The {idx}'th token is {token}"")
```

### Progress bar with TQDM
Progress bar appears in CLI or jupyter notebook, updating in real time
```
for token in tqdm(model.generate(**inputs, max_new_tokens=20, yield_tokens=True)):
   my_endpoint.post(token)
```
And there's many many more tools that would easily integrate!

In this case I proposed tokens because it's easier to think about that way, and it matches the current streamer implementation, but it may be easier to implement yielding a list of lists of tokens, since for beam search and similar multiple beams (multiple sequences) are being considered at any given time. This would enable more features on the developer side, esp in the case where you may want to generate multiple sequences in one call. But this is more of a sidenote and either of this or the base implementation would be really awesome.

### Your contribution

I'm not planning to put in a PR anytime soon, but I did have a look through the code before finding the new streamer WIP feature. It seems like it would be fairly easy to implement a version of what I am describing. You just need to add a flag to optionally
```
yield new_token
```
inside each of beam_search, beam_sample, greedy_search, etc- and then update the model.generate wrapper to also optionally yield the results from each of these.

In this case I proposed tokens because it's easier to think about that way, and it matches the current streamer implementation, but it may be easier to implement yielding a list of lists of tokens, since for beam search and such multiple beams are being considered at any given time.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",5,open
"Make it easy to get seperate ""prints"" for individual runs/ users when using Transformers Agent","### Feature request

I have started exploring the new Transformers Agent. And I would like to build a UI to help me speed up the process.

I might be running multiple runs in parallel or have multiple users using my application. I would like to be able to stream the information from the run as it arrives. I would like to store the information in a database containing all the runs I've done.

Currently all the valuable information about the run is printed I.e. you are using print to inform me like below

```bash
==Explanation from the agent==
I will use the following  tool: `image_generator` to generate an image.


==Code generated by the agent==
image = image_generator(prompt=""rivers and lakes"")


==Result==
<PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x7F8DDC11C4C0>
```

This is for example done in `agenst.py`

![image](https://github.com/huggingface/transformers/assets/42288570/ed635571-63f2-45b4-9cc5-8acd33dcbc71)

Using `print` makes it hard for me to distinguish between multiple runs/ users. Especially if run in parallel.

Please provide a simple to use method to stream each run individually. It could be as simple as adding a `print` (or `write`) argument to the `Agent.run`, `HFAgent.run` and `OpenAI.run` method.

Alternatively some `run_id` argument could be provided and printed as well. Then I can split the stream that comes in by `run_id`. This is less preferred though that this also adds some complexity.

### Motivation

This will make it much, much easier to create interesting AI apps.

### Your contribution

I might do it 😄 . But I hope someone with knowledge of the code base would do it.

### Additional Context

An async `.run_async` function would also be much appreciated as my UI is built on top of Tornado. This will help me keep the app responsive.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Flaky Whisper PT-TF & PT-Flax Equivalence Test,"### System Info

transformers 4.29.0 dev

### Who can help?

@ArthurZucker @sanchit-gandhi @ydshieh 

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Flaky test, so not reproducible. Example run where the error occurred: 

* https://app.circleci.com/pipelines/github/huggingface/transformers/64100/workflows/b4463c5d-b3dc-4b00-a7cd-19acd096cb07/jobs/792381
* https://app.circleci.com/pipelines/github/huggingface/transformers/64111/workflows/dc9092c4-0673-46c7-b89f-f805bc20128c/jobs/792557
* https://app.circleci.com/pipelines/github/huggingface/transformers/64230/workflows/e2d42ca4-f367-4a85-9054-a0ea99e49849/jobs/794534

Occasionally, the PT-TF and PT-Flax whisper equivalence test fails. The tolerance was increased in #23257 and #23288 but the reason for recent failures has not yet been found. 
 
### Expected behaviour

Equivalence tests reliably pass. ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
[New model] ImageBind: One Embedding Space To Bind Them All,"### Model description

As stated in their [blog post](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/), 

> ""[ImageBind is] the first AI model capable of binding information from six modalities. The [model](https://github.com/facebookresearch/ImageBind) learns a single embedding, or shared representation space, not just for text, image/video, and audio, but also for sensors that record depth (3D), thermal (infrared radiation), and inertial measurement units (IMU), which calculate motion and position.""



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

GitHub repo: https://github.com/facebookresearch/ImageBind
Paper: https://facebookresearch.github.io/ImageBind/paper
Blog: https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/
Demo: https://imagebind.metademolab.com/
Video: https://dl.fbaipublicfiles.com/imagebind/imagebind_video.mp4
Weights: https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth (currently only 1 that I can see)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
NSP Support for Zero-shot Text Classification Pipeline,"### Feature request

Zero-shot classification can be solved with NextSentencePrediction task of BERT, and it has shown competitive results to NLI-based zero-shot classification in some cases. There could be a parameter where we choose the type of submethod that we are going to use for the pipeline like `pipeline(task=""zero-shot-classification"", type_=""nsp"")` or we could just simply add a task named ""nsp-zeroshot-classification"". This is also possible for MLM, which is a more widely used pretraining task across LMs.

### Motivation

Like I said, NSP has proven to be useful especially for languages that do not have access to NLI dataset since only pre-training is enough. Although multilingual NLI models can also be used, they have been proven to be worse compared to smaller monolingual models in this task, as one would expect. Even if this is a small detail which would be unnecessary to put into the codebase, I wanted to share this implementation so that anyone who's interested can take a look and try different methods.

Here are some references, one of which is my study, that use NSP for zero-shot classification.

Sun, Y., Zheng, Y., Hao, C., & Qiu, H. (2021). NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction. arXiv preprint arXiv:2109.03564.

Çelik, E., & Dalyan, T. (2023). Unified benchmark for zero-shot Turkish text classification. Information Processing & Management, 60(3), 103298.

### Your contribution

I can open a PR, here's the implementation I did based on Sun et al. 2021. It is heaily based on the current NLI zeroshot pipeline class, but also adds a `reverse` argument which changes the order of the sentences for NSP.

```python
import numpy as np
from typing import List, Union

from transformers.utils import logging
from transformers.pipelines.base import ChunkPipeline, ArgumentHandler
from transformers.tokenization_utils import TruncationStrategy
from transformers.pipelines import ZeroShotClassificationArgumentHandler

logger = logging.get_logger(__name__)


class ZeroShotClassificationArgumentHandler(ArgumentHandler):
    def _parse_labels(self, labels):
        if isinstance(labels, str):
            labels = [label.strip() for label in labels.split("","") if label.strip()]
        return labels

    def __call__(self, sequences, labels, hypothesis_template, reverse):
        if len(labels) == 0 or len(sequences) == 0:
            raise ValueError(
                ""You must include at least one label and at least one sequence.""
            )
        if hypothesis_template.format(labels[0]) == hypothesis_template:
            raise ValueError(
                (
                    'The provided hypothesis_template ""{}"" was not able to be formatted with the target labels. '
                    ""Make sure the passed template includes formatting syntax such as {{}} where the label should go.""
                ).format(hypothesis_template)
            )

        if isinstance(sequences, str):
            sequences = [sequences]

        sequence_pairs = []
        for sequence in sequences:
            if reverse:
                sequence_pairs.extend(
                    [[hypothesis_template.format(label), sequence] for label in labels]
                )
            else:
                sequence_pairs.extend(
                    [[sequence, hypothesis_template.format(label)] for label in labels]
                )

        return sequence_pairs, sequences


class NSPZeroShotClassificationPipeline(ChunkPipeline):
    def __init__(
        self, args_parser=ZeroShotClassificationArgumentHandler(), *args, **kwargs
    ):
        self._args_parser = args_parser
        super().__init__(*args, **kwargs)

    @property
    def isNext_id(self):
        return 0

    def _parse_and_tokenize(
        self,
        sequence_pairs,
        padding=True,
        add_special_tokens=True,
        truncation=TruncationStrategy.ONLY_FIRST,
        **kwargs,
    ):
        return_tensors = self.framework
        if self.tokenizer.pad_token is None:
            logger.error(
                ""Tokenizer was not supporting padding necessary for zero-shot, attempting to use ""
                "" `pad_token=eos_token`""
            )
            self.tokenizer.pad_token = self.tokenizer.eos_token
        try:
            inputs = self.tokenizer(
                sequence_pairs,
                add_special_tokens=add_special_tokens,
                return_tensors=return_tensors,
                padding=padding,
                truncation=truncation,
            )
        except Exception as e:
            if ""too short"" in str(e):
                inputs = self.tokenizer(
                    sequence_pairs,
                    add_special_tokens=add_special_tokens,
                    return_tensors=return_tensors,
                    padding=padding,
                    truncation=TruncationStrategy.DO_NOT_TRUNCATE,
                )
            else:
                raise e

        return inputs

    def _sanitize_parameters(self, **kwargs):
        if kwargs.get(""multi_class"", None) is not None:
            kwargs[""multi_label""] = kwargs[""multi_class""]
            logger.warning(
                ""The `multi_class` argument has been deprecated and renamed to `multi_label`. ""
                ""`multi_class` will be removed in a future version of Transformers.""
            )
        preprocess_params = {}
        if ""candidate_labels"" in kwargs:
            preprocess_params[""candidate_labels""] = self._args_parser._parse_labels(
                kwargs[""candidate_labels""]
            )
        if ""hypothesis_template"" in kwargs:
            preprocess_params[""hypothesis_template""] = kwargs[""hypothesis_template""]
        if ""reverse"" in kwargs:
            preprocess_params[""reverse""] = kwargs[""reverse""]

        postprocess_params = {}
        if ""multi_label"" in kwargs:
            postprocess_params[""multi_label""] = kwargs[""multi_label""]
        return preprocess_params, {}, postprocess_params

    def __call__(
        self,
        sequences: Union[str, List[str]],
        *args,
        **kwargs,
    ):
        if len(args) == 0:
            pass
        elif len(args) == 1 and ""candidate_labels"" not in kwargs:
            kwargs[""candidate_labels""] = args[0]
        else:
            raise ValueError(f""Unable to understand extra arguments {args}"")

        return super().__call__(sequences, **kwargs)

    def preprocess(
        self,
        inputs,
        candidate_labels=None,
        hypothesis_template=""This example is {}."",
        reverse=False,
    ):
        sequence_pairs, sequences = self._args_parser(
            inputs, candidate_labels, hypothesis_template, reverse
        )

        for i, (candidate_label, sequence_pair) in enumerate(
            zip(candidate_labels, sequence_pairs)
        ):
            model_input = self._parse_and_tokenize([sequence_pair])

            yield {
                ""candidate_label"": candidate_label,
                ""sequence"": sequences[0],
                ""is_last"": i == len(candidate_labels) - 1,
                **model_input,
            }

    def _forward(self, inputs):
        candidate_label = inputs[""candidate_label""]
        sequence = inputs[""sequence""]
        model_inputs = {k: inputs[k] for k in self.tokenizer.model_input_names}
        outputs = self.model(**model_inputs)

        model_outputs = {
            ""candidate_label"": candidate_label,
            ""sequence"": sequence,
            ""is_last"": inputs[""is_last""],
            **outputs,
        }
        return model_outputs

    def postprocess(self, model_outputs, multi_label=False):
        candidate_labels = [outputs[""candidate_label""] for outputs in model_outputs]
        sequences = [outputs[""sequence""] for outputs in model_outputs]
        logits = np.concatenate([output[""logits""].numpy() for output in model_outputs])
        N = logits.shape[0]
        n = len(candidate_labels)
        num_sequences = N // n
        reshaped_outputs = logits.reshape((num_sequences, n, -1))

        if multi_label or len(candidate_labels) == 1:
            isNext_id = self.isNext_id
            notNext_id = 1
            isNext_contr_logits = reshaped_outputs[..., [notNext_id, isNext_id]]
            scores = np.exp(isNext_contr_logits) / np.exp(isNext_contr_logits).sum(
                -1, keepdims=True
            )
            scores = scores[..., 1]
        else:
            isNext_logits = reshaped_outputs[..., self.isNext_id]
            scores = np.exp(isNext_logits) / np.exp(isNext_logits).sum(
                -1, keepdims=True
            )

        top_inds = list(reversed(scores[0].argsort()))
        return {
            ""sequence"": sequences[0],
            ""labels"": [candidate_labels[i] for i in top_inds],
            ""scores"": scores[0, top_inds].tolist(),
        }

```

This task can be used by registering it to the tasks, shown in example below:
```python
from nsp import NSPZeroShotClassificationPipeline
from transformers.pipelines import PIPELINE_REGISTRY
from transformers import BertForNextSentencePrediction, TFBertForNextSentencePrediction

PIPELINES = [
    dict(
        task=""nsp-zeroshot-classification"",
        pipeline_class=NSPZeroShotClassificationPipeline,
        pt_model=BertForNextSentencePrediction,
        tf_model=TFBertForNextSentencePrediction,
        default={""pt"": (""bert-base-uncased"")},
        type=""text"",
    )
]

for p in PIPELINES:
    PIPELINE_REGISTRY.register_pipeline(**p)
```","[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNzcxMTg3OTI0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Pipeline', 'name': 'Core: Pipeline', 'color': 'FF7066', 'default': False, 'description': 'Internals of the library; Pipeline.'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
ASR example doesn't save tokenizer settings,"### System Info

- `transformers` version: 4.28.1
- Platform: Windows-10-10.0.22621-SP0
- Python version: 3.11.2
- Huggingface_hub version: 0.14.1
- Safetensors version: not installed
- PyTorch version (GPU?): 2.0.1+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: NO
- Using distributed or parallel set-up in script?: NO

### Who can help?

@sgugger

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Run training using [run_speech_recognition_ctc.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py) and the included json file.

[train.json.zip](https://github.com/huggingface/transformers/files/11425889/train.json.zip)

Next, attempt to infer using the trained model:

```py
import os.path

from datasets import load_dataset
from datasets import Audio
from transformers import pipeline, AutomaticSpeechRecognitionPipeline

cv13 = load_dataset(
    ""mozilla-foundation/common_voice_13_0"",
    ""eo"",
    split=""train[:10]"",
    )
print(cv13[0])
cv13 = cv13.cast_column(""audio"", Audio(sampling_rate=16000))
sampling_rate = cv13.features[""audio""].sampling_rate
audio_file = cv13[0][""audio""][""path""]
d, n = os.path.split(audio_file)
audio_file = os.path.join(d, ""eo_train_0"", n)
print(audio_file)

transcriber: AutomaticSpeechRecognitionPipeline = pipeline(
    ""automatic-speech-recognition"",
    model=""xekri/wav2vec2-common_voice_13_0-eo-demo2"",
)
print(transcriber(audio_file))
```

Output:

```
Found cached dataset common_voice_13_0 (C:/Users/rober/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/eo/13.0.0/22809012aac1fc9803eaffc44122e4149043748e93933935d5ea19898587e4d7)
{'client_id': 'b8c51543fe043c8f27d0de0428e060e309d9d824ac9ad33e40aba7062dafd99e2e87bbedc671007e31973afb599b1c290dbd922637b79132727b5f37bc1ee88e', 'path': 'C:\\Users\\rober\\.cache\\huggingface\\datasets\\downloads\\extracted\\1dea8f044902d398c6cb09bfb5629dc2fbd80a6309ddd435c4554fa38f730472\\common_voice_eo_20453647.mp3', 'audio': {'path': 'C:\\Users\\rober\\.cache\\huggingface\\datasets\\downloads\\extracted\\1dea8f044902d398c6cb09bfb5629dc2fbd80a6309ddd435c4554fa38f730472\\common_voice_eo_20453647.mp3', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,
       -1.16407300e-11,  1.07661449e-12, -1.71219774e-11]), 'sampling_rate': 48000}, 'sentence': 'Ĉu ili tiel plaĉas al vi?', 'up_votes': 2, 'down_votes': 0, 'age': 'twenties', 'gender': 'male', 'accent': 'Internacia', 'locale': 'eo', 'segment': '', 'variant': ''}
C:\Users\rober\.cache\huggingface\datasets\downloads\extracted\1dea8f044902d398c6cb09bfb5629dc2fbd80a6309ddd435c4554fa38f730472\eo_train_0\common_voice_eo_20453647.mp3
Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.27k/2.27k [00:00<?, ?B/s]
F:\eo-reco\.env\Lib\site-packages\huggingface_hub\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\rober\.cache\huggingface\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.26G/1.26G [01:56<00:00, 10.8MB/s]
Traceback (most recent call last):
  File ""F:\eo-reco\infer.py"", line 20, in <module>
    transcriber: AutomaticSpeechRecognitionPipeline = pipeline(
                                                      ^^^^^^^^^
  File ""F:\eo-reco\.env\Lib\site-packages\transformers\pipelines\__init__.py"", line 876, in pipeline
    tokenizer = AutoTokenizer.from_pretrained(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\eo-reco\.env\Lib\site-packages\transformers\models\auto\tokenization_auto.py"", line 723, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\eo-reco\.env\Lib\site-packages\transformers\tokenization_utils_base.py"", line 1795, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'xekri/wav2vec2-common_voice_13_0-eo-demo2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'xekri/wav2vec2-common_voice_13_0-eo-demo2' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.
```

Checking the uploaded repo, it seems that no tokenizer-related files (e.g. `vocab.json`, `tokenizer_config.json`, etc) were pushed.

I added some debug to `run_speech_recognition_ctc.py` and found that these files were generated locally, but got deleted locally during step 7 when `Trainer` was initialized (line 701).

The output from `run_speech_recognition_ctc.py` at that point was:

```
loading file vocab.json
loading file tokenizer_config.json
loading file added_tokens.json
loading file special_tokens_map.json
Adding <s> to the vocabulary
Adding </s> to the vocabulary
Cloning https://huggingface.co/xekri/wav2vec2-common_voice_13_0-eo-demo into local empty directory.
05/08/2023 15:06:23 - WARNING - huggingface_hub.repository - Cloning https://huggingface.co/xekri/wav2vec2-common_voice_13_0-eo-demo into local empty directory.
max_steps is given, it will override any value given in num_train_epochs
```

It seems that instantiating `Training` with `push_to_hub=true` creates a new repo and then empties anything in the local directory so that it can clone the repo into it. This deletes any files written to the local directory, which includes the tokenizer configs.


### Expected behavior

No error.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 6470596964, 'node_id': 'LA_kwDOCUB6oc8AAAABga15ZA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Audio', 'name': 'Audio', 'color': '760453', 'default': False, 'description': ''}]",8,open
"Detr Models cannot be loaded with `device_map=""auto""`","### System Info

- `transformers` version: 4.28.1
- Platform: macOS-13.1-x86_64-i386-64bit
- Python version: 3.9.2
- Huggingface_hub version: 0.12.1
- Safetensors version: not installed
- PyTorch version (GPU?): 1.13.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help?

_No response_

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

```python
from transformers import pipeline

p = pipeline(
    ""object-detection"", 
    model=""facebook/detr-resnet-50"", 
    image_processor=""facebook/detr-resnet-50"", 
    device_map=""auto""
)
```

### Expected behavior

This does not work because the `transformers.models.detr.modeling_detr.DetrConvEncoder` model init involves copy weights from `nn.BatchNorm2d` to `DetrFrozenBatchNorm2d` which is not allowed when on a meta device.

```
 File ""/Users/chiragjn/venv39/lib/python3.9/site-packages/transformers/pipelines/__init__.py"", line 779, in pipeline
    framework, model = infer_framework_load_model(
  File ""/Users/chiragjn/venv39/lib/python3.9/site-packages/transformers/pipelines/base.py"", line 262, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File ""/Users/chiragjn/venv39/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py"", line 471, in from_pretrained
    return model_class.from_pretrained(
  File ""/Users/chiragjn/venv39/lib/python3.9/site-packages/transformers/modeling_utils.py"", line 2629, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File ""/Users/chiragjn/venv39/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py"", line 1373, in __init__
    self.model = DetrModel(config)
  File ""/Users/chiragjn/venv39/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py"", line 1205, in __init__
    backbone = DetrConvEncoder(config)
  File ""/Users/chiragjn/venv39/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py"", line 354, in __init__
    replace_batch_norm(backbone)
  File ""/Users/chiragjn/venv39/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py"", line 314, in replace_batch_norm
    frozen.weight.data.copy_(bn.weight)
NotImplementedError: Cannot copy out of meta tensor; no data!
```

The model loads fine with a specific device with `device` argument. ","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",13,open
VideoMAEForVideoClassification does not support `device_map='auto'` yet.,"### Feature request

Support for `device_map = 'auto'` so that the VideoMAE models can be run with Int8 mixed precision. For reproducibility, here is what I get when I run the command in a collab notebook (w/ GPU) with accelerate and bitsandbytes installed:

```
from transformers import AutoModelForVideoClassification

model_name = 'MCG-NJU/videomae-base-finetuned-ssv2 #Example checkpoint
model = AutoModelForVideoClassification.from_pretrained(model_name,load_in_8bit=True,device_map='auto')
```

Which gives the following error message:

```
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ in <cell line: 4>:4                                                                              │
│                                                                                                  │
│ /usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471 in          │
│ from_pretrained                                                                                  │
│                                                                                                  │
│   468 │   │   │   )                                                                              │
│   469 │   │   elif type(config) in cls._model_mapping.keys():                                    │
│   470 │   │   │   model_class = _get_model_class(config, cls._model_mapping)                     │
│ ❱ 471 │   │   │   return model_class.from_pretrained(                                            │
│   472 │   │   │   │   pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   │
│   473 │   │   │   )                                                                              │
│   474 │   │   raise ValueError(                                                                  │
│                                                                                                  │
│ /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2703 in from_pretrained   │
│                                                                                                  │
│   2700 │   │   │   )                                                                             │
│   2701 │   │   │                                                                                 │
│   2702 │   │   │   if model._no_split_modules is None:                                           │
│ ❱ 2703 │   │   │   │   raise ValueError(f""{model.__class__.__name__} does not support `device_m  │
│   2704 │   │   │   no_split_modules = model._no_split_modules                                    │
│   2705 │   │   │   if device_map not in [""auto"", ""balanced"", ""balanced_low_0"", ""sequential""]:    │
│   2706 │   │   │   │   raise ValueError(                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
ValueError: VideoMAEForVideoClassification does not support `device_map='auto'` yet.
```




### Motivation

I saw a similar issue #22018 which got resolved really quickly. Hoping that this won't be a lot of work to incorperate into the VideoMAE models  :slightly_smiling_face: 

### Your contribution

Would prefer if someone more familiar with the repo did this instead (it doesn't appear to be much work if the update is like #22207 but I didn't understand what the change did and don't currently have time to study the codebase)","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 5151155822, 'node_id': 'LA_kwDOCUB6oc8AAAABMwhmbg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Big%20Model%20Inference', 'name': 'Big Model Inference', 'color': '006b75', 'default': False, 'description': 'Problems related to the Big Model Inference capabilities provided by Accelerate'}]",11,open
[New model] 🐸TTS advanced Text-to-Speech,"### Model description

🐸TTS is a library for advanced Text-to-Speech generation. It's built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality. 🐸TTS comes with pretrained models, tools for measuring dataset quality and already used in 20+ languages for products and research projects.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

GithHub repo: https://github.com/coqui-ai/TTS
Samples: http://erogol.com/ddc-samples/","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",20,open
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation ,"### System Info

transformers                 4.28.1
torch                              2.0.0
torchaudio                     2.0.0
torchvision                     0.15.0
huggingface-hub          0.13.4
trl                                    0.4.2.dev0

### Who can help?

Probably people from accelerate, trainer, and text: 
@pacman100, @sgugger, @ArthurZucker

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

1. Install the TRL package from (https://github.com/lvwerra/trl)
2. Clone the package and go to `trl/examples/summarization/scripts`
3. Setup `accelerate config` like this
```
compute_environment: LOCAL_MACHINE
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_offload_params: false
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_transformer_layer_cls_to_wrap: GPT2Block
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```
4. call `accelerate launch reward_summarization.py` 

This results in the following error:

```
/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: Error detected in WhereBackward0. Traceback of forward call that caused the error:
  File ""reward_summarization.py"", line 203, in <module>
    trainer.train(script_args.resume_from_checkpoint)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/trainer.py"", line 1662, in train
    return inner_training_loop(
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/trainer.py"", line 1929, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/trainer.py"", line 2699, in training_step
    loss = self.compute_loss(model, inputs)
  File ""reward_summarization.py"", line 185, in compute_loss
    rewards_j = model(input_ids=inputs[""input_ids_j""], attention_mask=inputs[""attention_mask_j""])[0]
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/nn/parallel/distributed.py"", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/nn/parallel/distributed.py"", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py"", line 1420, in forward
    transformer_outputs = self.transformer(
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py"", line 899, in forward
    outputs = block(
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py"", line 389, in forward
    attn_outputs = self.attn(
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py"", line 330, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py"", line 201, in _attn
    attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)
 (Triggered internally at /opt/conda/conda-bld/pytorch_1678402379298/work/torch/csrc/autograd/python_anomaly_mode.cpp:114.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File ""reward_summarization.py"", line 203, in <module>
    trainer.train(script_args.resume_from_checkpoint)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/trainer.py"", line 1662, in train
    return inner_training_loop(
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/trainer.py"", line 1929, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/transformers/trainer.py"", line 2717, in training_step
    loss.backward()
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/_tensor.py"", line 487, in backward
    torch.autograd.backward(
  File ""/home/ubuntu/miniconda3/envs/trl/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [CUDABoolType [1, 1, 385, 385]] is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
```


### Expected behavior

I expect it should run fine, but it ends in that error. Although it is not a native huggingFace code, it seems that it the issue is from the gpt2 trainer code.  ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",14,open
Need support for Sentence Similarity Pipeline,"### Feature request

HuggingFace now has a lot of Sentence Similarity models, but the pipeline does not yet support this: https://huggingface.co/docs/transformers/main_classes/pipelines

### Motivation

HuggingFace now has a lot of Sentence Similarity models, but the pipeline does not yet support this: https://huggingface.co/docs/transformers/main_classes/pipelines

### Your contribution

I can write a PR, but might need some one else's help.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
`prefix_allowed_tokens_fn` do not constrain when all allowed tokens have scores of `-inf`,"### System Info

transformer 4.25.1
python 3.8.16

### Who can help?

@gante

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

When using `generate()` with `prefix_allowed_tokens_fn`, (more precisely, when using `PrefixConstrainedLogitsProcessor`),
when all tokens returned by `prefix_allowed_tokens_fn` have scores of `-inf`, the model does not comply with the constraints and picks the token which is not on the allowed token list.

### Expected behavior

Even if all allowed tokens have score of `-inf`, the model should pick tokens from allowed token list by `prefix_allowed_tokens_fn`.
I think it can be solved by using some clamp function or adding epsilon value to this code.
https://github.com/huggingface/transformers/blob/04ab5605fbb4ef207b10bf2772d88c53fc242e83/src/transformers/generation/logits_process.py#L692-L698

This is my own code to solve it.
However, it might cause other bugs.

```python
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        masked_score = torch.full_like(scores, -math.inf)
        for batch_id, beam_sent in enumerate(input_ids.view(-1, self._num_beams, input_ids.shape[-1])):
            for beam_id, sent in enumerate(beam_sent):
                allowed_idx = batch_id * self._num_beams + beam_id, self._prefix_allowed_tokens_fn(batch_id, sent)
                filtered_scores = torch.clamp(scores[allowed_idx], min=-10 ** 6)
                masked_score[allowed_idx] = filtered_scores
        return masked_score
```
Edit: The model works well on `torch.clamp()` with `min=-10 ** 6`, not `min=-10 ** 8`, when all allowed token's score is -inf. Too low score token in the sequence may have affected the decoding step. I updated the above code.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Add an efficient vision transformer backbone in ICLR 2022: CrossFormer,"### Model description

The CrossFormer has three new things that does not exist in other ViTs (such as Swin):

1. The cross-scale embedding layer(CEL) that generate cross-scale embeddings as ViT's input.
2. The long-short distance attention (LSDA) mechanism, which is an efficient replacement of the vanilla self-attention and shows better performance than Swin
3. A dynamic relative position bias, a kind of relative position bias that support dynamic group size.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The open source website: https://github.com/cheerss/CrossFormer
The paper was accepted in ICLR 2022: https://openreview.net/forum?id=_PHymLIxuI","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Add CLIP-ViP,"### Model description

[CLIP-ViP](https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP) is a video-language model which is based on a pre-trained image-text model [CLIP](https://openai.com/blog/clip/) then further pre-trained (post-pretraining) on a large-scale video-text dataset [HD-VILA-100M](https://github.com/microsoft/XPretrain/tree/main/hd-vila-100m). This work is accepted by ICLR 2023.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

[The official implementation](https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP)
This repo has model implementation and pre-trained weights.
@hellwayxue","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",12,open
DeBERTa models produce nonsense fill-mask output,"### System Info

Python version: 3.8.15
Transformers version: 4.24.0

### Who can help?

@ArthurZucker, @younesbelkada

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

Both on the HF website and using transformers in Python scripts/interpreter, the DeBERTa models seem to produce nonsense outputs in a fill-mask task. This is demonstrated below using a fill-mask pipeline for ease of reproduction, but the same thing happens even when calling the models manually and inspecting the logits. I demonstrate with one model, but the other `microsoft/deberta` masked language models appear to have the same issue (i.e., not the ones fine-tuned on mnli or whatever, which I wouldn't test against).
```python
>>> from transformers import pipeline
>>> test_sentence = 'Do you [MASK] the muffin man?'

# for comparison
>>> bert = pipeline('fill-mask', model = 'bert-base-uncased')
>>> print('\n'.join([d['sequence'] for d in bert(test_sentence)]))
do you know the muffin man?
do you remember the muffin man?
do you mean the muffin man?
do you see the muffin man?
do you recognize the muffin man?

>>> deberta = pipeline('fill-mask', model = 'microsoft/deberta-v3-large')
>>> print('\n'.join([d['sequence'] for d in deberta(test_sentence)]))
Do you Moisturizing the muffin man?
Do you Kagan the muffin man?
Do youULA the muffin man?
Do you闘 the muffin man?
Do you aplica the muffin man?
```

Here's a screenshot from the HF website for the same model (`microsoft/deberta-v3-large`):
![deberta](https://user-images.githubusercontent.com/5022150/232265118-7d9e3126-839e-4654-a5c8-a8363f3052e5.png)

Based on the paper and the documentation on the model cards, it seems like these should be able to be used for masked language modeling out of the box since they were pre-trained on it, but they're clearly not doing a good job of it. Am I missing something about why these models shouldn't be used for MLM without fine-tuning, or is there a bug with them?

### Expected behavior

I'd expect sensible predictions for masked token locations (assuming these models can indeed be used for that without additional fine-tuning).","[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",5,open
Implement a decode method in transformers.BasicTokenizer,"### Feature request

Transformers has provided a nice BasicTokenizer for basic tokenizing when we don't need BPE tokenizers. For data processing (like data format converting), it is better to offer a decode method for basic use.

### Motivation

When doing data format converting in some data processing problems, we usually meet the requirement to recover a list of tokens into continuous, readable text.

### Your contribution

None.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
TypeError: export() got an unexpected keyword argument 'preprocessor',"### Model description

onnx_inputs, onnx_outputs = export(
            preprocessor=tokenizer, model=model, config=onnx_config, opset=10, output=onnx_model_path
        )

i got error: TypeError: export() got an unexpected keyword argument 'preprocessor'

### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Training Evaluation Display on VSCode,"### System Info

1. OSX Ventura 13.2
1. VSCode 1.77.1
 - Chromium 102.0.5005.196
 - Jupyter extension v2023.3.1000892223
3. Transformers 4.26.1


### Who can help?

Not sure. Please let me know if it is a VSCode issue

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb
Run the notebook (I commented out the parts pushing to hub)

### Expected behavior

The table of metrics  during evaluation phase in training fail to show up as html object in VSCode. There seems to be no similar issue on colab or AWS

Currently, the output looks like this (repeated by the number of times evaluation is run during training)
```
0.3564084804084804
{'eval_loss': 1.6524937152862549, 'eval_f1': 0.3564084804084804, 'eval_accuracy': 0.36, 'eval_runtime': 4.6151, 'eval_samples_per_second': 10.834, 'eval_steps_per_second': 1.517, 'epoch': 0.26}
***** Running Evaluation *****
  Num examples = 50
  Batch size = 8
{'loss': 1.6389, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.28}

```
","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",16,open
Add SwiftFormer,"### Model description

'SwiftFormer' paper introduces a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations in the self-attention computation with linear element-wise multiplications.  A series of models called 'SwiftFormer' is built based on this, which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Even their small variant achieves 78.5% top-1 ImageNet1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2× faster compared to MobileViT-v2.

I would like to add this model to Huggingface.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/abs/2303.15446
Original code and weights: https://github.com/Amshaker/SwiftFormer 
Author: @Amshaker

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Open AI GPT Model Implementation in Flax,"### Model description

https://huggingface.co/openai-gpt today supports tf and pytorch but not flax. I'd like to implement the support to enhance the current gpt offering by hugging face

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Given that the model is already implemented in other two frameworks, I'll try to infer the model from there. Please feel free to provide additional resources that can help me wrap this up better and faster","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",10,open
Add `output_hidden_state` and `output_scores` to Flax generate,"I need whisper's output_scores and output_hidden_states as the result of generate() method.
On Pytorch model, I can easily get the output_scores and output_hidden_states by setting these parameters in generate() method as follows:
```
whisper_output = model.generate(inputs=input_features, max_new_tokens=180, output_scores=True, output_hidden_states=True, return_dict_in_generate=True)
```
and the resulted `whisper_output` returns 'scores' and 'output_hidden_states' as it keys alongside 'sequences'

Now I want to do so for Flax whisper model. but setting these parameters as the static_argnames of model doesn't have effect to get output_scores.

Is there any solution for getting output_scores or logits from Flax whisper model?","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2934977194, 'node_id': 'MDU6TGFiZWwyOTM0OTc3MTk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Flax', 'name': 'Flax', 'color': '4862AD', 'default': False, 'description': ''}]",11,open
[DO NOT MERGE] Add Crop Transformation,"# What does this PR do?

Abstracts out cropping logic to be a more generic `crop` function which other, more specific cropping functions e.g. `center_crop` can call. 

Motivation: 
* The output of the CLIP feature extractor changed after #17628. This was due to a difference in how the `top` and `left` coordinates were calculated resulting in some values being off by one. 
* The original CLIP feature extractor matched the original implementation
* Having a more generic `crop` method enables each image processor to have its own center_crop logic with minimal code replication. 

[BEFORE MERGING]: Verify this doesn't have large impact on any popular CLIP dependant pipelines

Fixes #22505


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Add MobileViT v2,"### Model description

[MobileViT](https://openreview.net/forum?id=vh-0sUt8HlG) is a computer vision model that combines CNNs with transformers that has already been added to Transformers. 

[MobileViT v2](https://arxiv.org/abs/2206.02680) is the second version; it is constructed by replacing multi-headed self-attention in MobileViT v1 with the proposed separable self-attention. 

Does Hugging Face have plan to add MobileViT v2 to Transformers?

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The official implementation is from Apple at this link: [https://github.com/apple/ml-cvnets](https://github.com/apple/ml-cvnets)

The timm library also implemented it and has pre-trained weights at this link: [https://github.com/huggingface/pytorch-image-models/blob/82cb47bcf360e1974c00c35c2aa9e242e6b5b565/timm/models/mobilevit.py](https://github.com/huggingface/pytorch-image-models/blob/82cb47bcf360e1974c00c35c2aa9e242e6b5b565/timm/models/mobilevit.py)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Unable to import VGG16 model transformers,"### Model description

i have recently upload my trained vgg16 model to hugging face.After uploading i have a prompt of instructions to use my model.
Although i have followed the prompt i got errors.
[https://huggingface.co/Nvsai/DeviceClassification](url)
>>> from transformers import VGG16
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'VGG16' from 'transformers' (/mnt/mydrive/ubantu/programming/openvino/lib/python3.9/site-packages/transformers/__init__.py)
>>> 
>>> model = VGG16.fro
![Screenshot from 2023-04-04 20-45-24](https://user-images.githubusercontent.com/87435205/229841900-e12cee0f-69a1-4dd5-9332-2f65f177e8cf.png)
m_pretrained(""Nvsai/DeviceClassification"")
![Screenshot from 2023-04-04 20-45-56](https://user-images.githubusercontent.com/87435205/229841929-812f7eb6-58e1-4919-aff6-35200aee426c.png)


### Open source status

- [ ] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
[HOW TO FINETUNE CLIP OPENAI LAION2B MODELS FOR IMAGE CLASSIFICATION],"I try to finetune CLIP model by using pretrained: https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K
But I met a bug: 
![image](https://user-images.githubusercontent.com/124332581/229679584-34db65ad-5a43-423e-bcd9-c54902fe7d6b.png)
Help me! Thanks","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Add interpolation of position encodings to BLIP-2,"### Feature request

ViT implemented in Huggingface Transformers has the feature to enable finetuning with different resolution of images
https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTModel.forward.interpolate_pos_encoding while the newly implemented BLIP-2 model does not. Would like to add those following the ViT implementation.

### Motivation

I was playing around with the model whether different (mainly higher) resolution of input images helps downstream tasks. 

(Curious to get feedback on whether this feature would be needed or not for the sake of keeping the code simple.)

### Your contribution

It's mostly copying & pasting from the ViT implementation `interpolate_pos_encoding` but have a working code ready and ready for PR to get reviewed (and address bugs).","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Add Restormer,"### Model description

**Restormer: Efficient Transformer for High-Resolution Image Restoration** was published in CVPR 2022, which introduced a new Vision Transformer based architecture for Image Restoration tasks like Deraining, Motion Deblurring, Defocus Deblurring and Denoising. It reduced the time complexity of Self Attention in Vision Transformers from O(n<sup>2</sup>) to O(n) by introducing **Multi-Dconv Head Transposed Attention**.  It also introduced **Gated-Dconv Feed-Forward Network**. 

@manyana72 and I would like to add this model to Huggingface. 

cc: @NielsRogge 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

[Paper](https://arxiv.org/pdf/2111.09881.pdf), [Code Implementation](https://github.com/swz30/Restormer) and [pretrained model weights](https://github.com/swz30/Restormer/releases/tag/v1.0)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Multi-node training with Deepspeed hangs when `full_determinism = True`,"Hey, as I've described below, I think there are problems training Deepspeed in a multi-node setting when `full_determinism = True` in the `TrainingArguments`. I've replicated this on multiple hardware configurations (i.e. different nodes and GPU types — specifically A6000, V100, RTX 3090 — on the same large cluster system). Please take a look, thank you very much!

### System Info

### `transformers-cli env`
- `transformers` version: 4.27.3
- Platform: Linux-3.10.0-1160.76.1.el7.x86_64-x86_64-with-glibc2.10
- Python version: 3.8.1
- Huggingface_hub version: 0.13.3
- PyTorch version (GPU?): 2.0.0+cu117 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: Yes

### Additional info
- `deepspeed` version: 0.8.3
- gcc: 10.2
- cuda: 11.7.1
- pdsh: 2.34

### Who can help?

@sgugger @stas00 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

### Shell

Please set the following environment variables appropriately:

```bash
export NODELIST=""gpu1504 gpu1505""
export NUM_NODES=2
export GPUS_PER_NODE=1
export MASTER_ADDR=gpu1504
export MASTER_PORT=9901
```

Create `train.py` from the snippet below, then run with the following commands:

```bash
conda create -n ds-trainer python==3.8.1
conda activate ds-trainer
pip install transformers[deepspeed]

echo ""PATH=$PATH"" > .deepspeed_env

cat /dev/null >| hostfile
for i in $NODELIST; do
    echo ""$i slots=$GPUS_PER_NODE"" >> hostfile;
done

deepspeed --num_gpus $GPUS_PER_NODE --num_nodes $NUM_NODES --master_addr $MASTER_ADDR --master_port $MASTER_PORT --hostfile hostfile train.py
```

### `train.py`

```python
import torch
from torch.utils.data import Dataset
from transformers import BertForMaskedLM, Trainer, TrainingArguments
import copy


## Model

model = BertForMaskedLM.from_pretrained(""bert-base-uncased"")


## Dataset

class DummyDataset(Dataset):
    def __init__(self, max_text_length=16, num_samples=20000) -> None:
        super().__init__()
        self.input_ids = torch.randint(0, 30522, (num_samples, max_text_length))
        self.labels = copy.deepcopy(self.input_ids)

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, index):
        return {
            ""input_ids"": self.input_ids[index],
            ""labels"": self.labels[index],
        }

train_dataset = DummyDataset()

## Training

deepspeed_config = {
    ""optimizer"": {
        ""type"": ""AdamW"",
        ""params"": {
            ""lr"": ""auto"",
            ""betas"": ""auto"",
            ""eps"": ""auto"",
            ""weight_decay"": ""auto"",
        },
    },
    ""gradient_accumulation_steps"": ""auto"",
    ""gradient_clipping"": ""auto"",
    ""train_batch_size"": ""auto"",
    ""train_micro_batch_size_per_gpu"": ""auto"",
}

training_arguments = TrainingArguments(
    full_determinism = True,
    output_dir = ""output"",
    do_train = True,
    per_device_train_batch_size = 16,
    max_steps = 100,
    deepspeed = deepspeed_config
)

trainer = Trainer(
    model=model,
    args=training_arguments,
    train_dataset=train_dataset
)

trainer.train()
```

### Expected behavior

**When I run the above code (a minimal example for DeepSpeed training) in a multi-node setting, training seems to hang after the following output:**

<details>
<summary>Output (not working)</summary>

```Shell
[2023-03-24 10:26:48,202] [INFO] [multinode_runner.py:67:get_cmd] Running on the following workers: gpu1504,gpu1505
[2023-03-24 10:26:48,202] [INFO] [runner.py:550:main] cmd = pdsh -S -f 1024 -w gpu1504,gpu1505 export PYTHONPATH=/gpfs/data/csun45/akhand10/projects/test; exp
ort PATH=/gpfs/runtime/opt/pdsh/2.34/bin:/gpfs/runtime/opt/cuda/11.7.1/cuda/bin:/gpfs/runtime/opt/gcc/10.2/bin:/users/akhand10/.local/machine/bin:/users/akhan
d10/.local/machine/bin:/users/akhand10/.local/scripts:/users/akhand10/.local/bin:/users/akhand10/.local/machine/bin:/users/akhand10/palm.h/.local/miniconda3/e
nvs/ds-trainer/bin:/users/akhand10/.local/miniconda3/condabin:/users/akhand10/.local/scripts:/users/akhand10/.local/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/
usr/bin:/usr/local/sbin:/usr/sbin:/usr/lpp/mmfs/bin:/usr/lpp/mmfs/sbin:/opt/ibutils/bin:/gpfs/runtime/bin:/opt/singularity/2.5.2/bin:/users/akhand10/bin;  cd 
/gpfs/data/csun45/akhand10/projects/test; /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=
eyJncHUxNTA0IjogWzBdLCAiZ3B1MTUwNSI6IFswXX0= --node_rank=%n --master_addr=gpu1504 --master_port=9901 train.py
gpu1504: [2023-03-24 10:26:51,118] [INFO] [launch.py:142:main] WORLD INFO DICT: {'gpu1504': [0], 'gpu1505': [0]}
gpu1504: [2023-03-24 10:26:51,118] [INFO] [launch.py:148:main] nnodes=2, num_local_procs=1, node_rank=0
gpu1504: [2023-03-24 10:26:51,119] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'gpu1504': [0], 'gpu1505': [1]})
gpu1504: [2023-03-24 10:26:51,119] [INFO] [launch.py:162:main] dist_world_size=2
gpu1504: [2023-03-24 10:26:51,119] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0
gpu1505: [2023-03-24 10:26:53,517] [INFO] [launch.py:142:main] WORLD INFO DICT: {'gpu1504': [0], 'gpu1505': [0]}
gpu1505: [2023-03-24 10:26:53,517] [INFO] [launch.py:148:main] nnodes=2, num_local_procs=1, node_rank=1
gpu1505: [2023-03-24 10:26:53,517] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'gpu1504': [0], 'gpu1505': [1]})
gpu1505: [2023-03-24 10:26:53,517] [INFO] [launch.py:162:main] dist_world_size=2
gpu1505: [2023-03-24 10:26:53,517] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0
gpu1504: Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_rel
ationship.weight']
gpu1504: - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. 
initializing a BertForSequenceClassification model from a BertForPreTraining model).
gpu1504: - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a
 BertForSequenceClassification model from a BertForSequenceClassification model).
gpu1504: [2023-03-24 10:26:55,478] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
gpu1505: Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_r
elationship.bias']
gpu1505: - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. 
initializing a BertForSequenceClassification model from a BertForPreTraining model).
gpu1505: - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a
 BertForSequenceClassification model from a BertForSequenceClassification model).
```
</details>

In particular, the last line of relevance is: `[INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl`. 

<details>
<summary>Extra NCCL output</summary>

If I provide the vars: `NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=ALL CUDA_LAUNCH_BLOCKING=1`

```Shell
...
...
gpu1504: [2023-03-24 10:48:06,695] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
gpu1504: gpu1504:30024:30024 [0] NCCL INFO Bootstrap : Using ib0:172.25.211.4<0>
gpu1504: gpu1504:30024:30024 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
gpu1504: gpu1504:30024:30024 [0] NCCL INFO cudaDriverVersion 11070
gpu1504: NCCL version 2.14.3+cuda11.7
gpu1505: gpu1505:22060:22060 [0] NCCL INFO cudaDriverVersion 11070
gpu1504: gpu1504:30024:30024 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x7f62ffe00000
gpu1504: gpu1504:30024:30241 [0] NCCL INFO NET/IB : Using [0]mlx5_2:1/IB [1]mlx5_0:1/IB ; OOB ib0:172.25.211.4<0>
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Using network IB
gpu1505: gpu1505:22060:22060 [0] NCCL INFO Bootstrap : Using ib0:172.25.211.5<0>
gpu1505: gpu1505:22060:22060 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
gpu1505: gpu1505:22060:22060 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x7f97a3e00000
gpu1505: gpu1505:22060:22171 [0] NCCL INFO NET/IB : Using [0]mlx5_2:1/IB [1]mlx5_0:1/IB ; OOB ib0:172.25.211.5<0>
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Using network IB
gpu1505: gpu1505:22060:22171 [0] NCCL INFO NET/IB : GPU Direct RDMA Disabled for HCA 0 'mlx5_2'
gpu1505: gpu1505:22060:22171 [0] NCCL INFO NET/IB : GPU Direct RDMA Disabled for HCA 1 'mlx5_0'
gpu1505: gpu1505:22060:22171 [0] NCCL INFO === System : maxBw 12.5 totalBw 24.0 ===
gpu1505: gpu1505:22060:22171 [0] NCCL INFO CPU/0 (1/2/-1)
gpu1505: gpu1505:22060:22171 [0] NCCL INFO + SYS[5000.0] - CPU/1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO + PCI[24.0] - GPU/1000 (1)
gpu1505: gpu1505:22060:22171 [0] NCCL INFO + PCI[12.0] - NIC/23000
gpu1505: gpu1505:22060:22171 [0] NCCL INFO               + NET[12.5] - NET/1 (3ad0a20003723f04/1/12.500000)
gpu1505: gpu1505:22060:22171 [0] NCCL INFO CPU/1 (1/2/-1)
gpu1505: gpu1505:22060:22171 [0] NCCL INFO + SYS[5000.0] - CPU/0
gpu1505: gpu1505:22060:22171 [0] NCCL INFO + PCI[24.0] - NIC/C2000
gpu1505: gpu1505:22060:22171 [0] NCCL INFO               + NET[12.5] - NET/0 (82d0a20003723f04/1/12.500000)
gpu1505: gpu1505:22060:22171 [0] NCCL INFO ==========================================
gpu1505: gpu1505:22060:22171 [0] NCCL INFO GPU/1000 :GPU/1000 (0/5000.000000/LOC) CPU/0 (1/24.000000/PHB) CPU/1 (2/24.000000/SYS) NET/1 (3/12.000000/PHB) NET/0 (4/12.500000/SYS) 
gpu1505: gpu1505:22060:22171 [0] NCCL INFO NET/1 :GPU/1000 (3/12.000000/PHB) CPU/0 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
gpu1505: gpu1505:22060:22171 [0] NCCL INFO NET/0 :GPU/1000 (4/12.500000/SYS) CPU/0 (3/12.500000/SYS) CPU/1 (2/12.500000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Setting affinity for GPU 0 to 04
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type LOC/PHB, sameChannels 1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO  0 : NET/1 GPU/1 NET/1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 1, bw 24.000000/12.000000, type LOC/PHB, sameChannels 1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO  0 : NET/1 GPU/1 NET/1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type LOC/PIX, sameChannels 1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO NET/IB : GPU Direct RDMA Disabled for HCA 0 'mlx5_2'
gpu1504: gpu1504:30024:30241 [0] NCCL INFO NET/IB : GPU Direct RDMA Disabled for HCA 1 'mlx5_0'
gpu1504: gpu1504:30024:30241 [0] NCCL INFO === System : maxBw 12.5 totalBw 24.0 ===
gpu1504: gpu1504:30024:30241 [0] NCCL INFO CPU/0 (1/2/-1)
gpu1504: gpu1504:30024:30241 [0] NCCL INFO + SYS[5000.0] - CPU/1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO + PCI[24.0] - GPU/41000 (0)
gpu1504: gpu1504:30024:30241 [0] NCCL INFO + PCI[12.0] - NIC/23000
gpu1504: gpu1504:30024:30241 [0] NCCL INFO               + NET[12.5] - NET/1 (b4e3ff0003a1420c/1/12.500000)
gpu1504: gpu1504:30024:30241 [0] NCCL INFO CPU/1 (1/2/-1)
gpu1504: gpu1504:30024:30241 [0] NCCL INFO + SYS[5000.0] - CPU/0
gpu1504: gpu1504:30024:30241 [0] NCCL INFO + PCI[24.0] - NIC/C2000
gpu1504: gpu1504:30024:30241 [0] NCCL INFO               + NET[12.5] - NET/0 (d2cfa20003723f04/1/12.500000)
gpu1504: gpu1504:30024:30241 [0] NCCL INFO ==========================================
gpu1504: gpu1504:30024:30241 [0] NCCL INFO GPU/41000 :GPU/41000 (0/5000.000000/LOC) CPU/0 (1/24.000000/PHB) CPU/1 (2/24.000000/SYS) NET/1 (3/12.000000/PHB) NET/0 (4/12.500000/SYS) 
gpu1504: gpu1504:30024:30241 [0] NCCL INFO NET/1 :GPU/41000 (3/12.000000/PHB) CPU/0 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
gpu1504: gpu1504:30024:30241 [0] NCCL INFO NET/0 :GPU/41000 (4/12.500000/SYS) CPU/0 (3/12.500000/SYS) CPU/1 (2/12.500000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Setting affinity for GPU 0 to 10
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type LOC/PHB, sameChannels 1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO  0 : NET/1 GPU/0 NET/1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 1, bw 24.000000/12.000000, type LOC/PHB, sameChannels 1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO  0 : NET/1 GPU/0 NET/1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type LOC/PIX, sameChannels 1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/-1/-1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Tree 1 : 1 -> 0 -> -1/-1/-1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Channel 00/02 :    0   1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Channel 01/02 :    0   1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Ring 00 : 1 -> 0 -> 1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Ring 01 : 1 -> 0 -> 1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
gpu1504: gpu1504:30024:30241 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 1152 pointer 0x7f6301c00000
gpu1504: gpu1504:30024:30241 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 8 pointer 0x7f6301c00600
gpu1504: gpu1504:30024:30241 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 1152 pointer 0x7f6301c00800
gpu1504: gpu1504:30024:30241 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 8 pointer 0x7f6301c00e00
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Tree 0 : 0 -> 1 -> -1/-1/-1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Tree 1 : -1 -> 1 -> 0/-1/-1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Ring 00 : 0 -> 1 -> 0
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Ring 01 : 0 -> 1 -> 0
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
gpu1505: gpu1505:22060:22171 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 1152 pointer 0x7f97a5c00000
gpu1505: gpu1505:22060:22171 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 8 pointer 0x7f97a5c00600
gpu1505: gpu1505:22060:22171 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 1152 pointer 0x7f97a5c00800
gpu1505: gpu1505:22060:22171 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 8 pointer 0x7f97a5c00e00
gpu1505: gpu1505:22060:22174 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f96f00009c0
gpu1505: gpu1505:22060:22174 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-3elXjI
gpu1505: 
gpu1505: gpu1505:22060:22174 [0] NCCL INFO New proxy recv connection 0 from local rank 0, transport 2
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f96f0004010
gpu1504: gpu1504:30024:30248 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f62840008c0
gpu1504: gpu1504:30024:30248 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-HznK2t
gpu1504: 
gpu1504: gpu1504:30024:30248 [0] NCCL INFO New proxy recv connection 0 from local rank 0, transport 2
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f6284004010
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Channel 00/0 : 0[41000] -> 1[1000] [receive] via NET/IB/1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Channel 00/0 : 1[1000] -> 0[41000] [receive] via NET/IB/1
gpu1505: gpu1505:22060:22174 [0] NCCL INFO New proxy recv connection 1 from local rank 0, transport 2
gpu1504: gpu1504:30024:30248 [0] NCCL INFO New proxy recv connection 1 from local rank 0, transport 2
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f96f0004050
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f6284004050
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Channel 01/0 : 0[41000] -> 1[1000] [receive] via NET/IB/1
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Channel 01/0 : 1[1000] -> 0[41000] [receive] via NET/IB/1
gpu1504: gpu1504:30024:30248 [0] NCCL INFO New proxy send connection 2 from local rank 0, transport 2
gpu1505: gpu1505:22060:22174 [0] NCCL INFO New proxy send connection 2 from local rank 0, transport 2
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f6284004090
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Channel 00/0 : 0[41000] -> 1[1000] [send] via NET/IB/1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f96f0004090
gpu1504: gpu1504:30024:30248 [0] NCCL INFO New proxy send connection 3 from local rank 0, transport 2
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Channel 00/0 : 1[1000] -> 0[41000] [send] via NET/IB/1
gpu1505: gpu1505:22060:22174 [0] NCCL INFO New proxy send connection 3 from local rank 0, transport 2
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f62840040d0
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Channel 01/0 : 0[41000] -> 1[1000] [send] via NET/IB/1
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f96f00040d0
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Channel 01/0 : 1[1000] -> 0[41000] [send] via NET/IB/1
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x7f6284020000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x7f96f0020000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO NET/IB: Dev 1 Port 1 qpn 173 mtu 5 LID 1038
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x7f96f0034000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x7f97a7400000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO Mem Realloc old size 0, new size 768 pointer 0x7f96f0033ff0
gpu1504: gpu1504:30024:30248 [0] NCCL INFO NET/IB: Dev 1 Port 1 qpn 153 mtu 5 LID 1036
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x7f96f0035000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x7f6284034000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO NET/IB: Dev 1 Port 1 qpn 174 mtu 5 LID 1038
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x7f96f003f000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x7f97a9400000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x7f6303400000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO Mem Realloc old size 0, new size 768 pointer 0x7f6284034050
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x7f96f003f000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x7f96f0046000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x7f6284035000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x7f96f0049000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO NET/IB: Dev 1 Port 1 qpn 154 mtu 5 LID 1036
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x7f628403f000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x7f97ab400000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x7f6305400000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x7f628403f000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x7f6284046000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x7f6284049000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x7f6307400000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x7f96f0049000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x7f96f0050000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x7f96f0053000
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x7f97ad400000
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Connected all rings
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Connected all trees
gpu1505: gpu1505:22060:22171 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
gpu1505: gpu1505:22060:22171 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
gpu1505: gpu1505:22060:22174 [0] NCCL INFO New proxy send connection 4 from local rank 0, transport 2
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x7f6284049000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x7f6284050000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x7f6284053000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x7f6309400000
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Connected all rings
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Connected all trees
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Latency/AlgBw |    Tree/    LL |    Tree/ LL128 |    Tree/Simple |    Ring/    LL |    Ring/ LL128 |    Ring/Simple | CollNetDirect/    LL | CollNetDirect/ LL128 | CollNetDirect/Simple | CollNetChain/    LL | CollNetChain/ LL128 | CollNetChain/Simple |
gpu1504: gpu1504:30024:30241 [0] NCCL INFO  Max NThreads |            512 |            640 |            512 |            512 |            640 |            256 |              0 |              0 |            512 |              0 |              0 |            512 |
gpu1504: gpu1504:30024:30241 [0] NCCL INFO     Broadcast |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   3.0 |    14.0/   0.0 |    18.0/  12.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
gpu1504: gpu1504:30024:30241 [0] NCCL INFO        Reduce |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   3.0 |    14.0/   0.0 |    18.0/  12.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
gpu1504: gpu1504:30024:30241 [0] NCCL INFO     AllGather |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
gpu1504: gpu1504:30024:30241 [0] NCCL INFO ReduceScatter |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
gpu1504: gpu1504:30024:30241 [0] NCCL INFO     AllReduce |    14.4/   2.4 |    21.4/   0.0 |    56.0/   9.2 |    10.8/   3.0 |    21.0/   0.0 |    35.4/  12.0 |     4.4/   0.0 |     4.4/   0.0 |    10.7/   0.0 |     4.4/   0.0 |     4.4/   0.0 |     0.0/   0.0 |
gpu1504: gpu1504:30024:30241 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
gpu1504: gpu1504:30024:30241 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
gpu1505: gpu1505:22060:22171 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f96f0004110
gpu1505: gpu1505:22060:22171 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x7f97a5c01000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO New proxy send connection 4 from local rank 0, transport 2
gpu1505: gpu1505:22060:22174 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x7f97af400000
gpu1505: gpu1505:22060:22171 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x7f96ca000000
gpu1505: gpu1505:22060:22171 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x7f97a3e00200
gpu1505: gpu1505:22060:22171 [0] NCCL INFO comm 0x560589dc07a0 rank 1 nranks 2 cudaDev 0 busId 1000 - Init COMPLETE
gpu1505: gpu1505:22060:22060 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x7f97bc000000 recvbuff 0x7f97bc000000 count 93763584 datatype 0 op 0 root 0 comm 0x560589dc07a0 [nranks=2] stream 0x560589bf2f70
gpu1505: gpu1505:22060:22060 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
gpu1504: gpu1504:30024:30241 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f6284004110
gpu1504: gpu1504:30024:30241 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x7f6301c01000
gpu1504: gpu1504:30024:30248 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x7f630b400000
gpu1504: gpu1504:30024:30241 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x7f6226000000
gpu1504: gpu1504:30024:30241 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x7f62ffe00200
gpu1504: gpu1504:30024:30241 [0] NCCL INFO comm 0x55a62826f2d0 rank 0 nranks 2 cudaDev 0 busId 41000 - Init COMPLETE
gpu1504: gpu1504:30024:30024 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x7f6335e00000 recvbuff 0x7f6335e00000 count 93763584 datatype 0 op 0 root 0 comm 0x55a62826f2d0 [nranks=2] stream 0x55a628085b40
gpu1504: gpu1504:30024:30024 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
```
</details>

<details>
<summary>py-spy output</summary>

`py-spy dump --pid [pid]`

```Python
Thread 27321 (active): ""MainThread""
    broadcast (torch/distributed/distributed_c10d.py:1555)
    wrapper (torch/distributed/distributed_c10d.py:1436)
    broadcast (deepspeed/comm/torch.py:78)
    broadcast (deepspeed/comm/comm.py:228)
    log_wrapper (deepspeed/comm/comm.py:123)
    _broadcast_model (deepspeed/runtime/engine.py:1105)
    _configure_distributed_model (deepspeed/runtime/engine.py:1182)
    __init__ (deepspeed/runtime/engine.py:297)
    initialize (deepspeed/__init__.py:125)
    deepspeed_init (transformers/deepspeed.py:378)
    _inner_training_loop (transformers/trainer.py:1702)
    train (transformers/trainer.py:1633)
    <module> (train.py:64)
```
</details>

This code works fine in a single-node setup (i.e. with `deepspeed train.py`).

<details>
<summary>Continued output (for single-node, working)</summary>

```Shell
...
...
[2023-03-24 10:36:01,318] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.304279088973999 seconds
Using /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.23477387428283691 seconds
{'train_runtime': 12.686, 'train_samples_per_second': 126.124, 'train_steps_per_second': 7.883, 'train_loss': 1.1398809814453126, 'epoch': 0.08}              
[2023-03-24 10:36:18,214] [INFO] [launch.py:350:main] Process 24746 exits successfully.
```
</details>

## Problem: `full_determinism = True`

**If you set `full_determinism = False` in TrainingArguments, multi-node training does work:**

<details>
<summary>Working multi-node output</summary>

```Shell
[2023-03-23 16:40:59,614] [INFO] [multinode_runner.py:67:get_cmd] Running on the following workers: gpu1504,gpu1505
[2023-03-23 16:40:59,614] [INFO] [runner.py:550:main] cmd = pdsh -S -f 1024 -w gpu1504,gpu1505 export PYTHONPATH=/gpfs/data/csun45/akhand10/projects/test_ds; export PATH=/users/akhand10/.local/machine/bin:/users/akhand10/.local/machine/bin:/users/akhand10/.local/scripts:/users/akhand10/.local/bin:/gpfs/runtime/opt/cuda/11.7.1/cuda/bin:/gpfs/runtime/opt/gcc/10.2/bin:/users/akhand10/.local/machine/bin:/users/akhand10/.local/scripts:/users/akhand10/.local/bin:/gpfs/home/akhand10/.vscode-cli/server-stable/bin/ee2b180d582a7f601fa6ecfdad8d9fd269ab1884/bin/remote-cli:/users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/bin:/users/akhand10/.local/miniconda3/condabin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/lpp/mmfs/bin:/usr/lpp/mmfs/sbin:/opt/ibutils/bin:/gpfs/runtime/bin:/opt/singularity/2.5.2/bin:/users/akhand10/bin;  cd /gpfs/data/csun45/akhand10/projects/test_ds; /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJncHUxNTA0IjogWzBdLCAiZ3B1MTUwNSI6IFswXX0= --node_rank=%n --master_addr=gpu1504 --master_port=9901 deepspeed_trainer_mvp.py
gpu1504: [2023-03-23 16:41:01,849] [INFO] [launch.py:142:main] WORLD INFO DICT: {'gpu1504': [0], 'gpu1505': [0]}
gpu1504: [2023-03-23 16:41:01,849] [INFO] [launch.py:148:main] nnodes=2, num_local_procs=1, node_rank=0
gpu1504: [2023-03-23 16:41:01,849] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'gpu1504': [0], 'gpu1505': [1]})
gpu1504: [2023-03-23 16:41:01,849] [INFO] [launch.py:162:main] dist_world_size=2
gpu1504: [2023-03-23 16:41:01,850] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0
gpu1505: [2023-03-23 16:41:05,197] [INFO] [launch.py:142:main] WORLD INFO DICT: {'gpu1504': [0], 'gpu1505': [0]}
gpu1505: [2023-03-23 16:41:05,197] [INFO] [launch.py:148:main] nnodes=2, num_local_procs=1, node_rank=1
gpu1505: [2023-03-23 16:41:05,197] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'gpu1504': [0], 'gpu1505': [1]})
gpu1505: [2023-03-23 16:41:05,197] [INFO] [launch.py:162:main] dist_world_size=2
gpu1505: [2023-03-23 16:41:05,197] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0
gpu1504: Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
gpu1504: - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
gpu1504: - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
gpu1504: [2023-03-23 16:41:05,717] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
gpu1505: Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
gpu1505: - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
gpu1505: - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
gpu1505: Using /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
gpu1504: Using /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
gpu1505: Detected CUDA files, patching ldflags
gpu1505: Emitting ninja build file /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
gpu1505: Building extension module fused_adam...
gpu1505: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
gpu1505: [1/3] /gpfs/runtime/opt/cuda/11.7.1/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\""_gcc\"" -DPYBIND11_STDLIB=\""_libstdcpp\"" -DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" -I/users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include/TH -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include/THC -isystem /gpfs/runtime/opt/cuda/11.7.1/cuda/include -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -std=c++17 -c /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
gpu1505: [2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\""_gcc\"" -DPYBIND11_STDLIB=\""_libstdcpp\"" -DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" -I/users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include/TH -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include/THC -isystem /gpfs/runtime/opt/cuda/11.7.1/cuda/include -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++14 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
gpu1505: [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/gpfs/runtime/opt/cuda/11.7.1/cuda/lib64 -lcudart -o fused_adam.so
gpu1505: Loading extension module fused_adam...
gpu1505: Time to load fused_adam op: 41.71729230880737 seconds
gpu1505: Using /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
gpu1504: Loading extension module fused_adam...
gpu1504: Time to load fused_adam op: 41.791842222213745 seconds
gpu1504: Using /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
gpu1505: Emitting ninja build file /gpfs/home/akhand10/.cache/torch_extensions/py38_cu117/utils/build.ninja...
gpu1505: Building extension module utils...
gpu1505: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
gpu1505: [1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\""_gcc\"" -DPYBIND11_STDLIB=\""_libstdcpp\"" -DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include/TH -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/include/THC -isystem /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o 
gpu1505: [2/2] c++ flatten_unflatten.o -shared -L/users/akhand10/palm.h/.local/miniconda3/envs/ds-trainer/lib/python3.8/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so
gpu1505: Loading extension module utils...
gpu1505: Time to load utils op: 14.304872751235962 seconds
gpu1504: Loading extension module utils...
gpu1504: Time to load utils op: 14.238191604614258 seconds
gpu1505: {'train_runtime': 31.3539, 'train_samples_per_second': 102.061, 'train_steps_per_second': 3.189, 'train_loss': 0.9621941375732422, 'epoch': 0.16}
gpu1504: {'train_runtime': 31.3186, 'train_samples_per_second': 102.176, 'train_steps_per_second': 3.193, 'train_loss': 0.9663803863525391, 'epoch': 0.16}
100%|██████████| 100/100 [00:31<00:00,  3.13it/s]
100%|██████████| 100/100 [00:31<00:00,  3.13it/s]
gpu1505: [2023-03-23 16:42:40,309] [INFO] [launch.py:350:main] Process 26888 exits successfully.
gpu1504: [2023-03-23 16:42:40,966] [INFO] [launch.py:350:main] Process 37514 exits successfully.
```
</details>","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",5,open
XVector Finetuning process - Whisper XVector,"### Model description

The idea is to apply XVector to Whisper and, In the process, generate documentation to Finetune or Adapt XVector (Maybe something similar to SetFit for Audio)  @vaibva

### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Add MegatronT5,"### Model description

In NeMo Megatron, the T5 model is available, but there is currently no MegatronT5 class for huggingface, such as MegatronBERT or MegatronGPT2. I have recently finished the porting work and have tested the model internally. I would like to share this model with the community.



### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

- [NeMo Megatron models](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/intro.html)
- [NeMo](https://github.com/NVIDIA/NeMo)
- [Megatron-LM T5 model](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/t5_model.py)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Native support of ChatGLM-6b,"### Feature request

Support https://huggingface.co/THUDM/chatglm-6b (and its int4 variants) in the Transformers library instead of relying on remote code execution.


### Motivation


This model performs really well (despite being a small model compared to large ones) and got a LOT of attention recently. It might be the SD moment for LLM IMO as it runs perfectly on consumer GPUs.

It would be great if Transformers can have native support for this model, instead of relying on remote code execution. A native integration will also make it much easier to use the model on Inference API / Endpoints. 

### Your contribution

cc @sgugger @osanseviero ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Ernie-M for pretraining multilingual models,"### Feature request

Two things that might help in that regard: 

- To train TSDAE, one needs support as class ErnieMForPreTraining, just as for Ernie
https://huggingface.co/docs/transformers/model_doc/ernie#transformers.ErnieForPreTraining

- To train cross-encoders with contrastive loss, a bit like SimCSE, one needs standard support for getting the 'attention_mask' out of the tokenizer sbert uses. Sbert just expects those. Tried to hack it in into sbert, but failed. 

### Motivation

Suspect getting Ernie-M-large for pretraining multilingual sentence embeddings will yield close to sota results.  
According to mSimCSE, we can get top multilingual embeddings just on training on their 300k dataset of english pairs, alone (worked better than cross-lingual training). 
With a stronger base model (they used xlm-roberta), sota embeddings might just lie on the streets.  
https://github.com/yaushian/mSimCSE

### Your contribution

Can't do it alone, plz help. ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Add InternImage,"### Model description

InternImage is a new large-scale CNN-based foundation model, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that this model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs.

It is worth noting that InternImage relies on a custom cuda operator, so if this causes problems for model addition, you can replace [the cuda operator](https://github.com/OpenGVLab/InternImage/blob/master/classification/ops_dcnv3/modules/dcnv3.py#L218) with [a pytorch implementation](https://github.com/OpenGVLab/InternImage/blob/master/classification/ops_dcnv3/modules/dcnv3.py#L91).

In fact, we have already submitted [a version of the code on transformers](https://huggingface.co/OpenGVLab/internimage_t_1k_224/tree/main), however, due to security reasons, the code we submitted cannot call your web inference api, so we would like you to add InternImage to transformers.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://github.com/OpenGVLab/InternImage","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
export clip to text encoder and image encoder two onnxs,"### Model description

i want to export clip to text encoder and image encoder two onnx, but it seems can only convert the whole model, how can i seperate clip to two onnx models? 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",10,open
Positinal Encoding for T5 family of models,"### Feature request

Please create a hook to allow users to modify the T5 family of models and change the default positional embeddings to custom positional embeddings.

### Motivation

I am trying to build a T5 version that takes non-text input, and the traditional positional encodings are getting in the way, and there is no way to switch it off, or to make them learnable parameters, etc.
BART gives limited access to positional encodings, but T5 family gives nearly 0 access.

### Your contribution

If i knew where the positional encoding were calculated and added in to the input_ids, I could create this hook myself","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Add BEiTv3,"### Model description

Microsoft just open-sourced BEiTv3: https://github.com/microsoft/unilm/tree/master/beit3

This is a very powerful vision-language model that can be used as backbone for a variety of downstream tasks, from image classification to VQA to object detection.

Time to add it to HF Transformers! :)

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://github.com/microsoft/unilm/tree/master/beit3","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Add X-Decoder Model ,"### Model description

X-Decoder is a generalized decoding pipeline that can predict pixel-level segmentation and language tokens seamlessly. X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language (VL) tasks. 

The model exhibits strong transferability to a wide range of downstream tasks in both zero-shot and fine-tuning settings, achieving state-of-the-art open-vocabulary segmentation and referring segmentation on 10 settings of 7 datasets and should be a valuable addition to transformers library

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/pdf/2212.11270.pdf
Code: https://github.com/microsoft/X-Decoder
Weights: https://huggingface.co/spaces/xdecoder/Demo/blob/main/xdecoder_focalt_last.pt

Author: @eltociear 
Cc: @NielsRogge @alaradirik ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Add SpikeGPT model,"### Model description

**Abstract:**
>As the size of large language models continue to scale, so does the computational resources required to run it. Spiking neural networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the RWKV language model, we successfully implement `SpikeGPT', a generative language model with pure binary, event-driven spiking activation units. We train the proposed model on three model variants: 45M, 125M and 260M parameters. To the best of our knowledge, this is 4x larger than any functional backprop-trained SNN to date. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity to linear with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 5x less energy consumption when processed on neuromorphic hardware that can leverage sparse, event-driven activations.

Concretely, it is a GPT model using Receptance Weighted Key Value (RWKV) instead of regular attention, and an adapted FFN layer.

### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

[Paper](https://arxiv.org/abs/2302.13939) | [Code](https://github.com/ridgerchu/SpikeGPT)

Author: @ridgerchu","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Add TensorFlow Wav2Vec2 for sequence classification,"### Feature request

Wav2Vec2 is one of the most popular speech recognition models, used over 2 million times monthly. In the PyTorch modelling code, we have Wav2Vec2 for speech recognition _and_ Wav2Vec2 for audio classification. However, in TensorFlow, we only have Wav2Vec2 for speech recognition. It would be great to add Wav2Vec2 for audio classification to the TensorFlow modelling code for cross-framework equivalence!

### Motivation

The audio classification class for PyTorch Wav2Vec2 lives under `Wav2Vec2ForSequenceClassification`:
https://github.com/huggingface/transformers/blob/13489248fa8f2cda7503628204f8f43b108797a2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1745

For this feature request, we'll need to port this PyTorch code into TensorFlow to create an equivalent TensorFlow class, `TFWav2Vec2ForSequenceClassification`.

This means adding a projection layer and classification layer on top of the base `TFWav2Vec2Model`. See the PyTorch code for reference:
https://github.com/huggingface/transformers/blob/13489248fa8f2cda7503628204f8f43b108797a2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1753-L1758

To check our that our implementation is correct, we can do one forward pass of the PyTorch model and a forward pass of the TensorFlow model with the same inputs. If the output logits are to within 1e-5, we know that our TensorFlow model is correct ✅. We can then enable PT-TF cross tests in the modelling file such that these checks are performed by the CI.

### Your contribution

Opening this one up to the community! If you're interested in tackling this, free to drop a comment in this thread and open a PR when you're ready. More than happy to answer any questions / queries about this integration!","[{'id': 1834054694, 'node_id': 'MDU6TGFiZWwxODM0MDU0Njk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/TensorFlow', 'name': 'TensorFlow', 'color': 'FF6F00', 'default': False, 'description': 'Anything TensorFlow'}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",9,open
Add TensorFlow Whisper model for audio classification,"### Feature request

The PR https://github.com/huggingface/transformers/pull/21754 adds the PyTorch version of `WhisperForAudioClassification`. It would be great to add the TensorFlow equivalent.

### Motivation

Whisper is an encoder-decoder model for speech recognition. However, we can repurpose the model for other speech tasks, such as _audio classification_.

Audio classification is the task of mapping from an input speech sequence to a single class prediction. For more details, refer to the task page on the Hub: https://huggingface.co/tasks/audio-classification

For audio classification, we only require a _single_ model output. Thus, we do not need the auto-regressive generation capacities of the Whisper decoder (which is used to generate a _sequence_ of text tokens during speech recognition). Instead, we can just use the Whisper encoder to get hidden states, and add a classification head on top to make class label predictions.

This is analogous to using a Wav2Vec2 model for audio classification: the Wav2Vec2 encoder is used to get hidden states, and a classification head added on top to make class label predictions.

The PR https://github.com/huggingface/transformers/pull/21754 adds the PyTorch version of `WhisperForAudioClassification`. It required adding a projection layer and classification layer on top of the `WhisperEncoder`. For more details, refer directly to the pull request.

It would be great to add the TensorFlow equivalent of this model for cross-framework support.

The most difficult part of this PR will be getting the model tester to work. You can see from the PyTorch PR that we require a standalone tester for the audio classification model. This is because the original Whisper model is an encoder-decoder model, but the audio classification model is an encoder-only model. Thus, we require different testing logic.

### Your contribution

Opening this one up to the community! If you're interested in tackling this, free to drop a comment in this thread and open a PR when you're ready. More than happy to answer any questions / queries about this integration!","[{'id': 1834054694, 'node_id': 'MDU6TGFiZWwxODM0MDU0Njk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/TensorFlow', 'name': 'TensorFlow', 'color': 'FF6F00', 'default': False, 'description': 'Anything TensorFlow'}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
kros_test,"### Model description

1st test model trained by 100 pages

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Add SeaFormer model,"### Model description

The computational cost and memory requirement render many computer vision models unsuitable on the mobile device, especially for the high-resolution per-pixel semantic segmentation task. SeaFormer (Squeeze-enhanced Axial Transformer) designed a generic attention block characterized by the formulation of squeeze Axial and detail enhancement. Coupled with a light segmentation head, they achieve the best trade-off between segmentation accuracy and latency on the ARM-based mobile devices on the ADE20K and Cityscapes datasets. They beat both the mobile-friendly rivals and Transformer-based counterparts with better performance and lower latency.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/pdf/2301.13156.pdf
Code and weights: https://github.com/fudan-zvg/SeaFormer
Authors: @wwqq @lzrobots @speedinghzl

cc: @NielsRogge @alaradirik","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",11,open
Project_Test01,"### Model description

This model takes parameters - training data(optional) and rubrics(list of strings) to train. It gives us zero shot results for scoring logical reasoning answers provided by the students.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2301.08771.pdf","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
add MaxViT [TF],"### Model description

MaxViT: Multi-Axis Vision Transformer is one of the nice papers of late 2022 which is also published in [ECCV 2022](https://arxiv.org/pdf/2204.01697v4.pdf) by Google AI.  
* This paper introduces a new attention module called ""multi-axis attention"" which consists of blocked local and sparse global attention for efficient and scalable spatial interactions on arbitrary input resolutions.
* It demonstrates superior performance on various vision tasks including image classification, object detection, and so on.

I think it would be nice to have it on Hugging Face. I would be happy to contribute it on Hugging face.

cc: @alara @NielsRogge 

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Code and Weights: 
* TF1: https://github.com/google-research/maxvit
* TF2: https://github.com/leondgarse/keras_cv_attention_models","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
[i18n-fr] Translating docs to fr,"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the french-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @ArthurZucker, @sgugger for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [x] [index.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.mdx) (https://github.com/huggingface/transformers/pull/21458)
- [x] [quicktour.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.mdx)(https://github.com/huggingface/transformers/pull/21589)
- [x] [installation.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.mdx)(https://github.com/huggingface/transformers/pull/27657)

## Tutorial section
- [ ] [pipeline_tutorial.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.mdx)(https://github.com/huggingface/transformers/pull/28359)
- [x]  [autoclass_tutorial.mdx](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.mdx)(https://github.com/huggingface/transformers/pull/27659)
- [ ]  [preprocessing.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.mdx)
- [ ]  [training.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.mdx)
- [ ]  [accelerate.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.mdx)(https://github.com/huggingface/transformers/pull/28418)
- [ ]  [model_sharing.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.mdx)
- [ ]  [multilingual.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.mdx)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
How to sending my request about parameters in inference API?,"### Model description

<img width=""999"" alt=""image"" src=""https://user-images.githubusercontent.com/61076726/216067290-7969e29b-8c83-41fa-80a6-6c71bc4bac16.png"">

I can't find where I can modify, and, does generating inference not include beam search?

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Megatron-11B,"### Model description

I discovered two implementations of this facebook model on the hub, which was trained on the same corpus as Roberta/Bert. I want to try out some prompting, but when I try to download it with 

from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(""hyunwoongko/megatron-11B"")


I get a 
KeyError: 'megatron' exception. 

This is a relatively sizable model which is as interesting as gpt-j to me, does it work with transformers? 
I found one of the two model re-publishers below wrote their own library, but it depends on outdated versions of transformer. Can we use this model with transformers? 


### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The two megatron-to-pytorch models on the hub: 
https://huggingface.co/models?search=megatron-11

Extra py lib which should make it work: 
https://pypi.org/project/megatron-11b/","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
[pure bf16 training] w/ `AnyPrecisionAdamW` and Kahan summation,"This PR was prompted by [this discussion](https://github.com/pytorch/torchdistx/pull/52#discussion_r1082027732) with  @lessw2020.

The PR works, just keeping it as Draft for now as I haven't polished it to be ready for merging.

# How to perform pure bf16 training (not mixed) running with `AnyPrecisionAdamW` also in bf16 w/ Kahan summation

I think it should require x8 bytes per param, instead of x18 for mixed precision training - i.e. 1/2 memory usage for everything but activations memory.

(also included a hack into loading `load_from_disk` to get saved datasets, but it's unrelated to the actual feature - will remove at the end)

To test checkout this branch:
```
git clone https://github.com/huggingface/transformers transformers-bf16
cd transformers-bf16
git checkout full-bf16-train
```

## getting `AnyPrecisionAdamW`

You can try to install the bleed edge [`torchdistx`](https://github.com/pytorch/torchdistx/) but it's very difficult to do. Since the optimizer is just python code, we just hack-install it doing just this:

```
mkdir -p $CONDA_PREFIX/lib/python3.8/site-packages/torchdistx/optimizers
wget https://raw.githubusercontent.com/pytorch/torchdistx/main/src/python/torchdistx/optimizers/anyprecision_optimizer.py \
-O $CONDA_PREFIX/lib/python3.8/site-packages/torchdistx/optimizers/__init__.py
```
you will just need to update your destination path if you're not using CONDA or have a different python version. To be more specific adjust the location of your python's `site-packages` directory.

# Training

If you have an 80GB A100, you can do `opt-1.3b` setup below, otherwise for smaller cards choose one of the smaller setups.

You can of course do this for any model, this PR is model invariant.

And you can do either finetuning or training from scratch

## opt-1.3b / bf16-pure training from scratch

First, prep an initialized opt-1.3 model:
```

cat << EOT > prep-bf16.py
from transformers import AutoConfig, AutoModel, AutoTokenizer
import torch

mname = ""facebook/opt-1.3b""

config = AutoConfig.from_pretrained(mname)
model = AutoModel.from_config(config, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(mname)

path = ""opt-1.3b-bf16""

model.save_pretrained(path)
tokenizer.save_pretrained(path)
EOT

python prep-bf16.py
```

Train from scratch:

```
rm -rf save_dir;  PYTHONPATH=""src"" python -m torch.distributed.run \
--nproc_per_node=1 --nnode=1 --node_rank=0 \
--master_addr=127.0.0.1 --master_port=9901 \
examples/pytorch/language-modeling/run_clm.py --bf16 \
--half_precision_backend no_amp --seed 42 --model_name_or_path opt-1.3b-bf16 \
--dataset_name wikitext --dataset_config_name wikitext-103-raw-v1 --optim \
adamw_anyprecision --optim_args \
'use_kahan_summation=true, momentum_dtype=bfloat16, variance_dtype=bfloat16, compensation_buffer_dtype=bfloat16' \
--per_device_train_batch_size 12 --per_device_eval_batch_size 12 \
--gradient_accumulation_steps 1 --do_train --do_eval --logging_steps 10 \
--save_steps 1000 --eval_steps 100 --weight_decay 0.1 --num_train_epochs 1 \
--adam_beta1 0.9 --adam_beta2 0.95 --learning_rate 0.0002 --lr_scheduler_type \
linear --warmup_steps 500 --report_to tensorboard --output_dir save_dir
```

Let's check that I got the math right for opt-1.3B 

Theoretical memory allocation for optim states, weights, grads

```
breakdown:       n_params*(optim + grad + weights)
bf16 mixed precision: 1.3*(8     +   2  +  4+2   ) = 1.3*16 = 20.8GB
bf16 pure:            1.3*(4+2   +   2  +    2   ) = 1.3*10 = 13.0GB
-----------------------------------------------------
diff:                                                          7.8GB

```

Real memory allocation: (got by adding `--skip_memory_metrics 0` flag to get memory usage reports)

```
a. bf16 mixed precision:
  before_init_mem_gpu        =        0MB
  init_mem_gpu_alloc_delta   =     5019MB
  init_mem_gpu_peaked_delta  =        0MB
  train_mem_gpu_alloc_delta  =    20076MB
  train_mem_gpu_peaked_delta =      123MB
-----------------------------------------
  total                      =    25218MB             

b. bf16 pure:
  before_init_mem_gpu        =        0MB
  init_mem_gpu_alloc_delta   =     5019MB
  init_mem_gpu_peaked_delta  =        0MB
  train_mem_gpu_alloc_delta  =    12548MB
  train_mem_gpu_peaked_delta =      124MB
-----------------------------------------
  total                      =    17691MB             


diff: 7.53GB
```

So the theoretical and actual numbers check out memory wise.


## opt-125m / bf16-pure training from scratch

If you want to fit into a smaller card, let's do opt-125m

Then prep an empty opt-125m model:
```

cat << EOT > prep-bf16.py
from transformers import AutoConfig, AutoModel, AutoTokenizer
import torch

mname = ""facebook/opt-125m""

config = AutoConfig.from_pretrained(mname)
model = AutoModel.from_config(config, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(mname)

path = ""opt-125m-bf16""

model.save_pretrained(path)
tokenizer.save_pretrained(path)
EOT

python prep-bf16.py
```

Train from scratch in pure bf16:

```
rm -rf save_dir;  PYTHONPATH=""src"" python -m torch.distributed.run \
--nproc_per_node=1 --nnode=1 --node_rank=0 \
--master_addr=127.0.0.1 --master_port=9901 \
examples/pytorch/language-modeling/run_clm.py --bf16 \
--half_precision_backend no_amp --seed 42 --model_name_or_path opt-125m-bf16 \
--dataset_name wikitext --dataset_config_name wikitext-103-raw-v1 --optim \
adamw_anyprecision --optim_args \
'use_kahan_summation=true, momentum_dtype=bfloat16, variance_dtype=bfloat16, compensation_buffer_dtype=bfloat16' \
--per_device_train_batch_size 12 --per_device_eval_batch_size 12 \
--gradient_accumulation_steps 1 --do_train --do_eval --logging_steps 10 \
--save_steps 1000 --eval_steps 100 --weight_decay 0.1 --num_train_epochs 1 \
--adam_beta1 0.9 --adam_beta2 0.95 --learning_rate 0.0002 --lr_scheduler_type \
linear --warmup_steps 500 --report_to tensorboard --output_dir save_dir
```


## opt-125m / fp16-amp  training from scratch

Same for mixed precision fp16 (we want bf16 to give us a similar loss curve when everything else is the same):
```

cat << EOT > prep-fp16.py
from transformers import AutoConfig, AutoModel, AutoTokenizer
import torch

mname = ""facebook/opt-125m""

config = AutoConfig.from_pretrained(mname)
model = AutoModel.from_config(config, torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(mname)

path = ""opt-125m-fp16""

model.save_pretrained(path)
tokenizer.save_pretrained(path)
EOT

python prep-fp16.py
```
```
rm -rf save_dir;  PYTHONPATH=""src"" python -m torch.distributed.run \
--nproc_per_node=1 --nnode=1 --node_rank=0 \
--master_addr=127.0.0.1 --master_port=9901 \
examples/pytorch/language-modeling/run_clm.py --ff16 \
--seed 42 --model_name_or_path opt-125m-fp16 \
--dataset_name wikitext --dataset_config_name wikitext-103-raw-v1 \
--per_device_train_batch_size 12 --per_device_eval_batch_size 12 \
--gradient_accumulation_steps 1 --do_train --do_eval --logging_steps 10 \
--save_steps 1000 --eval_steps 100 --weight_decay 0.1 --num_train_epochs 1 \
--adam_beta1 0.9 --adam_beta2 0.95 --learning_rate 0.0002 --lr_scheduler_type \
linear --warmup_steps 500 --report_to tensorboard --output_dir save_dir
```
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Add support for BLIP and GIT in image-to-text and VQA pipelines,"### Feature request

BLIP and GIT are 2 recent additions in the library, providing state-of-the-art performance for tasks like image captioning and visual question answering (VQA). GIT is even capable of video captioning and video QA.

Hence it makes sense to support them in our image-to-text and VQA pipelines.

### Motivation

Having support for better models in pipelines is very desired!

See also a request for it here: https://discuss.huggingface.co/t/support-for-different-models-in-text-to-image-pipeline/29504

### Your contribution

I can assist in adding support, see #18446 as a very similar case","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",25,open
Models for low resource languages,"### Model description

Hi, I was wondering if there is any way to take leverage of STOA models like [this](https://huggingface.co/facebook/bart-large-mnli) one by facebook and come up with a model for a low resource language, say Filipino using something like student-teacher methods.

My main aim is to come up with some **Zero-Shot models** for such languages with similar accuracy when used with languages like english.



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
TokenGT,"### Model description

Adding the TokenGT graph transformer model with @Raman-Kumar (see [Graphormer issue](https://github.com/huggingface/transformers/issues/20962#issuecomment-1375361519))

@Raman-Kumar I'll create a PR with what I had ported of TokenGT at the end of the week, to give you a starting point! You'll need to read [this](https://huggingface.co/docs/transformers/add_new_model) first, to get an idea of the steps we follow when integrating a model. 
Then, 1st step will be checking the code with a checkpoint, so you need to look for one and download it, to compare results with the original implementation.
Does that work for you?

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",34,open
[i18n-<ao>] Translating docs to <am>,"<!--
Note: Please search to see if an issue already exists for the language you are trying to translate.
-->

Hi!

Let's bring the documentation to all the <languageName>-speaking community 🌐 (currently 0 out of 267 complete)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @ArthurZucker, @sgugger for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section

- [ ] [index.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.mdx) https://github.com/huggingface/transformers/pull/20180
- [ ] [quicktour.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.mdx) (waiting for initial PR to go through)
- [ ] [installation.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.mdx).

## Tutorial section
- [ ] [pipeline_tutorial.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.mdx)
- [ ]  [autoclass_tutorial.mdx](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.mdx)
- [ ]  [preprocessing.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.mdx)
- [ ]  [training.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.mdx)
- [ ]  [accelerate.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.mdx)
- [ ]  [model_sharing.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.mdx)
- [ ]  [multilingual.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.mdx)

<!--
Keep on adding more as you go 🔥
-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Train mobileBERT from scratch for other languages,"### Model description

Hi,
I am thinking of training a mobileBERT model from scratch for the German language. Can I use the [English mobileBERT model from HuggingFace](https://huggingface.co/google/mobilebert-uncased) to apply it to a dataset in another language? It makes sense that I would have to adapt the teacher model of mobileBERT to a BERT model of the corresponding language. Unfortunately, I could not find a parameter to adapt the teacher model.

Are there any other ideas on how best to train a mobileBERT model for another language?

Many greetings and many thanks!

### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Add GPT-2-climate,"### Model description

GPT-2 was pretrained on a climate change-related corpus consisting of over 500 thousand abstracts of top climate scientists' articles from trustable sources covering large temporal and spatial scales. The climate-gpt-2 model could further be used for downstream tasks in the climate change domain, including Classification, Fact-checking, and text generation (climate change-related texts).

paper: https://www.climatechange.ai/papers/neurips2022/27

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation


@seashr","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Flaky feature extraction tests for Donut,"### System Info

Note: I have observed with my own setup, but most recently this was seen in a CI environment  

- `transformers` version: 4.26.0.dev0
- Platform: macOS-13.0.1-arm64-arm-64bit
- Python version: 3.9.15
- Huggingface_hub version: 0.11.0
- PyTorch version (GPU?): 1.14.0.dev20221118 (False)
- Tensorflow version (GPU?): 2.10.0 (True)
- Flax version (CPU?/GPU?/TPU?): 0.5.3 (cpu)
- Jax version: 0.3.6
- JaxLib version: 0.3.5
- Using GPU in script?: Noe
- Using distributed or parallel set-up in script?: No

### Who can help?

@amyeroberts 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

pytest tests/models/donut/test_feature_extraction_donut.py::DonutFeatureExtractionTest

### Expected behavior

Tests don't randomly fail e.g. like in [this CI run](https://app.circleci.com/pipelines/github/huggingface/transformers/53575/workflows/40f5b896-c941-4a9f-8946-26f54bb14505/jobs/644204).

There is an issue occurring when the amount to pad is being calculated - some dimensions are having negative padding found. ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Is it possible to add simple custom pytorch-crf layer on top of TokenClassification model. It will make the model more robust.,"### Model description

Is it possible to add simple custom `pytorch-crf` layer on top of `TokenClassification model`. It will make the model more robust.
There should be simple `Notebook tutorial` which teaches us to add our own `custom layer` on top of `Hugging face models` for 

- Classification
- Token Classification ( BIO)

By taking an example from `dslim/bert-base-NER`. Then add `from torchcrf import CRF` on top of it.

I am planning to do this, but I don't know how to get this feature coded. Any leads or Notebook example would be helpful.

```
from torchcrf import CRF

model_checkpoint = ""dslim/bert-base-NER""
tokenizer = BertTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)
config = BertConfig.from_pretrained(model_checkpoint, output_hidden_states=True)
bert_model = BertForTokenClassification.from_pretrained(model_checkpoint,id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True)


class BERT_CRF(nn.Module):
    
    def __init__(self, bert_model, num_labels):
        super(BERT_CRF, self).__init__()
        self.bert = bert_model
        self.dropout = nn.Dropout(0.25)
        
        self.classifier = nn.Linear(4*768, num_labels)

        self.crf = CRF(num_labels, batch_first = True)
    
    def forward(self, input_ids, attention_mask,  labels=None, token_type_ids=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        
        **sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)**
        sequence_output = self.dropout(sequence_output)
        
        emission = self.classifier(sequence_output) # [32,256,17]
        labels=labels.reshape(attention_mask.size()[0],attention_mask.size()[1])
        
        if labels is not None:    
            loss = -self.crf(log_soft(emission, 2), labels, mask=attention_mask.type(torch.uint8), reduction='mean')
            prediction = self.crf.decode(emission, mask=attention_mask.type(torch.uint8))
            return [loss, prediction]
                
        else:         
            prediction = self.crf.decode(emission, mask=attention_mask.type(torch.uint8))
            return prediction

```

```
args = TrainingArguments(
    ""spanbert_crf_ner-pos2"",
    # evaluation_strategy=""epoch"",
    save_strategy=""epoch"",
    learning_rate=2e-5,
    num_train_epochs=1,
    weight_decay=0.01,
    per_device_train_batch_size=8,
    # per_device_eval_batch_size=32
    fp16=True
    # bf16=True #Ampere GPU
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_data,
    # eval_dataset=train_data,
    # data_collator=data_collator,
    # compute_metrics=compute_metrics,
    tokenizer=tokenizer)
```

I get error on line `    **sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)** `

As `outputs = self.bert(input_ids, attention_mask=attention_mask)` gives the logits for tokenclassification` . How can we get hidden states so that I can concate last 4 hidden states. so that I can do `outputs[1][-1]`?



### Open source status

- [ ] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
add MeMViT model,"### Model description

[MeMViT, CVPR 2022](https://arxiv.org/abs/2201.08383) is the most efficient transformer-based video understanding model, and META AI released it. Its efficient online attention calculation mechanism decreases computation by 30 times compared to SOTA video classification models.

It would be an excellent addition to the `transformers` library considering it is the current SOTA on AVA, EPIC-Kitchens-100 action classification, and action anticipation datasets.

### Your contribution

I want to work on adding this architecture to the HuggingFace.

### Open source status
- [x] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

Source code: https://github.com/facebookresearch/MeMViT
Weight files: https://github.com/facebookresearch/MeMViT#model-checkpoints

cc: @NielsRogge @alaradirik ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",9,open
Atlas: Few-shot Learning with Retrieval Augmented Language Model,"### Model description

Atlas is a retrieval-augmented seq2seq language model comprised of a Contriever retriever and fusion-in-decoder (FID) architecture (which uses T5), introduced in the paper [Atlas: Few-shot Learning with Retrieval Augmented Language Models](https://arxiv.org/pdf/2208.03299.pdf)

From the papers abstract:

> Large language models have shown impressive few-shot results on a wide range of tasks.
However, when knowledge is key for such results, as is the case for tasks such as question
answering and fact checking, massive parameter counts to store knowledge seem to be needed.
Retrieval augmented models are known to excel at knowledge intensive tasks without the
need for as many parameters, but it is unclear whether they work in few-shot settings. In this
work we present Atlas, a carefully designed and pre-trained retrieval augmented language
model able to learn knowledge intensive tasks with very few training examples. We perform
evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and
study the impact of the content of the document index, showing that it can easily be updated.
Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples,
outperforming a 540B parameters model by 3% despite having 50x fewer parameters.

### Open source status

- [X] The model implementation is available https://github.com/facebookresearch/atlas
- [X] The model weights are available https://github.com/facebookresearch/atlas

### Provide useful links for the implementation

Open-sourced implementation from Meta https://github.com/facebookresearch/atlas, with weights available.

Authored by @patrick-s-h-lewis and @gizacard","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Add BART-LS,"### Model description

BART-LS (Long Bart), presented in this [paper](https://arxiv.org/pdf/2209.10052.pdf), establishes a new SOTA on a number of  NLP tasks and long form datasets. It uses pooling-augmented block-wise attention and a novel pre-training strategy to achieve this.

Given my interest in long text summarisation I'm very keen to get this into the wonderful transformers library and to start benchmarking it against other models. Therefore, happy to take this on and ping any members of the team if I face any blockers. If this fits with the library's plans let me know and I'll start working on a PR for this.

### Open source status

- [X] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

Original Model Repo (which includes the model weights): https://github.com/facebookresearch/bart_ls","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
Add FlexiBERT,"### Model description

FlexiBERT is a suite of *flexible* and *heterogeneous* models. The design space was proposed in this [paper](https://arxiv.org/abs/2205.11656) accepted for publication in the Journal of Artificial Intelligence Research.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

The weights are available [here](https://huggingface.co/shikhartuli/flexibert-mini).","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
longformer_content,"### Model description

This model will generate a content score for a summary given the context and the summary itself.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Running the run_mlm_flax on TPU v4 pods,"### System Info

transformers 4.24.0

### Who can help?

@patil-suraj 

I am having problems scaling the run_mlm_flax scripts so that they run on TPU VM v4 Pods (ie the v4-16, v4-32 etc). When running ""out of the box"", the performance is exactly the same as when running on a v4-8. To me this indicates that I am feeding a lot of empty data. The max `per_device_train_batch_size` for 512 sequences in RoBERTa is 62 in both cases, but since the output is identical, it is obviously not scaling.

From trying to understand the code, it seems to be logical to multiply the batch size here with the `jax.process_count()` ([src example](https://huggingface.co/pere/roberta-base-exp-32B/blob/main/run_mlm_flax_stream.py#L452)). However, this does not seem to be the way to approach it.

Any ideas about how to approach this? Is the script tested on v4s?



### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

See explanation above.

### Expected behavior

Expect the batch size to scale automatically.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",36,open
🌐 [i18n-KO] Translating docs to Korean,"Hi!

Let's bring the documentation to all the Korean-speaking community 🌏  (currently 9 out of 77 complete)

Would you want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.

Some notes:

* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
* Please translate in a gender-neutral way.
* Add your translations to the folder called `ko` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
* Register your translation in `ko/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @ArthurZucker, @sgugger and @eunseojo for review.
* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).
* With the [HuggingFace Documentation l10n](https://pseudo-lab.com/HuggingFace-0558662add4949558f6b4c4d526547da) initiative of [Pseudo Lab](https://pseudo-lab.com/), full translation will be done even faster. 🎉 Please give us your support! Cheers to our team 👍@0525hhgus, @KIHOON71, @gabrielwithappy, @jungnerd, @sim-so, @HanNayeoniee, @wonhyeongseo

안녕하세요!

한국어를 사용하는 모두가 기술 문서를 읽을 수 있게 해보아요 🌏 (현재 77개 문서 중 9개 완료)

번역에 참여하고 싶으신가요? 🤗 [번역 가이드](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md)를 먼저 읽어보시기 바랍니다. 끝 부분에 번역해야할 파일들이 나열되어 있습니다. 작업하고 계신 파일이 있다면 여기에 간단히 알려주세요. 중복되지 않도록 `작업중`으로 표시해둘게요.

참고 사항:
* 기술 문서이지만 (친구에게 설명 듣듯이) 쉽게 읽히면 좋겠습니다. __존댓말__ 로 써주시면 감사하겠습니다.
* 성별은 일부 언어(스페인어, 프랑스어 등)에만 적용되는 사항으로, 한국어의 경우 번역기를 사용하신 후 문장 기호와 조사 등이 알맞는지 확인해주시기 바랍니다.
* [소스 폴더](https://github.com/huggingface/transformers/tree/main/docs/source) 아래 `ko` 폴더에 번역본을 넣어주세요.
* 목차(`ko/_toctree.yml`)도 함께 업데이트해주세요. [영어 목차](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml)와 순서가 동일해야 합니다.
* 모두 마치셨다면, 기록이 원활하도록 PR을 여실 때 현재 이슈(`#20179`)를 내용에 넣어주시기 바랍니다. 리뷰 요청은 @ArthurZucker님, @sgugger님, @eunseojo님께 요청해주세요.
*  🙋 커뮤니티에 마음껏 홍보해주시기 바랍니다! 🤗 [포럼](https://discuss.huggingface.co/)에 올리셔도 좋아요.
* [가짜연구소](https://pseudo-lab.com/)의 [이니셔티브](https://pseudo-lab.com/HuggingFace-0558662add4949558f6b4c4d526547da)로 번역이 더욱 빠르게 진행될 예정입니다. 🎉 많은 응원 부탁드려요! 우리팀 화이팅 👍 
  @0525hhgus, @KIHOON71, @gabrielwithappy, @jungnerd, @sim-so, @HanNayeoniee, @wonhyeongseo

## GET STARTED

- [x] 🤗 Transformers https://github.com/huggingface/transformers/pull/20180
- [x] Quick tour https://github.com/huggingface/transformers/pull/20946
- [x] Installation https://github.com/huggingface/transformers/pull/20948

## TUTORIAL
- [x] Pipelines for inference https://github.com/huggingface/transformers/pull/22508
- [x] Load pretrained instances with an AutoClass https://github.com/huggingface/transformers/pull/22533
- [x] Preprocess https://github.com/huggingface/transformers/pull/22578
- [x] Fine-tune a pretrained model https://github.com/huggingface/transformers/pull/22670
- [x] Train with a script https://github.com/huggingface/transformers/pull/22793
- [x] Distributed training with 🤗 Accelerate https://github.com/huggingface/transformers/pull/22830
- [x] Load and train adapters with 🤗 PEFT https://github.com/huggingface/transformers/pull/25706
- [x] Share a model
- [x] Agents https://github.com/huggingface/transformers/pull/24881
- [x] Generation with LLMs https://github.com/huggingface/transformers/pull/25791

## TASK GUIDES

### NATURAL LANGUAGE PROCESSING
- [x] Text classification https://github.com/huggingface/transformers/pull/22655
- [x] Token classification https://github.com/huggingface/transformers/pull/22945
- [x] Question answering
- [x] Causal language modeling
- [x] Masked language modeling https://github.com/huggingface/transformers/pull/22838
- [x] Translation https://github.com/huggingface/transformers/pull/22805
- [x] Summarization https://github.com/huggingface/transformers/pull/22783
- [x] Multiple choice

### AUDIO
- [x] Audio classification https://github.com/huggingface/transformers/pull/26200
- [x] Automatic speech recognition

### COMPUTER VISION
- [x] Image classification
- [x] Semantic segmentation https://github.com/huggingface/transformers/pull/26515
- [x] Video classification
- [x] Object detection
- [x] Zero-shot object detection
- [x] Zero-shot image classification
- [x] Depth estimation

### MULTIMODAL
- [x] Image captioning
- [x] Document Question Answering
- [x] Visual Question Answering https://github.com/huggingface/transformers/pull/25679
- [ ] Text to speech

### GENERATION
- [ ] Customize the generation strategy

## DEVELOPER GUIDES
- [x] Use tokenizers from 🤗 Tokenizers https://github.com/huggingface/transformers/pull/22956
- [x] Inference for multilingual models
- [x] Create a custom architecture https://github.com/huggingface/transformers/pull/22754
- [x] Sharing custom models https://github.com/huggingface/transformers/pull/22534
- [x] Run training on Amazon SageMaker https://github.com/huggingface/transformers/pull/22509
- [x] Export to ONNX https://github.com/huggingface/transformers/pull/22806
- [x] Export to TFLite
- [x] Export to TorchScript
- [ ] Benchmarks
- [ ] Notebooks with examples
- [x] Community resources https://github.com/huggingface/transformers/pull/25674
- [x] Custom Tools and Prompts
- [x] Troubleshoot

## PERFORMANCE AND SCALABILITY
- [x] Overview

### EFFICIENT TRAINING TECHNIQUES
- [ ] Training on one GPU https://github.com/huggingface/transformers/pull/25250
- [x] Training on many GPUs https://github.com/huggingface/transformers/pull/26244
- [x] Training on CPU https://github.com/huggingface/transformers/pull/24911
- [x] Training on many CPUs https://github.com/huggingface/transformers/pull/24923
- [ ] Training on TPUs
- [x] Training on TPU with TensorFlow
- [ ] Training on Specialized Hardware
- [x] Custom hardware for training https://github.com/huggingface/transformers/pull/24966
- [x] Hyperparameter Search using Trainer API

### OPTIMIZING INFERENCE
- [x] Inference on CPU https://github.com/huggingface/transformers/pull/24920
- [x] Inference on one GPU https://github.com/huggingface/transformers/pull/24978
- [x] Inference on many GPUs https://github.com/huggingface/transformers/pull/24943
- [ ] Inference on Specialized Hardware
- [x] Instantiating a big model https://github.com/huggingface/transformers/pull/26245
- [x] Debugging https://github.com/huggingface/transformers/pull/26246
- [x] XLA Integration for TensorFlow Models https://github.com/huggingface/transformers/pull/24904
- [ ] Optimize inference using `torch.compile`

### CONTRIBUTE
- [x] How to contribute to transformers? https://github.com/huggingface/transformers/pull/25877
- [x] How to add a model to 🤗 Transformers? https://github.com/huggingface/transformers/pull/24957
- [x] How to convert a 🤗 Transformers model to TensorFlow? https://github.com/huggingface/transformers/pull/25017
- [x] How to add a pipeline to 🤗 Transformers? https://github.com/huggingface/transformers/pull/25498
- [x] Testing https://github.com/huggingface/transformers/pull/24900
- [ ] Checks on a Pull Request

### CONCEPTUAL GUIDES
- [x] Philosophy https://github.com/huggingface/transformers/pull/25010
- [ ] Glossary
- [x] What 🤗 Transformers can do
- [x] How 🤗 Transformers solve tasks https://github.com/huggingface/transformers/pull/23844
- [x] The Transformer model family https://github.com/huggingface/transformers/pull/24625
- [x] Summary of the tokenizers https://github.com/huggingface/transformers/pull/26243
- [x] Attention mechanisms
- [x] Padding and truncation https://github.com/huggingface/transformers/pull/23823
- [x] BERTology https://github.com/huggingface/transformers/pull/23968
- [x] Perplexity of fixed-length models https://github.com/huggingface/transformers/pull/23850
- [x] Pipelines for webserver inference https://github.com/huggingface/transformers/pull/24828
- [x] Model training anatomy https://github.com/huggingface/transformers/pull/25755

<details>
<summary>

## Other relevant PRs along the way

</summary>

- Enable easy Table of Contents editing https://github.com/huggingface/transformers/pull/22581
- Added forgotten internal English anchors for `sagemaker.mdx` https://github.com/huggingface/transformers/pull/22549
- Fixed anchor links for `auto_class`, `training` https://github.com/huggingface/transformers/pull/22796
- Update ToC from upstream https://github.com/huggingface/transformers/pull/23112

</details>","[{'id': 1834067346, 'node_id': 'MDU6TGFiZWwxODM0MDY3MzQ2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Documentation', 'name': 'Documentation', 'color': '77cc3b', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",19,open
Compact Transformer,"### Model description

# Escaping the Big Data Paradigm with Compact Transformers

Abstract :

> With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper : https://arxiv.org/pdf/2104.05704.pdf
Github repository : https://github.com/SHI-Labs/Compact-Transformers","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
OWL-ViT training / fine-tuning code,"### Feature request

Hi,

I've noticed that recently Google Research added the training and fine-tuning code for OWL-ViT in Scenic. Are you planning to integrate it in HuggingFace Transformers? Thank you!","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 5769473378, 'node_id': 'LA_kwDOCUB6oc8AAAABV-MtYg', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Vision', 'name': 'Vision', 'color': 'C079EF', 'default': False, 'description': ''}]",12,open
Add in-layer TF Tokenizer to BPE tokenizers,"### Feature request

As what we have with `TFBertTokenizer`, but with models that use Byte Pair Encoding (e.g. `TFT5Tokenizer`, `TFClipTokenizer`) etc...

They were implemented in `keras-nlp` (https://github.com/keras-team/keras-nlp/pull/389) and we can now bring them here. 

### Motivation

With that feature we will be able to serve almost every model with TF Serving, which will make it much easier to serve models, as we won't have to write handlers and custom servers.

Having TF BPE Tokenizers is (I think) the last barrier to make `transformers` fully TF Serving-compliant.

### Your contribution

I can submit a PR, but there are a huge lot of models for which we would need to do that, so I expect a large number of subtasks if you decide to go for it.

Also, as `keras-nlp` implemented it (https://github.com/keras-team/keras-nlp/pull/389), should we copy-paste the code for each tokenizer or import from `keras-nlp`, while keeping the reference to their repo?","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",28,open
Add VATT model,"### Model description

Hey,
as discussed with @NielsRogge a few weeks back, I'd like to work on adding the ""VATT: Transformers for Multimodal
Self-Supervised Learning from Raw Video, Audio and Text"" model from Google.

It is basically three transformers(Video/Audio/Text) that are trained jointly in an unsupervised manner using contrastive loss functions. For downstreams tasks they fine-tune the Transformers separately, but also explore a version that shares the weights for all modalities.

For Pre-Traning they use text-video-audio triplets from HowTo100M and video-audio pairs from AudioSet. The authors describe how to fine-tune VATT for vision and audio classification tasks and provide weights for the fine-tuned versions.

The backbone for vision is ViT, for audio WaveFormTransformer and for text they are using BERT/T5

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/pdf/2104.11178.pdf
GitHub: https://github.com/google-research/google-research/tree/master/vatt","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
Add translation of docs to greek,"I thought it would be awsome to add greek translation to the documentation I currently have translated only the index.mdx
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Lite Transformer with Long-Short Range Attention,"### Model description

Abstract : 

> Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT’14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5× with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2×. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years.

### Open source status

- [x] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

[Paper](https://arxiv.org/pdf/2004.11886.pdf)
[Code](https://github.com/mit-han-lab/lite-transformer)
[Old version of the paper, when the model was called `Mobile Transformer (MBT)`](https://openreview.net/attachment?id=ByeMPlHKPH&name=original_pdf)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
Add EDSR and MDSR,"### Model description

EDSR (Enhanced Deep Residual Networks for Single Image Super-Resolution) is for image super resolution, here's the [paper](https://arxiv.org/abs/1707.02921).

### Open source status

- [x] The model implementation is available
- [x] The model weights are available

### Provide useful links for the implementation

Official Implementation https://github.com/sanghyun-son/EDSR-PyTorch and https://github.com/LimBee/NTIRE2017

## Your contribution

I'd like to work on incorporating this architecture into the HuggingFace. Please let me know if you think it's worth adding to huggingface.

@NielsRogge can you review this issue so that I can get started","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",10,open
ERNIE and tensorflow2,"### Feature request

Checked the transformersss Explanatory document , I found that only ErnieModel that supports torch exists. Is there any plan to release TFErnieModel later?

### Motivation

I am using the tensoflow2 framework  and want use ERNIE.

### Your contribution

sorry.","[{'id': 1834054694, 'node_id': 'MDU6TGFiZWwxODM0MDU0Njk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/TensorFlow', 'name': 'TensorFlow', 'color': 'FF6F00', 'default': False, 'description': 'Anything TensorFlow'}, {'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
INF encountered when using sampling with temperature.,"### System Info

latest transformers version == 4.24.0
When generating samples with mBART, I encounter this problem:
![image](https://user-images.githubusercontent.com/10862038/195253207-8229466b-8342-458d-ac40-0b40edd801af.png)

Looking deeply into the codes, I find the problem roots from the beam score added to next_token_scores here:
https://github.com/huggingface/transformers/blob/bc21aaca789f1a366c05e8b5e111632944886393/src/transformers/generation_utils.py#L2566

The original value of beam_scores is 0, but when using temperature like 0.5, the score is also divided the temperature value in logit_warper and gets larger and larger. And finally it causes the overflow of next_token_scores.  

### Who can help?

@patrickvonplaten @Narsil @gante 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

**I provide a simple code that can reproduce this issue.** 

import transformers
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")
model = AutoModelForSeq2SeqLM.from_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")        

model = model.cuda()

src = 'In einem Notruf erzählte Professor Shannon Lamb mit einer etwas zittrigen Stimme der Polizei, dass er seine Freundin erschossen habe und dass die Beamten zu seinem Haus kommen müssten.'

encoded_hi = tokenizer(src, return_tensors=""pt"", padding=True).to('cuda') # do_sample=True
generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.lang_code_to_id['en_XX'], temperature=0.5, do_sample=True, num_beams=10, num_return_sequences=10)

tgt_txt = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

### Expected behavior

I think this should be solved but I'm not sure about the effect of the beam_scores.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
openai whisper ASR pytorch to tflite,"### Model description

I'm trying to figure out how to create tflite models(int8/float32 ) for OpenAI->Whisper ASR model (Tiny.en.pt)
Somehow below generated tflite file getting crashed while running inference 
https://colab.research.google.com/github/usefulsensors/openai-whisper/blob/main/tinynn_pytorch_to_tflite_int8.ipynb



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",7,open
[WIP] Fix weights initialization of several vision models,"# What does this PR do?

This PR is a follow-up of #19341, to make sure weights are properly initialized when training vision models from scratch.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Create TF port of BigBird,"### Model description

[BigBird](https://arxiv.org/abs/2007.14062) is an open source transformer model architecture for longer sequences, and is implemented in the Transformer library already in PyTorch and Flax, but not yet in TensorFlow. This issue tracks the implementation of a TensorFlow version of the model.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

[Location of current implementations](https://github.com/huggingface/transformers/tree/main/src/transformers/models/big_bird)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Adding TensorFlow port of LeViT,"### Feature request

To add the TensorFlow port of the [LeViT](https://arxiv.org/abs/2104.01136) architecture. The architecture is currently present in the Transformers library in [PyTorch](https://github.com/huggingface/transformers/blob/main/src/transformers/models/levit/modeling_levit.py). 

### Motivation

[LeViT](https://arxiv.org/abs/2104.01136) is a family of architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. The TensorFlow port would be an addition to the hybrid architecutre families.

### Your contribution

I would like to make the contribution by building out the TensorFlow port.

Tagging: @amyeroberts who could assign me to the task of adding the TensorFlow port of the model.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
add Unified-IO,"### Model description

I'd like to request the addition of the Unified-IO model. It is a multimodal model capable of visual question answering, image generation and more...
the repo is this: https://github.com/allenai/unified-io-inference
the paper: [Unified-IO: Sequential Modeling for Generally Applicable Vision Models](https://arxiv.org/abs/2206.08916)

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://github.com/allenai/unified-io-inference","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",7,open
PhraseConstraints apearing only directly after input or at the end of the generated sentence,"### System Info

- `transformers` version: 4.22.0
- Platform: Linux-3.10.0-1160.25.1.el7.x86_64-x86_64-with-glibc2.17
- Python version: 3.9.12
- Huggingface_hub version: 0.9.1
- PyTorch version (GPU?): 1.12.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

### Who can help?

@patrickvonplaten @Narsil @cwkeam 

### Information

- [X] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

## Overview
In the [PR](https://github.com/huggingface/transformers/pull/15761) that introduced word constraints to the generation function we have an example script --> Example 2: A Mix of Strong Constraint and a Disjunctive Constraint.
Following up you see it slightly modified, but the modifications should not have an impact on the output
- I added the import for `GPT2LMHeadModel` and `GPT2Tokenizer`
- I removed the `.to(torch_device)` for me to run the script
- I redid the assertions, so we can run the script on its own --> removing  `self.....`

```py

from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained(""gpt2"")
tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")

force_word = ""scared""
force_flexible = [""scream"", ""screams"", ""screaming"", ""screamed""]

force_words_ids = [
    tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids,
    tokenizer(force_flexible, add_prefix_space=True, add_special_tokens=False).input_ids,
]

starting_text = [""The soldiers"", ""The child""]

input_ids = tokenizer(starting_text, return_tensors=""pt"").input_ids

outputs = model.generate(
    input_ids,
    force_words_ids=force_words_ids,
    num_beams=10,
    num_return_sequences=1,
    no_repeat_ngram_size=1,
    remove_invalid_values=True,
)

generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)

assert generated_text[0] == ""The soldiers, who were all scared and screaming at each other as they tried to get out of the""
assert generated_text[1] == ""The child was taken to a local hospital where she screamed and scared for her life, police said.""

```
## ToDo

- [ ] run  the script on `transformers==4.20.1`it works perfectly well
- [ ] run the script on a version above `4.20.1` it will not pass the assertions



### Expected behavior

## Problem

The constraining algorithm seems to be somewhat broken in versions above `4.20.1`
For example on version `4.22`we the script generates the following the outputs:
> _The soldiers, who had been stationed at the base for more than a year before being evacuated **screaming scared**_
>  _The child was taken to a local hospital where he died.\n 'I don't think **screaming scared**_

You can see that the constraints just get added to the end of the generated sentence. In fact, when trying around with constraints, I found out, that they are either placed right after the input:
--> example is made up to show what happens...
> _The soldiers **screaming scared**, who had been stationed at the base for more than a year before being evacuated _
>  _The child **screaming scared** was taken to a local hospital where he died.\n 'I don't think_

or at the end of the generated sentence:
> _The soldiers, who had been stationed at the base for more than a year before being evacuated **screaming scared**_
>  _The child was taken to a local hospital where he died.\n 'I don't think **screaming scared**_

---

- [ ] I expect for the constraints to appear naturally within the generated sentence (like in the testing-script). On versions above `4.20.1` they are just appended in a senseless manner? 

---

- hope that helps
- pls ask me if you have further questions, through I am a beginner myself

","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",19,open
AMOS,"### Model description

Abstract

""We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, we jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs’ outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax. For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models.""

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

HF Hub : https://huggingface.co/microsoft/amos
GitHUB : https://github.com/microsoft/AMOS
Paper : https://arxiv.org/pdf/2204.03243.pdf

Authors : @yumeng5 @xiongchenyan","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Encoder-decoder model is not working correctly for the latest versions,"### System Info

transformers==4.2.1

### Who can help?

@patrickvonplaten

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X ] My own task or dataset (give details below)

### Reproduction

Hi,

I'm working with a seq2seq problem,  in particular with `EncoderDecoderModel` model. The problem is that I can't have good results with the latest version (**4.21.3**). I also tried with **4.18.0** because of [this](https://github.com/huggingface/blog/issues/292#issuecomment-1122666099) but didn't work either.  It is however working when using version **4.2.1**

I have made an [public notebook](https://colab.research.google.com/drive/1obQcmRdX89eWJfk_qUMcxa4pBSPIc53X?usp=sharing) you can run to see the issue. Is an example to train a model to generate the written digits, given a number. 




### Expected behavior

You works nicely with version **4.2.1**, but very bad with the most recent versions. 
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",9,open
Follow ups to DocumentQuestionAnswering Pipeline,"### Feature request

PR https://github.com/huggingface/transformers/pull/18414 has a number of TODOs left over which we'd like to track as follow up tasks.

## Pipeline
- [x] Add support for documents which have more than the tokenizer span (e.g. 512) words
- [ ] Add support for multi-page documents (e.g. for Donut, we need to present one image per page)
- [x] Rework use of tokenizer to avoid the need for `add_prefix_space=True`
- [x] Re-add support for Donut
- [ ] Refactor Donut usage in the pipeline or move logic into the tokenizer, so that pipeline does not have as much Donut-specific code

## Testing
- [ ] Enable `test_small_model_pt_donut` once `hf-internal-testing/tiny-random-donut` is implemented

## Documentation / Website
- [x] Add DocumentQuestionAnswering demo to [Hosted Inference API](https://huggingface.co/impira/layoutlm-document-qa) so that model demos work
- [ ] Add tutorial documentation to [Task Summary](https://huggingface.co/docs/transformers/v4.21.3/en/task_summary#question-answering)

### Motivation

These are follow ups that we cut from the initial scope of PR #18414.

### Your contribution

Happy to contribute many or all of these.","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",16,open
Identifying backend compatibility versions,"We are currently working on identifying the backend versions with which we are compatible and with which we want to be compatible. These backends are PyTorch and TensorFlow. We will be considering Flax at a later point in time.

The first step was to identify the number of failures in each PyTorch/TensorFlow version and was done in https://github.com/huggingface/transformers/issues/18181.

Total number of tests: 38,991.

|     Framework | No. Failures | Release date | Older than 2 years |
| :--------------- | ---------- |  ---------- |  ---------- |
|  PyTorch 1.10 |           50 | Mar 10 2021 | No |
|  PyTorch  1.9 |          710 | Jun 15 2021 | No |
|  PyTorch  1.8 |         1301 | Mar 4 2021 | No |
|  PyTorch  1.7 |         1567 | Oct 27 2020 | No |
|  PyTorch  1.6 |         2342 | Jul 28 2020 | Yes |
|  PyTorch  1.5 |         3315 | Apr 21 2020 | Yes |
|  PyTorch  1.4 |         3949 | Jan 16 2020 | Yes |
| TensorFlow 2.8 |          118 | Feb 2 2022 | No |
| TensorFlow 2.7 |          122 | Nov 4 2021 | No |
| TensorFlow 2.6 |          122 | Aug 11 2021 | No |
| TensorFlow 2.5 |          128 | May 13 2021 | No |
| TensorFlow 2.4 |          167 | Dec 14 2020 | No |

We're proposing to drop versions older than 2 years old and to work towards providing support (support = 0 tests failing) for versions we aim to support. We will drop support for older versions once we reach their two-year-old date. 

Here is the proposed plan moving forward:
- [ ] Have a detailed breakdown of failures for the following versions: 
    - [ ] Torch 1.7
    - [ ] Torch 1.8
    - [ ] Torch 1.9
    - [ ] Torch 1.10
    - [ ] Torch 1.11
    - [ ] Torch 1.12
    - [ ] TensorFlow 2.4
    - [ ] TensorFlow 2.5
    - [ ] TensorFlow 2.6
    - [ ] TensorFlow 2.7
    - [ ] TensorFlow 2.8
    - [ ] TensorFlow 2.9
- [ ] Start with an initial compatibility document to mention which models are supported in which versions
- [ ] Open good first issues to improve compatibility for models not compatible with all versions, starting from the latest one and moving back in time.
- [ ] As versions become supported, run tests on older versions to ensure no regression.

Work by @ydshieh and @LysandreJik 

----------

### Some context and tips when working on Past CI

1. The Past CI runs against a specific commit/tag:
    - **Motivation**: To be able to run the test against the **same** commit to see if a set of fixes improves the overall backward compatibility without new issues introduced.
    - The chosen commit could be changed (to more recent ones) along the time, but it should never be `main`.
    - When working on the fix for Past CI , keeping in mind that we should **check the source code in the commit that is chosen for that particular Past CI run**. The commit given at the beginning of each report provided in the following comments.
2. For each report, there is an attached `errors.txt` where you can find more information to ease the fix process:
    -  The file contains a list whose elements have the following content:
        - The line where an error occurs
        - The error message
        - The complete name of the failed test
        - The link to the job that ran that failed test
    - The errors in the reports sometimes don't contain enough information to make the decision/action. You can use the corresponding links provided in `errors.txt` to see the full trackback on the job run pages.
3. One (possible) fix process would be like:
    - For a framework and a particular version, go to the corresponding reporting table provided in the following comments.
    - Make sure you have a preferred way to navigate the source code in a specific commit.
    - Download/Open the corresponding `errors.txt`.
    - From the `General` table, take a row whose `status` is empty. Ideally, take the ones with higher value in `no.` column.
    - Search in `errors.txt` for the `error` in the picked row. You get information about the failed line, failed test, and the job link.
    - Navigate to the failed line or failed test in your workspace (or in a browser) that checks out to the specific commit for the run.
    - Use the job link to go to the job run page if you need more information about the error.
    - Then you might come up with a solution :-), or decide a fix is not necessary with good reasons.
    - Update the `status` column with a comment once a fix or a decision is made.
4. Some guides/hints for the fix:
    - 🔥 To install a specific framework version, `utils/past_ci_versions.py` can help!
    - ⚠️ As the tests are run against a chosen commit, which may not contain some fixes in the `main` branch. (This is particular confusing if you try to run the failed test without checking out to that commit.).
        - If the test passes when you run a failed test (in the report) against the `main` branch, with the target framework version, it's very likely a fix exists on `main` that applies to the target framework version too.
        - In this case,
            - either update `status` with `fixed in #XXXXX` (if you know clearly that PR fixes that error)
            - or `works for commits since **b487096**` - a commit sha (It's not always trivial to find out which PR fixed a particular error - especially when working with Past CI) 
    - We decide to focus on the PyTorch and TensorFlow version, and not to consider other 3rd libraries.  Therefore, some packages are not installed, like `kenlm` or `detectorn2`. We could just simply update the `status` column with `XXX not installed`.
    - When an error is coming from a C/C++ exception, and the same code and inputs work for new framework versions, we could skip that failed test with a `@unittest.skipIf`, and update the status like `torch._C issue -> works wth PT >= 11 Fixed in #19122`.
        - PR [#19122](https://github.com/huggingface/transformers/pull/19122) is one such example.
    - If an error occurs in several framework versions, say, PT 11 and PT 10, and a status is updated for the newer version (here PT 11), we can simply put `see PT 11` in the report `status` column for older versions.
    - Some old framework versions lack attributes or arguments introduced in newer versions. See [#19201](https://github.com/huggingface/transformers/pull/19201)  and [#19203](https://github.com/huggingface/transformers/pull/19203) for how a fix would look like in such cases. If a similar warning (to the one in [#19203](https://github.com/huggingface/transformers/pull/19203)) already exists, we could update `status` with, for example, `Vilt needs PT >= 1.10`.
        - Adding such warning is not a fix in a strict sense, but at least it provides some information. Together with the updated `status`, we keep information tracked.
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",11,open
Implement a new model: Point-BERT 🌟,"### Model description

Hi, I’m Adonai vera; I’m looking to contribute with a new point cloud model; Point cloud has become very relevant in applications such as autonomous cars, inspections, architecture, reconstruction, and more. Segmentation algorithms allow us to extract valuable information from these types of files. That is why it is very important for me to contribute to the HF community with the architecture of a new Point-Bert model. Point-BERT is a Pre-training 3D Point Cloud Transformers with Masked Point Modeling. According to the state of the art, it is one of the best models, surpassing PointNet, Pointnet++, and Transformer-OcCo, among others.

**Short description of the model and link to the paper:**
They devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, they first divide a point cloud into several local point patches. A point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, they randomly mask out some patches of input point clouds and feed them into the backbone Transformers. (Abstract)

[Link to the paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.pdf)

**Link to the implementation if it is open-source:**

[Pre-trained model in github](https://github.com/lulutang0608/Point-BERT)

 [Talk about the new arquicture in CVPR 2022](https://www.youtube.com/watch?v=KMOCw68Veoo)

**Link to the model weights if they are available:**

[Cfgs](https://github.com/lulutang0608/Point-BERT/tree/master/cfgs)

[Architecture](https://github.com/lulutang0608/Point-BERT/blob/master/models/Point_BERT.py)

I’m happy to contribute to this process, but it's important to know If this model can be valuable to the community.

PDT: I'm trying to connect with the model's authors to get more insights into the implementation. 🤗 
[EDIT] I talked with Xumin, one of the model's authors, and they support all the deployment in HF. 🚀🚀🚀

The authors are: :woman_teacher: :man_teacher:
@lulutang0608
@yuxumin
@raoyongming


### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation
[Architecture](https://github.com/lulutang0608/Point-BERT/blob/master/models/Point_BERT.py)
[Cfgs](https://github.com/lulutang0608/Point-BERT/tree/master/cfgs)
[Talk about the new arquicture in CVPR 2022](https://www.youtube.com/watch?v=KMOCw68Veoo)
[Pre-trained model in github](https://github.com/lulutang0608/Point-BERT)
[Link to the paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.pdf)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Refactor Pytorch `model.generate` method to work on TPU,"### Feature request

Refactor PT version of the method `model.generate` for text generating models to make it compatible with XLA and speed up inference on TPU.

### Motivation

Right now, `model.generate` on PT is extremely slow on TPU compared to CPU and GPU. This is probably due to the fact that some operations done in the PT version of `model.generate` are not XLA compatible, and thus the generation process falls back on CPU. This makes inference on TPU infeasible. A major refactoring work has already been done on its TF counterpart, so it would be nice to have the PT version working as well.

A more in-depth discussion with @gante took place in #12322 and on this [huggingface discussion](https://huggingface.co/spaces/joaogante/tf_xla_generate_benchmarks/discussions/1).

### Your contribution

If there is some interest from the HF team, I can definitely assist during the work.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",20,open
Add TF VideoMAE,"### Feature request

Add the [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) model in TensorFlow.

### Motivation

There's an evident scarcity of SoTA and easy-to-use video models in TensorFlow. I believe having VideoMAE in TensorFlow would greatly benefit the community. 

### Your contribution

I am willing to contribute the model. Please assign it to me. 

@amyeroberts possible to assign this to me?","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
Support output_scores in XLA TF generate,"### Feature request

Support output_scores in XLA TF generate.

### Motivation

The scores would be critical and widely used in generation models for downstream applications. It is a confidence score for downstream thresholding or ranking. But it is not yet supported for XLA TF generate. 

### Your contribution

N/A","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
[WIP] Introduce NestLayer,"# What does this PR do?

Outline for a new layer class to replace `tf.keras.layers.Layer` in our models. It extends `tf.keras.layers.Layer` to include two methods `get_layer` and `layers` from `tf.keras.Model`.

## Motivation

All of our TF models' layers are subclasses of `tf.keras.layers.Layer`. Unfortunately, when there are nested layers, we are not able to access the layers below the first level using the typical keras `layers` API. 

The main reason for introducing to be able to use our models as backbones. In DETR, we replace the ResNet backbone's [batchnorm layers to frozen batchnorm layers](https://github.com/huggingface/transformers/blob/2ab790e82d0759b667cd848a4d49e6ad65e15d59/src/transformers/models/detr/modeling_detr.py#L306). We need to be able to perform the same or similar operations on our TF models. This requires being able to access all of the layers, which is currently not possible. 

For example - our `TFResNetModel` will only show `TFResNetMainLayer` when we call `model.summary(expand_nested=True)` and `TFResNetMainLayer` has no property `layers`. 

```
In [1]: from transformers import TFResNetModel
In [2]: model_checkpoint = ""microsoft/resnet-50""
In [3]: model = TFResNetModel.from_pretrained(model_checkpoint)
In [4]: model.summary(expand_nested=True)
Model: ""tf_res_net_model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 resnet (TFResNetMainLayer)  multiple                  23561152

=================================================================
Total params: 23,561,152
Trainable params: 23,508,032
Non-trainable params: 53,120
_________________________________________________________________

In [5]: model.layers
Out[5]: [<transformers.models.resnet.modeling_tf_resnet.TFResNetMainLayer at 0x17fb9daf0>]

In [6]: hasattr(model.layers[0], 'layers')
Out[6]: False
```

This is also necessary if we every want to be able to access the intermediate activation functions of our TF model.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
[Summary] Regarding memory issue in tests,"### Description

This is a short summary of the memory issue in our tests


### The following tests definitely have memory issues
- PyTorch (increase ~`15 MB` each call):
  - test_torch_fx
  - test_torch_fx_output_loss
- TensorFlow:
  - test_xla_fit 
  - test_xla_generate_fast (increase ~`100 MB` each call)
  - test_xla_generate_slow
  - test_xla_mode
  - test_onnx_runtime_optimize  (increase ~`8 MB` each call)
  - test_dataset_conversion (increase ~`0.2 M`B each call)

- **Flax**:
  - **Almost all test methods have memory issue!**
  -  [The CircleCI job run page](https://app.circleci.com/pipelines/github/huggingface/transformers/45317/workflows/5bcb8b8a-776c-4c58-ad99-cf2700304c05/jobs/528556/resources) demonstrates this issue too

### Some tests are also suspicious, but need more investigations.

- For example, the test `test_graph_mode` have the following memory *difference* in consecutive runs (in KB): 
    ```
    [936.0, 520.0, 260.0, 520.0, 0.0, 0.0, 260.0, 520.0, 0.0, 0.0, 260.0, 0.0, 0.0, 0.0, 260.0, 260.0, 260.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    ```
    (not always increase, but it continues to happen)

- For `test_saved_model_creation_extended` (in KB):
    ```
    [144436.0, -104552.0, 1280.0, -103908.0, -1536.0, 177868.0, -33572.0, 20240.0, 170852.0, -51704.0, -8448.0, 59904.0, -48128.0, 2440.0, 34856.0, 3068.0, -3420.0, -36864.0, -6756.0, 36136.0, -2048.0, -17400.0, -4608.0, -25896.0, 4096.0, 1024.0, 22344.0, 25784.0, -256.0]
    ```
    (sometimes some amount of memory is released, but still leaks in the long run?)

### Pytest itself will accumulate some memory usage as tests continue to run.

This is just my hypothesis: sometimes I see an increase of a few KB after a sequence of runs without leak.

### Possible actions to take

- (It's probably worth it to fix this issue for a few tests mentioned above to gain some experience):
  - In this case, we can only focus on `non-slow` tests 

- **[Not to go]** There is a `pytest` plugin `pytest-forked` to run each test in a forked subprocess. But it doesn't work well with TensorFlow and Flax (some tests will hang forever). I will provide some details in the comments.

- We can try to run the tests per model in each CircleCI job steps. However, the output on job run pages will be a bit noisy, but we can have an extra step to print the test failures in a cleaner way.
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",9,open
Add Mask2Former,"### Model description

Mask2Former is a single architecture for panoptic, instance and semantic segmentation.

**Mask2Former Paper Abstract**:
Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).


### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/abs/2112.01527

Github repo (and weights): https://github.com/facebookresearch/Mask2Former","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
Update no_trainer scripts to include gradient accumulation,"### Feature request

🤗 Accelerate has a gradient accumulation wrapper, and the `no_trainer` scripts should be updated to include it! 

An example can be seen [here](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/gradient_accumulation.py), below is an example diff of what the integration would look like:

```diff
-     accelerator = (
-         Accelerator(log_with=args.report_to, logging_dir=args.output_dir) if args.with_tracking else Accelerator()
-     )
+     accelerator = (
+         Accelerator(log_with=args.report_to, logging_dir=args.output_dir, gradient_accumulation_steps=args.gradient_accumulation_steps) if args.with_tracking else Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps)
+     )
```

As well as:
```diff
-     num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
+     num_update_steps_per_epoch = len(train_dataloader)

...


for step, batch in enumerate(train_dataloader):
+     with accelerator.accumulate(model):
```
```diff
-             loss = loss / args.gradient_accumulation_steps
            accelerator.backward(loss)
-             if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
                progress_bar.update(1)
                completed_steps += 1
```

The list of available scripts to update include:

- [ ] examples/pytorch/image-classification/run_image_classification_no_trainer.py
- [ ] examples/pytorch/language-modeling/run_clm_no_trainer.py
- [ ] examples/pytorch/language-modeling/run_mlm_no_trainer.py
- [ ] examples/pytorch/multiple-choice/run_swag_no_trainer.py
- [ ] examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
- [ ] examples/pytorch/question_answering/run_qa_no_trainer.py
- [ ] examples/pytorch/semantic-segmentation/run_semantic_segmentation_no_trainer.py
- [ ] examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py
- [ ] examples/pytorch/summarization/run_summarization_no_trainer.py

### Motivation

This is a great first issue for someone who wants to learn how to use some of the latest bits in Accelerate and get an easy beginner contribution to the library 🤗 

### Your contribution

If you decide to pick up this issue, feel free to ping myself (@muellerzr), @sgugger, or @pacman100 to review 🤗 ","[{'id': 1936351150, 'node_id': 'MDU6TGFiZWwxOTM2MzUxMTUw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Examples', 'name': 'Examples', 'color': 'd4c5f9', 'default': False, 'description': 'Which is related to examples in general'}, {'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",18,open
GFT: Generative Fundamental Training,"### Model description

Hi,
It is just an ad of our GFT.
It has a novel attention head by fusing attention and mlp layer. It has an ultra-thin, ultra-deep architecture that maximizing model performance with minimal parameters. More interestingly,  it equips a novel decoder call top-E(entropy) algorithm. 
Our model has 81 layers but only 300M parameters. 
Both Chinese and English(Biomedical) model are available.
It is coded from scratch by cuda and C++, no DL framework is needed.
Enjoy!

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

https://github.com/wangyi-fudan/GFT

Chinese Model Download

链接: https://pan.baidu.com/s/1HKw83YWttCIPdnZCZ-hdOA

密码: 0r1j

English Medical Model Download

链接: https://pan.baidu.com/s/1yazmdB8xFzLwXbKmXWslRA

密码: rqdc
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
[Flax] Add scan_with_axes,"# What does this PR do?

Adds `scan_with_axes` to Flax Bert and its derived models.

TODO:
- [ ] Fix cookie cutter template
- [ ] Run `make fix-copies` (after review)

Fixes [#17399](https://github.com/huggingface/transformers/issues/17399)

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Transformers documentation translation to French,"Hi!

Let's bring the documentation to all the French-speaking community :)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any and we'll add your name to the list.

Some notes:

- Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).
- Please translate in a gender-neutral way.
- Add your translations to the folder called fr inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
- Register your translation in [fr/_toctree.yml](https://github.com/huggingface/transformers/blob/main/docs/source/it/_toctree.yml); please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
- Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @omarespejel and @sgugger for review.
- 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).

## Get Started section
- [ ] [index.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.mdx) 
- [ ] [quicktour.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.mdx).
- [ ] [installation.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.mdx). 


## Tutorial section
- [ ] [pipeline_tutorial.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.mdx) 
- [ ] [autoclass_tutorial.mdx](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.mdx) 
- [ ] [preprocessing.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.mdx) 
- [ ] [training.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.mdx) 
- [ ] [accelerate.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.mdx) 
- [ ] [model_sharing.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.mdx) 
- [ ] [multilingual.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.mdx) ","[{'id': 1834067346, 'node_id': 'MDU6TGFiZWwxODM0MDY3MzQ2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Documentation', 'name': 'Documentation', 'color': '77cc3b', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",8,open
Onnx Runtime Errors With LongT5,"### System Info

- `optimum` version: 1.2.3 (installed via Github installation)
- `transformers` version: 4.20.1
- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.13
- Huggingface_hub version: 0.8.1
- PyTorch version (GPU?): 1.11.0+cu113 (False)
- Tensorflow version (GPU?): 2.8.2 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help?

@stancld @echarlaix @LysandreJik

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

LongT5 with TGlobal Attention isn't able to run sequences longer than **global_block_size * 2**. This is because during the model tracing [num_globals > 0](https://github.com/huggingface/transformers/blob/main/src/transformers/models/longt5/modeling_longt5.py#L191) is being converted to False. I originally posted the error in Optimum (https://github.com/huggingface/optimum/issues/285) but @echarlaix asked me to open an issue here because this error concerns the ONNX export.

Code to reproduce is below:

```
!pip install transformers
!pip install transformers[onnx]
!python -m pip install git+https://github.com/huggingface/optimum.git
!python -m pip install git+[https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]](https://github.com/huggingface/optimum.git#egg=optimum%5Bonnxruntime%5D)
!pip install datasets
```

```py
from optimum.onnxruntime import ORTModelForSeq2SeqLM

model = ORTModelForSeq2SeqLM.from_pretrained(""longt5-tglobal-base"", from_transformers=True)
from transformers import AutoTokenizer, pipeline

tokenizer = AutoTokenizer.from_pretrained('google/long-t5-tglobal-base')

onnx_summarization = pipeline(""summarization"", model=model, tokenizer=tokenizer)

text = # Something longer than 32 tokens if I don't change the number of global blocks
pred = onnx_summarization(text)`
```
```
RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running LessOrEqual node. Name:'LessOrEqual_648' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:603 onnxruntime::Broadcaster::Broadcaster(gsl::span, gsl::span) largest <= 1 was false. Can broadcast 0 by 0 or 1. 16 is invalid.
```
### Expected behavior

Should work for very large seq lens on default global block size without error","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}, {'id': 4364086132, 'node_id': 'LA_kwDOCUB6oc8AAAABBB6rdA', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/ONNX', 'name': 'ONNX', 'color': 'D4C5F9', 'default': False, 'description': ''}]",8,open
TF2 DeBERTaV2 runs super slow on TPUs,"### System Info

latest version of transformers, Colab TPU, tensorflow 2

### Who can help?

@kamalkraj @Rocketknight1 @BigBird01 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

It's currently hard to share code and access to the google bucket. But I believe any TF2 DeBERTaV2 code running on TPUs will have this issue

### Expected behavior

I've been trying to train a deberta v3 model on GPU and TPUs. I got it to work on multi-node and multi-gpus using Nvidia deeplearning examples libraries https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow2/LanguageModeling/ 
I basically used the training setup and loop from the BERT code, the dataset utils from the ELECTRA code, and the model from Huggingface transformers with some changes in order to share embeddings. 

On 6xA40 45gb gpus i get around 1370 sentences per seconds during training (which is lower than what Nvidia gets for Electra but it's fine).

Ok, now the problem....  on TPU i get **20** sentences per second

I traced the issue back to the tf.gather function here https://github.com/huggingface/transformers/blob/main/src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py#L525

I ran TPU profiling and this is the output:
![image](https://user-images.githubusercontent.com/44616226/180247092-6bb99a22-05aa-418a-a684-f6fa632918ce.png)

GatherV2 takes most of the time:
![image](https://user-images.githubusercontent.com/44616226/180248277-d6145680-963e-49ff-99f7-5837672d0e92.png)

zoomed in pictures of the fast ops
![image](https://user-images.githubusercontent.com/44616226/180248860-a7429388-0023-4c20-9f5d-5b9726c0dda0.png)

Also, I'm not sure if this is TPU specific since on GPUs the training ~30% slower compared to regular ELECTRA.","[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",36,open
Test summary with previous PyTorch/TensorFlow versions,"Initialized by @LysandreJik, we ran the tests with previous PyTorch/TensorFlow versions. The goal is to determine if we should drop (some) earlier PyTorch/TensorFlow versions.

- This is not exactly the same as the scheduled daily CI (`torch-scatter`, `accelerate` not installed, etc.)
- Currently we only have the global summary (i.e. there is no number of test failures per model)

Here is the results (running on ~June 20, 2022):
- PyTorch testing has ~27100 tests
- TensorFlow testing has ~15700 tests

|     Framework | No. Failures |
| :--------------- | ----------: |
|  PyTorch 1.10 |           50 |
|  PyTorch  1.9 |          710 |
|  PyTorch  1.8 |         1301 |
|  PyTorch  1.7 |         1567 |
|  PyTorch  1.6 |         2342 |
|  PyTorch  1.5 |         3315 |
|  PyTorch  1.4 |         3949 |
| TensorFlow 2.8 |          118 |
| TensorFlow 2.7 |          122 |
| TensorFlow 2.6 |          122 |
| TensorFlow 2.5 |          128 |
| TensorFlow 2.4 |          167 |

It looks like the number of failures in TensorFlow testing doesn't increase much.

### So far my thoughts:
- All TF >= 2.4 should be (still) kept in the list of supported versions

### Questions
- What's you opinion regarding which versions to drop support?
- Would you like to see the number of test failures per model?
- TensorFlow 2.3 needs CUDA 10.1 and requires the build of a special docker image. Do you think we should make the effort on it to have the results for `TF 2.3`?
","[{'id': 1834088753, 'node_id': 'MDU6TGFiZWwxODM0MDg4NzUz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Tests', 'name': 'Tests', 'color': 'a6fcca', 'default': False, 'description': 'Related to tests'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",11,open
dalle mega,"# What does this PR do?

This PR adds the DalleMega model from [dalle-mini](https://github.com/borisdayma/dalle-mini) for text-2-image generation.
The VQGAN model required for converting the tokens to image is in this PR #18150

- [ ] override the `sample` method for classifier-free guidance. 
- [ ] port and upload weights on the hub
- [ ] add tests
- [ ] add docs
- [ ] boom!","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Add VQGAN,"# What does this PR do?

Adds the VQGAN model, first step for adding the Dallemega model in transformers. 

- This model is different from most the models available in `Transformers`, it's an U-Net like encoder-decoder architecture with vector quantizer bottleneck.
-  This is only the generator part of the GAN, intended only for inference.
- It does not have common transformer style embeddings, blocks and other attributes.
- Currently it does not support `output_hidden_states` and `output_attentions`, since this is complex architecture and it's not  clear which `hidden_states` to return. Would love to hear your thoughts if we should support this.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
[TRACKER] Add alibi tests on BLOOM,"### Feature request

Add ALiBi tests on BLOOM ! 
We should add several tests to simply to test if alibi has been created correctly

- test padding 
- test expected output

@Narsil 

### Motivation

Build stronger CI tests

### Your contribution

Design and build the tests mentioned above","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
DeltaLM,"### Model description

DeltaLM is a multilingual encoder-decoder architecture that regards the decoder as the task layer of off-the-shelf pre-trained encoders. This architecture introduces an interleaved decoder, which has a more consistent structure with the encoder. Weights from pre-trained multilingual encoders are used to initialise both the encoder and decoder models before training on monolingual and bilingual data.

As of September 2021 DeltaLM ranks first on the [WMT21 multilingual translation task](http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html).

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation
The model implementation is available at: 
https://github.com/microsoft/unilm/tree/master/deltalm

Model weights are available:
[DeltaLM-base](https://deltalm.blob.core.windows.net/deltalm/deltalm-base.pt)
[DeltaLM-large](https://deltalm.blob.core.windows.net/deltalm/deltalm-large.pt)

Who are the authors:
@shumingma @gitnlp

I'd be happy to try work on contributing the model.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Model parallelism for m2m100,"### Model description

The translation model m2m100 proposed by Facebook is too huge to train using DDP, is there any open solution for model parallelism of m2m100, just like GPT2? Thank you.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

_No response_","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Adding TF Implementation of BEiT,"### Feature request

Addition of TF implementation of BEiT 

### Motivation

I have always seen that there is a discrepancy in the availability of models for PyTorch and the models available in TensorFlow, and want to have models for usage in both backends.

### Your contribution

I will add the implementation of BEiT in TF :)

cc - @gante","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",6,open
An example for finetuning FLAVA or any VLP multimodel using trainer (for example for classification),"### Feature request

There is no example of finetuning any VLP model using trainer. I would appreciate an example

### Motivation

The way to use trainers with any Vision and Language pretrained model is not clear. 

### Your contribution

None. ","[{'id': 1936351150, 'node_id': 'MDU6TGFiZWwxOTM2MzUxMTUw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Examples', 'name': 'Examples', 'color': 'd4c5f9', 'default': False, 'description': 'Which is related to examples in general'}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
Add TF implementation of LongT5 model,"### Feature request

Add TF implementation of LongT5 model

### Motivation

Add support for TF backend to allow using LongT5 with TF.

### Your contribution

I will add this :]

cc: @gante @patrickvonplaten ","[{'id': 1834054694, 'node_id': 'MDU6TGFiZWwxODM0MDU0Njk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/TensorFlow', 'name': 'TensorFlow', 'color': 'FF6F00', 'default': False, 'description': 'Anything TensorFlow'}, {'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Add DFFT,"### Model description

DFFT is a new fully Transformer-based object detector. The model doesn't require a decoder, unlike DETR.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Paper: https://arxiv.org/abs/2206.06829

Github repo (and weights): https://github.com/Pealing/DFFT","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",7,open
TF: XLA generation not working properly in some models,"This issue is used to track TensorFlow XLA generation issues, arising from #17857. There are three categories of issues, sorted in descending order by severity:

### Key model issues

These are heavily-used models, whose quality should be prioritized.

- [x] T5 -- The quality of the results decreases with `max_length`. See [here](https://github.com/huggingface/transformers/pull/17857/files#r906702367).
- [x] GPT-J -- fails simple generate tests with numerical issues

### Models failing basic tests

These models are failing `test_xla_generate_fast` -- a short greedy generation.

- [ ] LED
- [ ] Speech2Text
- [ ] XLNet
- [ ] XGLM

### Models failing complex tests

These are models failing `test_xla_generate_slow` -- a long beam search generation.

- [x] Bart
- [x] Blenderbot
- [x] Marian
- [x] mbart
- [x] OPT
- [x] Pegasus","[{'id': 1834054694, 'node_id': 'MDU6TGFiZWwxODM0MDU0Njk0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/TensorFlow', 'name': 'TensorFlow', 'color': 'FF6F00', 'default': False, 'description': 'Anything TensorFlow'}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",12,open
[WIP] DETR TF implementation,"# What does this PR do?

Add TF implementation of DETR model

Dependent on the TF implementation of ResNets being merged in to provide a backbone: https://github.com/huggingface/transformers/pull/17427


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Add Flax implementation for BLOOM,"### Model description

I'm interested in adding an implementation of BLOOM in Flax. 

The implementation shouldn't be too bad since the pytorch implementation can serve as a guide and a way to check correctness. 



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

@younesbelkada @stas00 @patrickvonplaten 

If someone is already planning to work on this then no worries, but if not I will start on this as soon as I have time!","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",7,open
Loading repository after rename does not work (with old name),"### System Info

```shell
- `transformers` version: 4.19.2
- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.13
- Huggingface_hub version: 0.7.0
- PyTorch version (GPU?): 1.11.0+cu113 (False)
- Tensorflow version (GPU?): 2.8.2 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>
```


### Who can help?

cc @LysandreJik

### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

Context
```
FROM LOCATION (old location): https://huggingface.co/KB/bert-base-swedish-cased
TO LOCATION (new location): https://huggingface.co/KBLab/bert-base-swedish-cased
```

The following breaks
```py
from transformers import AutoModel
model = AutoModel.from_pretrained(""KB/bert-base-swedish-cased"")
```

Error
```
OSError: Can't load config for 'KB/bert-base-swedish-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'KB/bert-base-swedish-cased' is the correct path to a directory containing a config.json file
```

What works here?
:white_check_mark: going to original path redirects to new path
:white_check_mark: doing git clone
:white_check_mark: using huggingface_hub model_info(""KB/bert-base-swedish-cased"")
:x: using transformers AutoModel.from_pretrained(""KB/bert-base-swedish-cased"")

### Expected behavior

```shell
Loading the model with the old name would still work so users loading a transferred model don't have an impacted experience
```
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
TFRemBertModelTest.test_resize_token_embeddings not working,"### System Info

```shell
- `transformers` version: 4.20.0.dev0
- Platform: Windows-10-10.0.22000-SP0
- Python version: 3.9.11
- Huggingface_hub version: 0.5.1
- PyTorch version (GPU?): 1.11.0+cu113 (True)
- Tensorflow version (GPU?): 2.8.0 (True)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No
```


### Who can help?

@gante @Rocketknight1 

### Reproduction

`TFRemBertModelTest.test_resize_token_embeddings` has CI failed [here](https://github.com/huggingface/transformers/runs/6682139350?check_suite_focus=true)

This method (called during `resize_token_embeddings`)
https://github.com/huggingface/transformers/blob/028d4b7c8be2c2fc1146fcc1e9bd253c1a7ea346/src/transformers/modeling_tf_utils.py#L1449
assumes that `word_embedding_weight` has the same shape as `old_lm_head_decoder`, but this is not the case for `TFRemBertModel`, as it has `input_embedding_size` and `output_embedding_size` in config.

An PR #17511 was opened, but we decided to not merge it. Instead, a cleaning up of  TF embeddings should be done first.

### Expected behavior

```shell
`resize_token_embeddings` should work for `TFRemBertModelTest`
```
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",5,open
_batch_encode_plus() got an unexpected keyword argument 'is_pretokenized' using BertTokenizerFast,"### System Info

```shell
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][""input_ids""]), training_set[0][""labels""]):
  print('{0:10}  {1}'.format(token, label))

The error I am getting is:
Traceback (most recent call last):
  File ""C:\Users\1632613\Documents\Anit\NER_Trans\test.py"", line 108, in <module>
    for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][""input_ids""]), training_set[0][""labels""]):
  File ""C:\Users\1632613\Documents\Anit\NER_Trans\test.py"", line 66, in __getitem__
    encoding = self.tokenizer(sentence,
  File ""C:\Users\1632613\AppData\Local\conda\conda\envs\ner\lib\site-packages\transformers\tokenization_utils_base.py"", line 2477, in __call__
    return self.batch_encode_plus(
  File ""C:\Users\1632613\AppData\Local\conda\conda\envs\ner\lib\site-packages\transformers\tokenization_utils_base.py"", line 2668, in batch_encode_plus
    return self._batch_encode_plus(
TypeError: _batch_encode_plus() got an unexpected keyword argument 'is_pretokenized'
```


### Who can help?

@SaulLu 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

1. Download the NER Dataset from the Kaggle link (https://www.kaggle.com/datasets/namanj27/ner-dataset)
2. Use the Script with the mentioned versions of transformers and tokenizers:
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][""input_ids""]), training_set[0][""labels""]):
  print('{0:10}  {1}'.format(token, label))


### Expected behavior

```shell
I expect to get the token, label from the script above.

Python Version: 3.9
tokenizers-0.12.1 
transformers-4.19.2

Anyone can shed some light please?
```
","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",20,open
Tranformers documentation translation to Italian,"Hi!

Let's bring the documentation to all the Italian-speaking community :)

Who would want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know here if you'd like to translate any and we'll add your name to the list.

Some notes:

- Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗). For example, use Tu instead of Lei.
- Please translate in a gender-neutral way.
- Add your translations  to the folder called  `it` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).
- Register your translation in [it/_toctree.yml](https://github.com/huggingface/transformers/blob/main/docs/source/it/_toctree.yml); please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).
- Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue.
- 🙋 If you'd like others to help you with the translation, you can also post in the  🤗 [forums](https://discuss.huggingface.co/).

## Get Started section
- [x] [index.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.mdx) @mfumanelli
- [x] [quicktour.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.mdx). @mfumanelli
- [x] [installation.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.mdx). @mfumanelli
## Tutorial section
- [x] [pipeline_tutorial.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.mdx) @nickprock
- [x] [autoclass_tutorial.mdx](https://github.com/huggingface/transformers/blob/master/docs/source/autoclass_tutorial.mdx) @mfumanelli
- [x] [preprocessing.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.mdx) @nickprock
- [x] [training.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.mdx) @nickprock
- [x] [accelerate.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.mdx) @mfumanelli
- [ ] [model_sharing.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.mdx) WIP @mfumanelli
- [ ] [multilingual.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.mdx) WIP @nickprock 

## How-to guides
- [x] [fast_tokenizers.mdx](https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.mdx ""fast_tokenizers.mdx"") @andreafailla
- [ ] [create_a_model.mdx](https://github.com/huggingface/transformers/blob/b9a768b3ffa80c4c19d024f9f42d5917e7d8109e/docs/source/en/create_a_model.mdx ""create_a_model.mdx"") WIP @F02934
- [x] [custom_models.mdx](https://github.com/huggingface/transformers/blob/b9a768b3ffa80c4c19d024f9f42d5917e7d8109e/docs/source/en/custom_models.mdx ""custom_models.mdx"") @Xpiri
- [x] [run_scripts.mdx](https://github.com/huggingface/transformers/blob/b9a768b3ffa80c4c19d024f9f42d5917e7d8109e/docs/source/en/run_scripts.mdx ""run_scripts.mdx"") @lorenzobalzani
- [x] [sagemaker.mdx](https://github.com/huggingface/transformers/blob/b9a768b3ffa80c4c19d024f9f42d5917e7d8109e/docs/source/en/sagemaker.mdx ""sagemaker.mdx"") @andreafailla
- [ ] [converting_tensorflow_models.mdx](https://github.com/huggingface/transformers/blob/b9a768b3ffa80c4c19d024f9f42d5917e7d8109e/docs/source/en/converting_tensorflow_models.mdx ""converting_tensorflow_models.mdx"") WIP @Xpiri
- [ ] [serialization.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/serialization.mdx ""serialization.mdx"") WIP @F02934
- [ ] [performance.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/performance.mdx ""performance.mdx"") WIP @machicomio
- [ ] [perf_train_gpu_one](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.mdx)
- [ ] [perf_train_gpu_many](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_many.mdx)
- [ ] [perf_train_cpu](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_cpu.mdx)
- [ ] [perf_train_cpu_many](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_cpu_many.mdx)
- [ ] [perf_train_tpu](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu.mdx)
- [ ] [perf_train_special](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_special.mdx)
- [ ] [perf_infer_cpu](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_cpu.mdx)
- [ ] [perf_infer_gpu_one](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.mdx)
- [ ] [perf_infer_gpu_many](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_many.mdx)
- [ ] [perf_infer_special](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_special.mdx)
- [ ] [perf_hardware](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_hardware.mdx)
- [ ] [big_models](https://github.com/huggingface/transformers/blob/main/docs/source/en/big_models.mdx)
- [x] [parallelism.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/parallelism.mdx ""parallelism.mdx"") WIP @Xpiri
- [ ] [benchmarks.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/benchmarks.mdx ""benchmarks.mdx"") WIP @mfumanelli 
- [ ] [migration.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/migration.mdx ""migration.mdx"") WIP @Baelish03
- [ ] [troubleshooting.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/troubleshooting.mdx ""troubleshooting.mdx"") WIP @F02934
- [ ] [debugging.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/debugging.mdx ""debugging.mdx"") WIP @nickprock
- [ ] notebooks
- [ ] [community.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/community.mdx ""community.mdx"") WIP @lorenzobalzani
- [ ] [add_new_model.mdx](https://github.com/huggingface/transformers/blob/b9a768b3ffa80c4c19d024f9f42d5917e7d8109e/docs/source/en/add_new_model.mdx ""docs/source/en/add_new_model.mdx"") WIP @Steboss89
- [ ] [add_new_pipeline.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/add_new_pipeline.mdx ""add_new_pipeline.mdx"") 
- [ ] [testing.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/testing.mdx ""testing.mdx"") 
- [ ] [pr_checks.mdx](https://github.com/huggingface/transformers/blob/9c9db751e29432e8924624ef44856cd9fa671ef3/docs/source/en/pr_checks.mdx ""pr_checks.mdx"") ","[{'id': 1834067346, 'node_id': 'MDU6TGFiZWwxODM0MDY3MzQ2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Documentation', 'name': 'Documentation', 'color': '77cc3b', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",74,open
[RFC] Scan & Gradient checkpointing in Flax,"### Feature request

We should add scan and remat (gradient checkpointing) to the most important Flax/JAX models (BERT, GPT2, OPT, T5, BART, Wav2Vec2).

### Motivation

Scan allows for much faster compilation and memory savings and `remat` is the equivalent of `gradient_checkpointing` in PyTorch.

@sanchit-gandhi already uses both features in the Flax Seq2Seq Speech project - see: https://github.com/sanchit-gandhi/seq2seq-speech so it'd be quite trivial to get them working.

**Implementation details:**

Given that both `scan` and `remat` are not related to the model architecture, they should IMO **not** be in the model's config (We've done this mistake in PyTorch and don't want to repeat it here).

I would advocate for the following API:

```python
model = FlaxBertForMaskedLM.from_pretrained(""bert-base-cased"")
model.scan()  # or model.scan_enable()
model.unscan()  # or model.scan_disable()
```

and 

```python
model = FlaxBertForMaskedLM.from_pretrained(""bert-base-cased"")
model.gradient_checkpoint_enable()
model.gradient_checkpoint_disable()
```

As can be seen here: https://github.com/sanchit-gandhi/seq2seq-speech/blob/b28d0c25c8fad0f9ffa6707f91f7aba320d44a4b/models/modeling_flax_wav2vec2.py#L504

We'll need to re-initialize the `flax.linen.module` inside the model. However this should be fine since it just means that we do

```
self.module = self.module_class(config=config, dtype=dtype, use_scan=True, **kwargs)
 self. _is_scan_enabled = True
```

similar to this line: https://github.com/huggingface/transformers/blob/71e602725b90f63f404109bae9f72cbdf755477b/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L868

We can see along the PR how much logic can reside in `modeling_flax_utils.py` and how much would go into the specific models, *e.g.* `modeling_flax_wav2vec2.py`.

The same API / logic could be used for the `gradient_checkpointing`.

### Your contribution

Happy to give this implementation a shot with @sanchit-gandhi  and @patil-suraj .

Also would love to hear feedback from @borisdayma @marcvanzee about the API","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",5,open
Add Google's Trillson Audio Classification Model,"# What does this PR do?
Add Google's Trillson Audio Classification Model #17339 

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [x] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).
- [x] Did you write any new necessary tests?


## Who can review?
@patrickvonplaten

","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",49,open
[WIP] [deepspeed] from_pretrained deal with ignore_mismatched_sizes,"An attempt to fix the issue reported https://github.com/huggingface/transformers/issues/17336

Fixes: https://github.com/huggingface/transformers/issues/17336","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Google's Trillson Audio Classification,"### Model description

The TRILLsson models are described in the publication TRILLsson: Distilling Universal Paralingistic Speech Representations. From audio, they generate generally-useful paralinguistic speech representations (paralinguistics are aspects of speech other than text, such as emotion, language identification, synthetic or real, etc). These representations are smaller, faster, and publicly available versions of the state-of-the-art CAP12 embeddings, which are described in [Universal Paralinguistic Speech Representations Using Self-Supervised Conformers](https://arxiv.org/abs/2110.04621) (ICASSP 2022).

### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Google recently has done some very nice work on better audio / speech representations and distilled audio / speech representations. See:
- https://arxiv.org/abs/2110.04621
- https://arxiv.org/abs/2203.00236

Some of the distilled models are open-sourced and could be made more available via an integration to HuggingFace's Transformer library.

E.g. the following notebook shows how the weights can be loaded and run with publicly accessible model code:
https://colab.research.google.com/drive/1-D6pyxFyquIO8pss_lngL_mncHa3kAAT?usp=sharing

The relevent models to add are:
- https://tfhub.dev/google/nonsemantic-speech-benchmark/trillsson3/1 and
- https://tfhub.dev/google/nonsemantic-speech-benchmark/trillsson2/1

and the relevant code is publicly available: https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark

The google colab shows exacty how the model can be run and debugged in TF.

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",18,open
UNETR: Transformers for 3D Medical Image Segmentation,"### Model description

I would like to add a new model:

Proposed in the paper: [UNETR: Transformers for 3D Medical Image Segmentation](https://arxiv.org/abs/2103.10504)

UNEt TRansformers (UNETR) utilize a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful ""U-shaped"" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output.



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

Model Implementation: https://github.com/Project-MONAI/research-contributions/tree/master/UNETR

Pretrained Model: https://drive.google.com/file/d/1kR5QuRAuooYcTNLMnMj80Z9IgSs8jtLO/view?usp=sharing (Based on BTCV dataset)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",18,open
GPT-neo generate is ignoring passed position ids,"### System Info

```shell
python version: 3.9
transformers version: 4.18
```


### Who can help?

@patil-suraj @patrickvonplaten 

### Information

- [ ] The official example scripts
- [X] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [X] My own task or dataset (give details below)

### Reproduction

``` 

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch 

text = ""hi there: ""
modelname = ""EleutherAI/gpt-neo-125M""
model = AutoModelForCausalLM.from_pretrained(modelname)
tokenizer = AutoTokenizer.from_pretrained(modelname)

inputs = tokenizer(text, return_tensors =""pt"")
inputs[""position_ids""] = inputs[""attention_mask""].cumsum(-1)
print(inputs)
a = model.generate(**inputs)


inputs[""position_ids""] = inputs[""position_ids""] + 10
print(inputs)
b = model.generate(**inputs)

# a and b should be different because the position ids are different
print(a)
print(b)
```
the result
```
a = tensor([[5303,  612,   25,  220,  220,  220,  220,  220,  220,  220,  220,  220,
          220,  220,  220,  220,  220,  220,  220,  220]])
b = tensor([[5303,  612,   25,  220,  220,  220,  220,  220,  220,  220,  220,  220,
          220,  220,  220,  220,  220,  220,  220,  220]])
```

### Expected behavior

```shell
The outputs a and b should (almost always) be different because different position ids should be passed to the model's forward function, resulting in different activations. The issue seems to be here: https://github.com/huggingface/transformers/blob/ee393c009a243bbb86fa11d2efa771a1704d85cf/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L699

Specifically, the else statement is such that if both an attention mask and position ids are passed, the position ids are erased. In such a scenario, the default position ids in the model's forward function (https://github.com/huggingface/transformers/blob/ee393c009a243bbb86fa11d2efa771a1704d85cf/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L537) are used, rather than the passed-in position ids. 

Proposed fix: remove the `else` block on line 699 and unindent the `if past` block.
```
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",12,open
add FAN model (vision),"### Model description

Fully attentional networks (FAN) is a family of general-purpose Vision Transformer backbones that are highly robust to unseen natural corruptions in various visual recognition tasks.

### Open source status

- [X] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

* https://github.com/NVlabs/FAN
* https://arxiv.org/abs/2204.12451","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
ALBEF: Align Before Fuse,"### Model description

Align Before Fuse (ALBEF) is a vision-language (VL) model that showed competitive results in numerous VL tasks such as image-text retrieval, visual question answering, visual entailment, and visual grounding. 

The authors propose to use text encoder (BERT's first half layers) and image encoder (ViT) to create an aligned representation for respective modality before fusing them together with a multi-modal encoder (BERT's second half layers). The model is trained on multi-modal representation tasks and momentum distillation to achieve state-of-the-art results in VL tasks.

As multi-modal models are gaining more attention in academia/industry, I think ALBEF could be a nice addition to the transformers library.



### Open source status

- [X] The model implementation is available
- [X] The model weights are available

### Provide useful links for the implementation

- There are an official implementation and pre-trained/fine-tuned weights by the authors at this [repo](https://github.com/salesforce/ALBEF)
- Link to the [paper](https://arxiv.org/abs/2107.07651)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",7,open
[Kernel Fusion] Training benchmarks of Torchdynamo + AOTAutograd + NVFuser(many models),"Note to maintainers: We are using this PR to collaborate and there is no intention yet to merge anything, so please ignore unless you want to experiment with the latest auto-speedups.

## What was the issue with the previous AOTAutograd integration?
So, there was some investigation into applying AOTAutograd a couple months ago in this PR (https://github.com/huggingface/transformers/pull/15264). Although the performance results were quite promising, @stas00 and I found one major blocker - the potential for incorrect semantics. AOTAutograd is a tracing-based approach, and as such, it's fairly difficult for it to guarantee that its semantics are always correct. For example, data-dependent control flow, use of third-party libraries (like Numpy), or modification of global state all posed problems for integrating AOTAutograd into HuggingFace. Considering that HF has >100 models (and is adding more every day!), the burden of needing to ensure that AOTAutograd produces correct results would have been quite burdensome.

## TorchDynamo to the rescue
Luckily, now, there's another solution in the form of [Torchdynamo](https://dev-discuss.pytorch.org/t/torchdynamo-an-experiment-in-dynamic-python-bytecode-transformation/361) (from @jansel)! In contrast to tracing based approaches like `jit.trace` and AOTAutograd, Torchdynamo is *sound* - it should never produce incorrect results (modulo bugs). In comparison to approaches like `jit.script`, Torchdynamo is much more *complete* - it should allow any PyTorch code to be able to run, although it may not always speed things up.

The central approach that TorchDynamo takes is that as opposed to trying to live at the AST level (i.e. `jit.script`) or the object-level (i.e. tracing like `jit.trace`), it lives at the Python bytecode level. This is similar to the approach that language JITs like Javascript's V8 or JVM's Hotspot take. By living at this level, it's able to ensure that it can support *all* Python, as it can always fall back to eager-mode execution. Let's take an example of some code that would have been very problematic previously.

```
def f(x):
    a = x * 2
    b = a + torch.from_numpy(np.randn(5))
    if b.sum() > 0:
        return d.sin().sin()
    else:
        return d.cos().cos()
```
Not only does this have data-dependent control flow - it also has calls to external libraries that aren't PyTorch! (numpy in this case). TorchDynamo (morally) would rewrite this code into something like this:
```
def block1(x, np_tensor):
    a = x * 2
    b = a + np_tensor
    return b

def block2(b):
    return b.sin().sin()

def block3(b):
    return b.sin().sin()

def f_dynamo(x):
    b = block1(x, torch.from_numpy(np.randn(5))
    if b.sum() > 0:
        return block2(b)
    else:
        return block3(b)
```

Note that `block1`, `block2`, and `block3` are just simple straight line functions - exactly what AOTAutograd can handle! So, we can now apply AOTAutograd to each of those blocks.

In this way, TorchDynamo and AOTAutograd complement each other - TorchDynamo resolves the dynamic/non-traceable behavior that AOTAutograd can't handle, and AOTAutograd then provides static compilation that handles things like PyTorch's autograd.

So, what do you need to do to use AOTAutograd with Torchdynamo? Well, it's simple!
```
import torchdynamo
from torchdynamo.optimizations.training import aot_autograd_speedup_strategy
with torchdynamo.optimize(aot_autograd_speedup_strategy):
    # run your model here!
```

All of the above is simply to capture the graphs in the first place. However, after capturing the graphs, we need to actually speed them up. To do so, we pass them to [NVFuser](https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31952/), a PyTorch-native compiler for GPUs.

## Results
This script primarily comes from a great effort from @anijain2305. However, I want to note a couple of things.

1. In contrast with the pure AOTAutograd integration, where our benchmark only covered 3.5 models (and had some tricky to debug correctness issues), it was fairly trivial to extend this benchmarking to 14 models (with correctness testing for all of them!) In fact, the main bottleneck to adding more is just figuring out how to run more models (I pretty much exhausted all of the AutoConfig ones I could run easily).
2. For the most part, TorchDynamo + AOTAutograd improves both performance and memory usage. On some models, quite significantly (1.4x+ for MobileBert, FNet, and Albert), but it generally improves performance for nearly all models.
3. For many of these models, we *can't* produce a single graph to compile, often due to Numpy usage. Here, it's crucial that torchdynamo passes multiple graphs to AOTAutograd.
4. Currently, we feed the graphs produced by TorchDynamo and AOTAutograd into NVFuser. But, in the future, other backends should have no issues integrating into this as well (and in fact, we *have* some extra integrations, like TensorRT).

Run on A100:
```
$ python hf_dynamo_aot.py --run-dynamo-aot-efficient --nvfuser
```
Results:

| model                      | dtype          | name                 |   time (s) |   mem (GB) |   speedup |   mem comp ression |
|:---------------------------|:---------------|---------------------|-----------:|-----------:|----------:|------------------:|
| BertForMaskedLM            | float32  |eager                |      0.040 |      3.521 |     1.000 |             1.000 |
| BertForMaskedLM            | float32  |dynamo_aot_efficient |      0.037 |      3.516 |     1.094 |             1.001 |
| BertForMaskedLM            | float16  |eager                |      0.027 |      1.880 |     1.000 |             1.000 |
| BertForMaskedLM            | float16  |dynamo_aot_efficient |      0.023 |      1.885 |     1.155 |             0.997 |
| BertForMaskedLM            | bfloat16 |eager                |      0.027 |      1.874 |     1.000 |             1.000 |
| BertForMaskedLM            | bfloat16 |dynamo_aot_efficient |      0.023 |      1.867 |     1.154 |             1.003 |
| AlbertForMaskedLM          | float32  |eager                |      0.081 |      6.070 |     1.000 |             1.000 |
| AlbertForMaskedLM          | float32  |dynamo_aot_efficient |      0.056 |      3.943 |     1.442 |             1.539 |
| AlbertForMaskedLM          | float16  |eager                |      0.046 |      2.908 |     1.000 |             1.000 |
| AlbertForMaskedLM          | float16  |dynamo_aot_efficient |      0.035 |      1.971 |     1.338 |             1.475 |
| AlbertForMaskedLM          | bfloat16 |eager                |      0.048 |      2.866 |     1.000 |             1.000 |
| AlbertForMaskedLM          | bfloat16 |dynamo_aot_efficient |      0.035 |      1.972 |     1.374 |             1.453 |
| GPT2LMHeadModel            | float32  |eager                |      0.055 |      4.632 |     1.000 |             1.000 |
| GPT2LMHeadModel            | float32  |dynamo_aot_efficient |      0.043 |      3.791 |     1.280 |             1.222 |
| GPT2LMHeadModel            | float16  |eager                |      0.036 |      2.426 |     1.000 |             1.000 |
| GPT2LMHeadModel            | float16  |dynamo_aot_efficient |      0.029 |      2.018 |     1.213 |             1.203 |
| GPT2LMHeadModel            | bfloat16 |eager                |      0.036 |      2.425 |     1.000 |             1.000 |
| GPT2LMHeadModel            | bfloat16 |dynamo_aot_efficient |      0.030 |      1.998 |     1.208 |             1.214 |
| LongformerForMaskedLM      | float32  |eager                |      0.121 |      4.591 |     1.000 |             1.000 |
| LongformerForMaskedLM      | float32  |dynamo_aot_efficient |      0.120 |      4.585 |     1.006 |             1.001 |
| LongformerForMaskedLM      | float16  |eager                |      0.096 |      2.711 |     1.000 |             1.000 |
| LongformerForMaskedLM      | float16  |dynamo_aot_efficient |      0.096 |      2.705 |     1.005 |             1.002 |
| T5ForConditionalGeneration | float32  |eager                |      0.103 |      8.300 |     1.000 |             1.000 |
| T5ForConditionalGeneration | float32  |dynamo_aot_efficient |      0.098 |      7.831 |     1.050 |             1.060 |
| DistilBertForMaskedLM      | float32  |eager                |      0.045 |      3.492 |     1.000 |             1.000 |
| DistilBertForMaskedLM      | float32  |dynamo_aot_efficient |      0.043 |      3.497 |     1.038 |             0.999 |
| DistilBertForMaskedLM      | float16  |eager                |      0.026 |      1.870 |     1.000 |             1.000 |
| DistilBertForMaskedLM      | float16  |dynamo_aot_efficient |      0.027 |      1.871 |     0.963 |             0.999 |
| DistilBertForMaskedLM      | bfloat16 |eager                |      0.026 |      1.860 |     1.000 |             1.000 |
| DistilBertForMaskedLM      | bfloat16 |dynamo_aot_efficient |      0.027 |      1.861 |     0.986 |             1.000 |
| RobertaForMaskedLM         | float32  |eager                |      0.157 |     12.366 |     1.000 |             1.000 |
| RobertaForMaskedLM         | float32  |dynamo_aot_efficient |      0.135 |     12.341 |     1.164 |             1.002 |
| RobertaForMaskedLM         | float16  |eager                |      0.098 |      6.573 |     1.000 |             1.000 |
| RobertaForMaskedLM         | float16  |dynamo_aot_efficient |      0.088 |      6.567 |     1.114 |             1.001 |
| RobertaForMaskedLM         | bfloat16 |eager                |      0.101 |      6.579 |     1.000 |             1.000 |
| RobertaForMaskedLM         | bfloat16 |dynamo_aot_efficient |      0.088 |      6.559 |     1.140 |             1.003 |
| GPT2LMHeadModel            | float32  |eager                |      0.123 |      9.292 |     1.000 |             1.000 |
| GPT2LMHeadModel            | float32  |dynamo_aot_efficient |      0.098 |      7.108 |     1.256 |             1.307 |
| GPT2LMHeadModel            | float16  |eager                |      0.080 |      4.610 |     1.000 |             1.000 |
| GPT2LMHeadModel            | float16  |dynamo_aot_efficient |      0.067 |      3.767 |     1.182 |             1.224 |
| GPT2LMHeadModel            | bfloat16 |eager                |      0.081 |      4.779 |     1.000 |             1.000 |
| GPT2LMHeadModel            | bfloat16 |dynamo_aot_efficient |      0.068 |      3.763 |     1.191 |             1.270 |
| ElectraForMaskedLM         | float32  |eager                |      0.074 |      6.257 |     1.000 |             1.000 |
| ElectraForMaskedLM         | float32  |dynamo_aot_efficient |      0.064 |      6.258 |     1.151 |             1.000 |
| ElectraForMaskedLM         | float16  |eager                |      0.042 |      3.356 |     1.000 |             1.000 |
| ElectraForMaskedLM         | float16  |dynamo_aot_efficient |      0.039 |      3.347 |     1.092 |             1.003 |
| ElectraForMaskedLM         | bfloat16 |eager                |      0.044 |      3.367 |     1.000 |             1.000 |
| ElectraForMaskedLM         | bfloat16 |dynamo_aot_efficient |      0.039 |      3.341 |     1.124 |             1.008 |
| FNetForMaskedLM            | float32  |eager                |      0.055 |      4.974 |     1.000 |             1.000 |
| FNetForMaskedLM            | float32  |dynamo_aot_efficient |      0.038 |      2.802 |     1.429 |             1.775 |
| ConvBertForMaskedLM        | float32  |eager                |      0.090 |      5.809 |     1.000 |             1.000 |
| ConvBertForMaskedLM        | float32  |dynamo_aot_efficient |      0.085 |      5.795 |     1.058 |             1.002 |
| ConvBertForMaskedLM        | float16  |eager                |      0.064 |      3.021 |     1.000 |             1.000 |
| ConvBertForMaskedLM        | float16  |dynamo_aot_efficient |      0.062 |      3.009 |     1.024 |             1.004 |
| MobileBertForMaskedLM      | float32  |eager                |      0.104 |      2.474 |     1.000 |             1.000 |
| MobileBertForMaskedLM      | float32  |dynamo_aot_efficient |      0.069 |      2.576 |     1.499 |             0.961 |
| MobileBertForMaskedLM      | float16  |eager                |      0.101 |      1.329 |     1.000 |             1.000 |
| MobileBertForMaskedLM      | float16  |dynamo_aot_efficient |      0.067 |      1.423 |     1.499 |             0.934 |
| MobileBertForMaskedLM      | bfloat16 |eager                |      0.100 |      1.330 |     1.000 |             1.000 |
| MobileBertForMaskedLM      | bfloat16 |dynamo_aot_efficient |      0.067 |      1.423 |     1.504 |             0.935 |
| CamembertForMaskedLM       | float32  |eager                |      0.075 |      6.312 |     1.000 |             1.000 |
| CamembertForMaskedLM       | float32  |dynamo_aot_efficient |      0.065 |      6.317 |     1.151 |             0.999 |
| CamembertForMaskedLM       | float16  |eager                |      0.047 |      3.376 |     1.000 |             1.000 |
| CamembertForMaskedLM       | float16  |dynamo_aot_efficient |      0.044 |      3.366 |     1.084 |             1.003 |
| CamembertForMaskedLM       | bfloat16 |eager                |      0.049 |      3.390 |     1.000 |             1.000 |
| CamembertForMaskedLM       | bfloat16 |dynamo_aot_efficient |      0.044 |      3.370 |     1.113 |             1.006 |
| LayoutLMForMaskedLM        | float32  |eager                |      0.077 |      6.305 |     1.000 |             1.000 |
| LayoutLMForMaskedLM        | float32  |dynamo_aot_efficient |      0.067 |      6.305 |     1.149 |             1.000 |
| LayoutLMForMaskedLM        | float16  |eager                |      0.045 |      3.371 |     1.000 |             1.000 |
| LayoutLMForMaskedLM        | float16  |dynamo_aot_efficient |      0.042 |      3.373 |     1.089 |             0.999 |
| LayoutLMForMaskedLM        | bfloat16 |eager                |      0.047 |      3.389 |     1.000 |             1.000 |
| LayoutLMForMaskedLM        | bfloat16 |dynamo_aot_efficient |      0.042 |      3.371 |     1.118 |             1.005 |


### Limitations
There are a couple of limitations today (that we're working on addressing).

1. Like AOTAutograd, this pipeline currently requires static shape specialization. That is, when the input shapes change, we'll need to recompile.
2. The interaction with PyTorch's distributed features is somewhat untested.

### Reading resources:

AOTAutograd: https://docs.google.com/presentation/d/1rTt0BR2KChDQQTks2hHUtvHxtHQKwgQHVNrmbhj0byk/edit?usp=sharing
TorchDynamo: https://dev-discuss.pytorch.org/t/torchdynamo-an-experiment-in-dynamic-python-bytecode-transformation/361
Min-Cut rematerialization: https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467/7
NVFuser: https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31952/","[{'id': 2604155188, 'node_id': 'MDU6TGFiZWwyNjA0MTU1MTg4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Benchmarks', 'name': 'Benchmarks', 'color': '2DF372', 'default': False, 'description': 'Issues related to Memory regressions in tests and scripts'}, {'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",12,open
Set Transformer,"### Model description

This issue proposes addition of the Set Transformer, a set2seq transformer for learning to order sets of items. 

## Short description of the model and link to the paper

The transformer is one of a family of models that implements permutation invariance in order to learn ordering relations. This particular implementation uses stacked attention blocks to achieve the invariance. Set transformers are good for a multitude of problems - the toy problem is the TSP, where vertices are ordered optimally, though the framing can also be applied to any sequence generation tasks where the sequence items are known ahead of time. See [this review](https://jair.org/index.php/jair/article/view/12839) for a description of the family of problems.

This particular transformer is the Set Transformer, presented in [Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks](http://proceedings.mlr.press/v97/lee19d.html).

This isn't immediately designed for text or images or speech, but is a distinct transformer architecture that has been applied to text and image data.

## Link to the implementation if it is open-source

There's an official PyTorch implementation at [https://github.com/juho-lee/set_transformer](https://github.com/juho-lee/set_transformer)

We've already got this up & running as a baseline in an upcoming IJCAI paper

## Link to the model weights if they are available.

not immediately available, but we could work something out

### Open source status

- [x] The model implementation is available
- [ ] The model weights are available

### Provide useful links for the implementation

Me & a colleague can get this up onto HF, we have a running implementation and the reference implementation is both on github and licensed MIT. Reference implementation by @juho-lee (author) and @yoonholee.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Collection of Tokenizer issues,"### System Info

```shell
Transformers + Tokenizers
```


### Who can help?

This Issue is a summary of multiple problems that we are currently encountering with Tokenizers. To solve them we'll need a more profound discussion of:
- To what extend fast and slow tokenizers should be aligned
- Whether all slow tokenizers should be kept
- How to treat special tokens
- Whether all internal methods of tokenizer should be exposed

Relevant issues/PRs:
https://github.com/huggingface/transformers/issues/15420
https://github.com/huggingface/transformers/issues/16336
https://github.com/huggingface/transformers/issues/16334
https://github.com/huggingface/transformers/issues/16337
https://github.com/huggingface/transformers/issues/15138
https://github.com/huggingface/transformers/issues/16339
https://github.com/huggingface/transformers/pull/15775

To community:
At the moment we sadly don't find the time to dive deeper here, but we're trying hard to allocate time to discuss the strategy here soon.



### Information

- [ ] The official example scripts
- [ ] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [ ] My own task or dataset (give details below)

### Reproduction

See issues above

### Expected behavior

```shell
Don't know yet
```
","[{'id': 1260952223, 'node_id': 'MDU6TGFiZWwxMjYwOTUyMjIz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Discussion', 'name': 'Discussion', 'color': '22870e', 'default': False, 'description': 'Discussion on a topic (keep it focused or open a new issue though)'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",7,open
Undocumented distributed inference behaviour for `run_summarization.py`,"### System Info

```shell
Fails with error


Traceback (most recent call last):
  File ""/scratches/neuron/anaconda3/envs/T5DST-SGD/bin/transformers-cli"", line 5, in <module>
    from transformers.commands.transformers_cli import main
  File ""/scratches/neuron/anaconda3/envs/T5DST-SGD/lib/python3.8/site-packages/transformers/commands/transformers_cli.py"", line 26, in <module>
    from .user import UserCommands
  File ""/scratches/neuron/anaconda3/envs/T5DST-SGD/lib/python3.8/site-packages/transformers/commands/user.py"", line 20, in <module>
    from huggingface_hub.hf_api import HfFolder, create_repo, list_repos_objs, login, logout, whoami
ImportError: cannot import name 'list_repos_objs' from 'huggingface_hub.hf_api' (/scratches/neuron/anaconda3/envs/T5DST-SGD/lib/python3.8/site-packages/huggingface_hub/hf_api.py)


However I am running `4.16.2` with python `3.8`.
```


### Who can help?

@sgugger @stevhliu @patil-suraj 

### Information

- [ ] The official example scripts
- [x] My own modified scripts

### Tasks

- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
- [x] My own task or dataset (give details below)

### Reproduction

I am working with a copy of the `run_summarization.py` (pytorch) example that the authors of this [paper](https://arxiv.org/pdf/2109.07506.pdf) modified to work for dialogue state tracking (implemented [here ](https://github.com/chiahsuan156/DST-as-Prompting) for reference)

The `run_summarization.py` script can be launched with `torch.distributed.launch` and the `--do_predict` option. This shards the examples in the test set to various GPUs and therefore generation and task-oriented metrics is accelerated. The predictions are written to the `generated_predictions.txt` file in the output directory. 

To be able to compute dialogue-relevant task oriented metrics, one ought to run a postprocessing script that uses the `generated_predictions.txt`. Because the trainer erases all the columns that are not keys to the model `forward` method from the dataset, the metadata that informs us of what training examples the predictions are related to is lost. Therefore, we rely on the ordering of the `generated_predictions.txt` to match the order of the examples in the dataset.

My question is: 
 - Does `predictions` (`L675`)  obey the order of the `dataset`? So if my dataset has 1m examples, will the 1m entries in the `predictions` list match the order of the dataset iterator? In my experience this depends on implementation* and the behaviour is not documented.
 
*For example, in frameworks such as `ray` you have to explicitly enforce the order in which the results are returned and the predictions may be returned out of order - if a process finishes, it returns its results so it can be given more work by an external load balancer.

### Expected behavior

```shell
Improved documentation about expected behaviour here. Happy to discuss where this should be added and contribute a small PR to clarify this important issue.
```
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3817266200, 'node_id': 'MDU6TGFiZWwzODE3MjY2MjAw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': None}]",5,open
pointer to transformer (big) model,"# 🌟 New model addition

## Model description

<!-- Important information -->
Hi, needed a pointer on how to instantiate a Transformer-big from the original Vaswani et. al. paper (Attention Is All You Need). I could only find versions of Transformer-like architectures, so would be useful if this could also be added.

## Open source status

* [x] the model implementation is available: (give details): https://research.google/pubs/pub46201/
* [ ] the model weights are available: (give details)
* [ ] who are the authors: (mention them, if possible by @gh-username)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Run the scheduled tests,":warning: Do not merge this PR!

PR 2/2: in order to finish running suite, rebase this PR on [`test-tokenizers-main`](https://github.com/huggingface/transformers/tree/test-tokenizers-main), branch of PR https://github.com/huggingface/transformers/pull/16708. 

---

This PR builds on top of https://github.com/huggingface/transformers/pull/16708.

It leverages the docker images created in the PR above, and updates the channel in which to report the tests to be a dummy one.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
[TODO] Investigate equivalence tests,"**(add a lot of assignees just to make you informed and kept updated in the future. Don't hesitate to remove yourself if you think it's irrelevant)**

Currently the PT/TF/Flax equivalence tests use `1e-5` as the tolerance for the absolute differences of outputs.

We see that these tests failed with a non-negligible (although not carefully defined) frequency.

Create this page to track a list of models to investigate.

- **FlaxWav2Vec2ModelTest** (2.2888184e-05 > 1e-5)
  - https://app.circleci.com/pipelines/github/huggingface/transformers/37363/workflows/a4b06424-0ba8-4fbc-9054-6ff52fbf8145/jobs/411654 

- **TFGPT2EncoderDecoderModelTest** (0.001009281724691391 > 1e-3)
  - https://app.circleci.com/pipelines/github/huggingface/transformers/37358/workflows/43c12161-33d8-4df5-ba3c-3e62a4507ee7/jobs/411579
    - This also happens to **TFBERTEncoderDecoderModelTest**
    -  This is caused by some sequence in a batch which gets all 0s as attention mask (generated by ids_tensor) - may happens on both encoder and decoder (especially after combining with the causal mask).
    - For **TFBERTEncoderDecoderModelTest**, the difference is smaller than *TFGPT2EncoderDecoderModelTest* (by a magnitude of 5x~10x) -> this is due to the last hidden states in GPT2 is after layer norm (not the case for BERT).
    - If we look the cross attention diff between PT/TF, it is clear that we have the same issue (both in the magnitude of `1e-3`)
    - The encoder attention diff between PT/TF is in the magnitude of `5e-8`: ~~**not very sure why this doesn't get much larger**~~.
      - This is because PT/TF (at least in BERT) has different `encoder_extended_attention_mask`: `1e-4` vs `1e-9`.

- **TFViTMAEModelTest** (1.013279e-05 > 1e-5)
  - https://app.circleci.com/pipelines/github/huggingface/transformers/37319/workflows/5adfba7a-d12b-4e1e-9a7a-e33c7d5fd6ee/jobs/411002","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",5,open
TEAMS: Training ELECTRA Augmented with Multi-word Selection,"Hi,

this [ACL paper](https://arxiv.org/abs/2106.00139) from last year (2021) proposed an ELECTRA extension:

> Pre-trained text encoders such as BERT and its variants have recently achieved state-of-the-art performances on many NLP tasks. While being effective, these pre-training methods typically demand massive computation resources. To accelerate pre-training, ELECTRA trains a discriminator that predicts whether each input token is replaced by a generator. However, this new task, as a binary classification, is less semantically informative. In this study, we present a new text encoder pre-training method that improves ELECTRA based on multi-task learning. Specifically, we train the discriminator to simultaneously detect replaced tokens and select original tokens from candidate sets. We further develop two techniques to effectively combine all pre-training tasks: (1) using attention-based networks for task-specific heads, and (2) sharing bottom layers of the generator and the discriminator. Extensive experiments on GLUE and SQuAD datasets demonstrate both the effectiveness and the efficiency of our proposed method.

Implementation is available in the TensorFlow models repository: https://github.com/tensorflow/models/tree/master/official/projects/teams

I would like to work on that, to see if it can easily be added (e.g. only writing a model conversion script).

Unfortunately, no model weights do exist at the moment. So I would like to pre-train a model and check if conversion can be done without changing the current ELECTRA implementation too much.

This issue tracks the integration into Transformers :hugs: 

","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Edgeformer,"# 🌟 New model addition

## Model description

[EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation](https://arxiv.org/abs/2202.07959)

EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation. Tao Ge and Furu Wei

March 2022: release code and pretrained checkpoints.

## Open source status

* [x] the model implementation is available: https://github.com/microsoft/unilm/blob/900f5416c8137a753b1c8f53cd5015d0ceca7061/edgelm/fairseq/models/transformer/transformer_legacy.py#L226
* [x] the model weights are available: https://github.com/microsoft/unilm/tree/900f5416c8137a753b1c8f53cd5015d0ceca7061/edgelm#pretrained-models
* [x] who are the authors: Maybe @gitnlp 

Happy to help with a model contribution here!","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",17,open
R3M: A Universal Visual Representation for Robot Manipulation,"# 🌟 New model addition

## Model description
We pre-train a visual representation using the Ego4D human video dataset using a combination of time-contrastive learning, video-language alignment,and an L1 penalty to encourage sparse and compact representations. The resulting representation, R3M, can be used as a frozen perception module for downstream policy learning. Across a suite of 12 simulated robot manipulation tasks, we find that R3M improves task success by over 20% compared to training from scratch and by over 10% compared to state-of-the-art visual representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika Panda arm to learn a range of manipulation tasks in a real, cluttered apartment given just 20 demonstrations.
<!-- Important information -->

## Open source status

* [x] the model implementation is available:(https://github.com/facebookresearch/r3m)
* [x] the model weights are available: https://github.com/facebookresearch/r3m/blob/main/r3m/example.py
* [x] who are the authors: @suraj-nair-1 
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
 [WIP] New Model Add FastPitch 1.1,"# 🌟 New model addition

## Model description
<!-- Important information -->

**What type of model is Fast Pitch 1.1?** 
It is a Mel spectrogram generator (part of a speech to text model engine) that mainly comprises of two Feed Forward Transformer stacks. 
Fast Pitch is used to transform text to a spectrogram **to be used in wave form generation in speech synthesis**.
Fast Pitch 1.1 include's multispeaker embeddings. 
**What is the novel feature of the model making it different from other spectrogram generators?**
A fully-parallel text-to-speech model
based on Fast Speech, conditioned on fundamental frequency
contours. The model predicts pitch contours during inference.
By altering these predictions, the generated speech can be
more expressive, better match the semantic of the utterance,
and in the end more engaging to the listener. 
Uniformly increasing or decreasing pitch with Fast Pitch generates speech
that resembles the voluntary modulation of voice.

Fast Pitch is meant to be used with a *neural vocoder* like Wave Net, or Wave Glow.

Text (Feature Extraction) → **Audio Synthesis  (**spectrogram)**→ Waveform Synthesis (wavform)**

From the [paper](https://arxiv.org/pdf/2006.06873.pdf) 
abstract:

> We present FastPitch, a fully-parallel text-to-speech model
> based on FastSpeech, conditioned on fundamental frequency
> contours. The model predicts pitch contours during inference.
> By altering these predictions, the generated speech can be
> more expressive, better match the semantic of the utterance,
> and in the end more engaging to the listener. Uniformly increasing or decreasing pitch with FastPitch generates speech
> that resembles the voluntary modulation of voice. Conditioning on frequency contours improves the overall quality of
> synthesized speech, making it comparable to state-of-the-art.
> It does not introduce an overhead, and FastPitch retains the
> favorable, fully-parallel Transformer architecture, with over
> 900× real-time factor for mel-spectrogram synthesis of a typical utterance.

## Open source status
**Samples**
https://fastpitch.github.io/
**My Own Generated Samples**
https://voca.ro/1eYmqidRhGi6
**Pro's of the model:** 
It plays a part in a high MOS score. Compariable to Tacotron2, without the high cost of inference. 

High performance and High Quality will be useful to provide voice or soul to digital assistants or metaverse assistants.

Training isn't sophisticated, unlike FastPitch 1.0 this model does not required durations or alignments to be generated from Tacotron2 or Montreal Forced Aligner.
 
**Con's of the model:** 
It isn't in the Hugging Face repository to be easily adapted to products use cases. =)

* [x] the model implementation is available: 
It is availible here.
https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/FastPitch
* [x] the model weights are available: 
Using Automatic Mixed Percision FP-1.1
https://catalog.ngc.nvidia.com/orgs/nvidia/models/fastpitch_pyt_amp_ckpt_v1_1/files?version=21.12.0
* [x] who are the authors: @alancucki
*I am sorry if I missed anyone.*

cc @anton-l @patrickvonplaten 
will assign to whom will be availible once the draft is complete.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
[Community Event] Doc Tests Sprint,"### This issue is part of our **Doc Test Sprint**. If you're interested in helping out come [join us on Discord](https://discord.gg/J8bW9u5abB) and talk with other contributors!

Docstring examples are often the first point of contact when trying out a new library! So far we haven't done a very good job at ensuring that all docstring examples work correctly in 🤗 Transformers - but we're now very dedicated to ensure that all documentation examples work correctly by testing each documentation example via Python's doctest (https://docs.python.org/3/library/doctest.html) on a daily basis.

In short we should do the following for all models for both PyTorch and Tensorflow:
1. - Check the current doc examples will run without failure
2. - Check whether the current doc example of the forward method is a sensible example to better understand the model or whether it can be improved. E.g. is the example of https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/bert#transformers.BertForQuestionAnswering.forward a good example of the model? Could it be improved?
3. -  Add an expected output to the doc example and test it via Python's doc test (see **Guide to contributing** below)

Adding a documentation test for a model is a great way to better understand how the model works, a simple (possibly first) contribution to Transformers and most importantly a very important contribution to the Transformers community 🔥 

If you're interested in adding a documentation test, please read through the **Guide to contributing** below.

This issue is a call for contributors, to make sure docstring exmaples of existing model architectures work correctly. If you wish to contribute, reply in this thread which architectures you'd like to take :)

### Guide to contributing:
1. Ensure you've read our contributing [guidelines](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md) 📜 
2. Claim your architecture(s) in this thread (confirm no one is working on it) 🎯 
3. Implement the changes as in https://github.com/huggingface/transformers/pull/15987 (see the diff on the model architectures for a few examples) 💪 
    - The file you want to look at is in `src/transformers/models/[model_name]/modeling_[model_name].py`, `src/transformers/models/[model_name]/modeling_tf_[model_name].py` or `src/transformers/doc_utils.py` or `src/transformes/file_utils.py`
    - Make sure to run the doc example doc test locally as described in https://github.com/huggingface/transformers/tree/master/docs#for-python-files
    - Optionally, change the example docstring to a more sensible example that gives a better suited result
    - Make the test pass
    - Add the file name to https://github.com/huggingface/transformers/blob/master/utils/documentation_tests.txt (making sure the file stays in alphabetical order)
    - Run the doc example test again locally

    In addition, there are a few things we can also improve, for example : 
    - Fix some style issues: for example, change **``decoder_input_ids```** to **\`decoder_input_ids\`**.
    - Using a small model checkpoint instead of a large one: for example, change **""facebook/bart-large""** to **""facebook/bart-base""** (and adjust the expected outputs if any)
4. Open the PR and tag me @patrickvonplaten @ydshieh or @patil-suraj (don't forget to run `make fixup` before your final commit) 🎊 
    - Note that some code is copied across our codebase. If you see a line like `# Copied from transformers.models.bert...`, this means that the code is copied from that source, and our scripts will automatically keep that in sync. If you see that, you should not edit the copied method! Instead, edit the original method it's copied from, and run make fixup to synchronize that across all the copies. Be sure you installed the development dependencies with `pip install -e "".[dev]""`, as described in the contributor guidelines above, to ensure that the code quality tools in `make fixup` can run.

### PyTorch Model Examples added to tests:
- [ ] **ALBERT** (@vumichien)
- [x] **BART** (@abdouaziz)
- [x] BEiT
- [ ] **BERT** (@vumichien)
- [ ] Bert
- [ ] BigBird (@vumichien)
- [x] BigBirdPegasus
- [x] Blenderbot
- [x] BlenderbotSmall
- [ ] CamemBERT  (@abdouaziz)
- [ ] Canine (@NielsRogge)
- [ ] **CLIP** (@Aanisha)
- [ ] ConvBERT (@simonzli)
- [x] ConvNext
- [ ] CTRL (@jeremyadamsfisher)
- [x] Data2VecAudio
- [ ] Data2VecText
- [ ] DeBERTa (@Tegzes)
- [ ] **DeBERTa-v2** (@Tegzes)
- [x] DeiT
- [ ] DETR
- [ ] **DistilBERT** (@jmwoloso)
- [ ] DPR
- [ ] **ELECTRA** (@bhadreshpsavani)
- [ ] Encoder
- [ ] FairSeq
- [ ] FlauBERT (@abdouaziz)
- [ ] FNet
- [ ] Funnel
- [ ] **GPT2** (@ArEnSc)
- [ ] GPT-J (@ArEnSc)
- [x] Hubert
- [ ] I-BERT (@abdouaziz)
- [ ] ImageGPT
- [ ] LayoutLM (chiefchiefling @ discord)
- [ ] LayoutLMv2
- [ ] LED
- [x] **Longformer** (@KMFODA)
- [ ] LUKE (@Tegzes)
- [ ] LXMERT
- [ ] M2M100
- [x] **Marian**
- [x] MaskFormer (@reichenbch)
- [x] **mBART**
- [ ] MegatronBert
- [ ] MobileBERT (@vumichien)
- [ ] MPNet
- [ ] mT5
- [ ] Nystromformer
- [ ] OpenAI
- [ ] OpenAI
- [x] Pegasus
- [ ] Perceiver
- [x] PLBart
- [x] PoolFormer
- [ ] ProphetNet
- [ ] QDQBert
- [ ] RAG
- [ ] Realm
- [ ] **Reformer**
- [x] ResNet
- [ ] RemBERT
- [ ] RetriBERT
- [ ] **RoBERTa** (@patrickvonplaten )
- [ ] RoFormer
- [x] SegFormer
- [x] SEW
- [x] SEW-D
- [x] SpeechEncoderDecoder
- [x] Speech2Text
- [x] Speech2Text2
- [ ] Splinter
- [ ] SqueezeBERT
- [x] Swin
- [ ] **T5** (@MarkusSagen)
- [ ] TAPAS (@NielsRogge)
- [ ] Transformer-XL (@simonzli)
- [ ] TrOCR (@arnaudstiegler)
- [x] UniSpeech
- [x] UniSpeechSat
- [x] Van
- [x] ViLT
- [x] VisionEncoderDecoder
- [ ] VisionTextDualEncoder
- [ ] VisualBert
- [x] **ViT**
- [x] ViTMAE
- [x] **Wav2Vec2**
- [x] WavLM
- [ ] XGLM
- [ ] **XLM**
- [ ] **XLM-RoBERTa** (@AbinayaM02)
- [ ] XLM-RoBERTa-XL
- [ ] XLMProphetNet
- [ ] **XLNet**
- [ ] YOSO


### Tensorflow Model Examples added to tests:
- [ ] **ALBERT** (@vumichien)
- [ ] **BART**
- [ ] BEiT
- [ ] **BERT** (@vumichien)
- [ ] Bert
- [ ] BigBird (@vumichien)
- [ ] BigBirdPegasus
- [ ] Blenderbot
- [ ] BlenderbotSmall
- [ ] CamemBERT
- [ ] Canine
- [ ] **CLIP** (@Aanisha)
- [ ] ConvBERT (@simonzli)
- [ ] ConvNext
- [ ] CTRL
- [ ] Data2VecAudio
- [ ] Data2VecText
- [ ] DeBERTa
- [ ] **DeBERTa-v2**
- [ ] DeiT
- [ ] DETR
- [ ] **DistilBERT** (@jmwoloso)
- [ ] DPR
- [ ] **ELECTRA** (@bhadreshpsavani)
- [ ] Encoder
- [ ] FairSeq
- [ ] FlauBERT
- [ ] FNet
- [ ] Funnel
- [ ] **GPT2** (@cakiki)
- [ ] GPT-J (@cakiki)
- [ ] Hubert
- [ ] I-BERT
- [ ] ImageGPT
- [ ] LayoutLM
- [ ] LayoutLMv2
- [ ] LED
- [x] **Longformer** (@KMFODA)
- [ ] LUKE
- [ ] LXMERT
- [ ] M2M100
- [ ] **Marian**
- [x] MaskFormer (@reichenbch)
- [ ] **mBART**
- [ ] MegatronBert
- [ ] MobileBERT (@vumichien)
- [ ] MPNet
- [ ] mT5
- [ ] Nystromformer
- [ ] OpenAI
- [ ] OpenAI
- [ ] Pegasus
- [ ] Perceiver
- [ ] PLBart
- [ ] PoolFormer
- [ ] ProphetNet
- [ ] QDQBert
- [ ] RAG
- [ ] Realm
- [ ] **Reformer**
- [ ] ResNet
- [ ] RemBERT
- [ ] RetriBERT
- [ ] **RoBERTa** (@patrickvonplaten)
- [ ] RoFormer
- [ ] SegFormer
- [ ] SEW
- [ ] SEW-D
- [ ] SpeechEncoderDecoder
- [ ] Speech2Text
- [ ] Speech2Text2
- [ ] Splinter
- [ ] SqueezeBERT
- [ ] Swin (@johko)
- [ ] **T5** (@MarkusSagen)
- [ ] TAPAS
- [ ] Transformer-XL (@simonzli)
- [ ] TrOCR (@arnaudstiegler)
- [ ] UniSpeech
- [ ] UniSpeechSat
- [ ] Van
- [ ] ViLT
- [ ] VisionEncoderDecoder
- [ ] VisionTextDualEncoder
- [ ] VisualBert
- [ ] **ViT** (@johko)
- [ ] ViTMAE
- [ ] **Wav2Vec2**
- [ ] WavLM
- [ ] XGLM
- [ ] **XLM**
- [ ] **XLM-RoBERTa** (@AbinayaM02)
- [ ] XLM-RoBERTa-XL
- [ ] XLMProphetNet
- [ ] **XLNet**
- [ ] YOSO","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",104,open
Gpt2 large for onnx exportation and int8 quantization,"Hi, for model big as 7GB, does transformers support export to onnx?? Any tutorial about big model?","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Initialized DETR backbone weights do not match with actual pretrained weights ,"Hello everybody! 

My task is to initialize DETR Object Detection model with my own pretrained backbone (for example, ResNet-50). So, in Detr class (I took the code from [this Hugging Face tutorial](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) as a basis), I create model from DetrConfig:
```
class Detr(pl.LightningModule):

    def __init__(self, lr, lr_backbone, weight_decay, backbone_name):
        super().__init__()

        self.model = DetrForObjectDetection(DetrConfig(num_labels=len(id2label)
                                                      ,ignore_mismatched_sizes=True
                                                      ,backbone=backbone_name
                                                       ))
    ...
```
Then I create model:
```
model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-3, backbone_name='resnet50')
```
And I want to see, for example, first layer weights in backbone:
```
model.model.model.backbone.conv_encoder.model.conv1.weight
```
Output:
```
Parameter containing:
tensor([[[[-1.0862e-02,  5.6558e-02, -2.5807e-02,  ..., -1.8801e-03,
            2.1483e-02, -1.1144e-02],
...
```
And I notice that this weights are different from timm backbone model, which is [fed to the input](https://github.com/huggingface/transformers/blob/68dec6bffd68e62d151758d8ac7378bf9b40ac20/src/transformers/models/detr/modeling_detr.py#:~:text=backbone%20%3D%20create_model(name%2C%20pretrained%3DTrue%2C%20features_only%3DTrue%2C%20out_indices%3D(1%2C%202%2C%203%2C%204)%2C%20**kwargs)) of the Detr model:
```
backbone_model = timm.create_model('resnet50', pretrained=True, features_only=True, out_indices=(1, 2, 3, 4))
backbone_model.conv1.weight
```
Output:
```
Parameter containing:
tensor([[[[-2.6017e-02, -1.7312e-02, -1.0648e-02,  ...,  1.1380e-02,
           -1.0471e-02, -1.1242e-02],
...
```

So... My question is, how can this happen? Why are the weights of the model that is fed into the Detr input differ from the weights of the same model in the already initialized Detr? Is this a bug or am I missing something?

One more detail: If I create model again, weights will also change! But when I restart Kernel and create Detr model, weights will be the same as in my first model creation at previous Kernel! So, the weights are initialized differently, but with a certain cycle? Why is this happening? In my understanding, backbone weights should always be the same and equal to the original ResNet weights.

Maybe, I'm setting the configuration wrong, but it could be a bug. I would really appreciate an answer and possible clarification!","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",7,open
Implement Maximal Update Parametrization (muP),"# 🚀 Feature request

<!-- A clear and concise description of the feature proposal.
     Please provide a link to the paper and code in case they exist. -->

This request is to open up a discussion on 1) whether it makes sense to implement [Maximal Update Parametrization (abbreviated muP)](http://arxiv.org/abs/2203.03466) in Huggingface, 2) if so, how to do it.



## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request
     related to a problem? e.g., I'm always frustrated when [...]. If this is related
     to another GitHub issue, please link here too. -->

Hi,

I'm a maintainer for the [mup package](https://github.com/microsoft/mup) ([paper](http://arxiv.org/abs/2203.03466)). This repo allows one to implement in their models a special parametrization called maximal update parametrization, or muP, that has the special property that narrow and wide networks share the same optimal hyperparameters (like learning rate, initialization, etc). This is demonstrated below on a Transformer trained with adam, where on the left we have the pytorch default parametrization and the right we have muP.
![image](https://user-images.githubusercontent.com/53244851/158121650-ceb833c4-3cd9-4861-a75e-18ec8346757c.png)
Most strikingly, this property can be used to tune hyperparameters for extremely large neural networks like GPT-3 that is too expensive to train more than once, by just tuning a tiny version of it. But even for ""regular joe"" users, muP can alleviate a lot of the pain when transitioning from exploration to scaling up and finding performance suffer for mysterious reasons. Transformers in particular is somewhat infamous for problems like training instability. So having muP integrated natively into Huggingface can benefit a lot of users at once.

muP can be implemented in a backward compatible way, as shown below, so users do not need to worry about it breaking existing codebases.

See this [twitter thread](https://twitter.com/TheGregYang/status/1501294412126560257?s=20&t=0B9TXBOHEaAPZWXA6xS8rQ) for more (but brief) information about how this works, and this [blog post](https://www.microsoft.com/en-us/research/blog/%C2%B5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/) for less brief overview.

## Your contribution

<!-- Is there any way that you could help, e.g. by submitting a PR?
     Make sure to read the CONTRIBUTING.MD readme:
     https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md -->

Now let's return to the two questions at the beginning: 1) whether it makes sense to implement Maximal Update Parametrization (abbreviated muP) in Huggingface, 2) if so, how to do it.

For 1), the popularity (or not) of this issue should serve as an indicator of community interest, and the above makes the case for the utility of this integration. 

For 2), we have examples of how to integrate muP with some common (PyTorch) Huggingface transformers in our [mutransformers repo](https://github.com/microsoft/mutransformers). 


### Current Example Implementation

In summary, to modify an existing Huggingface transformer to implement muP, one needs to

1. Switch any readout layer (dimensions: width -> number of labels) from `nn.Linear` to `mup.MuReadout`.
2. Modify the `_init_weights` method to use `mup.init.*` methods instead of `nn.init.*` methods (or equivalent).
3. Scale the attention logits like 1/d instead of 1/sqrt(d)
4. Use `mup.AdamW` instead of the pytorch or Huggingface version.

In addition, when using a `mutransformer`, one needs to provide a ""base shape file"" that lets the model know how to properly scale the learning rate and attention with width. This is designed so that if the model parameter shapes are the same as the ""base shapes"", then the model is in the original parametrization, i.e. backward compatible.
```python
from mutransformers import BertConfig, BertForMaskedLM
# instantiate model
model = BertForMaskedLM(config=BertConfig(...))
# set base shapes
set_base_shapes(model, path_to_base_shape_file)
# re-initialize
model.apply(model._init_weights)
```

### More Seamless Integration

Now, the [mutransformers repo](https://github.com/microsoft/mutransformers) is primarily designed to serve as examples of how to implement [muP](https://github.com/microsoft/mup) into existing transformers. So all of the above can be streamlined if we really want seamless integration into Huggingface.

For example, the user interface for instantiating a model could just be the same as it is now, but we just have an additional flag `mup=True` in `BertConfig` that says to switch on `mup`. `BertConfig` itself may carry a default set of base shapes for use in this scenario, which the user can also modify if necessary.
```python
# the model automatically sets base shapes based on defaults in BertConfig
# no need to re-initialize either
model = BertForMaskedLM(config=BertConfig(mup=True,...))
# use model immediately, e.g., train
```
In addition, `mup.MuAdamW` can be incorporated natively into Huggingface as well, so that there is no dependency on the `mup` package at all.

### muP for All Transformers?

As, currently, there is no automatic way of backfitting existing transformers, it could be quite a task to add muP to all of the transformers in Huggingface. So a good practical compromise is to just implement muP for the most commonly used models in Huggingface.

In the interim, research can be done on a method of such automatic backfitting. This could even involve a pull request into PyTorch core.

### Conclusion

Again, this issue is intended to start the discussion of whether and how to make muP available to Huggingface users natively. It could be that the best course forward is to have users implement muP transformers themselves as in `mutransformers`, or even to build `mutransformers` into such a repo of muP transformers. And even if we do decide to integrate muP into Huggingface, there could be many ways to do it.

I hope discussion here could elucidate the right course of action.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",16,open
Improve EncoderDecoderModel docs,"## **First good issue**

There have been quite some issues/questions with how to use the Encoder-Decoder model, e.g.: https://github.com/huggingface/transformers/issues/4483 and https://github.com/huggingface/transformers/issues/15479 . The main reason for this is that the model docs are quite outdated and we could need a nice How-to-guide. 

So I think we have two action items here:

1. Improve https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/encoder-decoder#encoder-decoder-models a.k.a.: https://github.com/huggingface/transformers/blob/master/docs/source/model_doc/encoder-decoder.mdx

We should mention here: 
a) How to create a model ? We should show how to use the `from_encoder_decoder_pretrained(...)` and then how to save the model?
b) How to fine-tune this model? We should mention that this model can then be fine-tuned just like any other encoder-decoder model (Bart, T5, ...)
c) Put a big warning that the config values have to be correctly set and how to set them, e.g. read: https://github.com/huggingface/transformers/issues/15479

This should be an `EncoderDecoderModel` specific text and be very concise and short.

In a second step, we should then write a How-to-guide that includes much more details.

More than happy to help someone tackle this first good issue","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",19,open
issues in `generate()`,"## A collection of issues for `generate()` method that we need to address

(**I don't assign these issues to anyone yet. I might work on some of these later. The purpose here is to make these issues transparent to the community, and to serve as a TODO list so we won't forget.**)

As an extension to the model equivalence tests across framework, I prepared a list of potential issues for `generate()` which gives different results across frameworks.

- Currently only look at the type and shape, not the values inside the tensors
- We might need to wait the current **`generate()` refactorization PR(s) merged** before starting address these issues.
- Here is the [Colab notebook](
https://colab.research.google.com/drive/1294oyVDrwvnuw_P3QtFE_CZ7DoMdthyq?usp=sharing) which demonstrates the issues.

Summary of issues (also included as comments in the above notebook)

- Flax's `generate()` doesn't support `return_dict_in_generate` (gives errors), `output_scores`, `output_attentions`, while PT/TF accept them
- PT/TF/Flax `BeamSearchEncoderDecoderOutput` outputs have different keys
- `scores` have different types: PT -> `tuple` , TF/Flax -> `tensor` 
- `scores` have different shape: PT -> (15, 4, 50257), TF -> (1, 4, 50257), Flax -> (1,)
- `sequences_scores` have different types: PT -> tensor , TF -> None , Flax -> no such attribute 
- `PT.sequences_scores` seems to be `Flax.scores`
- TF's `sequences` has a shorter length by 1 than PT's `sequences` ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
[WIP] Add Fusion-in-Decoder,"# What does this PR do?

This PR adds the Fusion-in-Decoder model to the repository.

Paper: https://arxiv.org/abs/2007.01282
Code: https://github.com/facebookresearch/FiD

## Who can review?
Anyone in the community is free to review the PR once the tests have passed.
@patil-suraj, @patrickvonplaten, @qqaatw","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Add OFA to transformers,"# 🌟 New model addition
We recently proposed OFA, a unified model for multimodal pretraining, which achieves multiple SoTAs on downstream tasks, including image captioning, text-to-image generation, referring expression comprehension, etc. We would like to implement OFA on transformers if it is possible.

## Model description
OFA is a unified multimodal pretrained model that unifies modalities (i.e., cross-modality, vision, language) and tasks (e.g., image generation, visual grounding, image captioning, image classification, text generation, etc.) to a simple sequence-to-sequence learning framework. For more information, please refer to our paper: [Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework](http://arxiv.org/abs/2202.03052).

<!-- Important information -->
Our code base is Fairseq. We wonder if there is any simpler solution for the transfer from Fairseq to transformers. To email us, please contact zheluo.wp@alibaba-inc.com or junyang.ljy@alibaba-inc.com.

## Open source status

* [x] the model implementation is available: ([OFA official repo](https://github.com/OFA-Sys/OFA))
* [x] the model weights are available: ([OFA checkpoints](https://github.com/OFA-Sys/OFA/blob/main/checkpoints.md))
* [x] who are the authors: (Peng Wang @[logicwong](https://github.com/logicwong), An Yang @[yangapku](https://github.com/yangapku), Rui Men @[jxst539246](https://github.com/jxst539246), Junyang Lin @[JustinLin610](https://github.com/JustinLin610), Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yangf)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
[Community] Add fairseq FastSpeech2,"# What does this PR do?

This PR adds FastSpeech2, a transformer-based text-to-speech model. Fixes #15166.

## Motivation

While HF transformers has great support for ASR and other audio processing tasks, there is not much support for text-to-speech. There are many transformer-based TTS models that would be great to have in the library. FastSpeech2 is perhaps the most well-known and popular of transformer-based TTS models. With TTS support, we could potentially have really cool demos that go from ASR -> NLU -> NLG -> TTS to build full-fledged voice assistants! 

## Who can review?

Anton, Patrick (will tag when PR is ready)","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",18,open
Add TUNet,"# 🌟 New model addition

The TUNet model for audio superresolution

## Model description

[TUNet: A Block-online Bandwidth Extension Model based on Transformers and Self-supervised Pretraining](https://arxiv.org/abs/2110.13492v3) is a paper by Viet-Anh Nguyen, Anh H. T. Nguyen, and Andy W. H. Khong, introducing a model for audio superresolution based on transformers.

Audio superresolution allows for the upsampling of audio with minimal loss of quality. This is very useful for ASR tasks that require audio to be resampled during preprocessing, which has a big impact on transcriptions depending on the native sample rate.

## Open source status

* [x] the model implementation is available: [The official repo](https://github.com/NXTProduct/TUNet)
* [x] the model weights are available: [ONNX weights](https://github.com/NXTProduct/TUNet/tree/master/lightning_logs)
* [x] who are the authors: Viet-Anh Nguyen, Anh H. T. Nguyen, and Andy W. H. Khong
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Fix segformer reshape last stage,"# What does this PR do?

This PR improves SegFormer by removing the `reshape_last_stage` attribute of the configuration. In fact, this attribute was not needed at all. 

Previously, the `reshape_last_stage` argument was set to `False` for `SegformerForImageClassification` models, however it's perfectly fine to reshape afterwards within this head model. 

@sgugger I know you'll warn me about this being a breaking change. Here's why I think this change will not have an impact:

- `reshape_last_stage` was set to `True` by default in the configuration. As this PR only removes the ability to set this to `False`, the only use case in which this might be a breaking change is in case one defined a `SegformerModel` with `config.reshape_last_stage = False`. I am not sure if anyone has done this because there are no use cases for this.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
add image2text generation,add image2text generation,"[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
DeepMind Retro,"# 🌟 New model addition

Basically a retrieval augmented model like RAG, but without expensive retriever end2end training



## Open source status

* [ ] the model implementation is available: Not available 
* [ ] the model weights are available:  Not available 
* [ ] who are the authors:(https://arxiv.org/pdf/2112.04426.pdf)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",11,open
vits support? ,"add support for vits, an tts transformer based e2e model.

https://github.com/jaywalnut310/vits","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Porting Compressive Transformer to Huggingface,"# 🌟 New model addition

As part of my Master-Thesis I will implement or adapt different transformer architecture for LM that are specifically designed for long-context situation. As part of this I started with porting the Compressive Transformer to the huggingface interface. And will probably do the same for other architectures in the future. Let me know if you're interested for a pull-request.

## Model description
[Paper](https://arxiv.org/pdf/1911.05507.pdf)

The Compressive Transformer is an extension of the Transformer-XL architecture with an additional compressed memory. Memories that would get discarded in the Transformer-XL get compressed and added to the compressed memory. 
The compression function can take different forms but the best performance on word-level LM is a Conv1d compression. 
Training of the C-Transformer happens the same way to how the Transformer-XL architecture is trained. But additional to that there is an ""attention-reconstruction-loss"" which compares the attention that we get from using the memory with the attention we get from its compressed counter-part. Using the MSE-loss we can perform gradient updates on the compression function


## Open source status

* [x] the model implementation is available: 
https://nn.labml.ai/transformers/compressive/index.html Is an open source implemention under the MIT license
https://github.com/deepmind/pg19 The data-set used in parts of the experiment by the Authors
https://github.com/vilmarzti/long_context_transformers/blob/main/longcontext/transformers/compressive_transformer.py My humble start of porting the architecture to the huggingface-format.
* [x] the model weights are available: 
None that I could find.
Weights (for wikitext-2 and 103) might become available as my thesis progresses and I start training
* [x] who are the authors: 
Jack W. Rae (https://github.com/dm-jrae)
Anna Potapenko (https://github.com/AnyaP)
Siddhant M. Jayakumar (github-profile not found)
Chloe Hillier (github profile not found)
Timothy P. Lillicrap (github profile not found)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Support for Monotonic Mulithead Attention based Simultaneous Speech-to-text Translation,"# 🌟 New model addition
Simultaneous Speech-to-text Translation using Monotonic Multihead Attention(MMA).
I am wondering if anybody is working on implementing this model for now.
However, I am worried that if this model is going to be supported by Hugging Face systems, since inference works in a particular way using frameworks like [SimulEval](https://github.com/facebookresearch/SimulEval) to simulate streaming input which may not be compatible with current Hugging Face's inference system?

## Model description
[MMA(Ma et al., 2019)](https://arxiv.org/abs/1909.12406) has been used to handle streaming text/speech inputs mostly for translation, where MMA extends the monotonic attention mechanism to multihead.

<!-- Important information -->

## Open source status
* [x] the model implementation is available: [Fairseq Implementation is available here](https://github.com/pytorch/fairseq/blob/fcca32258c8e8bcc9f9890bf4714fa2f96b6b3e1/examples/simultaneous_translation/models/convtransformer_simul_trans.py#L29~#L63)
* [ ] the model weights are available: (give details)
* [ ] who are the authors: (mention them, if possible by @gh-username) : Xutai Ma(@xutaima), Juan Pino, James Cross, Liezl Puzon, Jiatao Gu

Inference framework : [Facebook Research SimulEval](https://github.com/facebookresearch/SimulEval)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Number-specific tokenization changes,"# 🌟 New model addition

## Model description

I wanted to contribute a bunch of number-specific LMs proposed in recent work. Most of these are not architecture changes but simple tokenization tricks such as converting a number `329` to `3.29e2` (scientific; [Zhang et al. 2020](https://aclanthology.org/2020.findings-emnlp.439/)) or `3 2 9` (digit splitting; [Nogueira et al. 2021](https://arxiv.org/abs/2102.13019)) or `e2` (exponent only; [Spokoyny et al. 2020](https://aclanthology.org/2020.emnlp-main.385/) and [Thawani et al. 2021](https://aclanthology.org/2021.emnlp-main.557/)). The motivation is that several industrial applications require number-heavy NLP but struggle with existing models.

I discussed a specific way to do this on team slack with @SaulLu for, say, NumBERT (scientific notation) which involves adding a new model (tokenizer-only) and uploading the pretrained weights to the hub. I wanted to open a broader discussion here about more such number-tokenizer-only methods, some of which may not even have pretrained weights. The hope would be to make some abstract intervention (perhaps at the tokenizer level) to let the user configure GPT or BERT tokenizer as `number_tokenizer=exponent`. 

But perhaps clubbing methods from different papers into one model/tokenizer is against HF's philosophy? If so, I could proceed with trying to simply incorporate them as individual models - is it fine if some of them do not have pretrained weights available?

## Open source status

* [X] the model implementation is available: [T5 fine-tuned on arithmetic, based on HF transformers](https://github.com/castorini/transformers-arithmetic) and [NumBERT/scientific, based on google-bert original code](https://github.com/google-research/google-research/tree/master/numbert)
* [X] the model weights are available: [BERT pretrained with scientific notation](https://console.cloud.google.com/storage/browser/gresearch/numbert)
* [X] who are the authors: @spokoyny @XikunZhang @DeepakRamachandran @iftenney @yanaiela @rodrigonogueira4 @lintool 
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
New and better T5 checkpoints from scaling transformers paper,"# 🌟 New model addition

## Model description
This paper explores different ways of scaling T5:
Scaling Efficiently: Insights from Pre-training and Finetuning Transformers
https://arxiv.org/abs/2109.10686

They found new checkpoints (DeepNarrow) that perform significantly better than the previous models on downstream applications.

Here is a table from the paper that compares the old Base, Large, XL and XXL models to the new ones:

<img width=""551"" alt=""image"" src=""https://user-images.githubusercontent.com/37597043/152038969-de1fa56e-991d-493b-b94c-96cbc42d69be.png"">

The checkpoints were released today here:
https://github.com/google-research/google-research/tree/master/scaling_transformers

<!-- Important information -->

## Open source status

* [x] the model implementation is available: the current T5 implementation in transformers
* [x] the model weights are available: https://github.com/google-research/google-research/tree/master/scaling_transformers
* [x] who are the authors: Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",26,open
Adding RelationExtraction head to layoutLMv2 and layoutXLM models,"# 🌟 New model head addition
Relation Extraction Head for LayoutLMv2/XLM
## Addition description
Hey all,

I've see a bunch of different requests across huggingface issues [[0]](https://github.com/huggingface/transformers/issues/14330), unilm issues [[0]](https://github.com/microsoft/unilm/issues/286)[[1]](https://github.com/microsoft/unilm/issues/465) and on @NielsRogge Transformer Tutorials issues [[0]](https://github.com/NielsRogge/Transformers-Tutorials/issues/6)[[1]](https://github.com/NielsRogge/Transformers-Tutorials/issues/39) about adding the relation extraction head from layoutlmv2 to the huggingface library. As the model is quite difficult to use in it's current state I was going to write my own layer ontop but I saw in this [issue](https://github.com/NielsRogge/Transformers-Tutorials/issues/39) that it may be a good idea to add it to transformers as a separate layoutlmv2/xlm head and thought it would be a good way to contribute back to a library I use so much.

I've gone ahead and added it under my own [branch](https://github.com/R0bk/transformers/tree/layoutlm-relation-extraction) and got it successfully working with the library. [Here](https://colab.research.google.com/drive/16wqA3oTUf7yzUKsSSZxiMf1443_ZO3wC?usp=sharing) is a colab using my branch of transformers if you want to test it yourself.

Before I add tests/ write more docs I just wanted to post here first to see if there's interest in potentially merging this in. If there is interest I have a few questions that it would be helpful to get some info on to ensure that I've correctly done the integration.
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",31,open
[activations] pytorch-1.11+ Tanh Gelu Approximation ,"# 🚀 Feature request

As kindly flagged by @vadimkantorov pt-1.11 will have a fast Tanh Gelu Approximation as implemented here https://github.com/pytorch/pytorch/pull/61439 so we could replace our manual implementation with the fast one when pt>=1.11 is detected.

for additional context please see this thread: https://github.com/pytorch/pytorch/issues/39853","[{'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",14,open
Evidentiality-guided Generator - Retrieval model,"# 🌟 New model addition

## Model description

In this paper, we introduce Evidentiality-guided GGenerator, which incorporates evidentiality of passages---whether a passage contains correct evidence to support the output---into training the generator via multi-task learning of answer generation and evidentiality prediction for retrieval-augmented generation. Experimental results show large improvements across three knowledge intensive tasks: open question answering, fact verification and knowledge-enhanced dialogue.

## Open source status

* [x] the model implementation is available: https://github.com/AkariAsai/evidentiality_qa/tree/main/evi_gen
* [x] the model weights are available: https://github.com/AkariAsai/evidentiality_qa#fine-tuned-models
* [x] who are the authors: @AkariAsai

Happy to guide anyone who is interested through adding this model! Seems like it gives some nice improvements over RAG!

@qqaatw - maybe interesting for you ;-)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",10,open
GeneratorExp aren't supported by torch.jit.script when I try to export a previously trained model  'google/vit-base-patch16-224-in21k'.,"## Environment info
- `transformers` version: 4.15.0
- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.12
- PyTorch version (GPU?): 1.10.0+cu111 (True)
- Tensorflow version (GPU?): 2.7.0 (True)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No


### Who can help

Models:

ViTModel

If the model isn't in the list, ping @LysandreJik who will redirect you to the correct contributor.

Library:

- Vision: @NielsRogge, @sgugger

Documentation: @sgugger

Model hub:

- for issues with a model, report at https://discuss.huggingface.co/ and tag the model's creator.



## Information

GeneratorExp aren't supported by torch.jit.script when I try to export a previously trained model  'google/vit-base-patch16-224-in21k'.

Model I am using (ViTModel):

The problem arises when using:
* [X] my own modified scripts: (give details below)
* 
model_x = ViTForImageClassification.from_pretrained(
    'google/vit-base-patch16-224-in21k',
    num_labels=len(label2id),
    label2id=label2id,
    id2label=id2label
)
model_scripted = torch.jit.script(model_x) # Export to TorchScript

---------------------------------------------------------------------------
UnsupportedNodeError                      Traceback (most recent call last)
<ipython-input-12-bc467d8ea1c0> in <module>()
      6     id2label=id2label
      7 )
----> 8 model_scripted = torch.jit.script(model_x) # Export to TorchScript
      9 model_scripted.save('model_scripted.pt') # Save

14 frames
/usr/local/lib/python3.7/dist-packages/torch/jit/frontend.py in __call__(self, ctx, node)
    284         method = getattr(self, 'build_' + node.__class__.__name__, None)
    285         if method is None:
--> 286             raise UnsupportedNodeError(ctx, node)
    287         return method(ctx, node)
    288 

UnsupportedNodeError: GeneratorExp aren't supported:
  File ""/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py"", line 987
        activations"".
        """"""
        return any(hasattr(m, ""gradient_checkpointing"") and m.gradient_checkpointing for m in self.modules())
                   ~ <--- HERE

## To reproduce

Steps to reproduce the behavior:

1.  from transformers import ViTForImageClassification
2.   Instantiate a previously created mode  'google/vit-base-patch16-224-in21k'  using ViTForImageClassification.from_pretrained() API.
3. Try invoking  torch.jit.script(model_x) and you will see the error.

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",4,open
feat(flax): leave restored weights on CPU,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

When restoring a flax model with `.from_pretrained()`, leave the weights on CPU.

The removed section was linked to issue https://github.com/google/flax/issues/1261 which is now closed.
When calling `jnp.array()`, the tensors are converted from numpy arrays and placed on default device (typically GPU or TPU), which can cause issues when loading very large models that don't fit on one single instance.


## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/master/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/master/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

@patil-suraj @patrickvonplaten 
<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @LysandreJik

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @n1t0, @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",7,open
[Kernel Fusion] training benchmarks of AOTAutograd (multiple models),"Note to maintainers: We are using this PR to collaborate and there is no intention yet to merge anything, so please ignore unless you want to experiment with the latest auto-speedups.

We are experimenting with the latest https://github.com/pytorch/functorch against pytorch nightly to automatically speed up the execution and reduce memory usage:

So the idea is this. Given an existing `model`, you speed it up by doing just this:

```
from functorch.compile import memory_efficient_fusion
aot_model = memory_efficient_fusion(model)
with torch.jit.fuser(""fuser2""):
    train(aot_model)
```

So for example HF Trainer could automate this with just adding a new flag, like `--fusion aot`.

Notably, as long as the part being compiled with AOTAutograd is static, you can do whatever you want outside of the model, and autograd will still work with the AOTAutograd compiled model.

So, things like this work fine
```
foo = aot_model(*inps)
loss = foo.sum()
if loss < 0:
    print(""awesome"")
loss.backward()
```

Here are some benchmarks:

-------------

A100 training (from @Chillee):
```
$ python  scripts/aot_albert.py
Current memory requirement: 5.69 GB
eager 0.08250337839126587
Current memory requirement: 4.00 GB
aot 0.05442763566970825

Maximum output error:  9.5367431640625e-07
Maximum gradient error:  1.5096273273229599e-05
```

| model                 | dtype          | name   |   time (s) |   mem (GB) |   time % |   mem % |
|:----------------------|:---------------|:-------|-----------:|-----------:|---------:|--------:|
| AlbertForMaskedLM     | torch.float32  | eager  |      0.087 |      7.133 |        0 |       0 |
| AlbertForMaskedLM     | torch.float32  | aot    |      0.057 |      5.438 |      -35 |     -24 |
| AlbertForMaskedLM     | torch.float16  | eager  |      0.051 |      3.901 |        0 |       0 |
| AlbertForMaskedLM     | torch.float16  | aot    |      0.034 |      3.054 |      -34 |     -22 |
| AlbertForMaskedLM     | torch.bfloat16 | eager  |      0.053 |      3.931 |        0 |       0 |
| AlbertForMaskedLM     | torch.bfloat16 | aot    |      0.034 |      3.083 |      -36 |     -22 |
| GPT2LMHeadModel       | torch.float32  | eager  |      0.056 |      5.174 |        0 |       0 |
| GPT2LMHeadModel       | torch.float32  | aot    |      0.045 |      4.328 |      -19 |     -16 |
| GPT2LMHeadModel       | torch.float16  | eager  |      0.033 |      4.645 |        0 |       0 |
| GPT2LMHeadModel       | torch.float16  | aot    |      0.029 |      4.223 |      -13 |      -9 |
| GPT2LMHeadModel       | torch.bfloat16 | eager  |      0.034 |      4.965 |        0 |       0 |
| GPT2LMHeadModel       | torch.bfloat16 | aot    |      0.029 |      4.541 |      -15 |      -9 |
| BertForMaskedLM       | torch.float32  | eager  |      0.041 |      6.764 |        0 |       0 |
| BertForMaskedLM       | torch.float32  | aot    |      0.036 |      6.759 |      -13 |       0 |
| BertForMaskedLM       | torch.float16  | eager  |      0.025 |      6.228 |        0 |       0 |
| BertForMaskedLM       | torch.float16  | aot    |      0.021 |      6.226 |      -16 |       0 |
| BertForMaskedLM       | torch.bfloat16 | eager  |      0.026 |      6.505 |        0 |       0 |
| BertForMaskedLM       | torch.bfloat16 | aot    |      0.021 |      6.503 |      -19 |       0 |
| LongformerForMaskedLM | torch.float32  | eager  |      0.122 |      9.921 |        0 |       0 |
| LongformerForMaskedLM | torch.float32  | aot    |      0.111 |      9.933 |       -9 |       0 |


On rtx3090 (from @stas00):

| model                 | dtype          | name   |   time (s) |   mem (GB) |   time % |   mem % |
|:----------------------|:---------------|:-------|-----------:|-----------:|---------:|--------:|
| AlbertForMaskedLM     | torch.float32  | eager  |      0.173 |      7.078 |        0 |       0 |
| AlbertForMaskedLM     | torch.float32  | aot    |      0.125 |      5.382 |      -28 |     -24 |
| AlbertForMaskedLM     | torch.float16  | eager  |      0.089 |      3.829 |        0 |       0 |
| AlbertForMaskedLM     | torch.float16  | aot    |      0.064 |      2.982 |      -28 |     -22 |
| AlbertForMaskedLM     | torch.bfloat16 | eager  |      0.092 |      3.852 |        0 |       0 |
| AlbertForMaskedLM     | torch.bfloat16 | aot    |      0.064 |      3.005 |      -30 |     -22 |
| GPT2LMHeadModel       | torch.float32  | eager  |      0.112 |      4.822 |        0 |       0 |
| GPT2LMHeadModel       | torch.float32  | aot    |      0.094 |      3.977 |      -16 |     -18 |
| GPT2LMHeadModel       | torch.float16  | eager  |      0.060 |      4.013 |        0 |       0 |
| GPT2LMHeadModel       | torch.float16  | aot    |      0.051 |      3.591 |      -15 |     -11 |
| GPT2LMHeadModel       | torch.bfloat16 | eager  |      0.061 |      4.736 |        0 |       0 |
| GPT2LMHeadModel       | torch.bfloat16 | aot    |      0.051 |      4.313 |      -16 |      -9 |
| BertForMaskedLM       | torch.float32  | eager  |      0.086 |      6.343 |        0 |       0 |
| BertForMaskedLM       | torch.float32  | aot    |      0.076 |      6.338 |      -11 |       0 |
| BertForMaskedLM       | torch.float16  | eager  |      0.046 |      5.717 |        0 |       0 |
| BertForMaskedLM       | torch.float16  | aot    |      0.041 |      5.714 |      -11 |       0 |
| BertForMaskedLM       | torch.bfloat16 | eager  |      0.046 |      5.952 |        0 |       0 |
| BertForMaskedLM       | torch.bfloat16 | aot    |      0.040 |      5.950 |      -13 |       0 |
| LongformerForMaskedLM | torch.float32  | eager  |      0.209 |      9.080 |        0 |       0 |
| LongformerForMaskedLM | torch.float32  | aot    |      0.194 |      9.092 |       -7 |       0 |


------------

Instructions from @stas00 on how to build functorch to reproduce these results.

```
# install torch-nightly
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly

# install functorch (and reinstall after `git pull` later if need to sync up)
git clone https://github.com/pytorch/functorch
cd functorch
rm -rf build
pip install -e .[aot]
```

As this is a constantly evolving code-base, make sure to `git pull` and rebuild above if you have the version that is some days old. Or at least if you try the code and it fails the first thing to do is to update and rebuild `functorch` and then retry the benchmarks.

Note that there is currently a correctness issue on one of the gradients on PyTorch nightly, the above was run with this patch (https://github.com/pytorch/pytorch/pull/71542), which fixes the correctness issue.

--------------------------

Notes:

- AOT = Ahead of Time
- eager = normal python/pytorch code - i.e. the way our models are written now

Q: What is the pytree registration [here](https://github.com/huggingface/transformers/pull/15264/files#diff-a271afd7d9556b5f3a5aa2df08fa1d10114569272db373956b108446468f476fR25) for? 
A: AOTAutograd tries to present the simplest possible graphs for backends, and so it primarily works with lists of tensors for both the input and the output. So, these pytrees are needed so that we can flatten the input data structures into a list, and unflatten the output back into the correct data structure. PS: This is very much inspired by Jax <3

Resources on AOTAutograd:
AOTAutograd: https://docs.google.com/presentation/d/1rTt0BR2KChDQQTks2hHUtvHxtHQKwgQHVNrmbhj0byk/edit?usp=sharing
Min-Cut recomputation: https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467","[{'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3770033588, 'node_id': 'LA_kwDOCUB6oc7gtiW0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Kernel%20Fusion', 'name': 'Kernel Fusion', 'color': 'A956E3', 'default': False, 'description': ''}]",6,open
Add FastSpeech2,"# 🌟 New model addition

## Model description

FastSpeech2 is a TTS model that outputs mel-spectrograms given some input text. From the [paper](https://arxiv.org/abs/2006.04558) abstract:

> Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at [this https URL](https://speechresearch.github.io/fastspeech2/).

## Open source status

* [x] the model implementation is available
* [x] the model weights are available
* [x] who are the authors: @RayeRen

The authors have not open-sourced their code implementation. However, the first author replied to an email inquiry and pointed me to the official implementation of DiffSinger, which includes FastSpeech2 code. This is likely the closest original implementation we can access.

* [DiffSinger](https://github.com/MoonInTheRiver/DiffSinger/tree/master/modules/fastspeech)

LJ Speech model weights are available [here](https://drive.google.com/file/d/1Zp45YjKkkv5vQSA7woHIqEggfyLqQdqs/view).

Other notable unofficial implementations include:

* [ming024](https://github.com/ming024/FastSpeech2)
* [ESPmet](https://espnet.github.io/espnet/_modules/espnet2/tts/fastspeech2/fastspeech2.html)
* [TensorFlowTTS](https://github.com/TensorSpeech/TensorFlowTTS/blob/master/tensorflow_tts/models/fastspeech2.py)


## Additional Context

This issue is a revisiting of https://github.com/huggingface/transformers/pull/11135.

cc @anton-l @patrickvonplaten ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",7,open
[JAX/FLAX]: CLM Tokenizer Training confusion,"Hi,

after looking at the current readme of the [CLM tokenizer training example](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling#train-tokenizer-1), there's something strange in the model configuration:

The `config.json` file looks like this:

```json
GPT2Config {
  ""_name_or_path"": ""./"",
  ""activation_function"": ""gelu_new"",
  ""architectures"": [
    ""GPT2LMHeadModel""
  ],
  ""attn_pdrop"": 0.0,
  ""bos_token_id"": 50256,
  ""embd_pdrop"": 0.0,
  ""eos_token_id"": 50256,
  ""initializer_range"": 0.02,
  ""layer_norm_epsilon"": 1e-05,
  ""model_type"": ""gpt2"",
  ""n_ctx"": 1024,
  ""n_embd"": 768,
  ""n_head"": 12,
  ""n_inner"": null,
  ""n_layer"": 12,
  ""n_positions"": 1024,
  ""reorder_and_upcast_attn"": false,
  ""resid_pdrop"": 0.0,
  ""scale_attn_by_inverse_layer_idx"": false,
  ""scale_attn_weights"": true,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""task_specific_params"": {
    ""text-generation"": {
      ""do_sample"": true,
      ""max_length"": 50
    }
  },
  ""transformers_version"": ""4.16.0.dev0"",
  ""use_cache"": true,
  ""vocab_size"": 50257
}

```

Vocab size is 50257, and `eos_token_id` is set to 50256. I think that setting `eos_token_id` is wrong, because of the following example:

```bash
In [10]: tokenizer.convert_ids_to_tokens([1797, 705, 225, 50256])
Out[10]: ['hal', 'lo', 'Ġ', 'Ġgeestigheid']
```

Id *50256* is originally set to `'Ġgeestigheid'`. I'm not 100% sure, but it should be set to 50257 (and thus outside the vocabulary), because of:

```bash
In [7]: tokenizer.encode(""hallo <|endoftext|>"")
Out[7]: [1797, 705, 225, 50257]
```

It shows that `eos_token` is set to `<|endoftext|>` and from the tokenizer part, `eos_token_id` then should be set to `50257`?!

Now I'm using the official GPT-2 model as reference:

It uses `""eos_token_id"": 50256` in the `config.json` file, some tokenizer tests:

```bash
In [6]: tokenizer.eos_token
Out[6]: '<|endoftext|>'

In [7]: tokenizer.eos_token_id
Out[7]: 50256

In [8]: tokenizer.encode(""Hello <|endoftext|>"")
Out[8]: [15496, 220, 50256]
```

Which is correct.

And there's another issue: after looking at the `tokenizer.json` file for GPT-2, the following entry exists:

```bash
""<|endoftext|>"":50256}
```

which is perfect, but: for the own trained vocab this entry does not exist! I'm not sure if this is a bug in the Tokenizers library or intended :thinking: ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
[Benchmark] HF Trainer on A100,"# 🖥 Benchmarking `transformers` w/ HF Trainer on a single A100 40GB

We are going to use a special benchmarking tool that will do all the work for us. https://github.com/huggingface/transformers/pull/14934

This is the index post and specific benchmarks are in their own posts below:

1. [fp16 vs bf16 vs tf32 vs fp32](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189)
2. [gradient accumulation steps](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004592231)
3. [batch size](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957)
4. [gradient checkpointing](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005034578)
5. [optimizers](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005220263)
6. [combining winning strategies](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005227577) **~3x speed improvement!**
7. [RTX-3090 vs A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005235845)

Note that each benchmark was run only once, so multiple runs and averaging is probably going to give slightly different results. The purpose here though is to see relative differences roughly and not try to give an exact number.

See also the [same benchmarks for RTX-3090](https://github.com/huggingface/transformers/issues/14608)
","[{'id': 2604155188, 'node_id': 'MDU6TGFiZWwyNjA0MTU1MTg4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Benchmarks', 'name': 'Benchmarks', 'color': '2DF372', 'default': False, 'description': 'Issues related to Memory regressions in tests and scripts'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",17,open
[Benchmarks] index,"This issue is to document the important `transformers` benchmarks in one place, so that they are easy to find.

To add a new benchmark entry post it in an Issue (separately or as a comment in an existing issue) and then link from here. If you have edit rights please add a link directly to this post, otherwise please add a note in the comments and I will update this post. 

Please do not post actual benchmarks in the comments of this Issue. This is only an index. 

Thank you!


## Fastest speed combinations
- [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1005229426)
- [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005227577)


## Precision: fp16 vs bf16 vs tf32 vs fp32
- [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803)
- [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189)


## Batch size / gradient accumulation steps 
-  gradient accumulation steps: [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537), [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004592231)
- batch size [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004470417), [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957)
 


## Gradient checkpointing
- [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004422281)
- [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005034578)


## Optimizers:
- Adam torch vs. apex vs HF vs adafactor: [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1005219385), [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005220263)
- re-run the above a year later with the same list of optimizers, plus BNB's 8bit optimizer and fused torch AdamW [PCIe 80GB A100](https://github.com/huggingface/transformers/issues/22101)

## Network / Interconnects:
- [DP/DDP/NVLink](https://github.com/huggingface/transformers/issues/9371#issuecomment-768656711)
","[{'id': 2604155188, 'node_id': 'MDU6TGFiZWwyNjA0MTU1MTg4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Benchmarks', 'name': 'Benchmarks', 'color': '2DF372', 'default': False, 'description': 'Issues related to Memory regressions in tests and scripts'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
Fine-tuning GPT-J-6B in colab: 8-bit weights with low-rank adaptors,"# 🌟 New model addition

## Model description

This is a version of EleutherAI's GPT-J with 6 billion parameters that is modified so you can generate and fine-tune the model in colab or equivalent desktop GPU (e.g. single 1080Ti).

The original GPT-J takes 22+ GB memory for float32 parameters. Even if you cast everything to 16-bit, it will still not fit onto most single-GPU setups short of A6000 and A100. You can inference it on TPU or CPUs, but fine-tuning is way more expensive.

## Implementation
Proof-of-concept notebook is available here: [![colab](https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es?usp=sharing#scrollTo=P8Y75B6WDIN-)

[Model card](https://huggingface.co/hivemind/gpt-j-6B-8bit) has more detailed explanations and auxiliary notebooks (e.g. model conversion and perplexity check).

The current implementation is somewhat hacky, but it can be integrated easily with modelling_gptj.py if you like the idea.

## Open source status

* the model implementation is available [here](https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es)
* the model weights are available [here](https://huggingface.co/hivemind/gpt-j-6B-8bit)
* who are the authors:
   - the [original GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B) was trained by [Eleuther AI](https://www.eleuther.ai/) (citation: Ben Wang and Aran Komatsuzaki)
   - fast quantization from [bitsandbytes](https://github.com/facebookresearch/bitsandbytes) by [Tim Dettmers](https://github.com/TimDettmers)
   - low-rank adapters were proposed for GPT-like models by [Hu et al (2021)](https://arxiv.org/abs/2106.09685)
   - this notebook was created by me ( @deniskamazur ) with some help from Yozh ( @justheuristic)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 3716956449, 'node_id': 'LA_kwDOCUB6oc7djEEh', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Quantization', 'name': 'Quantization', 'color': '971399', 'default': False, 'description': ''}]",34,open
Support on Mixture of expert models,"Hi, I find that there are emerging works in the field of NLP on **Mixture of experts** based models, such as Switch Transformers from Google. However, I do not find such mixture of expert models in huggingface transformers. Do you have the plan to support such models? Thanks !","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",8,open
[WIP] [performance doc] faster/leaner optimizers,"documenting faster / leaner optimizers

TODO:

- add `--optim adamw_bnb_8bit` for HF Trainer.
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
[Benchmark] HF Trainer on RTX-3090,"# 🖥 Benchmarking `transformers` w/ HF Trainer on RTX-3090

We are going to use a special benchmarking tool that will do all the work for us. https://github.com/huggingface/transformers/pull/14934

This is the index post and specific benchmarks are in their own posts below:

1. [fp16 vs bf16 vs tf32 vs fp32](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803)
2. [gradient accumulation steps](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)
3. [gradient checkpointing](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004422281)
4. [batch size](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)
5. [optimizers](https://github.com/huggingface/transformers/issues/14608#issuecomment-1005219385)
6. [combining winning strategies](https://github.com/huggingface/transformers/issues/14608#issuecomment-1005229426) **~2x speed improvement!**
7. [RTX-3090 vs A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005235845)

See also the [same benchmarks for A100](https://github.com/huggingface/transformers/issues/15026)

TODO:
- other suggestions?

Note that each benchmark was run only once, so multiple runs and averaging is probably going to give slightly different results. The purpose here though is to see relative differences roughly and not try to give an exact number.
","[{'id': 2604155188, 'node_id': 'MDU6TGFiZWwyNjA0MTU1MTg4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Benchmarks', 'name': 'Benchmarks', 'color': '2DF372', 'default': False, 'description': 'Issues related to Memory regressions in tests and scripts'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",13,open
FLAX core dump error on CloudTPU when running run_clm_flax.py,"Hi, I'm having a weird problem trying to train a gpt-neo model from scratch on a v3-8 cloud TPU. Something similar to the [closed issue here](https://github.com/huggingface/transformers/issues/12404). Getting: 
```
https://symbolize.stripped_domain/r/?trace=7fb5dbf8a3f4,7fb5dbfe020f,7f&map= 
*** SIGTERM received by PID 64823 (TID 64823) on cpu 26 from PID 63364; stack trace: ***                                                                                                                                     | 0/1 [00:00<?, ?ba/s]
PC: @     0x7fb5dbf8a3f4  (unknown)  do_futex_wait.constprop.0
    @     0x7fb52fa377ed        976  (unknown)
    @     0x7fb5dbfe0210  440138896  (unknown)                                                                                                                                                                               | 0/1 [00:00<?, ?ba/s]
    @               0x80  (unknown)  (unknown)                                                                                                                                                                               | 0/1 [00:00<?, ?ba/s]
https://symbolize.stripped_domain/r/?trace=7fb5dbf8a3f4,7fb52fa377ec,7fb5dbfe020f,7f&map=44c8b163be936ec2996e56972aa94d48:7fb521e7d000-7fb52fd90330 
E1122 14:13:36.933620   64823 coredump_hook.cc:255] RAW: Remote crash gathering disabled for SIGTERM.                                                                                                                        | 0/1 [00:00<?, ?ba/s]
E1122 14:13:36.960024   64823 process_state.cc:776] RAW: Raising signal 15 with default behavior
```
randomly during preprocessing/loading the dataset. 

The env is clean, setup according to the Quickstart Flax guide from google's [help page](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm), and as well from [here](https://github.com/huggingface/transformers/tree/master/examples/research_projects/jax-projects#how-to-install-relevant-libraries). Jax is installed okay, sees 8 TPUs. I tried the standard pip install as well as the local install as some people suggested in the [issue](https://github.com/huggingface/transformers/issues/12404) above, still getting the same behavior. 

This error does **not** kill the training. 
So, question number 1 would be **how to get rid of this error ?**

Something else happens that _might_ be related: Running a dummy 300MB Wiki dataset for training only produces the error above, but training progresses. However, when running the full 40GB dataset, at a point during the first epoch I get:

``list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, .... (many 1s) .. 1, 1, 1])]]' of type <class 'numpy.ndarray'> is not a valid JAX type.``

This error kills the training. I've found this related [issue](https://github.com/huggingface/transformers/issues/12502), but the last suggestion of  increasing ``max_seq_len`` does not apply here, as the preprocessing should automatically concatenate and cut the model len (and it is set in the config file). The dataset itself is clean, does not contain long words or chars or anything weird.

Thus, question 2: **Any pointers on how to solve this second error?**

Unfortunately I cannot share the dataset as it's private :disappointed: so I don't know how to help reproduce this error. There are 2 questions in this single issue as maybe there's a chance they are related (?). 

Thanks a bunch!

Update: [here is the output of the run_clm_flax.py](https://wtools.io/paste-code/b7US). Because there's a limit on how much you can paste online, I've deleted a few chunks of repeating lines in the output. ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",7,open
Add TAPAS trained on NQ,"The authors of the ""Open Domain Question Answering over Tables via Dense Retrieval"" papers released the weights for TAPAS retriever trained on NQ here: https://github.com/google-research/tapas/blob/master/DENSE_TABLE_RETRIEVER.md

It would be nice if those weights could be converted to Huggingface model (I would presume it's fairly similar the other finetuned model since they share the same architecture, and I'd be happy to do it myself if there's some scripts I can run)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
[New Model] DocFormer: End-to-End Transformer for Document Understanding,"# 🌟 New model addition

## Model description

See _""DocFormer: End-to-End Transformer for Document Understanding"", Appalaraju et al (ICCV 2021)_ on [CVF](https://openaccess.thecvf.com/content/ICCV2021/papers/Appalaraju_DocFormer_End-to-End_Transformer_for_Document_Understanding_ICCV_2021_paper.pdf) and [arXiv](https://arxiv.org/abs/2106.11539)

DocFormer is a multi-modal transformer model for 2D/visual documents from Amazon (where, fair disclosure, I also currently work but not in research) - which I would characterize at a high level as being broadly along the same use cases as LayoutLMv2 (already in `transformers`), but achieving better (state-of-the-art) results with smaller datasets per the benchmarks in the paper. 

I've found this kind of multi-modal, spatial/linguistic model very useful in the past (actually released an [AWS sample](https://github.com/aws-samples/amazon-textract-transformer-pipeline) and [blog post](https://aws.amazon.com/blogs/machine-learning/bring-structure-to-diverse-documents-with-amazon-textract-and-transformer-based-models-on-amazon-sagemaker/) with Hugging Face LayoutLMv1 earlier this year) and would love the improvements from DocFormer could be available through HF Transformers.

## Open source status

* [X] the model implementation is available: (give details)
    * Looks like there's an (MIT-0) implementation at https://github.com/shabie/docformer
* [ ] the model weights are available: (give details)
    * Not currently as far as I can tell? 
* [X] who are the authors: (mention them, if possible by @gh-username)
    * Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha - all of AWS AI. Not sure of GitHub usernames
    * @shabie for the currently available implementation","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",18,open
[Deepspeed Inference] HF Integration,"This PR is working on an integration of [Deepspeed Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/) which implements Tensor Parallelism. This is different from Deepspeed ZeRO inference.

This is a very early draft.

To try:

```
cd transformers
export BS=16; rm -r output_dir; PYTHONPATH=src USE_TF=0  \
deepspeed --num_gpus=2 examples/pytorch/translation/run_translation.py \
--model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 \
--evaluation_strategy=steps --do_eval --label_smoothing 0.1 --learning_rate \
3e-5 --logging_first_step --logging_steps 500 --max_source_length 128 \
--max_target_length 128 --overwrite_output_dir --per_device_eval_batch_size $BS \
--predict_with_generate --sortish_sampler --source_lang en --target_lang ro \
--dataset_name wmt16 --dataset_config ro-en --source_prefix \
'translate English to Romanian: ' --val_max_target_length 128 --warmup_steps \
50 --max_eval_samples 50 --deepspeed_inference --skip_memory_metrics 0
```

and it currently hangs with `--num_gpus > 1`. One gpu finishes processing and the other is stuck in preparing inputs. So need to figure out the synchronization of the gpus.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 3716964643, 'node_id': 'LA_kwDOCUB6oc7djGEj', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Inference', 'name': 'Inference', 'color': '7966AF', 'default': False, 'description': ''}]",1,open
Export LayoutLMv2 to onnx ,"I am trying to export LayoutLMv2 model to onnx but there is no support for that available in transformers library.
I have tried to follow the method available for layoutLM but that is not working. 
Here is config class for LayoutLMv2
```
class LayoutLMv2OnnxConfig(OnnxConfig):
    def __init__(
        self,
        config: PretrainedConfig,
        task: str = ""default"",
        patching_specs: List[PatchingSpec] = None,
    ):
        super().__init__(config, task=task, patching_specs=patching_specs)
        self.max_2d_positions = config.max_2d_position_embeddings - 1

    @property
    def inputs(self) -> Mapping[str, Mapping[int, str]]:
        return OrderedDict(
            [
                (""input_ids"", {0: ""batch"", 1: ""sequence""}),
                (""bbox"", {0: ""batch"", 1: ""sequence""}),
                (""image"", {0: ""batch"", 1: ""sequence""}),
                (""attention_mask"", {0: ""batch"", 1: ""sequence""}),
                (""token_type_ids"", {0: ""batch"", 1: ""sequence""}),
            ]
        )

    def generate_dummy_inputs(
        self,
        tokenizer: PreTrainedTokenizer,
        batch_size: int = -1,
        seq_length: int = -1,
        is_pair: bool = False,
        framework: Optional[TensorType] = None,
    ) -> Mapping[str, Any]:
        """"""
        Generate inputs to provide to the ONNX exporter for the specific framework
        Args:
            tokenizer: The tokenizer associated with this model configuration
            batch_size: The batch size (int) to export the model for (-1 means dynamic axis)
            seq_length: The sequence length (int) to export the model for (-1 means dynamic axis)
            is_pair: Indicate if the input is a pair (sentence 1, sentence 2)
            framework: The framework (optional) the tokenizer will generate tensor for
        Returns:
            Mapping[str, Tensor] holding the kwargs to provide to the model's forward function
        """"""

        input_dict = super().generate_dummy_inputs(tokenizer, batch_size, seq_length, is_pair, framework)

        # Generate a dummy bbox
        box = [48, 84, 73, 128]

        if not framework == TensorType.PYTORCH:
            raise NotImplementedError(""Exporting LayoutLM to ONNX is currently only supported for PyTorch."")

        if not is_torch_available():
            raise ValueError(""Cannot generate dummy inputs without PyTorch installed."")
        import torch

        batch_size, seq_length = input_dict[""input_ids""].shape
        input_dict[""bbox""] = torch.tensor([*[box] * seq_length]).tile(batch_size, 1, 1)
        return input_dict

onnx_config = LayoutLMv2OnnxConfig(model.config)


export(tokenizer=tokenizer, model=model, config=onnx_config, opset=12, output=Path('onnx/layoutlmv2.onnx'))
```

Running the export line is raising this error,

```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-25-99a1f167e396> in <module>()
----> 1 export(tokenizer=tokenizer, model=model, config=onnx_config, opset=12, output=Path('onnx/layoutlmv2.onnx'))

3 frames
/usr/local/lib/python3.7/dist-packages/transformers/models/layoutlmv2/tokenization_layoutlmv2.py in __call__(self, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
    449 
    450         words = text if text_pair is None else text_pair
--> 451         assert boxes is not None, ""You must provide corresponding bounding boxes""
    452         if is_batched:
    453             assert len(words) == len(boxes), ""You must provide words and boxes for an equal amount of examples""

AssertionError: You must provide corresponding bounding boxes
```","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",24,open
[WIP] Adding support for `flax` for `pipelines`.,"# What does this PR do?

<!--
Congratulations! You've made it this far! You're not quite done yet though.

Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.

Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.

Once you're done, someone will review your PR shortly (see the section ""Who can review?"" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.
-->

<!-- Remove if not applicable -->

Fixes # (issue)

```python
from transformers import FlaxRobertaModel, RobertaTokenizerFast,
RobertaForMaskedLM, pipeline
import numpy as np
import datetime
import tqdm
import random

def data(n):
    for _ in range(n):
        yield ""JAX/Flax is amazing <mask>""

def flax(n):
    print(""----"")
    print(""Flax"")
    start = datetime.datetime.now()
    pipe = pipeline(
        model=""distilbert-base-uncased-finetuned-sst-2-english"",
        device=0,
        framework=""flax"",
        model_kwargs={""from_pt"": True},
    )
    print(""Loading flax"", datetime.datetime.now() - start)

    for out in tqdm.tqdm(pipe(data(n), batch_size=512)):
        pass

def tf(n):
    print(""----"")
    print(""TF"")
    start = datetime.datetime.now()
    pipe =
pipeline(model=""distilbert-base-uncased-finetuned-sst-2-english"",
device=0, framework=""tf"")
    print(""Loading TF"", datetime.datetime.now() - start)

    for out in tqdm.tqdm(pipe(data(n), batch_size=512)):
        pass

def pt(n):
    print(""----"")
    print(""PT"")
    start = datetime.datetime.now()
    pipe =
pipeline(model=""distilbert-base-uncased-finetuned-sst-2-english"",
device=0, framework=""pt"")
    print(""Loading PT"", datetime.datetime.now() - start)

    for out in tqdm.tqdm(pipe(data(n), batch_size=512)):
        pass
        # print(out)

if __name__ == ""__main__"":
    n = 20000
    # pt(n)
    # flax(n)
    tf(n)
```

## Before submitting
- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).
- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#start-contributing-pull-requests),
      Pull Request section?
- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link
      to it if that's the case.
- [ ] Did you make sure to update the documentation with your changes? Here are the
      [documentation guidelines](https://github.com/huggingface/transformers/tree/master/docs), and
      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/master/docs#writing-source-documentation).
- [ ] Did you write any new necessary tests?


## Who can review?

Anyone in the community is free to review the PR once the tests have passed. Feel free to tag
members/contributors who may be interested in your PR.

<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @

 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @LysandreJik

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @n1t0, @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",8,open
[Flax] Fix eval and data_args usage in streaming example,"# What does this PR do?

This PR fixes the evaluation loop in `run_mlm_flax_stream.py`. Current behavior didn't update the correct variable, which leads to data leakage during evaluation.

It also takes the opportunity to improve some `DataTrainingArguments` usages.

-------

It's a draft PR because there is an open improvement that could be made: the script splits train-eval based solely in `data_args.{dataset_name,num_eval_samples}`, but also accepts unused args `train_file, validation_file, train_ref_file, validation_ref_file, validation_split_percentage`. Other data args that are unused: `pad_to_max_length`, `line_by_line`.

My suggestion would be to remove all these unused args. May I proceed with that?


## Before submitting
- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#start-contributing-pull-requests),  Request section?
- [x] Did you make sure to update the documentation with your changes?
    The script is mentioned in [`jax-projects/dataset-streaming/README`](https://github.com/huggingface/transformers/blob/95bab53868a91b4809bd5281a72b5f326853e31f/examples/research_projects/jax-projects/dataset-streaming/README.md#train-model), but no changes are required.


## Who can review?
@patrickvonplaten 
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
`T5ForSequenceClassification`,"# 🚀 Feature request

T5 to classify sequences by using only the encoder of T5 and a `ClassificationHead`.

## Motivation

This gives the benefits of fine-tuning a model with no maximum sequence length (useful for long sequence tasks) without having to load the decoder weights into memory/treat it as a generative task.

## Your contribution

I already have working code for this, and saw some requests for it in other forums (slack, torch, huggingface) so if it's a welcome addition I'd be happy to add it to the library.
","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",27,open
[performance/precision] adding `jit.script` to activation functions,"# 🚀 Feature request

### Switch our activation functions to use `@torch.jit.script`

Over at BigScience we have been trying to figure out mismatches between Megatron-LM and HF Transformers when it comes to inference under fp16. There are several mismatches, this one discusses activation functions. And proposes to improve HF's models based on that.

So Megatron uses `@torch.jit.script` for its activation functions, which leads to 2 things:

1. faster performance
2. more correct math under fp16 or amp/fp16. Quoting @ngimel:

> ... that’s due to fusion. Fuser does intermediate operations in fp32, and thus produces more accurate results than simple function that truncates each intermediate to half.

(I need to double check on amp/fp16 - I'm making an assumption here)

So perhaps we should switch our activation functions to use `@torch.jit.script` too?

Caveats: 

1. it appears that when using `@torch.jit.script` one may have to write out the `bwd` part explicitly, see:

https://github.com/NVIDIA/Megatron-LM/blob/b31e1296354e979722627a6c4dedafe19b51fa97/megatron/model/fused_bias_gelu.py#L27-L56

2. This will change the results slightly (back-compat OK?) but it should produce more correct results!

You can see how the 2 functions diverge:

```
import torch
import random

seed = 42
random.seed(seed)    # python RNG
torch.manual_seed(seed)          # cpu + cuda
torch.cuda.manual_seed_all(seed) # multi-gpu
torch.backends.cudnn.enabled = True

width = 128
input = torch.rand((1,5,width*4)).cuda().half()

@torch.jit.script
def gelu_megatron_fwd_jit(x):
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))

def gelu_megatron_fwd(x):
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
    
output = gelu_megatron_fwd(input)
output_jit = gelu_megatron_fwd_jit(input)
# have to run 2nd time for jit to kick in!
output_jit = gelu_megatron_fwd_jit(input)

torch.testing.assert_equal(output, output_jit, check_stride=False)    
```

gives:

```
AssertionError: Tensors are not equal!

Mismatched elements: 800 / 2560 (31.2%)
Greatest absolute difference: 0.00048828125 at (0, 0, 1)
Greatest relative difference: 0.0009828009828009828 at (0, 0, 2)
```

@patrickvonplaten, @patil-suraj, @sgugger, @LysandreJik ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
[performance doc] add tables with DDP/DP/no-DP benchmarks,"Users ask of DDP vs. DP vs. single GPU vs w/ w/o Nvlink and this summary is insufficient 
https://huggingface.co/transformers/performance.html#nvlink
luckily we have great numbers posted - so should link to those from the guide:

1. table 1 where DDP wins by miles over any other setup: https://github.com/huggingface/transformers/issues/9371#issuecomment-768656711
2. table 2 where DDP is terrible https://github.com/huggingface/transformers/issues/9371#issuecomment-771302472

","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
[RFC] Add `modeling_xxx_fusion.py` to support kernel fusion,"## Introduction
I am an engineer currently working on 3D model parallelism for transformers. When the tensor model parallelism (https://github.com/huggingface/transformers/pull/13726) is done, I am going to introduce [kernel fusion](https://stackoverflow.com/questions/53305830/cuda-how-does-kernel-fusion-improve-performance-on-memory-bound-applications-on) feature to transformers. 

![image](https://user-images.githubusercontent.com/38183241/135726581-ee305818-c78a-439f-90b4-30cd1edbc1fe.png)

For this, I want to create a new modeling file called `modeling_xxx_fusion.py`. This work is currently being discussed with @stas00 and @RezaYazdaniAminabadi (DeepSpeed team). 

## Kernel fusion API
```python
from transformers import BertForMaskedLM

# create model
model = BertForMaskedLM.from_pretrained(""bert-base-cased"")

# 1. fuse_modules 
# `fuse_modules` is function level fusion, It supports a wide variety of models.
# all arguments is `True` as default
model.fuse_modules()  

# fuse selective modules
model.fuse_modules(
    word_embedding=True,
    scale_mask_softmax=True,
    layer_norm=True,
    bias_act=True,
    bias_dropout_residual=False,
    cross_entropy=True,
)

# 2. fuse_layers 
# `fuse_layers` is block level (attention & mlp) fusion, only a few models are supported.
# argument (`inference`) is `None` -> `not self.training` of `torch.nn.Module` as default.
model.fuse_layers(inference=None)

# fuse layers for inference
model.fuse_layers(inference=True)

# fuse layers for training
model.fuse_layers(inference=False)
```

## Implementation
The internal module of each model will be re-implemented using kernel fusion method, and the existed module will be replaced with the fused module. The following example is an example of `BertOutput(nn.Module)`.

```python
# transformers/models/bert/modeling_bert.py

class BertOutput(nn.Module):
      def __init__(self, config):
            super().__init__()
            self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
            self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
            self.dropout = nn.Dropout(config.hidden_dropout_prob)

      def forward(self, hidden_states, input_tensor):
            hidden_states = self.dense(hidden_states)
            hidden_states = self.dropout(hidden_states)
            hidden_states = self.LayerNorm(hidden_states + input_tensor)
            return hidden_states
```
```python
# transformers/models/bert/modeling_bert_fusion.py

class FusedBertOutput(BertOutput):
      def forward(self, hidden_states, input_tensor):
            hidden_states = hidden_states @ self.dense.weight.t()
            hidden_states = FusedBiasDropoutResidual.apply(hidden_states, self.dense.bias, input_tensor)
            hidden_states = FusedLayerNorm.apply(hidden_states, self.LayerNorm.weight, self.LayerNorm.bias)
            return hidden_states
```
When the user calls the `fuse_modules()` method, the kernel fusion engine finds `BertOutput` and replaces it with `FusedBertOutput`. and user calls `fused_layers` method, engine finds `BertLayer` and replcases it with `FusedBertLayer`. This is the method that `parallelformers` parallelized transformers models flexibly, and the `deepspeed` also supports kernel fusion in this way. 

However, the current version of `deepspeed` fuses the entire transformer layer, so the supported models are very limited. For example, bigbird requires random attention mechanism. in this case random attention must be implemented in the custom cuda kernel. However, because the number of models is so large, it is impossible to implement them all. So I propose a flexible way to fuse the kernel on a per-function. This is a strategy of triage. The area that can be fused performs fusion, and the area that can not be fused uses the torch's default module.

```python
# kernel_fusion_utils.py

class KernelFusionMixin(object):
    
    def fuse_modules(...):
        assert self._is_able_to_fuse, ""error message""
        ... implementation ...

    def fuse_layers(...)
        assert self._is_able_to_fuse, ""error message""
        ... implementation ...
```
```python
# modeling_utils.py

class PreTrainedModel(..., KernelFusionMixin):
    _is_parallelizable = ...
    _is_able_to_fuse = False. # <--- Only models that can be fused have `True`.
```

This is a draft. The API can be changed at any time. I look forward to feedback. I'm going to show you this soon with a framework I'm making. (Like parallelformers, we will pre-open the repositories on our side and merge them later on transformers and deepspeed.)

cc. @Stas00 @RezaYazdaniAminabadi @Sylvain
 ","[{'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",11,open
How to tokenize big dataset ,"Based on examples, I am trying to train a tokenizer and a model for T5.
I use Google Colab pro,
when I tried to run the following code:

```
import datasets

from t5_tokenizer_model import SentencePieceUnigramTokenizer


vocab_size = 32_000
input_sentence_size = None # change to 100_000 works

# Initialize a dataset
dataset = datasets.load_dataset(""oscar"", name=""unshuffled_deduplicated_fa"", split=""train"")

tokenizer = SentencePieceUnigramTokenizer(unk_token=""<unk>"", eos_token=""</s>"", pad_token=""<pad>"")

print(""len dataset:"", len(dataset))

# Build an iterator over this dataset
def batch_iterator(input_sentence_size=None):
    if input_sentence_size is None:
        input_sentence_size = len(dataset)
    batch_length = 100
    for i in range(0, input_sentence_size, batch_length):
        yield dataset[i: i + batch_length][""text""]


# Train tokenizer
tokenizer.train_from_iterator(
    iterator=batch_iterator(input_sentence_size=input_sentence_size),
    vocab_size=vocab_size,
    show_progress=True,
)

# Save files to disk
tokenizer.save(""/content/drive/MyDrive/Pouramini/tokenizer.json"")
```
It get stuck in `train_from_iterator` because the size of dataset is large (`input_sentence_size` is around 8M sentences)
How can I divide the dataset and run the code on each block and then merge them to a tokenizer output?
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Feature: Tail Free Sampling,"# 🚀 Feature request

I would like to have Huggingface implement Tail Free Sampling, or TFS for short, to the official repo. The original paper for this can be found here: https://trentbrick.github.io/Tail-Free-Sampling/#tail-free-sampling-algorithm

Implementation of this code can be found here: https://github.com/finetuneanon/transformers/blob/gpt-neo-localattention3-rp-b/src/transformers/generation_logits_process.py#L243-L284

## Motivation

At the moment KoboldAI uses Finetuneanon's version of Transformers for creating stories, however, this version has several changes made to the repository that make it impossible for me to use (eg: I use an aarch64 architecture with an integrated Nvidia chip, which is only supported by Huggingface's main branch). There are calls from the KoboldAI devs to migrate due to the integrated support of GPT-J in Huggingface, however, there are some features in Finetuneanon's Transformer which are not found in the main branch. Tail Free Sampling is one such feature.

## Your contribution

I would like to file a PR with the code, however, I do lack the knowledge on how to implement the required tests and checks. An example is available on Finetuneanon's branch but it will require cherry-picking the commit to the main branch.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Update FNet Fourier Transform,"# What does this PR do?

This PR changes the Fourier Transform according to the [latest updates](https://github.com/google-research/google-research/commit/4f7815fe3aaf71b811a0fb82ac8768a4e8ab0428#diff-4a2be83f4d2b702b206d3f64b2aeb209ac7a2d65fd28382916225463b984bc2c) to the FNet repository.

I will update the configurations of the checkpoints if the changes look fine.

@patrickvonplaten @LysandreJik @sgugger @patil-suraj ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",20,open
add non auto regressive model?,"This research area is developing very fast in the past two years, which could improve latency  significantly with quality on par. Will huggingface team be interested into the implementation of it? Thanks.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Unified freezing interface,"# 🚀 Feature request

We can freeze the backbone of individual models by finding the field name of the backbone (e.g., `bert` in `BertForSequenceClassification`) and setting `.requires_grad`. However, because the name of this field differs for different models, I don't think there's currently an easy way to do this that works for all models. This contradicts the philosophy that `AutoModel(ForXXX)` should hide all the implementation details.

Ideally, we can pass a `trainable_backbone` flag in the config or to the `__init__` function directly that controls this behavior.","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",13,open
Dependency parsing head for pretrained models,"# 🚀 Feature request

Add a new classification head for pretrained models, for dependency parsing.

## Motivation

Current heads, such as `AutoModelForTokenClassification`, does not work well for finetuning a pretrained model for dependency parsing. [There are now such heads available](https://openreview.net/pdf?id=Hk95PK9le), so adding such would make it a lot easier to finetune models for such tasks. A `PyTorch` implementation of that paper (and others) can be found [in this repo](https://github.com/yzhangcs/parser).

## Your contribution

I could assist with a PR, but for now I'd like to start the discussion to see if this is something that the HF team and others would be interested in.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",8,open
[model loading] framework-agnostic dtype parameter,"This is a split off from one of the discussions at https://github.com/huggingface/transformers/pull/13209:

1. It all started with trying to load torch models under either the desired dtype or the the dtype of the pretrained model - and thus avoid 2x memory usage needs e.g. if the model needs to be just fp16. So we added `torch_dtype` to `from_pretrained` and `from_config`.
2. Then we started storing `torch_dtype` in the config file for future possibly automatic loading model in the optimal ""regime"".
3. This resulted in a discrepancy where the same symbol sometimes means `torch.dtype` at other times a string like ""float32"" as we can't store `torch.dtype` in json.
4. then in https://github.com/huggingface/transformers/pull/13209#discussion_r693292542 we started discussing how `dtype` is really the same across pt/tf/flux and perhaps we should just use `dtype` in the config and variables and have it consistently to be a string (""float32"") and convert it to the right dtype object of the desired framework at the point of use, e.g. `getattr(torch, ""float32"")`

A possible solution is to deprecate `torch_dtype` and replace it with `dtype` string both in config and in the function argument.

Possible conflicts with the naming:

1. we already have the `dtype` attribute in modeling_utils, which returns `torch.dtype` based on the first param's dtype.

   https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_utils.py#L205

   The context is different, but still this is something to consider to avoid ambiguity.

I may have missed some other areas. So please share if something else needs to be added.

Additional notes:
- wrt flux: https://github.com/huggingface/transformers/pull/13209#discussion_r694511759
>  #13098 - the idea of the PR is exactly to disentangle parameter dtype from matmul/computation dtype. In Flax, it's common practice that the dtype parameter defines the matmul/computation dtype, see: https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.Dense.html#flax.linen.Dense.dtype instead of the parameter dtype and not the parameter dtype.
> So for Flax, I don't really think it would make sense to use a config.dtype to define weights dtype as it would be quite confusing with Flax's computation dtype parameter.


@LysandreJik, @sgugger, @patrickvonplaten ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Tapas tokenization Different from Tensorflow Code,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.9.1
### Who can help
@LysandreJik @sgugger @NielsRogge

## Information

Model I am using (Bert, XLNet ...): Tapas

When I am trying to replicate the TAPAS table retrieval results using Huggingface Tapas implementation, I find that [Tapas tokenization in Huggingface](https://github.com/huggingface/transformers/blob/master/src/transformers/models/tapas/tokenization_tapas.py#L1314) is different from the original [Tensorflow code ](https://github.com/google-research/tapas/blob/master/tapas/utils/tf_example_utils.py#L391). The original code first checks whether the table cell is ""n/a"", ""?"" or empty. If so, it would return ""[EMPTY]"" token. The Huggingface code has implemented [the same tokenization](https://github.com/huggingface/transformers/blob/master/src/transformers/models/tapas/tokenization_tapas.py#L370) with the tensorflow code, but it is not used to tokenize the tables. It could be easily fixed by changing all the calls of function `self.tokenize` to `self._tokenize` in the `_tokenize_table` function. After fixing this, I could use the released table retrieval model to replicate their results on NQ dataset with Huggingface Tapas.

","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",10,open
"""Resource exhausted"" when loading Flax GPT-Neo 2.7B","## Environment info

- `transformers` version: 4.10.0.dev0
- Platform: Linux-5.4.0-1043-gcp-x86_64-with-glibc2.29
- Python version: 3.8.5
- PyTorch version (GPU?): 1.8.1+cu102 (False)
- Tensorflow version (GPU?): 2.6.0 (False)
- Flax version (CPU?/GPU?/TPU?): 0.3.4 (cpu)
- Jax version: 0.2.19
- JaxLib version: 0.1.70
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help
@patrickvonplaten @patil-suraj @LysandreJik

## Information

I am not able to load the Flax GPT-Neo 2.7B model in my TPU VM v3-8 instance. 
```python
tokenizer = AutoTokenizer.from_pretrained(""EleutherAI/gpt-neo-2.7B"", pad_token=""</s>"", padding_side=""left"")
model = FlaxAutoModelForCausalLM.from_pretrained(""EleutherAI/gpt-neo-2.7B"", pad_token_id=tokenizer.eos_token_id)
```
The model will download but will fail to load with
```
RuntimeError: Resource exhausted: Failed to allocate request for 100.00MiB (104857600B) on device ordinal 0
```
However, the pytorch version will load and run just fine.
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",8,open
Advice needed: Adding more FSMT models,"# 🌟 New model addition

## Model description

I am planning to contribute a series of FSMT models to the model hub. The models have been trained for a paper that is currently under review.

Before working on a PR I wanted to ask for some advice:

### normalize_before
The new models have been trained with Fairseq's option `normalize_before=True`, while the existing FSMT implementation uses `normalize_before=False`. I understand that copy-pasting model code is preferred to extending the configuration. This would mean that a near-duplicate module `fsmt_prenorm` needs to be created. Is this correct?

### Adequate base branch
The FSMT module is currently being refactored (https://github.com/huggingface/transformers/pull/11218). Do you recommend that I start from the master branch or from the PR's feature branch, which is nearly completed?","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Support OpenNMT models,It will be great if OpenNMT (https://opennmt.net/) and CTranslate2 (https://github.com/OpenNMT/CTranslate2) model support is provided out of the box. ,"[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Problem about using mBART50 for Russian to Chinese translation,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.9.1
- Platform: ubuntu 18.04 
- Python version: 3.6.9. 
- PyTorch version (GPU?): 1.8.0 
- Tensorflow version (GPU?): 2.3.0 
- Using GPU in script?: yes
- Using distributed or parallel set-up in script?: no 

### Who can help

mbart-large-50-many-to-many-mmt：@LysandreJik @patrickvonplaten  

## Information

Model I am using: mbart-large-50-many-to-many-mmt

The problem arises when using:  
  
* my own modified scripts: (give details below)  

We originally wanted to do a Russian-Chinese translation task, but our translation results showed a lot of English. We used a script to test.

## To reproduce

Steps to reproduce the behavior:

1.The code is as follow:  
```python
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

text_list = ['Это позволит облегчить транспортировку грузов для Китая и Германии.',
             'Россия останется одним из лидеров, возможности для наращивания экспорта есть.',
             'Это позволит оптимизировать торговые отношения.']

src_lang = 'ru_RU'
tgt_lang = 'zh_CN'
model = MBartForConditionalGeneration.from_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")
tokenizer = MBart50TokenizerFast.from_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")


for text in text_list:
    tokenizer.src_lang = src_lang
    encoded_hi = tokenizer(text, return_tensors=""pt"")
    generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])
    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    print(translated)
```  
when the src_lang was ‘ru_RU’ and the tgt_lang was ‘zh_CN’, the results were: 
```
['This will facilitate the transport of goods for China and Germany.']
['Russia will remain one of the leaders, there are opportunities to increase export.']
['This will allow to optimize trade relations.']
```


<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
We wanted to obtain a set of Chinese translations. Here are the Chinese translations for reference.
```
['这将使中国和德国更容易运输货物。']
['俄罗斯仍将是一个领导者，有机会增加出口。']
['这将有助于改善贸易关系。']
```

","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",9,open
Weird behavior with mBART-50 and Spanish,"## Environment info
- `transformers` version: 4.9.1
- Platform: Linux-5.4.0-1054-aws-x86_64-with-debian-buster-sid
- Python version: 3.7.10
- PyTorch version (GPU?): 1.9.0+cu102 (True)

## Who can help
@patrickvonplaten

## Information
I am seeing weird behavior with mBART-50 and Spanish. Please look at the code below:
```
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

text = ""http://www.ted.com/talks/stephen_palumbi_following_the_mercury_trail.html""

model = MBartForConditionalGeneration.from_pretrained(""facebook/mbart-large-50-many-to-one-mmt"")
tokenizer = MBart50TokenizerFast.from_pretrained(""facebook/mbart-large-50-many-to-one-mmt"")
tokenizer.src_lang = ""es_XX""

encoded = tokenizer(text, return_tensors=""pt"")
generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer.lang_code_to_id[""en_XX""])
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```
The output is:
```
['(b) To continue to cooperate closely with the Special Rapporteur on extrajudicial, summary or arbitrary executions, the Special Rapporteur on torture and other cruel, inhuman or degrading treatment or punishment, the Special Rapporteur on the sale of children, child prostitution and child pornography, the Special Rapporteur on torture and other cruel, inhuman or degrading treatment or punishment, the Special Rapporteur on the sale of children, child prostitution and child pornography, the Special Rapporteur on the sale of children, child prostitution and child pornography, the Special Rapporteur on the sale of children, child prostitution and child pornography, the Special Rapporteur on violence against women, its causes and consequences, the Special Rapporteur on the sale of children, child prostitution and child pornography, the Special Rapporteur on the sale of children, child prostitution and child pornography, the Special']
```

However if I change the source language to french `tokenizer.src_lang = ""fr_XX""` or any other language, I get the following output (which is what you expect):
```
['http://www.ted.com/talks/stephen_palumbi_following_the_mercury_trail.html']
```

This behavior is similar with other texts as well (e.g., ""888""). Do you know why this behavior is unique to Spanish? Also, do you have any idea how to correct this behavior?

Thanks!
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
Truncating the prefix of a sequence rather than the suffix,"# 🚀 Feature request

Hi, tokenizers get `truncation` as an argument. When set to `True` the tokenizer will truncate the suffix of a sequence so it does not surpass the specified `max_length`. I'd like to have a functionality that truncates the prefix of the sequence, so the model will see the suffix of the sequence.

## Motivation

In many applications (e.g. Dialog, and QA) the most important part of the sequence is the suffix (e.g. the question after the context, or the last response of the dialog).

## Your contribution

Perhaps I'll submit a PR, but it might take me some time as I'm close to some deadlines of mine :(
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",11,open
BatchFeature should cast to `np.float32` by default,"Currently the default dtype for Speech Feature Extractors is `numpy.float64` which leads to two problems:

1) It makes the data processing extremely expensive for the RAM. Many sound formats are stored in int16 (such as `.wav`) and are then transformed to float64 which unnecessarly increases RAM by a factor of 4. We should at least stick to `float32`
2) Currently we have added some hacks to the Wav2Vec2 and Speech2TextTransformer feature extractors to prevent Double vs. Float dtype mismatches: https://github.com/huggingface/transformers/blob/f6e254474cb4f90f8a168a599b9aaf3544c37890/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L87

The main problem is that `np.asarray([....])` by default creates a np.float64 array and that we just pass that format along.
=> We should either always cast to float32 in BatchFeature (see here: https://github.com/huggingface/transformers/blob/f6e254474cb4f90f8a168a599b9aaf3544c37890/src/transformers/feature_extraction_utils.py#L151) or add a flag `dtype` to BatchFeature.

@patrickvonplaten ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
Got `ONNXRuntimeError` when try to run BART in ONNX format,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.9.0
- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.11
- PyTorch version (GPU?): 1.9.0+cu102 (True)
- Using GPU in script?: Yes

### Who can help
@mfuntowicz

<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.



Models:

- 

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

Model hub:

- for issues with a model report at https://discuss.huggingface.co/ and tag the model's creator.

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## To reproduce

I was using Google Colab and trying to export model `facebook/bart-large-cnn` to the onnx format. I ran the command `python -m transformers.onnx -m=facebook/bart-large-cnn onnx/bart-large-cnn`, and the outputs seem okay.

```
2021-07-22 23:14:33.821472: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
Using framework PyTorch: 1.9.0+cu102
Overriding 1 configuration item(s)
	- use_cache -> False
/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py:212: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):
/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py:218: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if attention_mask.size() != (bsz, 1, tgt_len, src_len):
/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py:249: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py:863: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if input_shape[-1] > 1:
tcmalloc: large alloc 1625399296 bytes == 0x5595ce83a000 @  0x7f1780d9f887 0x7f177f695c29 0x7f177f696afb 0x7f177f696bb4 0x7f177f696f9c 0x7f17670dcbb7 0x7f17670dd064 0x7f175b75ba1c 0x7f176bf8eaff 0x7f176b949b88 0x55949fda8bf8 0x55949fe1c6f2 0x55949fe16c35 0x55949fda973a 0x55949fe1893b 0x55949fe16c35 0x55949fda973a 0x55949fe1bf40 0x55949fe16c35 0x55949fda973a 0x55949fe1893b 0x55949fda965a 0x55949fe17b0e 0x55949fda965a 0x55949fe17b0e 0x55949fe16c35 0x55949fe16933 0x55949fe14da0 0x55949fda7ea9 0x55949fda7da0 0x55949fe1bbb3
tcmalloc: large alloc 1625399296 bytes == 0x55962f654000 @  0x7f1780d9f887 0x7f177f695c29 0x7f177f696afb 0x7f177f696bb4 0x7f177f696f9c 0x7f17670dcbb7 0x7f17670dd064 0x7f175b75ba1c 0x7f176bf8ecab 0x7f176b949b88 0x55949fda8bf8 0x55949fe1c6f2 0x55949fe16c35 0x55949fda973a 0x55949fe1893b 0x55949fe16c35 0x55949fda973a 0x55949fe1bf40 0x55949fe16c35 0x55949fda973a 0x55949fe1893b 0x55949fda965a 0x55949fe17b0e 0x55949fda965a 0x55949fe17b0e 0x55949fe16c35 0x55949fe16933 0x55949fe14da0 0x55949fda7ea9 0x55949fda7da0 0x55949fe1bbb3
tcmalloc: large alloc 1625399296 bytes == 0x5595ce83a000 @  0x7f1780d9d1e7 0x55949fdd9a18 0x55949fda4987 0x7f176bf8ece2 0x7f176b949b88 0x55949fda8bf8 0x55949fe1c6f2 0x55949fe16c35 0x55949fda973a 0x55949fe1893b 0x55949fe16c35 0x55949fda973a 0x55949fe1bf40 0x55949fe16c35 0x55949fda973a 0x55949fe1893b 0x55949fda965a 0x55949fe17b0e 0x55949fda965a 0x55949fe17b0e 0x55949fe16c35 0x55949fe16933 0x55949fe14da0 0x55949fda7ea9 0x55949fda7da0 0x55949fe1bbb3 0x55949fe16c35 0x55949fda973a 0x55949fe17b0e 0x55949fe16c35 0x55949fce8eb1
tcmalloc: large alloc 1625399296 bytes == 0x55962f654000 @  0x7f1780d9f887 0x7f177f695c29 0x7f177f695d47 0x7f177f6977a5 0x7f176bd60368 0x7f176bfbc844 0x7f176b949b88 0x55949fda8010 0x55949fda7da0 0x55949fe1bbb3 0x55949fe16c35 0x55949fda973a 0x55949fe1893b 0x55949fe16c35 0x55949fda973a 0x55949fe1bf40 0x55949fe16c35 0x55949fda973a 0x55949fe1893b 0x55949fda965a 0x55949fe17b0e 0x55949fda965a 0x55949fe17b0e 0x55949fe16c35 0x55949fe16933 0x55949fe14da0 0x55949fda7ea9 0x55949fda7da0 0x55949fe1bbb3 0x55949fe16c35 0x55949fda973a
Validating ONNX model...
	-[✓] ONNX model outputs' name match reference model ({'last_hidden_state', 'encoder_last_hidden_state'}
	- Validating ONNX Model output ""last_hidden_state"":
		-[✓] (2, 8, 1024) matchs (2, 8, 1024)
		-[✓] all values close (atol: 0.0001)
	- Validating ONNX Model output ""encoder_last_hidden_state"":
		-[✓] (2, 8, 1024) matchs (2, 8, 1024)
		-[✓] all values close (atol: 0.0001)
All good, model saved at: onnx/bart-large-cnn/model.onnx
```
Then I tried to execute the model in `onnxruntime`,

```
import onnxruntime as ort

ort_session = ort.InferenceSession('onnx/bart-large-cnn/model.onnx')

# Got input_ids and attention_mask using tokenizer

outputs = ort_session.run(None, {'input_ids': input_ids.detach().cpu().numpy(), 'attention_mask': attention_mask.detach().cpu().numpy()})
```
And I got the error,

```
---------------------------------------------------------------------------
RuntimeException                          Traceback (most recent call last)
<ipython-input-30-380e6a0e1085> in <module>()
----> 1 outputs = ort_session.run(None, {'input_ids': input_ids.detach().cpu().numpy(), 'attention_mask': attention_mask.detach().cpu().numpy()})

/usr/local/lib/python3.7/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py in run(self, output_names, input_feed, run_options)
    186             output_names = [output.name for output in self._outputs_meta]
    187         try:
--> 188             return self._sess.run(output_names, input_feed, run_options)
    189         except C.EPFail as err:
    190             if self._enable_fallback:

RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_109' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:42 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, std::vector<long int>&, bool) gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{2}, requested shape:{1,1}
```

I see that BART is recently supported for ONNX in the latest release, but there isn't any code to exactly explain how to run the inference in `onnxruntime`. Maybe I'm doing something wrong here, so any help will be appreciated!


<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",9,open
Model Request: Blenderbot 2.0,"# 🌟 New model addition

## Model description

Facebook released Blenderbot 2.0, a chatbot that builds on RAG and Blenderbot 1.0. It can save interactions for later reference and use web search to find information on the web.

https://parl.ai/projects/blenderbot2/

## Open source status

* [x] the model implementation is available: in Parl.ai
* [x] the model weights are available: https://parl.ai/docs/zoo.html#wizard-of-internet-models
* [ ] who are the authors: (mention them, if possible by @gh-username)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",29,open
Feature Request: El-Attention,"# 🚀 Feature request
I've looked into the paper titled ""[EL-Attention: Memory Efficient Lossless Attention for Generation](https://arxiv.org/abs/2105.04779)"".
It proposes a method for calculating attention that forgoes creating multi-head attention from the hidden state. This saves computational time and frees memory. 

## Motivation
El-attention seems to have no downsides, and promises significant memory and performance gains during training and inference. 

## Your contribution
The main difficulty may be in that it requires being added directly in to each model's attention mechanism code, or requires a ton of new subclasses for each part of each model. Maybe an easier solution to this would be a pipeline to use custom  attention mechanism code. ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Alphafold 2.0,"# 🌟 New model addition

It would be so amazing to have Alphafold model in huggingface. 😍
I don't know if there is any plan to add these kind of models to huggingface repo.

<!-- Important information -->
## Model description


## Open source status

* [x] the model implementation is available: ([github](https://github.com/deepmind/alphafold))
* [] the model weights are available: ([github](https://github.com/deepmind/alphafold))
* [x] who are the authors: (@deepmind)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Inconsistency between the tokenization of `CLIPTokenizer` and `CLIPTokenizerFast` with `openai/clip-vit-base-patch32`,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.8.2
- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.10
- PyTorch version (GPU?): 1.9.0+cu102 (False)
- Tensorflow version (GPU?): 2.5.0 (False)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help
@patil-suraj, I think you worked on CLIP, maybe you could help me by confirming that this behavior is not normal. If it is and no one can deal with it first, I'd be happy to try to fix it.
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @Rocketknight1

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

Model hub:

- for issues with a model report at https://discuss.huggingface.co/ and tag the model's creator.

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...): CLIP

## To reproduce

The easiest way to reproduce is to open [this google colab](https://colab.research.google.com/drive/1JzlYtuG4MdAKl8lPI5PkqGcYbdM3N24x?usp=sharing)

Steps to reproduce the behavior:

1. Import the slow and fast CLIP tokenizers from the transformers library and eventualy the tokenizer of https://github.com/openai/CLIP
```
from transformers import CLIPTokenizer, CLIPTokenizerFast
tokenizer_slow = CLIPTokenizer.from_pretrained(""openai/clip-vit-base-patch32"")
tokenizer_fast = CLIPTokenizerFast.from_pretrained(""openai/clip-vit-base-patch32"")
```
```
from CLIP import clip as clip_orig
```
2. Tokenize the same text with the 3 tokenizers
```
text = ""A photo of a cat""
context_length = 77
```
```
tokens_ids_orig = clip_orig.tokenize(text)
tokens_ids_slow = tokenizer_slow.encode(text, padding=""max_length"", max_length=context_length, return_tensors='pt')
tokens_ids_fast = tokenizer_fast.encode(text, padding=""max_length"", max_length=context_length, return_tensors='pt')
```
3. Compare the outputs
```
(tokens_ids_orig == tokens_ids_slow).sum() == context_length
```
Output: `True`
```
(tokens_ids_orig == tokens_ids_fast).sum() == context_length
```

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

I think I would have expected the slow and fast versions to tokenize the text in the same way. 

<!-- A clear and concise description of what you would expect to happen. -->
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",8,open
`TestMarian_MT_EN::test_batch_generation_mt_en` Failing due to randomly generated tokens,"The test fails with the following:

```
_________________ TestMarian_MT_EN.test_batch_generation_mt_en _________________
[gw0] linux -- Python 3.6.9 /usr/local/bin/python

self = <tests.test_modeling_tf_marian.TestMarian_MT_EN testMethod=test_batch_generation_mt_en>

    @slow
    def test_batch_generation_mt_en(self):
>       self._assert_generated_batch_equal_expected()

tests/test_modeling_tf_marian.py:390: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_modeling_tf_marian.py:366: in _assert_generated_batch_equal_expected
    self.assertListEqual(self.expected_text, generated_words)
E   AssertionError: Lists differ: ['Tou[19 chars] healed a man who was affected by the sad disease of leprosy.'] != ['Tou[19 chars] healed a man who was affected by▁kifkażUnjonik ill.']
E   
E   First differing element 0:
E   'Touc[17 chars]s healed a man who was affected by the sad disease of leprosy.'
E   'Touc[17 chars]s healed a man who was affected by▁kifkażUnjonik ill.'
E   
E   - ['Touching gently, Jesus healed a man who was affected by the sad disease of '
E   ?                                                          ^^^^^^ ^^^ ^^^^^^^^^
E   
E   + ['Touching gently, Jesus healed a man who was affected by▁kifkażUnjonik ill.']
E   ?                                                          ^^^^^ ^^^^^^ ^^^^^^ +
E   
E   -  'leprosy.']
```","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",6,open
Long-Short Transformer,"# 🌟 New model addition

## Model description

https://arxiv.org/abs/2107.02192

In this paper, they propose Long-Short Transformer, an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity.

## Open source status

* [x] the model implementation is available: https://github.com/NVIDIA/transformer-ls
* [x] the model weights are available: https://github.com/NVIDIA/transformer-ls
* [x] who are the authors: Chen Zhu (@zhuchen03) and Wei Ping and Chaowei Xiao and Mohammad Shoeybi and Tom Goldstein and Anima Anandkumar and Bryan Catanzaro (NVIDIA, University of Maryland)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
GPTNeo Error Attempting to Generate Text,"## Environment info

- `transformers` version: 4.9.0.dev0
- Platform: Linux-5.4.0-1043-gcp-x86_64-with-glibc2.29
- Python version: 3.8.10
- PyTorch version (GPU?): 1.9.0+cpu (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): 0.3.4 (tpu)
- Jax version: 0.2.16
- JaxLib version: 0.1.68
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no
- Using TPU: Yes

### Who can help
@patil-suraj and @patrickvonplaten

Models:
GPTNeo

Library:

- flax transformers

 -->

## Information

Model I am using (Bert, XLNet ...): GPTNeo


## To reproduce

Steps to reproduce the behavior:

Here is a Google Colab for reproducing: https://colab.research.google.com/drive/1tba52h5t-BP3g13FMdPXVjKqpoLTlGvP?usp=sharing

For convenience here is the error msg:
```
TypeError: dynamic_update_slice update shape must be smaller than operand shape, got update shape (1, 45) for operand shape (1, 20).
```

I was originally getting the same error as #12081. However, when I attempted to implement the same fix as in that issue, I got the above error. The error might be because I am using the ""ForCausalLM"" version of GPTNeo. However, there is no LMHead version

## Expected behavior
Generate the output sequence
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Add Flax Models to Pipelines,"# 🚀 Feature request

Hi y'all, I am trying a GPTNeo Flax model and am wanting to use it in the text generation pipeline. However, it is currently not supported. From looking at the current implementation of flax models and the text generation pipeline, it should be a relatively easy (famous last words) addition.

## Motivation

HF is heavily integrating Flax models (which I think is awesome!) into the library and has parroted many of the already existing parts of the transformers library to flax models, similar to what was done for TF models. The addition of the support of Flax models in the pipeline API will help those who are working with pure Flax models, especially for applications that will be use the model to accomplish some task.

## Your contribution

I would be willing to open a PR if one is not currently underway (I looked for one and didn't find any). However, I am new to flax so if the task is more difficult than I expect I probably will be not able to complete it.
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
[Flax] from_pretrained does not consider the passed dtype,"## Environment info

When loading a flax model with `from_pretrained` the type argument is not used. The weights are initialized with the dtype of saved weights.

So if you do 
```python
model = FlaxGPT2ForCausalLM.from_pretrained(""gpt2"", dtype=jnp.dtype(""bfloat16""))

# check the dtype of one of the params 
model.params[""transformer""][""wpe""][""embedding""].dtype
=> dtype(""float32"")
```

We should probably cast the weights to `self.dtype`.

As a workaround for `bf16`,  one could manually cast the weighs with
```
def to_bf16(t):
    return jax.tree_map(lambda x: x.astype(jnp.bfloat16) if x.dtype == jnp.float32 else x, t)

model.params = to_bf16(model.params)
```

cc @patrickvonplaten ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",7,open
Extractive summarization pipeline,"# 🚀 Feature request

An extractive summarization pipeline similar to the one for abstractive summarization. 

A central place for researchers to upload new models for others to use, without having to run the code from various git repo's. 

Currently, extractive summarization is the only safe choice for producing textual summaries in practices. Therefore, it seems relevant for Huggingface to include a pipeline for this task. 

This has previously been brought up here:  https://github.com/huggingface/transformers/issues/4332, but the issue remains closed which is unfortunate, as I think it would be a great feature. 

## Motivation

The current abstractive summarization pipeline is certainly very useful, and a great feature for all working on NLG tasks. 

However, given the significant problems with factual consistency in asbtractive summaries (see in example: https://arxiv.org/abs/2104.13346, https://arxiv.org/abs/2104.14839), abstractive summaries are still very risky to use in practice, as even the state of the art models are riddled with factual errors. 

Any thoughts on this? :) 
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
[Wav2Vec2] Better names for internal classes,"Wav2Vec2's classes have too many names, *e.g.*: FlaxWav2Vec2EncoderLayerStableLayerNormCollection. 

We should make those names easier (reminder for myself @patrickvonplaten to do this)","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
"Expand text-generation pipeline support for other causal models e.g., BigBirdForCausalLM","# 🚀 Feature request

Tried using the text generation pipeline (TextGenerationPipeline) with BigBirdForCausalLM but seems like the pipeline currently only supports a limited number of models. Is there a reason for this? Is there a workaround short of implementing the pipeline myself? Thank you.
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
🌟 New model addition: FNet,"# 🌟 New model addition: FNet

FNet is a highly efficient Transformer-like encoder architecture, wherein the self-attention sublayers have been wholly replaced by standard, unparameterized Fourier Transforms.


I would like to help adding this!

## Open source status

* [x] the model implementation is available: https://github.com/google-research/google-research/tree/master/f_net
* [x] the model weights are available: https://github.com/google-research/google-research/tree/master/f_net
* [x] who are the authors: (@ilyaeck @santiontanon) (Not sure, googled the authors' name + github, sorry if it's incorrect)","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
New Model: Charformer: Fast Character Transformers via Gradient-based Subword Tokenization ,"# 🌟 New model addition

## Model description

arXiv = https://arxiv.org/pdf/2106.12672.pdf (pre-print; under review)

In this paper, they introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. More importantly, is the introduction of Charformer, a deep Transformer model that integrates GBST and operates on the byte level. 

> Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that Charformer outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, Charformer is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28%-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end.

## Open source status

* [ ] the model implementation is available: ----] [Implementation and weights](https://github.com/google-research/google-research/charformer)
* [ ] the model weights are available: ------------] [to be released soon here](https://github.com/google-research/google-research/charformer)
* [x] who are the authors: Yi Tay, Vinh Q. Tran, Sebastian Ruder*, Jai Gupta, Hyung Won Chung, Dara BahriZhen Qin, Simon Baumgartner, Cong Yu, Donald Metzler (Google and DeepMind-->`*`)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
A model or a config like 'transformer_iwslt_de_en' for machine translation,"# 🌟 New model addition

Does huggingface have some models like `transformer_iwslt_de_en` or `transformer_wmt_en_de`  in fairseq for machine translation? 

I plan to write a model for machine translation on huggingface. It would be great to be able to compare directly with the baseline model on huggingface.

@patil-suraj ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",3,open
Request: New LM Adapted checkpoints for T5,"# 🌟 New LM Adapted checkpoints for T5

## Description

Google released a new set of checkpoints for T5 v1.1. here:
https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511

Especially interesting for most people will be the checkpoints below, as finetuning T5 with a prompt or using T5 for conditional text generation is quite common and these checkpoints promise better performance. The default T5 v1.1 checkpoints have never seen sequences without sentinel tokens.

### LM-Adapted: t5.1.1.lm100k (copied from the readme)

These ""LM adapted"" models are initialized from t5.1.1 (above) and train for an
additional 100K steps on the LM objective discussed in the [T5 paper][paper].
This adaptation improves the ability of the model to be used for [prompt
tuning](https://arxiv.org/abs/2104.08691).

* **t5.1.1.lm100k.small** (~77 million parameters): [gs://t5-data/pretrained_models/t5.1.1.lm100k.small](https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/t5.1.1.lm100k.small/)
* **t5.1.1.lm100k.base** (~250 million parameters): [gs://t5-data/pretrained_models/t5.1.1.lm100k.base](https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/t5.1.1.lm100k.base/)
* **t5.1.1.lm100k.large** (~800 million parameters): [gs://t5-data/pretrained_models/t5.1.1.lm100k.large](https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/t5.1.1.lm100k.large/)
* **t5.1.1.lm100k.xl** (~3 billion parameters): [gs://t5-data/pretrained_models/t5.1.1.lm100k.xl](https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/t5.1.1.lm100k.xl/)
* **t5.1.1.lm100k.xxl** (~11 billion parameters): [gs://t5-data/pretrained_models/t5.1.1.lm100k.xxl](https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/t5.1.1.lm100k.xxl/)

## Open source status

* [x] the model implementation is available: t5 v1.1. with geglu
* [x] the model weights are available: see links above
* [x] who are the authors: Brian Lester, Rami Al-Rfou, Noah Constant
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
A fast tokenizer for BertJapaneseTokenizer,"We would like a fast tokenizer for BertJapaneseTokenizer. This is because the current token classification model (run_ner.py) requires using the fast tokenizer but BertJapaneseTokenizer does not have it. Because of this, we cannot do token classification for Japanese using cl-tohoku's BERT models.
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Generate text until condition,"Is there a simple way to have the model generate text until a condition is met? I'm interested in data memorization and want to prompt the model with some tokens from the training data and then have it generate text until it makes a mistake (aka deviates from the training data). The naive approach with a while loop has significant overhead, and I was wondering if there was something smarter I can be doing.","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
[Deepspeed] [performance] inefficient load with `from_pretrained` w/ zero3,"# 🚀 Feature request

Currently under Deepspeed stage3 with `from_pretrained` we:

a. loop over each sub-module in zero.Init
1. init the sub-module
2. shard and scatter the shards

b. then to load pre-trained weights we loop over each sub-module:
1. gather the shards
2. `load_state_dict` for the one layer layer
3. shard and scatter the shards

c. any sub-module params that weren't in the pretrained state_dict
1. run the postponed `module_init` as it was done in https://github.com/huggingface/transformers/pull/11471
2. shard and scatter the shards XXX: I actually don't think `deepspeed.zero.GatheredParameters` was handled here. so these params don't get ZeRO'ed - need to fix that https://github.com/huggingface/transformers/issues/12272

Because we unnecessarily do scatter/gather/scatter, this takes much longer than just:

a. init the modules w/o allocating any storage as it has been implemented in pt-1.9.0/1.9.1 https://pytorch.org/tutorials/prototype/skip_param_init.html#implementation-details

b. for each sub-module with pretrained weights
1. load_state_dict 
2. shard and scatter the shards

c. any sub-module params that weren't in the pretrained state_dict
1. materialize and module_init
2. shard and scatter the shards

Solving this will most likely require support from Deepspeed, https://github.com/microsoft/DeepSpeed/issues/1142 or perhaps we can just try to remove `zero.Init` if the weights aren't materialized during model creation. So the very first sharding will get postponed to the `load_state_dict` stage (and `module_init` for the sub-modules that don't have pre-trained weights).","[{'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
[Deepspeed zero3] lazy weights init ,"I'm pretty sure we need to follow up to the lazy weights init feature https://github.com/huggingface/transformers/pull/11471
and add under zero3 `deepspeed.zero.GatheredParameters` here (or inside `_init_weights`):

https://github.com/huggingface/transformers/pull/11471/files#diff-6b72b98c4c2dcfc6cc606843917733f5d858374fbc22a735ff483bbc0c1e63eaR1275-R1276

plus need a test.","[{'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",0,open
Add TFSpeech2Text,"# 🚀 Feature request
Add TensorFlow implementation of Speech2Text model.

## Your contribution

I'll try to do this.

**Reviewers:** @patil-suraj ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Modify BERT encoder layers?,"Hello I would like to modify the encoder layers of the BERT model, to insert FC and ReLu layers.

This idea allows you to reproduce the use of [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)

How to use an nn.module class to handle encoder outputs?

Example:

```
import torch.nn as nn
from transformers import BertModel

class CustomBERTModel(nn.Module):
    def __init__(self):
          super(CustomBERTModel, self).__init__()
          self.bert = BertModel.from_pretrained(""bert-base-uncased"")
          # add your additional layers here, for example a dropout layer followed by a linear classification head
          self.dropout = nn.Dropout(0.3)
          self.out = nn.Linear(768, 2)

    def forward(self, ids, mask, token_type_ids):
          sequence_output, pooled_output = self.bert(
               ids, 
               attention_mask=mask,
               token_type_ids=token_type_ids
          )

          # we apply dropout to the sequence output, tensor has shape (batch_size, sequence_length, 768)
          sequence_output = self.dropout(sequence_output)
    
          # next, we apply the linear layer. The linear layer (which applies a linear transformation)
          # takes as input the hidden states of all tokens (so seq_len times a vector of size 768, each corresponding to
          # a single token in the input sequence) and outputs 2 numbers (scores, or logits) for every token
          # so the logits are of shape (batch_size, sequence_length, 2)
          logits = self.out(sequence_output)

          return logits
```","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Clearer indication for overridden method in generation,"The expectation for the `prepare_inputs_for_generation` function to be overridden can be made clearer by changing https://github.com/huggingface/transformers/blob/700cee344691afc41f68aa18fedea463b22f95f1/src/transformers/generation_utils.py#L369-L374

to raise a `NotImplementedError` that provides the information mentioned in the function's comment.

@patrickvonplaten ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Better documentation for generation parameter defaults,"# Generation default params documentation

It's very hard to follow how the generation parameters are set when running generation. When looking at the official function: https://github.com/huggingface/transformers/blob/700cee344691afc41f68aa18fedea463b22f95f1/src/transformers/generation_utils.py#L644 all parameters default to `None`, but are then later overwritten by the config's default parameters, *e.g.* here: https://github.com/huggingface/transformers/blob/700cee344691afc41f68aa18fedea463b22f95f1/src/transformers/generation_utils.py#L878 . This is very hard to trace or follow. We should at least put a warning or note that clearly states that all generation parameters (and actually all forward) parameters **always** default to the config.

What do you think @LysandreJik @patil-suraj @sgugger ? 

If you agree, I'll open a PR for it :-) ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
KeyError: 'labels' during Distilling Zero Shot Classification,"EDIT: I confirmed that this happens with the example script as it is, so no other changes are required to reproduce this.

## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.6.1
- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.10
- PyTorch version (GPU?): 1.8.1+cu101 (True)
- Tensorflow version (GPU?): 2.5.0 (True)
- Using GPU in script?: Yes (NVIDIA P100)
- Using distributed or parallel set-up in script?: No


### Who can help

Tagging @VictorSanh @sgugger, @patil-suraj (please correct me if I'm wrong)

<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @Rocketknight1

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

Model hub:

- for issues with a model report at https://discuss.huggingface.co/ and tag the model's creator.

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...):

Student: `distilbert-base-uncased`
Teacher: `roberta-large-mnli`

The problem arises when using:
* [x] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [ ] my own task or dataset: (give details below)
* [x] other: distillation of zero shot text classification models (research_projects)

I'm simply running the official colab script `Distilling Zero Shot Classification.ipynb`, but get a key error when performing the first epoch of the student training.

## To reproduce

Steps to reproduce the behavior:

1. Open the official script https://t.co/JAJ6Eb78vM?amp=1 (you can find this link here as well https://twitter.com/joeddav/status/1363543296166002688?lang=en)
2. Run all the required cells before training
3. Run the cell that runs `transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py`
4. Witness the `KeyError: 'labels'` on the first epoch of the student model training

Full logs:

`2021-06-16 15:33:19.328924: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
06/16/2021 15:33:20 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
06/16/2021 15:33:20 - INFO - __main__ -   Training/evaluation parameters DistillTrainingArguments(output_dir='./distilbert-base-uncased-agnews-student', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=128, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Jun16_15-33-20_9d2a3f891a99', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=0, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='./distilbert-base-uncased-agnews-student', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='')
06/16/2021 15:33:20 - INFO - __main__ -   Generating predictions from zero-shot teacher model
[INFO|configuration_utils.py:517] 2021-06-16 15:33:21,219 >> loading configuration file https://huggingface.co/roberta-large-mnli/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fab42bdbd5cb5e6ff7cabeb9bcc12728f56022f50b9644a3079904564f2bc704.ddc5961cccf081d6ca7f4f58ee119c21895aa9b19f0044f01954cd2ff42fefcb
[INFO|configuration_utils.py:553] 2021-06-16 15:33:21,220 >> Model config RobertaConfig {
  ""_num_labels"": 3,
  ""architectures"": [
    ""RobertaForSequenceClassification""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""eos_token_id"": 2,
  ""gradient_checkpointing"": false,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 1024,
  ""id2label"": {
    ""0"": ""CONTRADICTION"",
    ""1"": ""NEUTRAL"",
    ""2"": ""ENTAILMENT""
  },
  ""initializer_range"": 0.02,
  ""intermediate_size"": 4096,
  ""label2id"": {
    ""CONTRADICTION"": 0,
    ""ENTAILMENT"": 2,
    ""NEUTRAL"": 1
  },
  ""layer_norm_eps"": 1e-05,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 16,
  ""num_hidden_layers"": 24,
  ""pad_token_id"": 1,
  ""position_embedding_type"": ""absolute"",
  ""transformers_version"": ""4.6.1"",
  ""type_vocab_size"": 1,
  ""use_cache"": true,
  ""vocab_size"": 50265
}

[INFO|modeling_utils.py:1155] 2021-06-16 15:33:21,507 >> loading weights file https://huggingface.co/roberta-large-mnli/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/63cbd98723b89863bcd86a8002e823de3004a139513559246690c65521cdc9b9.38ef55c51c84ab2e78e5a0e2ea9c25830fd074df70d2f10071eb9a1bc1586ca0
[WARNING|modeling_utils.py:1331] 2021-06-16 15:33:44,205 >> Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1348] 2021-06-16 15:33:44,205 >> All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
[INFO|configuration_utils.py:517] 2021-06-16 15:33:47,683 >> loading configuration file https://huggingface.co/roberta-large-mnli/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fab42bdbd5cb5e6ff7cabeb9bcc12728f56022f50b9644a3079904564f2bc704.ddc5961cccf081d6ca7f4f58ee119c21895aa9b19f0044f01954cd2ff42fefcb
[INFO|configuration_utils.py:553] 2021-06-16 15:33:47,684 >> Model config RobertaConfig {
  ""_num_labels"": 3,
  ""architectures"": [
    ""RobertaForSequenceClassification""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""eos_token_id"": 2,
  ""gradient_checkpointing"": false,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 1024,
  ""id2label"": {
    ""0"": ""CONTRADICTION"",
    ""1"": ""NEUTRAL"",
    ""2"": ""ENTAILMENT""
  },
  ""initializer_range"": 0.02,
  ""intermediate_size"": 4096,
  ""label2id"": {
    ""CONTRADICTION"": 0,
    ""ENTAILMENT"": 2,
    ""NEUTRAL"": 1
  },
  ""layer_norm_eps"": 1e-05,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 16,
  ""num_hidden_layers"": 24,
  ""pad_token_id"": 1,
  ""position_embedding_type"": ""absolute"",
  ""transformers_version"": ""4.6.1"",
  ""type_vocab_size"": 1,
  ""use_cache"": true,
  ""vocab_size"": 50265
}

[INFO|tokenization_utils_base.py:1717] 2021-06-16 15:33:49,522 >> loading file https://huggingface.co/roberta-large-mnli/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/64a1d72b2bd05b0aff1a4dd9e7a90a6eea0312b4f914e80b0a923aa8f72219bd.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1717] 2021-06-16 15:33:49,522 >> loading file https://huggingface.co/roberta-large-mnli/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/425529714b758f50b6d3f93f8093d859856fd41cf1cec7c8edf2ab44aee632b6.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1717] 2021-06-16 15:33:49,522 >> loading file https://huggingface.co/roberta-large-mnli/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d077eac6b48c43618a441cba6eab600a5cc6383b98e7eada6d1ad4d3f3cc457e.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|tokenization_utils_base.py:1717] 2021-06-16 15:33:49,522 >> loading file https://huggingface.co/roberta-large-mnli/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2021-06-16 15:33:49,522 >> loading file https://huggingface.co/roberta-large-mnli/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2021-06-16 15:33:49,522 >> loading file https://huggingface.co/roberta-large-mnli/resolve/main/tokenizer_config.json from cache at None
100% 15000/15000 [1:15:16<00:00,  3.32it/s]
06/16/2021 16:49:06 - INFO - __main__ -   Initializing student model
[INFO|file_utils.py:1532] 2021-06-16 16:49:07,106 >> https://huggingface.co/distilbert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpy7f4tyyh
Downloading: 100% 442/442 [00:00<00:00, 348kB/s]
[INFO|file_utils.py:1536] 2021-06-16 16:49:07,540 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361
[INFO|file_utils.py:1544] 2021-06-16 16:49:07,540 >> creating metadata file for /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361
[INFO|configuration_utils.py:517] 2021-06-16 16:49:07,540 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361
[INFO|configuration_utils.py:553] 2021-06-16 16:49:07,541 >> Model config DistilBertConfig {
  ""activation"": ""gelu"",
  ""architectures"": [
    ""DistilBertForMaskedLM""
  ],
  ""attention_dropout"": 0.1,
  ""dim"": 768,
  ""dropout"": 0.1,
  ""hidden_dim"": 3072,
  ""id2label"": {
    ""0"": ""LABEL_0"",
    ""1"": ""LABEL_1"",
    ""2"": ""LABEL_2"",
    ""3"": ""LABEL_3""
  },
  ""initializer_range"": 0.02,
  ""label2id"": {
    ""LABEL_0"": 0,
    ""LABEL_1"": 1,
    ""LABEL_2"": 2,
    ""LABEL_3"": 3
  },
  ""max_position_embeddings"": 512,
  ""model_type"": ""distilbert"",
  ""n_heads"": 12,
  ""n_layers"": 6,
  ""pad_token_id"": 0,
  ""qa_dropout"": 0.1,
  ""seq_classif_dropout"": 0.2,
  ""sinusoidal_pos_embds"": false,
  ""tie_weights_"": true,
  ""transformers_version"": ""4.6.1"",
  ""vocab_size"": 30522
}

[INFO|file_utils.py:1532] 2021-06-16 16:49:07,820 >> https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptuo3f4g2
Downloading: 100% 268M/268M [00:04<00:00, 62.4MB/s]
[INFO|file_utils.py:1536] 2021-06-16 16:49:12,343 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a
[INFO|file_utils.py:1544] 2021-06-16 16:49:12,343 >> creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a
[INFO|modeling_utils.py:1155] 2021-06-16 16:49:12,343 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a
[WARNING|modeling_utils.py:1331] 2021-06-16 16:49:12,787 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1342] 2021-06-16 16:49:12,787 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|configuration_utils.py:517] 2021-06-16 16:49:13,073 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361
[INFO|configuration_utils.py:553] 2021-06-16 16:49:13,074 >> Model config DistilBertConfig {
  ""activation"": ""gelu"",
  ""architectures"": [
    ""DistilBertForMaskedLM""
  ],
  ""attention_dropout"": 0.1,
  ""dim"": 768,
  ""dropout"": 0.1,
  ""hidden_dim"": 3072,
  ""initializer_range"": 0.02,
  ""max_position_embeddings"": 512,
  ""model_type"": ""distilbert"",
  ""n_heads"": 12,
  ""n_layers"": 6,
  ""pad_token_id"": 0,
  ""qa_dropout"": 0.1,
  ""seq_classif_dropout"": 0.2,
  ""sinusoidal_pos_embds"": false,
  ""tie_weights_"": true,
  ""transformers_version"": ""4.6.1"",
  ""vocab_size"": 30522
}

[INFO|file_utils.py:1532] 2021-06-16 16:49:13,357 >> https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps3o1_gw9
Downloading: 100% 232k/232k [00:00<00:00, 1.83MB/s]
[INFO|file_utils.py:1536] 2021-06-16 16:49:13,766 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|file_utils.py:1544] 2021-06-16 16:49:13,766 >> creating metadata file for /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|file_utils.py:1532] 2021-06-16 16:49:14,049 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1n0mi2iy
Downloading: 100% 466k/466k [00:00<00:00, 3.48MB/s]
[INFO|file_utils.py:1536] 2021-06-16 16:49:14,616 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|file_utils.py:1544] 2021-06-16 16:49:14,616 >> creating metadata file for /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|file_utils.py:1532] 2021-06-16 16:49:15,461 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmperm21jrj
Downloading: 100% 28.0/28.0 [00:00<00:00, 22.2kB/s]
[INFO|file_utils.py:1536] 2021-06-16 16:49:15,745 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|file_utils.py:1544] 2021-06-16 16:49:15,745 >> creating metadata file for /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2021-06-16 16:49:15,746 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2021-06-16 16:49:15,746 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|tokenization_utils_base.py:1717] 2021-06-16 16:49:15,746 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2021-06-16 16:49:15,746 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2021-06-16 16:49:15,746 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
100% 120000/120000 [00:32<00:00, 3647.18ex/s]
06/16/2021 16:49:49 - INFO - __main__ -   Training student model on teacher predictions
[INFO|trainer.py:516] 2021-06-16 16:49:49,272 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.
[INFO|trainer.py:1156] 2021-06-16 16:49:49,285 >> ***** Running training *****
[INFO|trainer.py:1157] 2021-06-16 16:49:49,285 >>   Num examples = 120000
[INFO|trainer.py:1158] 2021-06-16 16:49:49,285 >>   Num Epochs = 1
[INFO|trainer.py:1159] 2021-06-16 16:49:49,285 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1160] 2021-06-16 16:49:49,285 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1161] 2021-06-16 16:49:49,285 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1162] 2021-06-16 16:49:49,286 >>   Total optimization steps = 3750
  0% 0/3750 [00:00<?, ?it/s]Traceback (most recent call last):
  File ""transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py"", line 338, in <module>
    main()
  File ""transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py"", line 328, in main
    trainer.train()
  File ""/usr/local/lib/python3.7/dist-packages/transformers/trainer.py"", line 1272, in train
    tr_loss += self.training_step(model, inputs)
  File ""/usr/local/lib/python3.7/dist-packages/transformers/trainer.py"", line 1734, in training_step
    loss = self.compute_loss(model, inputs)
  File ""transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py"", line 119, in compute_loss
    target_p = inputs[""labels""]
KeyError: 'labels'
  0% 0/3750 [00:00<?, ?it/s]
`

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

Not throw a `KeyError`

<!-- A clear and concise description of what you would expect to happen. -->
","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",8,open
Feature request for encoding more than one pair of texts,"# 🚀 Feature request

Currently, tokenizer may take only inputs like [['text_0', 'text_1']], would be beneficially to expand is as
[['text_0', 'text_1', ..., 'text_n']]

## Motivation

This would open a convenient way to deal with a new set of processing tasks.

## Your contribution

Don't have any.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
How to train the new wav2vec unsupervised model using hugging face ?,"# 🚀 Feature request

<!-- A clear and concise description of the feature proposal.
     Please provide a link to the paper and code in case they exist. -->
How to train the new wav2vec unsupervised model using hugging face ? , The paper link is : https://ai.facebook.com/research/publications/unsupervised-speech-recognition
## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request
     related to a problem? e.g., I'm always frustrated when [...]. If this is related
     to another GitHub issue, please link here too. -->

## Your contribution

<!-- Is there any way that you could help, e.g. by submitting a PR?
     Make sure to read the CONTRIBUTING.MD readme:
     https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md -->
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
[Performance] Tracking open Issues and PRs (pytorch transformers),"Let's use this Issue to track performance issues and enhancement requests, so it's easier to prioritize the work.

**This is for pytorch `transformers`**

Also I will label it as a `Good Difficult Issue` in case someone is ready for a challenging but rewarding experience of figuring things out. If you do want to take the challenge comment in the corresponding Issue/PR that resonates with you so others would know you're working on it.

If I missed any other relevant open performance-related Issues/PRs that need attention please comment below.

## Regression:
- [ ] https://github.com/huggingface/transformers/pull/11218 Regression after Bart-like refactoring - need to compare the original Bart refactoring PR since most likely the regression happened there.
- [ ]

## Odd slowness:
- [ ] https://github.com/huggingface/transformers/issues/10816 figuring out why eval with --fp16_full_eval is 25% slower
- [ ]

## Fused kernels possibilities:
- [ ] https://github.com/huggingface/transformers/issues/11368 Megatron fused CUDA kernels to improve Hugging Face model classes' scalability 
- [ ] research pytorch kernels?
- [ ] I know Deepspeed has various kernels that we might be able to use

## Faster / leaner startup / module loading
- [ ] https://github.com/huggingface/transformers/issues/12274 - skip storage allocation which gets dropped for pretrained weights

## Faster optimizers
- [ ] https://github.com/huggingface/transformers/issues/12084 - a proposal to port `MemoryEfficientFP16Optimizer` from fairseq
- [ ] https://github.com/huggingface/transformers/issues/9965 - `torch.optim._multi_tensor` faster optimizers - having some bottleneck in the test script - need to profile

## Scalability
- [ ] https://github.com/huggingface/transformers/issues/10321 Tensor Parallelism

## Deepspeed-specific features
- [ ] https://github.com/huggingface/transformers/issues/9606 a list of features that can be integrated
- [ ] https://github.com/huggingface/transformers/issues/12273 - make `from_pretrained` loading faster

## Tests
- [ ] No issue yet, but we really need to add performance regression tests

","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}, {'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMDgxMTM2NTM2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Difficult%20Issue', 'name': 'Good Difficult Issue', 'color': '684CC7', 'default': False, 'description': ''}]",2,open
Issue with mBART50 es-en translation ,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.6.1
- Platform: colab
- Python version: 3.7
- PyTorch version (GPU?): 
- Tensorflow version (GPU?):
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @Rocketknight1

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

Model hub:

- for issues with a model report at https://discuss.huggingface.co/ and tag the model's creator.

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...): mBART-large-50-many-to-one-nmt

The problem arises when using:
* [x] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is: Translation
* [x] an official GLUE/SQUaD task: (give the name)
* [ ] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behaviour:

The below notebook can be used to reproduce the results
1. https://colab.research.google.com/drive/1LEY3bI9mS7D-n6rJ70iKq3lN9_DQCQh7?usp=sharing

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->

I've used this model to translate a lot of Spanish text. But I observed that for some examples it's printing completely random things. 

The above example should return something like this `1980 Mount St. Helens eruption`

The current output is `The Committee recommends that the State party take all necessary measures to ensure the full implementation of the present recommendations, inter alia, by transmitting them to the members of the Council of Ministers, the Parliament, the Parliamentary Assembly and the Senate, the Parliamentary Assembly and the National Assembly, for appropriate consideration and further action.`

Tagging @patrickvonplaten, @patil-suraj here. I believe this is not really a code issue, but something intrinsic to the model. Any ideas why this is happening?
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
ViT tensorflow Implementation,"# 🚀 Feature request

<!-- A clear and concise description of the feature proposal.
     Please provide a link to the paper and code in case they exist. -->

I was reading about ViT in the HuggingFace document and noticed there is no TF implementation of it. It would be great to have it in HuggingFace repo.

## Motivation

I have seen [this](https://keras.io/examples/vision/image_classification_with_vision_transformer/) and think it wouldn't be so hard. We can convert pytorch pretrain weights and use it for tensroflow model. 
 
<!-- Please outline the motivation for the proposal. Is your feature request
     related to a problem? e.g., I'm always frustrated when [...]. If this is related
     to another GitHub issue, please link here too. -->","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
[testing] making network tests more reliable,"We have a group of tests that require a reliable network, which is never 100% so they fail for many months.

I propose that those tests will be rewritten with unstable network in mind and include:

1. `time.sleep(3)`
2. retry 3-5 times

e.g. one of the candidates is:

`tests/test_hf_api.py::HfApiEndpointsTest::test_list_repos_objs`

but also recent tests that push to hub.

Perhaps a simple retry context manager can be added to `testing_utils.py`, which would trap exceptions and retry after a pause. And then simply wrap the content of existing tests into that context manager, e.g.:

```
with RetryAfterSleepTest():
    # normal test code
```

it could accept the number of retries and sleep time between retries for optional arguments.

Of course, it's probably even better to make it also a decorator. e.g. `@unreliable_network_retry`

@LysandreJik ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",24,open
Add SENet Blocks in Encoding Layers,"# 🚀 Feature Request

I read the article ""[SesameBERT: Attention for Anywhere](https://arxiv.org/pdf/1910.03176.pdf)"" and would like to add SENet blocks in the Huggingface implementation. The article's authors made an implementation with [Tensorflow](https://github.com/ICLR2020Sesame/SesameBert/blob/master/modeling.py), but I would like to use the lib in pytorch.

## Motivation

The use of ([Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)) SENet Blocks has obtained state-of-the-art results. And they seem to be promising in NLP.

## Your contribution

I know that it is possible to modify the [[BertLayer()](https://github.com/huggingface/transformers/blob/61c506349134db0a0a2fd6fb2eff8e29a2f84e79/src/transformers/models/bert/modeling_bert.py#L430)] and [[BertEnconder()](https://github.com/huggingface/transformers/blob/61c506349134db0a0a2fd6fb2eff8e29a2f84e79/src/transformers/models/bert/modeling_bert.py#L513)] classes

Any suggestions on how to modify the code so that you can apply the idea used in the article?

![image](https://user-images.githubusercontent.com/25322394/120578580-0f3ca000-c3f4-11eb-9a14-93bf610c3a60.png)
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Request addition of 'GPT2ForwardBackward' models,"# 🌟 Request addition of 'GPT2ForwardBackward' models

## Model description

Code for running forward and backward versions of GPT-2 XL. These were trained for the paper:

**Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models**; Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena Hwang, and Yejin Choi; ACL (2021) 

https://arxiv.org/abs/2010.08566

## Open source status

* [ X] the model implementation is available: (https://github.com/peterwestuw/GPT2ForwardBackward)
* [ X] the model weights are available: (same link as above)
* [ X] who are the authors: (See arvix credits above)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
Import `SPIECE_UNDERLINE` from `file_utils` instead of WET definition,"Many places define `SPIECE_UNDERLINE` in the code like this: `SPIECE_UNDERLINE = ""▁""`

Instead it should me imported: `from transformers.file_utils import SPIECE_UNDERLINE`.

I can provide a PR...","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Routing Transformers / Add Google PG-19 Models,"# 🌟 New model addition - Google PG-19 Models

## Model description

Model checkpoints finally released as discussed in ""Efficient Content-Based Sparse Attention with Routing Transformers'
Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier (https://arxiv.org/abs/2003.05997)

## Open source status

* [X ] the model implementation is available: (same link as below)
* [ X] the model weights are available: ( https://github.com/google-research/google-research/tree/master/routing_transformer)
* [X ] who are the authors: (see above)

Note: These tf2 models require proper conversion to pytorch versions and modifications to scripts to enable training and inference.
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Key Error: 'pre-processing' during conversion from tatoeba to Marian model,"## Environment info

- `transformers` version: `4.6.0.dev0`
- Platform: `CentOS Linux release 7.7.1908 (Core)`
- Python version: `3.8.5`
- PyTorch version: `1.8.1 + cuda 10.2`
- Tensorflow version: N/A
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help

Marian: @patrickvonplaten , @patil-suraj 

## Information

Model I am using (Bert, XLNet ...): Marian

The problem arises when using:
* [x] the official example scripts: tatoeba to marian model script
* [ ] my own modified scripts

The tasks I am working on is:
* [x] an official GLUE/SQUaD task: machine translation
* [ ] my own task or dataset

## To reproduce


Following the script from [scripts/tatoeba/README.md ](https://github.com/huggingface/transformers/tree/master/scripts/tatoeba)
1. 
```git clone git@github.com:huggingface/transformers.git
cd transformers
pip install -e .
pip install pandas GitPython wget
```
2. 
```
curl https://cdn-datasets.huggingface.co/language_codes/language-codes-3b2.csv  > language-codes-3b2.csv
curl https://cdn-datasets.huggingface.co/language_codes/iso-639-3.csv > iso-639-3.csv
```
3. `git clone git@github.com:Helsinki-NLP/Tatoeba-Challenge.git`
4. `python src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py --models kor-eng eng-kor --save_dir converted/`

Error message:
```
Traceback (most recent call last):
  File ""src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py"", line 1267, in <module>
    resolver = TatoebaConverter(save_dir=args.save_dir)
  File ""src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py"", line 58, in __init__
    reg = self.make_tatoeba_registry()
  File ""src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py"", line 258, in make_tatoeba_registry
    return [(k, v[""pre-processing""], v[""download""], v[""download""][:-4] + "".test.txt"") for k, v in results.items()]
  File ""src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py"", line 258, in <listcomp>
    return [(k, v[""pre-processing""], v[""download""], v[""download""][:-4] + "".test.txt"") for k, v in results.items()]
KeyError: 'pre-processing'
```

## Expected behavior

Conversion of the model from Tatoeba to Marian for the chosen language pair with no errors.
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",3,open
Felix,"# 🌟 New model addition

## Model description

New model released by [google](https://github.com/google-research/google-research/tree/master/felix)

## Open source status

* [x] the model implementation is available: https://github.com/google-research/google-research/tree/master/felix
* [x] the model weights are available: https://github.com/google-research/google-research/tree/master/felix (pretrained `bert-base-cased` can be used)
* [x] who are the authors: @Jmallins
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",10,open
Distil BART for text simplification,"# 🌟 New model addition

## Model description
Not completed yet. I will notify very soon when it is done.
If anyone has any leads regarding the same issue, please share. It would be great help.","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
[resume optimization] skip loading pretrained weights on resume,"This is similar to what was discussed in https://github.com/huggingface/transformers/issues/9205, which proposed not to random init weights on `from_pretrained`, but this time it's about resume - currently we load pretrained weights and immediately drop them on resume from checkpoint in Trainer.

To solve this we, for example, could change examples:

1. to figure out the checkpoint immediately after we init `TrainingArguments` and just before model is created. 
2. then change `from_pretrained()` API to do keep everything as is, except loading the weights from `state_dict`, if say `skip_weights_load=True` is passed:

So the code becomes:
```

    if training_args.do_train:
        if last_checkpoint is not None:
            checkpoint = last_checkpoint
        elif os.path.isdir(model_args.model_name_or_path):
            checkpoint = model_args.model_name_or_path
        else:
            checkpoint = None

    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_args.model_name_or_path, 
        [...],
        skip_weights_load=checkpoint is not None,
    )

    if training_args.do_train:
        train_result = trainer.train(resume_from_checkpoint=checkpoint)
```

Any flaws in my thinking?

@patrickvonplaten, @sgugger ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",6,open
[CI] solving the pytest crashing and hanging CI job,"So as of recent we have the `run_tests_torch` CI job randomly and frequently failing.

We couldn't find any fault with any tests because there is never a traceback, just hanging `pytest` that sends no output.

This usually is a symptom that the process used more resources than it was allowed and it was killed - of course the python interpreted doesn't get a chance to make a peep - so no traceback. e.g. on colab processes get killed in the same way.

## Diagnostics

1. Go to the CI report and ""rerun job with SSH""

it then enables SSH and gives you the cmd to access the CI instance. Use those instructions which it shows to you in `Enable SSH` to ssh to the instance.

when done remember to exit the ssh shells and `Cancel Job`, since otherwise the instance will continue running at $$$.

2. CI doesn't run docker with `--priveleged` flag, so most normal system tools are disabled, so it's almost impossible to debug anything. Things like `dmesg` or `/var/sys/log` are not there, you can `sudo`, but you can't do almost anything with it.

Ideally in such situations it'd be a good idea to switch from `docker` back to `machine` where we would have full root access.

3. Resource limit
```
resource_class: xlarge
```
as of this writing gives you 16GB RAM.

This is very confusing since when you log into the instance there are 70GB of memory reported in the top. And if you try to monitor %MEM you get a very misleading low usage. It gives you the report for out of 70GB, not out of the cgroups memory limit of 16GB.

How do we know the real limit:

```
$ cat /sys/fs/cgroup/memory/memory.limit_in_bytes | perl -ne 'print $_ / 2**30'
16
```

Yup, 16GB

4. Now it's very difficult to measure how much memory several forked processes use together, you can't use `top` for that.

I had 2 consoles opened, one with top and another with running `pytest -n 8` that I started manually 

I did notice that the once all 8 processes were around 2-2.5GB RSS after awhile one of the workers crashed,

Then I found this handy tool thanks to https://unix.stackexchange.com/a/169129/291728
```
apt install smem
```

```
circleci@fc02c746bf66:~$ smem -t
  PID User     Command                         Swap      USS      PSS      RSS
    6 circleci /bin/sh                            0       88       88       92
    1 circleci /sbin/docker-init -- /bin/s        0       48      123      740
17567 circleci /usr/bin/time -v python -m         0       96      145     1216
17568 circleci tee tests_output.txt               0      140      225     1828
  495 circleci /bin/bash -eo pipefail -c w        0      292      526     1692
 1511 circleci -bash                              0      608     1066     3140
  476 circleci -bash                              0      620     1079     3148
18170 circleci /usr/bin/python /usr/bin/sm        0    13160    13286    15424
    7 circleci /bin/circleci-agent --confi        0    29424    29424    29428
17569 circleci python -m pytest -n 8 --dis        0   151172   163118   254684
17588 circleci /usr/local/bin/python -u -c        0   348860   371932   526452
17594 circleci /usr/local/bin/python -u -c        0  1863416  1887735  2048128
17579 circleci /usr/local/bin/python -u -c        0  2028784  2052674  2210400
17591 circleci /usr/local/bin/python -u -c        0  2031872  2056217  2214712
17574 circleci /usr/local/bin/python -u -c        0  2098124  2122054  2282392
17585 circleci /usr/local/bin/python -u -c        0  2226080  2247464  2401880
17582 circleci /usr/local/bin/python -u -c        0  2226864  2249367  2404832
17597 circleci /usr/local/bin/python -u -c        0  2643552  2665199  2818968
-------------------------------------------------------------------------------
   18 1                                           0 15663200 15861722 17219156
```

The PSS column seems to be able to do correct totals on, so I did:
```
watch -n 1 'smem -t | tail -1'
```
and indeed, once the total PSS hit ~16GB pytest crashed.

The failure we get is intermittent because the tests are run randomly and sometimes we get 4 ""fatter"" tests run concurrently, and at all other times when it succeeds we are lucky not to hit the bad combination.

I tried to switch to:

```
resource_class: 2xlarge
```
which would give us 32GB, but apparently we aren't allowed to do so and need to ask for a special permission from CircleCI admins.

5. what happens to the hanging processes? clearly `pytest` doesn't recover from crash. I think it can recover from other failures of its workers, but not when a kernel nukes one of its workers.

When the resource limit gets hit, all but one workers were hanging in some strange place:
```
Thread 0x00007f65d91bb700 (most recent call first):
  File ""/home/circleci/.local/lib/python3.7/site-packages/execnet/gateway_base.py"", line 400 in read
  File ""/home/circleci/.local/lib/python3.7/site-packages/execnet/gateway_base.py"", line 432 in from_io
  File ""/home/circleci/.local/lib/python3.7/site-packages/execnet/gateway_base.py"", line 967 in _thread_receiver
  File ""/home/circleci/.local/lib/python3.7/site-packages/execnet/gateway_base.py"", line 220 in run
  File ""/home/circleci/.local/lib/python3.7/site-packages/execnet/gateway_base.py"", line 285 in _perform_spawn
```
if I look in `top` 7 but 1 pytest workers stop working blocking on the above.

I figured that out by adding to `tests/conftest.py`:
```
import faulthandler
faulthandler.dump_traceback_later(20, repeat=True)
```

So now every 20 secs I was getting tb reports on where things were hanging...

But I'm not 100% sure it's why they are hanging, I will have to spend more time with it if we really want to understand why the other workers stop processing. So please don't take it as a truth, it's just one of the possibilities to check. But since it doesn't help our situation to understand why they can't recover I'm not going to waste time on it. 

## Summary

1. we probably have a very small leak that grows over hundreds of tests as the memory usage slowly, but consistently goes up
2. 16GB is just enough for our `pytest -n 4` - probably 75% of the time, until we add more tests
3. so we either need to ask for the 2xlarge instance, or use `-n 3`
4. ~probably it'd be a good idea to add~ (see next comment)
```
apt install time
```
and run `pytest` with:
```
/usr/bin/time -v python -m pytest ...
```
which will give us an indepth resource usage report - so overtime we should see if our test suite consumes more and more resources.


@LysandreJik, @sgugger 
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}, {'id': 2991663546, 'node_id': 'MDU6TGFiZWwyOTkxNjYzNTQ2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Testing', 'name': 'Testing', 'color': '19A601', 'default': False, 'description': ''}]",5,open
Megatron fused CUDA kernels to improve Hugging Face model classes' scalability,"# 🚀 Feature request

Support for custom fused CUDA kernels with HF model classes.

## Motivation

It appears that Hugging Face model classes do not scale very well as-is unlike Megatron-LM, even when the latter is configured with a degree of model-parallelization = 1 for a ""fair"" performance comparison.

One of the presumed reasons for this is that Megatron-LM leverages custom fused CUDA kernels written by NVIDIA, specifically [these](https://github.com/NVIDIA/Megatron-LM/blob/aed2f75e209e525c842aec7c044af7acae2a4614/megatron/model/transformer.py#L26L27).

Could we get variants of existing HF classes (perhaps for `GPT2Model`, `GPT2LMHeadModel`, etc.) such that the variants leverage some/all of these fused CUDA kernels? All this while still ensuring that one can load the original pre-trained weights into these variant classes.

Any guidance/low-level thoughts towards making this happen would also be greatly useful!

@thomwolf @patrickvonplaten @LysandreJik @stas00 ","[{'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Getting time offsets of beginning and end of each word in Wav2Vec2,"# 🚀 Feature request

Hello I was thinking it would be of great help if I can get the time offsets of start and end of each word .

## Motivation
I was going through Google Speech to text documentation and found this  [feature](https://cloud.google.com/speech-to-text/docs/async-time-offsets) and thought will be really amazing if i can have something similar here. 

<!-- Please outline the motivation for the proposal. Is your feature request
     related to a problem? e.g., I'm always frustrated when [...]. If this is related
     to another GitHub issue, please link here too. -->

## Your contribution

I can really use some help in this task and would love to implement something similar. 

<!-- Is there any way that you could help, e.g. by submitting a PR?
     Make sure to read the CONTRIBUTING.MD readme:
     https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md -->
","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",25,open
Running gpt-neo 2.7B with less than 13GB of system memory like Colab,"# 🚀 Feature request

A way to conserve regular system memory while loading large models. On systems without much system memory, the process crashes because it tries to load both the weight checkpoint and the model into system memory. In the case of gpt-neo 2.7B, this is even worse because the checkpoint is offered only in float32 format, taking up twice as much space.

## Motivation

Free Colab often offers GPUs with 16GB of VRAM but only about 13GB of RAM. This increases the barrier of entry for people to play with these kinds of models. I have found a way to load models in this kind of situation, but it is not general or well integrated. By posting it here, I hope that a more general implementation can be built at some point.

This would also help #11271.

## Your contribution

It is possible to work around this by loading the checkpoint directly into VRAM, casting it to float16, instantiating the model in VRAM and only then applying the weights from the checkpoint.

To do this, first a patch has to be applied to src/transformers/models/gpt_neo/modeling_gpt_neo.py. This is based on the 4.5.1 release.

    703c703
    <         self.h = nn.ModuleList([GPTNeoBlock(config, layer_id=i) for i in range(config.num_layers)])
    ---
    >         self.h = nn.ModuleList([GPTNeoBlock(config, layer_id=i).half().cuda() for i in range(config.num_layers)])
    890,891c890,891
    <         self.transformer = GPTNeoModel(config)
    <         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
    ---
    >         self.transformer = GPTNeoModel(config).half().cuda()
    >         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False).half().cuda()

This causes space for the biggest part of the model to be allocated directly on the GPU, which has more space in the free Colab scenario. It also moves the other parts of the model to GPU. Now the model can be instantiated like this:

    from transformers.file_utils import cached_path, WEIGHTS_NAME, hf_bucket_url
    model_name = ""EleutherAI/gpt-neo-2.7B""
    archive_file = hf_bucket_url(model_name, filename=WEIGHTS_NAME)
    resolved_archive_file = cached_path(archive_file)
    checkpoint = torch.load(resolved_archive_file, map_location=""cuda:0"")
    for k in checkpoint.keys():
      checkpoint[k] = checkpoint[k].half()
    model = GPTNeoForCausalLM.from_pretrained(model_name, state_dict=checkpoint).half().to(""cuda"")
    for k in list(checkpoint.keys()):
      del checkpoint[k]","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
[RFC] introduce `config.trained_precision`,"# 🚀 Feature request

As we are discovering that `bf16`-pretrained models don't do well on `fp16` ""regime"" (and surely vice-versa), and some models are pre-trained in `fp32` and surely won't do well on either `bf16` or `fp16`, and the problem is going to grow as more `bf16`-supporting hardware comes out, I propose we start requiring that the model tells the user which mode it was pretrained under.

So I suggest we add `config.trained_precision` which currently would be one of `fp16`, `bf16`, `fp32`, `unknown`.

I haven't thought it through on how to derive this automatically during `save_pretrained`, but when porting checkpoints the porter can figure that out and manually set this in the conversion script.

For example, from what I understood gtp-neo if `bf16` for all but `2.7B` version, which is `fp32`.

@sgugger, @LysandreJik ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",15,open
Sequential constraints?,"fairseq has an implementation of unordered and ordered multi-token constraints: see their [PR](https://github.com/pytorch/fairseq/pull/2402) and [example](https://github.com/pytorch/fairseq/tree/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/constrained_decoding).

This is more advanced than the single-token constraints that have been occasionally [requested here](https://github.com/huggingface/transformers/issues/10485) mainly due to the bookkeeping involved; see the papers referenced in the fairseq PR.

Has anyone looked into porting this feature? fairseq's [constraint tracking logic](https://github.com/pytorch/fairseq/blob/master/fairseq/token_generation_constraints.py) looks to be well-factored and could probably be adopted verbatim, license permitting. The beam search modifications ([fairseq implementation](https://github.com/pytorch/fairseq/blob/ee0d5a0f65a25e5f5372776402aac5cb9c4adbf1/fairseq/search.py#L210)) may be able to be implemented as a `LogitsProcessor`, or maybe even just a `prefix_allowed_tokens_fn`, but the papers propose some additional logic around making sure that a partial constraint stays in the beam; I'm not sure whether those hooks are sufficient to implement that logic (or how essential it is to the functionality).

(I've found one [related issue](https://github.com/huggingface/transformers/issues/1163).)","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
"denoising with sentence permutation, and language sampling","# 🚀 Feature request

<!-- A clear and concise description of the feature proposal.
     Please provide a link to the paper and code in case they exist. -->

## Motivation

When training or fine tuning models, data collator provided in huggingface isn't enough.
For example, if we want to further pretrain `mBART` or `XLM-R`, where language sampling or sentence permutation are needed, which is hard to do with huggingface datasets API since it loads all language datasets at first.   


Thanks!
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Model config is logged twice on startup,"Currently, the model config is logged twice during startup:

1. via `AutoConfig.from_pretrained`
2. via `AutoTokenizer.from_pretrained` -> `AutoConfig.from_pretrained`

Should there be a state variable that prevents the logging of the same config twice?

This happens for example with all example scripts:

Example log when running `run_clm.py`:
```
  File ""examples/language-modeling/run_clm.py"", line 444, in <module>
    main()
  File ""examples/language-modeling/run_clm.py"", line 275, in main
    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
  File ""/mnt/nvme1/code/huggingface/transformers-gpt-neo-nan/src/transformers/models/auto/configuration_auto.py"", line 401, in from_pretrained
    return config_class.from_dict(config_dict, **kwargs)
  File ""/mnt/nvme1/code/huggingface/transformers-gpt-neo-nan/src/transformers/configuration_utils.py"", line 526, in from_dict
    traceback.print_stack()
[INFO|configuration_utils.py:527] 2021-04-06 21:16:04,999 >> Model config GPT2Config {
  ""_num_labels"": 1,
  ""activation_function"": ""gelu_new"",
  ""architectures"": [
    ""GPT2LMHeadModel""
  ],
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 50256,
  ""embd_pdrop"": 0.1,
  ""eos_token_id"": 50256,
  ""gradient_checkpointing"": false,
  ""id2label"": {
    ""0"": ""LABEL_0""
  },
  ""initializer_range"": 0.02,
  ""label2id"": {
    ""LABEL_0"": 0
  },
  ""layer_norm_epsilon"": 1e-05,
  ""model_type"": ""gpt2"",
  ""n_ctx"": 1024,
  ""n_embd"": 768,
  ""n_head"": 12,
  ""n_inner"": null,
  ""n_layer"": 6,
  ""n_positions"": 1024,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""task_specific_params"": {
    ""text-generation"": {
      ""do_sample"": true,
      ""max_length"": 50
    }
  },
  ""transformers_version"": ""4.5.0"",
  ""use_cache"": true,
  ""vocab_size"": 50257
}

[INFO|configuration_utils.py:490] 2021-04-06 21:16:05,277 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/stas/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631
  File ""examples/language-modeling/run_clm.py"", line 444, in <module>
    main()
  File ""examples/language-modeling/run_clm.py"", line 289, in main
    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)
  File ""/mnt/nvme1/code/huggingface/transformers-gpt-neo-nan/src/transformers/models/auto/tokenization_auto.py"", line 390, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File ""/mnt/nvme1/code/huggingface/transformers-gpt-neo-nan/src/transformers/models/auto/configuration_auto.py"", line 401, in from_pretrained
    return config_class.from_dict(config_dict, **kwargs)
  File ""/mnt/nvme1/code/huggingface/transformers-gpt-neo-nan/src/transformers/configuration_utils.py"", line 526, in from_dict
    traceback.print_stack()
[INFO|configuration_utils.py:527] 2021-04-06 21:16:05,279 >> Model config GPT2Config {
  ""_num_labels"": 1,
  ""activation_function"": ""gelu_new"",
  ""architectures"": [
    ""GPT2LMHeadModel""
  ],
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 50256,
  ""embd_pdrop"": 0.1,
  ""eos_token_id"": 50256,
  ""gradient_checkpointing"": false,
  ""id2label"": {
    ""0"": ""LABEL_0""
  },
  ""initializer_range"": 0.02,
  ""label2id"": {
    ""LABEL_0"": 0
  },
  ""layer_norm_epsilon"": 1e-05,
  ""model_type"": ""gpt2"",
  ""n_ctx"": 1024,
  ""n_embd"": 768,
  ""n_head"": 12,
  ""n_inner"": null,
  ""n_layer"": 6,
  ""n_positions"": 1024,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""task_specific_params"": {
    ""text-generation"": {
      ""do_sample"": true,
      ""max_length"": 50
    }
  },
  ""transformers_version"": ""4.5.0"",
  ""use_cache"": true,
  ""vocab_size"": 50257
}
```

To get the traceback I just added:
```
        import traceback
        traceback.print_stack()
```

@LysandreJik, @sgugger ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",1,open
Enable multiple `eval_dataset` in `Trainer` API,"# 🚀 Feature request

Allow for two or more (equally long) validation sets to be passed to the `Trainer` API which are evaluated sequentially each `eval_steps`.

## Motivation

You can find my motivation in this [thread](https://discuss.huggingface.co/t/use-trainer-api-with-two-valiation-sets/5212) and the referenced paper. My idea would be to evaluate language model pre-training on an overlapping validation set (coming from the same data distribution as the training set) and a non-overlapping validation set (sampled from future periods or another domain). Ideally, I would like to track and log the validation loss during pre-training for both validation sets.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
use `BaseModelOutput` as common interface for all different `BaseModelOutputWith*`?,"Hello team,

I have been taking a look at the `different` output models from your models, and I wonder if it would make sense to inherit all the `BaseModelOutputWithPool` and all the other flavours of modeling output, instead of using `ModelOutput`.

https://github.com/huggingface/transformers/blob/c301c26370dfa48f6a6d0408b5bb9eb70ca831b3/src/transformers/modeling_outputs.py#L24

We are trying to build a wrapper around many of the public models hosted on hugging face, and it would be useful to know if we can assume that all the potential `outputs` of the models will contain `hidden_states`. Since now they all only inherit from `ModelOutput`  it seems a little confusing.

Am I missing something? Is it not something that can be assumed? ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
ReduceLROnPlateau-like functionality?,"# 🚀 Feature request

<!-- A clear and concise description of the feature proposal.
     Please provide a link to the paper and code in case they exist. -->

**Description:** Dynamic learning rate reduction upon metric saturation, as in `torch.optim.lr_scheduler.ReduceLROnPlateau`, integrated into the `Trainer` API.

Alternately, if there's any way (if hacky) to get dynamic learning rate reduction using the `Trainer` API as it is, that would be extremely helpful as well.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request
     related to a problem? e.g., I'm always frustrated when [...]. If this is related
     to another GitHub issue, please link here too. -->

LR schedules are a commonly used trick for ML optimization, and the `transformers` library already provides a significant number of baseline schedules (i.e. linear, cosine schedulers, warmup/no-warmup, restarts). However, these schedules are all static: updates to them occur at fixed steps in the optimization -- one can always tell what the learning rate at, say, step 1000 will be given these fixed schedules.

Reducing learning rate dynamically is also a common practical technique, usually applied when loss saturates (fails to improve after N iterations).

The difficulty is that dynamic learning rate reduction follows a non-fixed update schedule, meaning that working within the `LambdaLR` framework used by the other scheduler is less straightforward.

## Your contribution

I don't have a working implementation yet. At a high level, I tried to implement this myself as a `TrainerCallback` modeled on both the `EarlyStoppingCallback` in the `transformers` library as well as the `ReduceLROnPlateau` implementation in PyTorch. I was able to modify the optimizer object; however, learning rate updates to the optimizer would get overwritten by the scheduler. In any case, I also don't know if it's good style/even possible to modify the optimizer and scheduler in this way using a Callback -- seems like the `control` object is the only thing that changes within a `TrainerCallback`.

<!-- Is there any way that you could help, e.g. by submitting a PR?
     Make sure to read the CONTRIBUTING.MD readme:
     https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md -->
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Add GPT Neo models to Write With Transformer,"# 🚀 Feature request
Would it be possible to get the newly-added GPT Neo models usable on Write With Transformer?

## Motivation
It would be helpful to use the new models in the Write With Transformer app since it supports newlines.

CC @LysandreJik ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
Returning Confidence Score For Extractive QA Task When Using Non-Pipeline Approach ,"# 🚀 Feature request

HF's Extract QA pipeline provides an excellent interface for start. It returns 4 values including a **probability score / confidence score**. Unfortunately same is not the case when using the non-pipeline approach i.e using model and tokenizer for question answering. 
[Both methods are mentioned here, The pipeline one and the other](https://huggingface.co/transformers/task_summary.html#extractive-question-answering)

## Motivation
The confidence score will help a lot in various tasks. For example when I am developing a complete pipeline for QA, consisting of recall, retriever and some other models for entity matching and etc. I need to calculate scores of each models and then rank the final list of documents based on the weighted sum of score from each model. I believe this is a very common practice among NLP practitioners and not just for QA task. The point is confidence scores are usually a pretty standard requirement for each model output because we have to take further actions based on its score.

## Your contribution
I want to. but unfortunately I am not at the level where I can understand the code. I have went through the code and I believe its the ""decode"" function in ""QuestionAnsweringPipeline"" class which has the code the which generates the probability scores. If you guys can just provide an interface for it or provide docs for how to calculate this score using the model and tokenizer approach then that would be great too. And if you do decide to do this then please also add this addition to docs in the link mentioned at the top.

Thanks. ","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Please implement DUMA: Reading Comprehension with Transposition Thinking,"# 🚀 Feature request

<!-- A clear and concise description of the feature proposal.
     Please provide a link to the paper and code in case they exist. -->
This one is on the race leaderborad top, will you guys consider implement this?

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request
     related to a problem? e.g., I'm always frustrated when [...]. If this is related
     to another GitHub issue, please link here too. -->

## Your contribution

<!-- Is there any way that you could help, e.g. by submitting a PR?
     Make sure to read the CONTRIBUTING.MD readme:
     https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md -->
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Add DALL-E: Zero-Shot Text-to-Image Generation,"# 🚀 Feature request

Please add DALLE-E model to huggingface's Transformers library.

1. [Announcement](https://openai.com/blog/dall-e/)
2. [Abstract](https://arxiv.org/abs/2102.12092v2)
3. [Paper](https://arxiv.org/pdf/2102.12092v2.pdf)
4. Code:
    - [openai/DALL-E](https://github.com/openai/DALL-E) (official)
    - [lucidrains/DALLE-pytorch](https://github.com/lucidrains/DALLE-pytorch) ([Colab](https://colab.research.google.com/drive/1dWvA54k4fH8zAmiix3VXbg95uEIMfqQM?usp=sharing))

## Motivation

> DALL·E is a 12-billion parameter version of [GPT-3](https://huggingface.co/transformers/model_doc/gpt.html) trained to generate images from text descriptions, using a dataset of text–image pairs
>
> We (Open AI) decided to name our model using a portmanteau of the artist Salvador Dalí and Pixar’s WALL·E.
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",5,open
"Summarization length not controlled by max_length, min_length","I am using the pertained ctrlsum-cnndm model from transformers. I noticed that summarization text length is not exactly controlled by max_length, min_length arguments of model.generate(). Not sure why. It appears that empty spaces are included, but not sure. Please help. Thanks.

```
text1=""The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained(""hyunwoongko/ctrlsum-cnndm"")
model = AutoModelForSeq2SeqLM.from_pretrained(""hyunwoongko/ctrlsum-cnndm"")

inputs = tokenizer.encode(text1, return_tensors=""pt"", max_length=1024)#16
outputs = model.generate(inputs, max_length=100, min_length=50, num_beams=5, early_stopping=True)
print(tokenizer.decode(outputs[0]))
```

Results:
max_length=100, min_length=50, actually 36 words
`</s> The Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. It is the tallest structure in Paris and the second tallest free-standing structure in France after the Millau Viaduct.</s>
`


max_length=200, min_length=100, actually 83 words
`</s> The Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. It was the tallest man-made structure in the world for 41 years until the Chrysler Building in New York City was finished in 1930. It is the second tallest free-standing structure in France after the Millau Viaduct, which measures 125 metres (410 ft) on each side. The tower is now taller than the Chrysler building by 5.2 metres (17 ft)</s>
`","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",11,open
[trainer] figuring out why eval with `--fp16_full_eval` is 25% slower,"Recently HF trainer was extended to support full fp16 eval via `--fp16_full_eval`. I'd have expected it to be either equal or faster than eval with fp32 model, but surprisingly I have noticed a 25% slowdown when using it.

This may or may not impact deepspeed as well, which also runs eval in fp16, but we can't compare it to a baseline, since it only runs fp16.

I wonder if someone would like to research where the slowdown comes from.

I'd probably isolate the `model.half()` call which should be a constant and focus on the rest of the eval. I'm thinking that some component doesn't take well to fp16 variables. e.g.  label smoothing was problematic and now should be fixed in https://github.com/huggingface/transformers/pull/10815, but I tested w/ and w/o label smoothing and it's not adding to the slowdown. 

Here are the script and the corresponding metrics. 

First w/o  `--fp16_full_eval`, 

```
export BS=16; rm -r output_dir; PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 \
./examples/seq2seq/run_translation.py --model_name_or_path t5-small --output_dir /tmp/zero3 \
--overwrite_output_dir --max_train_samples 10 --max_val_samples 100 --max_source_length 12 \
--max_target_length 128 --val_max_target_length 128 --do_train --num_train_epochs 1 \
--per_device_train_batch_size 2 --learning_rate 3e-3 --warmup_steps 8 --predict_with_generate \
--logging_steps 0 --save_steps 2 --eval_steps 1 --group_by_length --adafactor --dataset_name wmt16 \
--dataset_config ro-en --source_lang en --target_lang ro \
--source_prefix ""translate English to Romanian: "" --do_eval 

***** train metrics *****
  epoch                      =    1.0
  init_mem_cpu_alloc_delta   =    2MB
  init_mem_cpu_peaked_delta  =    0MB
  init_mem_gpu_alloc_delta   =  230MB
  init_mem_gpu_peaked_delta  =    0MB
  train_mem_cpu_alloc_delta  =   60MB
  train_mem_cpu_peaked_delta =   63MB
  train_mem_gpu_alloc_delta  =  231MB
  train_mem_gpu_peaked_delta =  194MB
  train_runtime              = 7.7162
  train_samples              =     10
  train_samples_per_second   =  0.648
  
***** eval metrics *****
  epoch                     =    1.0
  eval_bleu                 = 2.4612
  eval_gen_len              =  18.53
  eval_loss                 =  5.017
  eval_mem_cpu_alloc_delta  =    0MB
  eval_mem_cpu_peaked_delta =    0MB
  eval_mem_gpu_alloc_delta  =    0MB
  eval_mem_gpu_peaked_delta =  244MB
  eval_runtime              = 4.6481
  eval_samples              =    100
  eval_samples_per_second   = 21.514
```

now let's add `--fp16_full_eval`:

```
export BS=16; rm -r output_dir; PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 \
./examples/seq2seq/run_translation.py --model_name_or_path t5-small --output_dir /tmp/zero3 \
--overwrite_output_dir --max_train_samples 10 --max_val_samples 100 --max_source_length 12 \
--max_target_length 128 --val_max_target_length 128 --do_train --num_train_epochs 1 \
--per_device_train_batch_size 2 --learning_rate 3e-3 --warmup_steps 8 --predict_with_generate \
--logging_steps 0 --save_steps 2 --eval_steps 1 --group_by_length --adafactor --dataset_name wmt16 \
--dataset_config ro-en --source_lang en --target_lang ro \
--source_prefix ""translate English to Romanian: "" --do_eval  \
--fp16_full_eval

***** train metrics *****
  epoch                      =    1.0
  init_mem_cpu_alloc_delta   =    2MB
  init_mem_cpu_peaked_delta  =    0MB
  init_mem_gpu_alloc_delta   =  230MB
  init_mem_gpu_peaked_delta  =    0MB
  train_mem_cpu_alloc_delta  =   60MB
  train_mem_cpu_peaked_delta =   63MB
  train_mem_gpu_alloc_delta  =  231MB
  train_mem_gpu_peaked_delta =  194MB
  train_runtime              = 7.1477
  train_samples              =     10
  train_samples_per_second   =    0.7

***** eval metrics *****
  epoch                     =    1.0
  eval_bleu                 = 2.4612
  eval_gen_len              =  18.53
  eval_loss                 = 5.0168
  eval_mem_cpu_alloc_delta  =    0MB
  eval_mem_cpu_peaked_delta =    0MB
  eval_mem_gpu_alloc_delta  = -231MB
  eval_mem_gpu_peaked_delta =  262MB
  eval_runtime              = 6.0125
  eval_samples              =    100
  eval_samples_per_second   = 16.632
```

As you can see w/o `--fp16_full_eval`: we get ~22 samples per sec and w/ it only ~17/ - that's a huge difference.

I also tested with a larger sample and the gap remains constant.

The halving happens here:

https://github.com/huggingface/transformers/blob/21e86f99e6b91af2e4df3790ba6c781e85fa0eb5/src/transformers/trainer.py#L1800

Thank you!
","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}, {'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",11,open
EncoderDecoderModel with different model dimensions,"## Who can help
@patrickvonplaten, @patil-suraj

## Information

When instantiating an `EncoderDecoderModel` from two pretrained models whose model dimensions are different, a `RunTimeError` occurs at the `CrossAttention` calculation step. 

The reason is, that regardless of a potentially different encoder model dimension, the projection layers for key and value are initialized with the decoder model dimension. 

This leads to a dimensionality mismatch when performing the matrix multiplication of encoder outputs (encoder model dimension) in the key and value projection layers (decoder model dimension).

Looking a little bit deeper in the API I would suspect it should be easy to provide the correct encoder model dimension to the `Attention` module in most Model implementations and their key/value projection layers, if the `add_cross_attention=True` argument is set. Also, I think the encoder model dimension should be easily accessible via `self.encoder.config.d_model` or something along these lines.

Generally, I think there is no reason against using `EncoderDecoderModel` with `encoder='bert-large-cased'` (`d_model=1024`) and `decoder='gpt2'` (`d_model=768`), but currently this setup doesnt't work.

Thanks a lot for looking into it :) 

Best regards
Lars","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Implementing efficient self attention in T5,"# 🌟 New model addition

My teammates and I (including @ice-americano) would like to use efficient self attention methods such as Linformer, Performer and Nystromformer

## Model description

These new methods serve as approximations of regular attention, but reduce complexity from quadratic in the inputs to linear.  We would like to add a parameter to T5 where users can specify an efficient attention method to use instead of regular attention.  Ideally, this would be implemented across all models, but the models tend to have varying implementations of attention, rendering this generalization fairly tedious.

## Open source status

* [x] the model implementation is available: repos are https://github.com/mlpen and https://github.com/lucidrains/performer-pytorch
* [ ] the model weights are available: N/A
* [x] who are the authors: @mlpen and @lucidrains
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
"Adding option to truncation from beginning instead of end, for both longest_first and longest_second","# 🚀 Feature request

<!-- A clear and concise description of the feature proposal.
     Please provide a link to the paper and code in case they exist. -->

The current [truncation strategies](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation) only provide truncation from end of the input sentence. This may be enough for most cases, but is not applicable for dialog tasks.

Say, you are concatenating an input dialog context to BERT. If the input length > max_length (256/ 512), then the tokens are truncated from the ending i.e. the most recent utterances in the dialog.

In some cases, you want the most recent utterances to be included in the input and if truncation needs to be done, then the oldest utterances be truncated i.e. truncation from the beginning.

This can be done manually in own code base outside of Transformers library, but it's not ideal.

Truncation outside of model will be done most likely on full words to truncate from beginning and fit them to model max length (say you truncate from beginning and reduce input to 254 words). But, these words will be converted to subwords when fed to BertTokenizer and the final input will be > 256, thus resulting in words being dropped from the last utterance again.

To do this properly outside of the Transformers library, one would need to instantiate the same Tokenizer object, tokenize each input and then truncate from beginning, then convert ids back to tokens and then either reconstruct the input from the truncated tokenized version or skip the tokenizer call inside the Transformers library.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request
     related to a problem? e.g., I'm always frustrated when [...]. If this is related
     to another GitHub issue, please link here too. -->

## Your contribution

<!-- Is there any way that you could help, e.g. by submitting a PR?
     Make sure to read the CONTRIBUTING.MD readme:
     https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md -->
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Load pretrained model except the head layer for a specific downstream task,"# 🚀 Feature request

It would be nice to have a flag for `from_pretrained` method that indicates whether to load last layer or not. This feature is needed for transfer learning.

## Motivation

I have trained a model with a specific dataset for a downstream task. Now, I need to train another model that needs to be trained on a similar dataset with different labels. I know that previous model have learned the features from the previous dataset and the new model doesn't need to start from scratch. When I try to load the first model with `from_pretrained` method, it returns size mismatch error due to last layer that has different shape for different number of labels. If there is a flag to load/not to load the last layer, I can initialize last layer randomly and go on my training with transfer learning.

","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Knowledge Retrieval missing from BlenderBot Implementation,"# 🚀 Feature request

<!-- A clear and concise description of the feature proposal.
     Please provide a link to the paper and code in case they exist. -->

The original Blenderbot [paper](https://arxiv.org/pdf/2004.13637.pdf) considered three transformer based models (Retrieval, Generator and RetNRef), however from what I can see only the generator model is implemented within this repository: [transformers/src/transformers/models/blenderbot/modeling_blenderbot.py](https://github.com/huggingface/transformers/blame/master/src/transformers/models/blenderbot/modeling_blenderbot.py).

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request
     related to a problem? e.g., I'm always frustrated when [...]. If this is related
     to another GitHub issue, please link here too. -->

As part of my academic work I am generating topic bound conversations and wish to compare Blenderbot as well as make modifications to its knowledge retrieval component. It would be useful if someone could point me towards this (if it is already implemented) or inform me if this feature is planned to be added in the future. 

![image](https://user-images.githubusercontent.com/77219112/108953393-d8e5db00-76b6-11eb-8b26-4ee0b123755d.png)

## Contribution

<!-- Is there any way that you could help, e.g. by submitting a PR?
     Make sure to read the CONTRIBUTING.MD readme:
     https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md -->

Prior to making a contribution I want to confirm that feature is not implemented and there is no intention of implementing this in the near future.

Thanks,
Alex

@patrickvonplaten @patil-suraj ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",0,open
tokenizer.Tokenizer compatibility with Inference API or Auto* classes,"# 🚀 Feature request
Make tokenizers created with the tokenizers library compatible with the Inference API or Auto classes

## Motivation

I have trained a model on a specific domain by modeling a sequence generation problem as a language modeling problem to predict the next token in the set. The tokenizer associated with the model I used (TransformerXL) was not compatible with my domain since my tokens contained whitespace so I created my own using the `WordLevelTrainer` class in the `tokenizers` library. **Now that I have a complete working solution I would like to use this tokenizer and model in the huggingface Inference API, however it does not work because it requires the tokenizer associated with the model**. Making the `transformers` models compatible with `tokenizers` library could make all kinds of use cases outside of NLP possible with these libraries. 

## Your contribution

Is it possible to hack the saved config for a tokenizer created through the `tokenizers` library to work directly with the `Auto` classes? If so I can document this approach for other users.
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
[Tensor Parallelism] Megatron-LM to transformers,"# 🚀 Feature request

Splitting the discussion that started here: https://github.com/huggingface/transformers/pull/10301#issuecomment-782917393 to add the potential future feature of transformers and it's Tensor Parallelism (Horizontal Model Parallelism) - for bigger context please see [Parallelism notes](https://github.com/huggingface/transformers/issues/9766).

Let's start with important clarification: MP can mean many different things
1. Vertical MP - slice the layers vertically - one or more full layers placed on each gpu = Vertical MP - in which case VertMP is a simple version of PP with chunks=1
2. Horizontal MP - slice the layers horizontally - place a slice of a full model on each gpu - Example Megatron-LM

At the moment I think it's only Megatron-LM that implements Horizontal MP. @anthon-l has ported that model to `transformers`, except the Horizontal MP parts, since currently `transformers` doesn't yet have support for it. There is already naive Vertical MP in t5 and gpt2 thanks to @alexorona's work, I ported Bart too but it's unmerged, and there is an ongoing effort to figure out how to implement the Pipeline. All these will have to co-operate with each other and also share common tools.

@anton-l [started sharing](https://github.com/huggingface/transformers/pull/10301#issuecomment-782917393) what needs to be done to make that important feature available - and then down the road potentially make it available to other (all?) `transformers` models.

@anton-l, the floor is yours.","[{'id': 2760822153, 'node_id': 'MDU6TGFiZWwyNzYwODIyMTUz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Tensor%20Parallel', 'name': 'Tensor Parallel', 'color': '1AD0A8', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",9,open
Request to add Switch Transformer,Google has come up with yet another transformer: https://arxiv.org/pdf/2101.03961.pdf ,"[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
Multilabel Sequence Classification in trainer,"# 🚀 Feature request

We need to be able to use the trainer for multilabel classification problems.

## Motivation

Right now we create our models in the old fashioned way, with a sigmoid layer at the end so we can do multilabel. However if we could use the trainer directly, we wouldn't need to maintain different training scripts. Are there any plans for adding this to the trainer?

## Your contribution

I could try to help but I don't even know where to start with.

Thanks you very much for reading
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",15,open
Better Fine-Tuning by Reducing Representational Collapse,"# 🚀 Feature request

Add r3f/r4f to some popular objective functions as suggested by [Armen et. al](https://arxiv.org/abs/2008.03156).

## Motivation

Finetuning is a primary use case of many users of the transformers library.
We can use r3f/r4f to reduce representation collapse by vocabulary inefficiencies.
This is also a relatively cheap feature to implement.

## Your contribution

Understand that this is a relatively new paper with not too much benchmarking done. Will do PR if requested.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Multiple Mask support in Pipeline,"# 🚀 Feature request

The [fill mask](https://huggingface.co/bert-base-uncased?text=Paris+is+the+capital+of+%5BMASK%5D+%3F) feature as a part of the pipeline currently only supports a single mask for the inputs. It could be expanded to predict and return the results for multiple masks in the same sentence too.

## Motivation

There are use cases where one would ideally have more than just a single mask where they would need a prediction from the model. For example, smarter template filling in outputs returned to users etc. Could also be used in better study of the implicit knowledge that BERT models have accumulated during pre-training.

## Your contribution

I should be able to raise a PR for the same. The output JSON schema would have to be slightly modified, but I can go ahead and complete the same if there is no other obvious issue that slipped my mind as to why only a single [MASK] token needs to be supported.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
"Text to Speech Generalized End-To-End Loss for Speaker Verification, Real Time Voice Cloning","# 🌟 New model addition

## Model description
Generalized End-To-End Loss for Speaker Verification implements Real time voice cloning, a way to generate a Text-To-Speech model adapted to a certain speaker with a short audio sample. The model implements the following paper.
https://arxiv.org/pdf/1806.04558.pdf and the code is available on github.

https://github.com/CorentinJ/Real-Time-Voice-Cloning

<!-- Important information -->

## Open source status

* [ ] the model implementation is available: (give details)
https://colab.research.google.com/drive/1SUq5RLOI0TIMkrBzMHMms01aaVNgkO7c?usp=sharing

The model can be run through Colaboratory. Here is an example of a generated voice.
https://soundcloud.com/birger-mo-ll/generated-voice

* [ ] the model weights are available: (give details)
Here are the model weights that are used.

encoder.load_model(project_name / Path(""encoder/saved_models/pretrained.pt""))
synthesizer = Synthesizer(project_name / Path(""synthesizer/saved_models/logs-pretrained/taco_pretrained""))
vocoder.load_model(project_name / Path(""vocoder/saved_models/pretrained/pretrained.pt""))

* [ ] who are the authors: @CorentinJ
The author is not currently working on the repo, but since it is a fairly popular repo (25.000 stars) it might be reasonable to take the time to explore how to recreate / adapt the model to work with Huggingface transformer.


","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Allow `do_lower_case=True` for any tokenizer,"# 🚀 Feature request

Extract the `do_lower_case` option to make it available for any tokenizer. Not just those that initially supported this, like the `BERT` tokenizers.

## Motivation

Sometimes we want to specify `do_lower_case=True` in the `tokenizer_config.json` of a custom tokenizer to activate the lowercasing. The problem is that this obviously works only for tokenizers based on one that originally used this option.

I think we should extract this feature to make it a shared one, that could be used with any tokenizer.

Example of a model that would need this described here: https://github.com/huggingface/transformers/issues/9518

## Special care points

- Make sure the `convert_slow_tokenizer` script also handles this, to activate the option in the resulting fast tokenizer.
- Maybe some other options could have the same treatment?

cc @LysandreJik @sgugger","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
PruneTrain: Fast Neural Network Training by Dynamic Sparse Model Reconfiguration,"# 🚀 Feature request

PruneTrain. {...} By using a structured-pruning approach and additional reconfiguration techniques we introduce, the pruned model can still be efficiently processed on a GPU accelerator. Overall, **PruneTrain achieves a reduction of 39% in the end-to-end training time of ResNet50 for ImageNet by reducing computation cost by 40% in FLOPs, memory accesses by 37% for memory bandwidth bound layers, and the inter-accelerator communication by 55%.**

## Motivation

I'm pre-training some midsize language models from scratch. If you tell me that I can pretrain a network with 1% drop in performance while cutting down the energy demand of the training by up to 40% and speeding inference time at the same time, I will buy it.

## Your contribution

https://arxiv.org/abs/1901.09290. I can not understand why the authors did not open source the code, since it could reduce the global warming, speedup experimentation and reduce energy consumption. ","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
Supporting truncation from both ends of the sequence in BertTokenizerFast,"# 🚀 Feature request

For `BertTokenizerFast` (inherited from `PreTrainedTokenizerFast`), it seems like `__call__` only supports truncating from the end of the sequences if we set `truncation` to be `longest_first`, `only_first` or `only_second`. For example, assuming `max_length` is 6 and `truncation` is `longest_first`:

(`I have a pen`, `I have an apple`) --> truncation --> (`I have a`, `I have an`)

However, if we take a closer look at [Google's original data-preprocessing script for BERT](https://github.com/google-research/bert/blob/master/create_pretraining_data.py#L430), truncation can happen at both ends of the sequence randomly:

(`I have a pen`, `I have an apple`) --> truncation --> (`I have a`, `have an apple`) or (`have a pen`, `I have an`) or (`I have a`, `I have an`) or (`have a pen`, `have an apple`)

For `BertTokenizer`, perhaps I could reassigned its `truncate_sequences` member function (https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L2887) with a new function that implements Google's truncation scheme; however, for `BertTokenizerFast`, truncation is handled completely in Rust, about which I can't do anything.

An alternative is to call `tokenize` first, then truncate the sequence using Google's scheme, ~~then call `__call__` and passing `is_split_into_words` as `True`~~. However, this approach has significant performance impact comparing to calling `__call__` on a batch of sequences directly (the average total tokenization latency doubled in our experiments).

> PS: Turned out `is_split_into_words` doesn't work this way (since when it sees a subword `##abc`, `__call__` would further tokenize it into `#` `#` `abc` even if `is_split_into_words==True`). Thus, the actual (but slow) alternative is to 1) call `tokenize` 2) implement the truncation scheme and making sure a subword starting with `##` won't be at the boundary 3) call `convert_tokens_to_string` 4) call `__call__`. Effectively, this alternative tokenizes the same sequence twice.

I'm wondering if's possible to add official support for random truncation from both ends of the sequence?

## Motivation

To match Google's truncation scheme exactly and minimizing artificial impacts on pretraining convergence.

## Your contribution

Unfortunately I'm not very familiar with Rust (I can read it, but I neve learned/wrote Rust before), thus I can't help much.
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",2,open
Adversarial/amnesic heads,"# 🚀 Feature request

Task heads that backpropagate deliberately reversed gradients to the encoder. A flag requesting this behavior when constructing a task head.

## Motivation

Transfer learning experiments lend themselves to questions about the extent to which two tasks rely on the same information about a word/sentence, and to experiments probing whether and how word encodings contain/correspond to syntax trees, lemmas, frequencies, and other objects of linguistic/psycholinguistic study.

A difficulty is that a pretrained model, without fine-tuning, may already encode certain information too thoroughly and accessibly for intermediate training to make much of a difference. For example, BERT's masked language modeling objective produces word encodings in which syntax information is readily accessible. Intermediate training on a syntax task requires training a task head to extract this information, of course, but it will result in very little reorganization of the encoder itself.

Adversarial training, such as the amnesic probing of Elazar et al. 2020, can avoid this pitfall. Intermediate training can aim to burn particular information *out* of the encodings, and measure how much this impairs trainability of the target task. Strictly reversing the sense of the training data won't do it though; getting all the answers exactly wrong requires just as much domain knowledge as getting them all right does. And randomizing the labels on training data may just result in a feckless task head, one that discards useful information passed to it from the encoder, rather than affecting the encoder itself. 

Ideally, then, the task head would be trained toward correctly reproducing gold-standard labels, but would flip all its gradients before backpropagating them to the shared encoder, thus training it not to produce precisely the signals that the task head found most informative. The following work by Cory Shain illustrates flipping gradients in this way (although it's not applied to shared-encoder transfer learning, but rather to development of encoders that disentangle semantics from syntax).

	https://docs.google.com/presentation/d/1E89yZ8jXXeSARDLmlksOCJo83QZdNbd7phBrR_dRogg/edit#slide=id.g79452223cd_0_19
	https://github.com/coryshain/synsemnet

## Your contribution

I am deeply unfamiliar with pytorch, unfortunately, and utterly ignorant of tensorflow. I can't offer much.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",1,open
[trainer] new in pytorch: `torch.optim._multi_tensor` faster optimizers ,"Back in September pytorch introduced `torch.optim._multi_tensor` https://github.com/pytorch/pytorch/pull/43507 which should be much more efficient for situations with lots of small feature tensors (`transformers`) and thus should show an appreciable speed up in training. If someone is interested in the progress of this project here is the stack to track: https://github.com/pytorch/pytorch/pull/48223

This feature is currently an alpha stage, so users can try to use it by simply replacing `torch.optim` with `torch.optim._multi_tensor` in HF Trainer or their own trainer.

Eventually it'll replace `torch.optim` so there is nothing that we need to do otherwise.

@blefaudeux who alerted me to this improvement suggested it should have good speed ups for the DDP/Sharded DDP training.

If resources allow it'd be good to run some benchmarks. Please feel free to beat me to it.

Thanks to @blefaudeux for the heads up, and @izdeby for working on this enhancement and clarifying where things are at. 

heads up to: @sgugger, @patrickvonplaten - nothing else that needs to be done. ","[{'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",8,open
[2D Parallelism] Tracking feasibility,"### Background

ZeRO-DP (ZeRO Data Parallel) and PP (Pipeline Parallelism) provide each a great memory saving over multiple GPUs. Each 1D allows for a much more efficient utilization of the gpu memory, but it's still not enough for very big models - sometimes not even feasible with any of the existing hardware. e.g. a model that's 45GB-big with just model params (t5-11b) can't fit even on a 40GB GPU.

The next stage in Model Parallelism that can enable loading bigger models onto smaller hardware is 2D Parallelism. That's combining Pipeline Parallelism (PP) with ZeRO-DP. 

3D Parallelism is possible too and it requires adding a horizontal MP (ala [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), but we don't quite have any way to implement that yet. Need to study Megatron-LM first. So starting with a relatively low hanging fruit of 2D.

------------------

### Tracking

We have 3 implementations that provide the required components to build 2D Parallelism:

1. DeepSpeed (**DS**)
2. FairScale (**FS**)
3. PyTorch (native) (**PT**)

and the purpose of this issue is to track the feasibility/status/inter-operability in each one of them. And also which parts have been back-ported to PyTorch core.

Plus it tracks the status of where transformers models are at with regards to the above 3 implementations.

The 2 main questions are:

1. native 2D: how do we integrate a native PP with native ZeRO-DP (sharded) (e.g. can fairscale PP work with fairscale ZeRO-DP)
2. inter-operability 2D: is there a chance one implementation of PP/ZeRO-DP could work with one or both others ZeRO-DP/PP (e.g. can fairscale PP work with DeepSpeed ZeRO-DP).

------------------

### Notes

* 3D Parallelism is possible too and it requires adding a horizontal MP (ala Megatron-LM), but we don't quite have any way to implement that yet. Need to study Megatron-LM first. So starting with low hanging fruit of 2D.

* MPU = Model Parallel Unit - a little helper module that helps each 1D to know which gpu groups it can use for PP, which for MP, which for DP. So that one 1D doesn't interfere with another 1D. e.g. in the case of 4 gpus and PP+DP, one may want:
   ```
         pp
   dp0 [0, 1]
   dp1 [2, 3] 
   ```
   
   So here there are 2 pipelines: 0-1, and 2-3, and DP sees gpus 0 and 2 as the entry points.

-------------------------- 

### TLDR

ZeRO-DP / PP inter-operability status

|    | DS | FS | PT |
|----|----|----|----|
| DS |  :heavy_check_mark:  | :question:  |  :x: |
| FS |  :question: | :question:  |  :question: |
| PT | :x:| :question:  |  :question: |


-------------------------- 

### 1. DeepSpeed

1D status:
* [x] [PP](https://www.deepspeed.ai/tutorials/pipeline/) 
* [x] [ZeRO-DP](https://www.deepspeed.ai/tutorials/zero/)

2D native status:
* [ ] :question: native PP + ZeRO-DP - untested yet, as it requires porting transformers to native PP first 

2D inter-operability status:
- [ ] :x: pytorch PP + DeepSpeed ZeRO-DP. I tried using pytorch PP with DeepSpeed ZeRO-DP and couldn't figure out how to make it work: https://github.com/microsoft/DeepSpeed/issues/710
- [ ] :question: fairscale PP + DeepSpeed ZeRO-DP  (unknown)

Important components:
 * [original megatron-lm MPU](https://github.com/microsoft/DeepSpeedExamples/blob/master/Megatron-LM/mpu/initialize.py)
 * [WIP DeepSpeed MPU](https://github.com/jeffra/DSE/blob/megatron-deepspeed-pipeline/megatron/mpu/initialize.py)

-------------------------- 

### 2. FairScale

Just started gather information on this one - will update once I have it.

1D status:
* [x] [PP](https://fairscale.readthedocs.io/en/latest/tutorials/pipe.html)
* [x] [ZeRO-DP](https://fairscale.readthedocs.io/en/latest/tutorials/oss.html)

2D native status:
* [ ] :question: native PP + ZeRO-DP - gathering info https://github.com/facebookresearch/fairscale/issues/351

2D inter-operability status:
- [ ] :question:  pytorch PP + fairscale ZeRO-DP gathering info
- [ ] :question:  DeepSpeed PP + fairscale ZeRO-DP gathering info

Important components:
* [MPU](https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/model_parallel/initialize.py#L41)


-------------------------- 


### 3. PyTorch

pytorch has been integrating from what I understand primarily fairscale version into its core.

1D status:
* [x] [PP](https://pytorch.org/docs/master/pipeline.html) - experimental support. have PoC t5 working: https://github.com/huggingface/transformers/pull/9765 [example](https://github.com/pytorch/pytorch/blob/master/benchmarks/distributed/pipeline/pipe.py) 
* [ ] ZeRO-DP - plans to implement that (primarily integrating fairscale implementation)

2D native status:
- [ ] :grey_exclamation: native PP + ZeRO-DP (Pytorch ZeRO-DP doesn't exists yet)

2D inter-operability status:
- [ ] :grey_exclamation: DeepSpeed PP + Pytorch ZeRO-DP (Pytorch ZeRO-DP doesn't exists yet)
- [ ] :grey_exclamation: fairscale PP + Pytorch ZeRO-DP  (Pytorch ZeRO-DP doesn't exists yet)

Important components:
* MPU:  ?

Ported components:
* ZeRO-DP stage 1: ZeroRedundancyOptimizer: an implementation of a standalone sharded optimizer wrapper https://github.com/pytorch/pytorch/pull/46750


Issues to track:
* The main discussion around integrating Deepspeed ZeRO into pytorch core: https://github.com/pytorch/pytorch/issues/42849



--------------------

### Transformers

To make 2D Parallelism working we need to of course support all these stages in `transformers`, so here is a status on what we have working or what is a work in progress. Some components (like bart-mp) work but are unmerged since we are still unsure how to move forward project-wide.

* ZeRO-DP
   - [x] works across all models with fairscale and DeepSpeed integrated.

* Naive vertical MP (aka PP w/ a single stage)
   - [x] t5
   - [x] gpt2
   - [ ] bart - unmerged https://github.com/huggingface/transformers/pull/9384

* Pytorch PP
   - [ ] t5 - unmerged https://github.com/huggingface/transformers/pull/9765

* Horizontal MP - unresearched!

","[{'id': 2627272588, 'node_id': 'MDU6TGFiZWwyNjI3MjcyNTg4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Model%20Parallel', 'name': 'Model Parallel', 'color': '8B66A5', 'default': False, 'description': 'Model Parallelilsm Implementations'}, {'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}, {'id': 2682576896, 'node_id': 'MDU6TGFiZWwyNjgyNTc2ODk2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Pipeline%20Parallel', 'name': 'Pipeline Parallel', 'color': '1F75CB', 'default': False, 'description': ''}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Implementing ELECTRIC training for ELECTRA,"# 🚀 Feature request

Google released Electric this summer at EMNLP (see: [here](https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf)). Electric is like ELECTRA, but trained using a Noise Contrastive Estimation loss instead of a negative sampling loss. 

## Motivation

Electric is well-suited for modeling perplexity scores, and can model these very efficiently. Modeling these perplexity scores using BERT requires N passes over the input sentence, where N is the number of tokes in the sentence (see [here](https://arxiv.org/abs/1910.14659)).

## Your contribution

Electric has been implemented in the Google Electra repository. From I can see, moving from an Electra to Electric-style training is not a huge code change, but I'm not that familiar with the inner workings of transformers to be able to make a judgment call on this.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",0,open
[seq2seq] some logging for all processes in distributed mode,"In 2D Parallelism, e.g. Pipeline + DeepSpeed I need to log unique device maps per process for the user to see, but currently `logger.info()` is only activated for the main process via `if is_main_process`. Currently only in `examples/seq2seq/run_seq2seq.py`, `examples/seq2seq/finetune_trainer.py`, but it'll be needed for other scripts as well down the road.

Any idea how I could accomplish that while keeping things as they are? I guess I could use `logger.warn` as a workaround, since it's not disabled for other processes. But it's not a good approach, since it's a WARNING after all. And I don't quite want to use `print()` as it might not be what the user wants if they want things quiet.

Perhaps you have some other ideas on how I could go about doing that. 

I think perhaps adding another logger that's INFO-activated for all distributed processes, and is used only occasionally when the normal logger won't do.

I think as we are getting more and more into distributed training we will need to be able to log specific things for specific processes.

Thank you.

@LysandreJik, @patrickvonplaten, @sgugger ","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",15,open
rfc: new benchmark tool,"This issue is to collect notes and ideas on creating a new benchmarking tool.

This is not about the other speed/memory regression project we have been discussing elsewhere. 

This is about integration and various comparisons that we need to run in order to give users the best advice on how to deploy transformers in the most efficient way.

Please share the comments ideas/suggestions/concerns/needs, and I will compile them here. 

- important: not part of examples - the goal is performance and integration tooling and not user-facing - totally different needs and priorities
- the cmd line has to continue working the same months later - so that old benchmarks could be re-run - ok to change interface with back-compat option so that the old benchmarks can be still re-validated and compared to
- ideally work with any transformers model - a single tool to rule them all
- minimal amount of arguments - just the important ones
- ability to generate markdown table entries directly and json files that contain not just the outcome but also the key variables that are being tested - 
- the report to include critical hardware/software params as well in a compact form and allow these to be merged from multiple recordings - i.e. if the hw/sw are the same - they can be merged into a single report. will need to figure out how to record hardware nuances 
    *  e.g. the same DDP test with 2 gpus connected w/ NVLink gives dramatically different results than the same 2 gpus w/o NVLink.
    * not sure how to record CPU-capacity/ free RAM, etc., since all these impact the outcome
- crucial to be able to truncate the dataset","[{'id': 2604155188, 'node_id': 'MDU6TGFiZWwyNjA0MTU1MTg4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Benchmarks', 'name': 'Benchmarks', 'color': '2DF372', 'default': False, 'description': 'Issues related to Memory regressions in tests and scripts'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",2,open
Translation Model in ONNX: Choosable Output Formats,"# 🚀 Feature request

I am requesting to provide an option to specify the output format for the `translation_xx_to_yy` export to ONNX models. Currently, the output of [convert_graph_to_onnx.convert](https://github.com/huggingface/transformers/blob/6a346f0358a40f89ec384d441233bf54cac44f6a/src/transformers/convert_graph_to_onnx.py#L330) will provide the raw tensors as output (working prototype code under #9722)

## Motivation

When putting the models into production it would be great if one could chose, whether one wants to have the actual tensors or the output-tokens returned when exporting a translation pipeline to ONNX. Thereby, one is not forced to do a custom re-implementation of the [model.generate](https://github.com/huggingface/transformers/blob/c4d4e8bdbd25d9463d41de6398940329c89b7fb6/src/transformers/generation_utils.py#L101) function, which then uses the ONNX model instead of the torch one.

As for now, the part which is could be replaced by an ONNX inference session lives under the [model.generate](https://github.com/huggingface/transformers/blob/c4d4e8bdbd25d9463d41de6398940329c89b7fb6/src/transformers/generation_utils.py#L385) function. Using this in production would mean to keep a TranslationPipeline object with all corresponding model information and config plus an ONNX inference session.

## Your contribution

There may be multiple solutions to this problem:
1. User-specific re-implementation of model.generate (This is what Ill try to accomplish in the future)

2. Is it possible to rewrite the code under model.generate to full torch? Then it should be possible to create a custom model for all translation models, that just places this ""generate layer"" on top of it. I have provided an example [here](https://github.com/oborchers/sentence-transformers/blob/master/examples/onnx_inference/onnx_inference.ipynb) which adds a simple pooling layer on an already extant transformers model. (That would require more study from my side to develop a prototype and follows step 1)
3. Provide support for the [ort-customops](https://github.com/microsoft/ort-customops) library by Microsoft. Essentially, this enables ONNX to handle strings (but introduces dependency to a very experimental extension). For example, that way one can export the universal sentence encoder (including tokenizer) to ONNX. Example [here](https://github.com/onnx/tensorflow-onnx/issues/1260). I cannot provide anything useful here.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
Easier perplexity computation,"# 🚀 Feature request

The docs provide a method to evaluate perplexity for a GPT-2 model, one example at a time (https://huggingface.co/transformers/perplexity.html). However this can potentially be included in the library with the computation being done in a batched manner. 

## Motivation

This would make it easier and faster for people to evaluate their language models in terms of perplexity.

If not a solution integrated in the library, the example given in the docs can be updated to do computation in a batched manner for speed.
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
[DeepSpeed] Features to integrate / Optimizations to add / Experiments to do,"# 🚀 Feature request

While we have the support for main DeepSpeed features integrated, there are other powerful features that haven't been explored yet and which can provide even more various performance boosts. Some will probably require no changes on our side, while others require changes in the model and/or trainer.

This issue is to track what's possible and the priorities if any.

## Features to integrate

* [ ] [1-bit Adam](https://www.deepspeed.ai/tutorials/onebit-adam/) - Up to 5x less communication volume and up to 2x faster training 
* [ ] [Progressive Layer Dropping](https://www.deepspeed.ai/tutorials/progressive_layer_dropping/) - Accelerating Training of Transformer-Based Language Models
* [ ] [DeepSpeed Sparse Attention](https://www.deepspeed.ai/tutorials/sparse-attention/)  (Seems to be limited only to NVIDIA V100 )
* [ ] [DeepSpeed Transformer Kernel](https://www.deepspeed.ai/tutorials/transformer_kernel/) [api](https://deepspeed.readthedocs.io/en/latest/kernel.html)

Irrelevant to `transformers`:

* [ ] [DeepSpeed Activation Checkpointing](https://www.deepspeed.ai/docs/config-json/#activation-checkpointing) and extra discussion [here](https://github.com/microsoft/DeepSpeed/issues/665#issuecomment-760512582) - reduce the activation memory during model parallel training by partitioning activation checkpoints across model parallel GPUs, or offloading them to CPU. Since we don't use DS's PP there is no use for it.


## Experiments

Things to experiment with as well:

* [ ] try to profile model performance with DeepSpeed's `FlopsProfiler`

## Optimizations

* [ ] the new zero3 has a special requirement for inference with `--predict_with_generate` that all gpus run all `forward` calls even if they finished completing the predicted sequence early in `generate` - otherwise other gpus will hang waiting for the one that finished early. So currently the workaround is to simply always run till `max_length` in the `while` loop is reached. Which might be inefficient if we have a lot of short sequences, so need to use a synchronization trick to simultaneously quit the `while` loop when all gpus know it's safe to do so. @samyam posted a proof-of-concept for how to do that:

> We could maybe simplify by doing a single all_reduce, where gpus that are done will use a tensor with 0.0 and those that are not done will use 1.0. If the result of all reduce is 0.0 then everyone can stop, otherwise gpus that are done will do fake forward.
```
while sync.item() > 0.0:
    p = model.forward(fake_input if am_i_done() else real_input)
    sync =torch.tensor(0.0 if am_i_done() else 1.0)
    torch.distributed.allreduce(sync)
```

At the moment this needs to be done in 5 places in the various search functions that `generate` may call.

For the full context please see: [this thread](https://github.com/microsoft/DeepSpeed/issues/860#issuecomment-799936583).


-------------------

If anybody would like to work on any of these items please open a dedicated issue so it'd be easier to track and please tag @stas00 to it.
","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}, {'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNjU5MjY3MDI1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed', 'name': 'DeepSpeed', 'color': '4D34F7', 'default': False, 'description': ''}]",13,open
Siamese Multi-depth Transformer-based Hierarchical Encoder,"# 🌟 New model addition

## Model description
Recently Google is published paper titled [""Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching""](https://arxiv.org/abs/2004.12297). And according to paper for long-form document matching SMITH model outperforms the previous state-of-the-art models including hierarchical attention, multi-depth attention-based hierarchical recurrent neural network, and BERT.

I feel it is will add value to already awesome transformers models collection :slightly_smiling_face:

<!-- Important information -->

## Open source status

* [X] the model implementation is available: https://github.com/google-research/google-research/tree/master/smith
* [X] the model weights are available: [SMITH-WP+SP model checkpoint](http://storage.googleapis.com/gresearch/smith_gwikimatch/smith_wsp_pretrain_ckpt_opensource.zip) and [GWikiMatch data](http://storage.googleapis.com/gresearch/smith_gwikimatch/gwikimatch_open_source.zip)
* [X] who are the authors: https://github.com/yangliuy, https://github.com/eladeban
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",3,open
Good Second Issue: T5 FP16 in Pytorch,"# 🚀 Feature request

This ""Good second issue"" should revisit some of the problems we were having with FP16 for `T5ForConditionalGeneration`: https://github.com/huggingface/transformers/issues/4586 and help to make T5 compatible with fp16.

**_Requirements:_**

- use transformers master
- use newest pytorch version
- have access to GPU

**_Context:_**

To better explain the context, let's define the three different pre-trained T5 models types we have:

- **T5v1** (original T5): => this corresponds to all those checkpoints: `t5-small`, `t5-base`, `t5-large`, `t5-3b`, `t5-11b`
- **T5v1_1** (improved T5): => this corresponds to all those checkpoints: `google/t5-v1_1-small`, `google/t5-v1_1-base`, `google/t5-v1_1-large`, `google/t5-v1_1-xl`, `google/t5-v1_1-xxl`. **T5v1_1** has a slightly different architecture than **T5v1**. More info on differences can be found here: https://github.com/huggingface/transformers/issues/6285
- **MT5** (multi-lingual T5): => this model is identical in architecture to **T5v1_1** but has different pre-trained weights and a much larger word embedding matrix.

As shown in this issue https://github.com/huggingface/transformers/issues/4586 , training **T5v1** in fp16 mode led in the past to numerical overflow in the `T5LayerFF` forward pass: https://github.com/huggingface/transformers/blob/6189ae99603bd5dc14c5631f1b4562f78e24d575/src/transformers/models/t5/modeling_t5.py#L279. 

At the time of this issue: https://github.com/huggingface/transformers/issues/4586, **T5v1** was added with a small bug that led to slightly wrong outputs that was only fixed by this PR: https://github.com/huggingface/transformers/pull/8518. 

Also, now there are new T5 checkpoints, notably the **T5v1_1** and **MT5** checkpoints, where it would be very interesting to see whether fp16 can work with those.

**_Feature Request_**

So for this feature request, we should two scenarios:

1) Inference:

For each T5 model type we should test when the models break during inference. This can be as easy as testing the following script for a bunch of different checkpoints on different `input_str`:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

checkpoint = ""t5-small""  # ""google/mt5-small"", ""google/t5-v1_1-small""

model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to('cuda')
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

input_str = ""Hello there. This is the input.""  # here it would be better to test much larger inputs

input_ids = tokenizer(input_str, return_tensors=""pt"").input_ids.to('cuda')

# FP32
output_fp32 = model.generate(input_ids)

# FP16
model.half()
output_fp16 = model.generate(input_ids)

if output_fp32.tolist() == output_fp16.tolist():
    print(""SUCCESS: Output is equal!"")
else:
    print(""Output is different!"")
    print(""FP32"", output_fp32)
    print(""FP16"", output_fp16)
```

2) Training (the more interesting part):

This is probably more important and will require more time / skill. In order to check how T5 does in FP16 training, I'd recommend to use the newly added `Seq2SeqTrainer`: https://github.com/huggingface/transformers/blob/6189ae99603bd5dc14c5631f1b4562f78e24d575/src/transformers/trainer_seq2seq.py#L38. I would recommend to train on a summarization task, such as CNN/Dailymail. One could closely follow, this notebook: https://colab.research.google.com/drive/1Ekd5pUeCX7VOrMx94_czTkwNtLN32Uyu?usp=sharing, but replacing Bert2Bert with the different T5 models. Ideally different ""fp16 backends"" should be tested: https://github.com/huggingface/transformers/blob/6189ae99603bd5dc14c5631f1b4562f78e24d575/src/transformers/training_args.py#L216 and one should try to see whether hacks as proposed in https://github.com/huggingface/transformers/issues/4586#issuecomment-748336815 can solve the problem. It would be very interesting to see whether the error happens only for **T5v1** or also for **T5v1_1** and **MT5** and it what point. For each type it would be great to test for ""small"", ""base"" and if possible even ""large"". Ideally, one should first create a short summarization fine-tuning script (happy to help here) and then run a bunch of different experiments with different fp16 backends and different models. 

**_Possible Outcome_**

The results of those experiments should be documented here or even better on https://discuss.huggingface.co/. Ideally, a solution to the problem is found and one could publish a nice blog post explaining how to effectively train T5.

## Motivation

T5 is one of the most widely used models of Transformers at the moment so that more results to this issue would be extremely useful for the community. In addition, this issue can be a great opportunity to learn more about the limits of fp16 and why some models still do require full fp32 support (or at least until bfloat16 is better supported in torch). This is not an easy issue to tackle, but an extremely important one.

## Your contribution

I'm happy to help along the way, starting with making a nice T5 summarization training pipeline that lets one easily test on different models, and fp16 backends. 
","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",11,open
CharacterBERT,"# 🌟 New model addition

## Model description
**CharacterBERT** is a **variant of BERT** that uses a CharacterCNN module **instead of** WordPieces. As a result, the model:
1. Does not require/rely on a WordPiece vocabulary
2. Produces a single embedding for any (reasonable) input token
3. Is more robust to misspellings

Paper: https://www.aclweb.org/anthology/2020.coling-main.609/

<!-- Important information -->

## Open source status

* [x] the model implementation is available: https://github.com/helboukkouri/character-bert
* [x] the model weights are available: https://github.com/helboukkouri/character-bert/blob/main/download.py#L16
* [x] who are the authors: @helboukkouri @osf9018 @Jekub @hiroshinoji @PierreZweigenbaum and Junichi Tsujii

I am willing to work on a PR but I will probably need some guidance 😊","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",14,open
overflow_to_sample_mapping missing in in documentation,"In the [documentation ]( https://huggingface.co/transformers/master/main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast.__call__)of the fast tokenizer, the `overflow_to_sample_mapping` field is missing.
Instead the `overflowing_tokens`  is listed there, which is only part of  the base tokenizer.
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",4,open
🌟 CTRLsum,"# 🌟 New model addition

## Model description

>Current summarization systems yield generic summaries that are disconnected from users’ preferences and expectations. To address this limitation, we present **CTRLsum**, a novel framework for controllable summarization.
> 
> Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts.
Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training.
We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects:
> 1) entity-centric
> 2) length-controllable summarization
> 3) contribution summarization on scientific papers
> 4) invention purpose summarization on patent filings
> 5) question-guided summarization on news articles in a reading comprehension setting
> 
> Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.

## Open source status

* [x] the model implementation is available: https://github.com/salesforce/ctrl-sum
* [x] the model weights are available: _Download link available in the README of the repo_
* [x] who are the authors: @jxhe @muggin
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",1,open
AlbertTokenizer handles special tokens incorrectly,"If I download the pretrained vocab `https://huggingface.co/albert-base-v1/resolve/main/spiece.model` to the local file system and use the following snippet, the tokenizer does not handle the special tokens properly:
```
tokenizer = AlbertTokenizer('spiece.model')
tokenizer.tokenize('[CLS] Hello World ! [SEP]')
['▁[', 'cl', 's', ']', '▁hello', '▁world', '▁', '!', '▁[', 's', 'ep', ']']
```
If I use `from_pretrained` to load the vocab, it works well:
```
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v1')
tokenizer.tokenize('[CLS] Hello World ! [SEP]')
['[CLS]', '▁hello', '▁world', '▁', '!', '[SEP]']
```","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",12,open
Marge - Pre-training via Paraphrasing,"# 🌟 New model addition

## Model description


","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
Sparse Transormer,"# 🌟 New model addition

## Model description

Sparse Transformers (https://openai.com/blog/sparse-transformer/) are one of the two most efficient transformers for long range problems, according to Google's Long Arena paper: https://arxiv.org/pdf/2011.04006.pdf (Big Bird) is the other one.

The original Sparse Transformers work shows great results on text, images, and audio. Further OpenAI work Jukebox (https://openai.com/blog/jukebox/) uses Sparse Transformers to generate incredibly long raw music audio with style transfer. Lastly https://proceedings.icml.cc/static/paper_files/icml/2020/6095-Paper.pdf uses Sparse Transformers to achieve state-of-the-art CIFAR performance.

## Open source status

* [x] the model implementation is available:

latest version, for CIFAR: https://github.com/openai/distribution_augmentation
original, but not maintained: https://github.com/openai/sparse_attention
Alternate implementation from FAIR: https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sparse_multihead_attention.py

* [x] the model weights are available: 

https://github.com/openai/distribution_augmentation (CIFAR work) has model weights available, as described in the README: https://openaipublic.blob.core.windows.net/distribution-augmentation-assets/models/c10-15m-baseline.npz

Jukebox is open-source and has model weights, but is a larger pipeline that includes VQ-VAEs so it may not be of interest for a transformers-only library.

* [x] who are the authors: @rewonc @myleott  @cclauss
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",2,open
how to use EncoderDecoderModel to do en-de translation?,"I have trained a EncoderDecoderModel from huggging face to do english-German translation task. I tried to overfit a small dataset (100 parallel sentences), and use `model.generate()` then `tokenizer.decode()` to perform the translation. However, the output seems to be proper German sentences, but it is definitely not the correct translation.

Here are the code for building the model

```
encoder_config = BertConfig()
decoder_config = BertConfig()
config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)
model = EncoderDecoderModel(config=config)
```

Here are the code for testing the model

```
model.eval()
input_ids = torch.tensor(tokenizer.encode(input_text)).unsqueeze(0)
output_ids = model.generate(input_ids.to('cuda'), decoder_start_token_id=model.config.decoder.pad_token_id)
output_text = tokenizer.decode(output_ids[0])
```

Example input: ""iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould .""

Ground truth translation: ""iron cement ist eine gebrauchs ##AT##-##AT## fertige Paste , die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken ( Winkel ) der Stahlguss -Kokille aufgetragen wird .""

What the model outputs after trained 100 epochs: ""[S] wenn sie den unten stehenden link anklicken, sehen sie ein video uber die erstellung ansprechender illustrationen in quarkxpress"" which is totally nonesense.

Where is the problem?","[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",19,open
Converting all model Config classes to dataclasses,"It seems that we could save a lot of boilerplate code and potentially prevent some bugs if we migrated all of the model config classes over to being dataclasses. Already many of our classes (BaseModelOutput, TrainingArguments, etc.) are dataclasses, so we are already committed to having dataclasses as a dependency.

It's relatively low priority, but I would be willing to help implement the change since I'm kind of a neat freak about code.","[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",10,open
Model Parallelism and Big Models,"# 🚀 Feature request

This is a discussion issue for training/fine-tuning very large transformer models. Recently, model parallelism was added for gpt2 and t5. The current implementation is for PyTorch only and requires manually modifying the model classes for each model. Possible routes (thanks to @stas00  for identifying these):
- `fairscale` to avoid individual model implementation
- `deepspeed` to possibly enable even larger models to be trained","[{'id': 2627272588, 'node_id': 'MDU6TGFiZWwyNjI3MjcyNTg4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Model%20Parallel', 'name': 'Model Parallel', 'color': '8B66A5', 'default': False, 'description': 'Model Parallelilsm Implementations'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",68,open
Add POINTER model,"# 🌟 New model addition

## Model description

[POINTER](https://github.com/dreasysnail/POINTER) is a progressive and non-autoregressive text generation pre-training approach, published on EMNLP 2020 by Microsoft Research. POINTER generates fluent text in a progressive and parallel manner. With empirical logarithmic time, POINTER outperforms existing non-autoregressive text generation approaches in hard-constrained text generation.

The model uses basically BERT-large architecture. However, an additional token is added to the vocab. The inference is performed by passing the input iteratively to the model. Since there is no existing model architecture in Huggingface that is compatible, I am not sure how to incorporate this into the model card. 

## Open source status

* [x] the model implementation is available: (https://github.com/dreasysnail/POINTER)
* [x] the model weights are available: [here](https://yizzhang.blob.core.windows.net/insertiont/ckpt.tar.gz?st=2020-08-18T20%3A49%3A02Z&se=2024-01-16T20%3A49%3A00Z&sp=rl&sv=2018-03-28&sr=b&sig=PKrSJt38cmY0P%2FBcZuyK%2Btm3bXyYzzfazaqTu1%2F%2FDtc%3D)
* [x] who are the authors: @dreasysnail 
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Adding kNN language modeling and Machine Translation,"# 🌟 Adding the kNN-LM and kNN-MT models

## Model description

The kNN [Language Model](https://arxiv.org/pdf/1911.00172.pdf) and [Machine Translation](https://arxiv.org/pdf/2010.00710.pdf) leverage additional data at test time by looking up examples in the data store that are similar to the test example and using them to inform the prediction. This leads to decreased perplexity for language modeling and increased BLEU for MT.

Ideally, we'd have a general KNNDecoder class which can use any model in the library along with a [🤗datasets](https://github.com/huggingface/datasets) data store, similarly to what RAG does currently.

## Open source status

* [x] the model implementation is available: for kNN-LM, an implementation using Fairseq and FAISS can be found [here](https://github.com/urvashik/knnlm)
* [x] the model weights are available: the methods use pre-trained models which are already in the library, [GPT-2](https://huggingface.co/gpt2) and the [Facebook WMT'19 models](https://huggingface.co/facebook/wmt19-de-en)
* [x] who are the authors: first author @urvashik
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",7,open
"Unexpected/wrong handling of added special tokens in special_tokens_mask (GPT1, BERT, possibly others)","## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.1.0
- Platform: Linux-5.4.0-52-generic-x86_64-with-glibc2.29
- Python version: 3.8.5
- PyTorch version (GPU?): 1.6.0+cpu (False)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: **No**
- Using distributed or parallel set-up in script?: **No**

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu
 examples/token-classification: @stefan-it
 documentation: @sgugger
 -->
Most appropriate seems @mfuntowicz (tokenization), blame says @thomwolf.
## Information

Model I am using (Bert, XLNet ...): OpenAI GPT (also  BERT)

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [X] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [X] my own task or dataset: (give details below)

I am adding special tokens (`BOS`, `SEP` and `EOS`) to GPT1 tokenizer in order to format and fine-tune a GPT model a bit differently. I am also making use of the convenient `return_special_tokens_mask` argument in `encode_plus()`, though it does not seem to mark the added custom special tokens as special in the returned mask.

The same is also true when adding custom special tokens to BERT tokenizer. I did not check beyond these two.
The problem for GPT seems to be that `get_special_tokens_mask()` in `tokenization_utils.py` does not seem to take into account any special tokens.
```python
def get_special_tokens_mask(
    self, token_ids_0: List, token_ids_1: Optional[List] = None, already_has_special_tokens: bool = False
) -> List[int]:
    return [0] * ((len(token_ids_1) if token_ids_1 else 0) + len(token_ids_0))
```

For BERT, it only seems to take into account `[CLS]` and `[SEP]`.



## To reproduce

```python
from transformers import OpenAIGPTTokenizer

tokenizer = OpenAIGPTTokenizer.from_pretrained(""openai-gpt"")
tokenizer.add_special_tokens({
    ""bos_token"": ""<bos>"",
    ""sep_token"": ""<sep>"",
    ""eos_token"": ""<eos>""
})

# Does not work this way either
# tokenizer.add_special_tokens({
#     ""additional_special_tokens"": [""<bos>"", ""<sep>"", ""<eos>""]
# })

encoded = tokenizer.encode_plus(""<bos> State your name, rank and intention <sep> The Doctor, doctor, fun. <eos>"",
                                return_special_tokens_mask=True)
print(encoded[""input_ids""])
print(encoded[""special_tokens_mask""])  # This returns all zeros
```

## Expected behavior
I would expect that the additional special tokens also get marked as special, i.e. that the `special_tokens_mask` in above snippet returns `[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]`
","[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",6,open
"examples/rag: test coverage, tiny model","Disclaimer: I don't know this code very well, this may be much harder than it seems.
Blocking PR: #7713 


[`examples/rag/finetune.py`, `examples/rag/finetune.sh`, `eval_rag.py`] do not seem to be tested at all.
It would be good to have a `test_finetune.py` like `examples/seq2seq` that tested these.

cc @stas00 if interested, rag is a cool new retrieval model https://arxiv.org/pdf/2005.11401.pdf","[{'id': 1108649053, 'node_id': 'MDU6TGFiZWwxMTA4NjQ5MDUz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Help%20wanted', 'name': 'Help wanted', 'color': '008672', 'default': False, 'description': 'Extra attention is needed, help appreciated'}, {'id': 1834088753, 'node_id': 'MDU6TGFiZWwxODM0MDg4NzUz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Tests', 'name': 'Tests', 'color': 'a6fcca', 'default': False, 'description': 'Related to tests'}, {'id': 2373468354, 'node_id': 'MDU6TGFiZWwyMzczNDY4MzU0', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/rag', 'name': 'rag', 'color': 'e58e85', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",6,open
Add FAVOR+ / Performer attention,"# 🌟 FAVOR+ / Performer attention addition

Are there any plans to add this new attention approximation block to Transformers library?

## Model description
The new attention mechanism with linear time and space complexity was introduced in
 _Rethinking Attention with Performers_ [[https://arxiv.org/abs/2009.14794](https://arxiv.org/abs/2009.14794)].
Authors of the paper claim that the new attention mechanism is backward-compatbile with already existing models 
> Backwards compatibility with pretrained models is available as a benefit from softmax approximation, via small finetuning (required due to error propagation)


<!-- Important information -->

## Open source status

* [x] the model implementation is available: it's an original Trax implementation from Google: https://github.com/google-research/google-research/tree/master/performer/fast_self_attention
* [ ] the model weights are available: probably not required as it's a building block for models rather than fully new architecture
* [x] who are the authors: Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",46,open
Add DistilBERTGeneration comparable to BertGeneration,"# 🚀 Feature request

I noticed the new `BertGeneration` class, which uses BERT-style models as both encoder and decoder, as well as the more general `EncoderDecoder` class. This is all great stuff! It would also be great to be able to use distilled models. I believe this is possible for the encoder, but for the decoder a language head must be added.

Since DistilBert is implemented as its own model, and not as a BertModel, I don't think it's possible (or at least it's not easy) for the end user to do this. At least not loading pretrained models, since any pretrained model needs to be a type approved by `AutoModelForCausalLM`.

## Motivation

Same motivation as using distilled models in general. Same results at higher speed, this time applied to an `EncoderDecoder` model.

## Your contribution

Happy to be an alpha tester for this feature
","[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMzkyMDQ2MzU5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20Second%20Issue', 'name': 'Good Second Issue', 'color': 'dd935a', 'default': False, 'description': 'Issues that are more difficult to do than ""Good First"" issues - give it a try if you want!'}]",7,open
Add new PET Model,"# 🌟 New model addition

## Model description

A new article just landed on ArXiv: https://arxiv.org/pdf/2009.07118.pdf
An implementation will eventually be available at https://github.com/timoschick/pet

Authors are @timoschick and Hinrich Schutze.

I didn't see any pre-trained models linked on the GitHub README, but the model is pretty small and easy to train.

Update: the code is available open source along, and it can presumably use pretrained BERT models(I do not know how this works, bu the GitHub page states that the roberta-large pretrained model can be used). The model also works unsupservised.

## Open source status

* [x] the model implementation is available: (give details)
* [x] the model weights are available: (give details)
* [x] who are the authors: (mention them, if possible by @gh-username)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",4,open
BERT and SpanBERT for Coreference Resolution,"# 🌟 New model addition

## Model description

This is a recent approach for co-reference resolution using BERT, implemented from the papers [BERT for Coreference Resolution: Baselines and Analysis](https://arxiv.org/abs/1908.09091) and [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/abs/1907.10529), which is the current state of the art on OntoNotes (79.6 F1). It uses tensorflow 1.14.0.

Reason why this is interesting is it achieves strong improvements on the OntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. Also, I think it would be a nice addition to huggingface library, as it has only the neuralcoref as the coreference resolution module.

## Open source status

* [x] the model implementation is available: (https://github.com/mandarjoshi90/coref)
* [x] the model weights are available: (https://github.com/facebookresearch/SpanBERT)
* [x] who are the authors: (@mandarjoshi90, @jkkummerfeld, @wenyudu)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",4,open
Test BART's memory  consumption,"- this can run on GPU only and be marked `@slow`
- check how much memory bart is using at `__init__`
- assert that it doesn't use more than 110% of that.
- check how much memory bart uses on a single forward pass. (optionally test this in fp16).
- assert that it doesn't use more than 110% of that.
- check how much memory bart uses on a single forward and backward pass.
- assert that it doesn't use more than 110% of that.

### Bonus:
- add similar asserts for timing! 
- let the test run and check memory on CPU (make sure that if pytest is run with `-n -8` the test still passes!
- add a test to `test_modeling_common.py` to make it easy for all models to test this.
- add a test to `test_modeling_common_tf.py` to make it easy for all TF models to test this.

The benchmarking utilities may help.
It may also help to use `torch.cuda.max_memory...`

@patrickvonplaten may have further thoughts!
","[{'id': 1108649053, 'node_id': 'MDU6TGFiZWwxMTA4NjQ5MDUz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Help%20wanted', 'name': 'Help wanted', 'color': '008672', 'default': False, 'description': 'Extra attention is needed, help appreciated'}, {'id': 1834088753, 'node_id': 'MDU6TGFiZWwxODM0MDg4NzUz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Tests', 'name': 'Tests', 'color': 'a6fcca', 'default': False, 'description': 'Related to tests'}, {'id': 2604155188, 'node_id': 'MDU6TGFiZWwyNjA0MTU1MTg4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Benchmarks', 'name': 'Benchmarks', 'color': '2DF372', 'default': False, 'description': 'Issues related to Memory regressions in tests and scripts'}, {'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNzk2NjI4NTYz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/WIP', 'name': 'WIP', 'color': '234C99', 'default': False, 'description': 'Label your PR/Issue with WIP for some long outstanding Issues/PRs that are work in progress'}]",11,open
Tabert,"# 🌟 New model addition

## Model description
a pre-trained language model for learning joint representations of natural language utterances and (semi-)structured tables for semantic parsing. TaBERT is pre-trained on a massive corpus of 26M Web tables and their associated natural language context, and could be used as a drop-in replacement of a semantic parsers original encoder to compute representations for utterances and table schemas (columns).
<!-- Important information -->

## Open source status

* [X] the model implementation is available: (give details)
https://github.com/facebookresearch/TaBERT
* [X] the model weights are available: (give details)
https://github.com/facebookresearch/TaBERT
* [ ] who are the authors: (mention them, if possible by @gh-username)
","[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}, {'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",5,open
Trying to add support for GPT2 as decoder in EncoderDecoder model,"# 🚀 Feature request

Hi, 
I am trying to add the option of using GPT2 as the decoder in the EncoderDecoder model, which only support 

## Motivation

For a generation problem, it usually better to use GPT2 as the decoder, over BERT.

## Your contribution

I've made the following changes in `modeling_gpt2.py` file:

- Added crossattention layer if the model is a decoder, to the `Block` class:
```python
class Block(nn.Module):
    def __init__(self, n_ctx, config, scale=False):
        super().__init__()
        nx = config.n_embd
        self.ln_1 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
        self.attn = Attention(nx, n_ctx, config, scale)
        self.ln_2 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
        self.mlp = MLP(4 * nx, config)
        self.is_decoder = config.is_decoder
        if self.is_decoder:
            self.crossattention = Attention(nx, n_ctx, config, scale)
...
    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, encoder_hidden_states=None,
                encoder_attention_mask=None):
        output_attn = self.attn(
            self.ln_1(x),
            layer_past=layer_past,
            attention_mask=attention_mask,
            head_mask=head_mask,
            use_cache=use_cache,
        )
        a = output_attn[0]  # output_attn: a, present, (attentions)
        outputs = []
        if self.is_decoder and encoder_hidden_states is not None:
            cross_attention_outputs = self.crossattention(
                a, layer_past, attention_mask, head_mask, encoder_hidden_states=encoder_hidden_states,
                                            encoder_attention_mask=encoder_attention_mask
            )
            a = cross_attention_outputs[0]
            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights

        x = x + a
        m = self.mlp(self.ln_2(x))
        x = x + m

        outputs = [x] + output_attn[1:] + outputs

        return outputs  # x, present, (attentions)
```

- Added 3 Linear layers instead of the Conv1d layer:
```python
class Attention(nn.Module):
    def __init__(self, nx, n_ctx, config, scale=False):
...
        # self.c_attn = Conv1D(n_state * 3, nx)
        self.query = nn.Linear(n_state, nx)
        self.key = nn.Linear(n_state, nx)
        self.value = nn.Linear(n_state, nx)
...
```

- Added `encoder_attention_mask` and `encoder_hidden_states` to the forward function of the `Attention` class, and using them for the key and the value if they are provided:
```python
def forward(self, x, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, encoder_hidden_states=None,
                encoder_attention_mask=None):
        query = self.query(x)
        if encoder_hidden_states is not None:
            key = self.key(encoder_hidden_states)
            value = self.value(encoder_hidden_states)
            attention_mask = encoder_attention_mask
        else:
            key = self.key(x)
            value = self.value(x)
        query = self.split_heads(query)
        key = self.split_heads(key, k=True)
        value = self.split_heads(value)
...
```

- Added the `encoder_attention_mask` and `encoder_hidden_states` arguments to the `GPT2Model` forward function, and processed `encoder_attention_mask` same as attention_mask:
```python
class GPT2Model(GPT2PreTrainedModel):
...
    def forward(
        self,
        input_ids=None,
        past=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        use_cache=True,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
    ):
...
        # Encoder attention mask. (same action as for regular attention mask)
        if encoder_attention_mask is not None:
            assert batch_size > 0, ""batch_size has to be defined and > 0""
            encoder_attention_mask = encoder_attention_mask.view(batch_size, -1)
            encoder_attention_mask = encoder_attention_mask.unsqueeze(1).unsqueeze(2)
            encoder_attention_mask = encoder_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility
            encoder_attention_mask = (1.0 - encoder_attention_mask) * -10000.0
...
        for i, (block, layer_past) in enumerate(zip(self.h, past)):
            if self.output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)

            outputs = block(
                hidden_states,
                layer_past=layer_past,
                attention_mask=attention_mask,
                head_mask=head_mask[i],
                use_cache=use_cache,
                encoder_hidden_states=encoder_hidden_states,
                encoder_attention_mask=encoder_attention_mask,
            )
...
```

- Added the `encoder_attention_mask` and `encoder_hidden_states` arguments to the `GPT2LMHeadModel`forward function, as well as `lm_lables` and `masked_lm_labels` for EncoderDecoder model compatibility (probably it's better to use `GPT2DoubleHeadsModel`):
```python
class GPT2LMHeadModel(GPT2PreTrainedModel):
...
    def forward(
        self,
        input_ids=None,
        past=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        use_cache=True,
        lm_labels=None,
        masked_lm_labels=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
    ):
...
        if lm_labels is not None:
            if labels is not None:
                raise ValueError(""You cannot specify both labels and lm_labels at the same time"")
            labels = lm_labels

        transformer_outputs = self.transformer(
            input_ids,
            past=past,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
        )
...
```
My biggest concern is with the second bullet, and I wanted to ask you if this implementation seems right (for now it's look like I am able to train and test an EncoderDecoder with BERT2GPT architecture).
Of course that if needed, I can provide the full code to all of my changes, but all of my changes is listed above.
Most (if not all) of the code I've add is adapted from huggingface `modeling_bert.py`file, so all of the credit goes to them.

Thanks","[{'id': 1843738573, 'node_id': 'MDU6TGFiZWwxODQzNzM4NTcz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Encoder-Decoder', 'name': 'Core: Encoder-Decoder', 'color': 'ef536d', 'default': False, 'description': ''}, {'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",31,open
